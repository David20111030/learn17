-- MySQL dump 10.13  Distrib 5.5.56, for Linux (i686)
--
-- Host: localhost    Database: resource
-- ------------------------------------------------------
-- Server version	5.5.56-log

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `resource_techn_article`
--

DROP TABLE IF EXISTS `resource_techn_article`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_techn_article` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `uid` int(11) NOT NULL DEFAULT '0' COMMENT '用户id',
  `title` varchar(30) NOT NULL DEFAULT '' COMMENT '文章标题',
  `summary` varchar(200) DEFAULT '' COMMENT '文章摘要',
  `img` varchar(255) NOT NULL DEFAULT '' COMMENT '文章图片',
  `content` text NOT NULL COMMENT '文章内容',
  `pid` int(11) NOT NULL DEFAULT '0' COMMENT '文章所属分类id',
  `state` tinyint(1) DEFAULT '0' COMMENT '添加状态 0 后台添加 1 前后添加',
  `views` int(11) DEFAULT '0' COMMENT '浏览量',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `mid` int(10) unsigned DEFAULT '0' COMMENT '修改者',
  `updatetime` int(10) unsigned DEFAULT '0' COMMENT '修改时间',
  `uuid` int(10) unsigned DEFAULT '0' COMMENT '创建者',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=421 DEFAULT CHARSET=utf8 COMMENT='技术文章表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_techn_article`
--

LOCK TABLES `resource_techn_article` WRITE;
/*!40000 ALTER TABLE `resource_techn_article` DISABLE KEYS */;
INSERT INTO `resource_techn_article` VALUES (1,1,'什么是搜索','','','百度：我们比如说想找寻任何的信息的时候，就会上百度去搜索一下，比如说找一部自己\r\n喜欢的电影，或者说找一本喜欢的书，或者找一条感兴趣的新闻（提到搜索的第一印象）\r\n百度 != 搜索，这是不对的\r\n\r\n垂直搜索（站内搜索）\r\n\r\n互联网的搜索：电商网站，招聘网站，新闻网站，各种app\r\nIT系统的搜索：OA软件，办公自动化软件，会议管理，日程管理，项目管理，员工管理，\r\n搜索“张三”，“张三儿”，“张小三”；有个电商网站，卖家，后台管理系统，搜索“牙膏”，\r\n订单，“牙膏相关的订单”\r\n\r\n搜索，就是在任何场景下，找寻你想要的信息，这个时候，会输入一段你要搜索的关键字，\r\n然后就期望找到这个关键字相关的有些信息',8,0,0,1514601542,0,0,0),(2,1,'如果用数据库做搜索会怎么样','','','做软件开发的话，或者对IT、计算机有一定的了解的话，都知道，数据都是存储在数据库里面的，比如说电商网站的商品信息，招聘网站的职位信息，新闻网站的新闻信息，等等吧。所以说，很自然的一点，如果说从技术的角度去考虑，如何实现如说，电商网站内部的搜索功能的话，就可以考虑，去使用数据库去进行搜索。\r\n\r\n1、比方说，每条记录的指定字段的文本，可能会很长，比如说“商品描述”字段的长度，有长达数千个，甚至数万个字符，这个时候，每次都要对每条记录的所有文本进行扫描，懒判断说，你包不包含我指定的这个关键词（比如说“牙膏”）\r\n2、还不能将搜索词拆分开来，尽可能去搜索更多的符合你的期望的结果，比如输入“生化机”，就搜索不出来“生化危机”\r\n\r\n用数据库来实现搜索，是不太靠谱的。通常来说，性能会很差的',8,0,0,1514601743,0,0,0),(3,1,'什么是全文检索(.png)和Lucene','','','（1）全文检索，倒排索引\r\n数据库里的数据 一共有100万条 按照之前的思路，其实就要扫描100万次,而且每次扫描，都需要匹配那个文本所有的字符，确认是否包含搜索的关键词，而且还不能将搜索词拆解开来进行检索\r\n利用倒排索引的话,假设100万条数据，拆分出来的词语，假设有1000成个词语，那么在倒排索引中，就有1000万行,我们可能并不需要搜索1000万次，很可能说，在搜索到第一次的时候，我们就可以找到这个搜索词对应的数据，也可能是第100次，或者第1000次\r\n（2）lucene，就是一个jar包，里面包含了封装好的各种建立倒排索引，以及进行搜索的代码，包括各种算法。我们就用java开发的时候，引入lucene jar，然后基于lucene的api进行去进行开发就可以了。用lucene，我们就可以去将已有的数据建立索引，lucene会在本地磁盘上面，给我们组织索引的数据结构。另外的话，我们也可以用lucene提供的一些功能和api来针对磁盘上额\r\n',8,0,0,1514602341,0,0,0),(4,1,'什么是Elasticsearch','','','Elasticsearch，分布式，高性能，高可用，可伸缩的搜索和分析系统\r\n\r\n1.自动维护数据的分布到多个节点的索引的建立，还有搜索请求分布到多个节点的执行\r\n2.自动维护数据的冗余副本，保证说，一些机器宕机了，不会丢失任何的数据\r\n3.封装了更多的高级功能，以给我们提供更多高级的支持，让我们快速的开发应用，开发更加复杂的应用，复杂性的搜索功能，聚合分析的功能，基于地理位置的搜索(距离我当前位置1公里以内的烤肉店)\r\n',8,0,0,1514602804,0,0,0),(5,1,'Elasticsearch的功能','','','（1）分布式的搜索引擎和数据分析引擎\r\n\r\n搜索：百度，网站的站内搜索，IT系统的检索\r\n数据分析：电商网站，最近7天牙膏这种商品销量排名前10的商家有哪些；新闻网站，最近1个月访问量排名前3的新闻版块是哪些\r\n分布式，搜索，数据分析\r\n\r\n（2）全文检索，结构化检索，数据分析\r\n\r\n全文检索：我想搜索商品名称包含牙膏的商品，select * from products where product_name like &quot;%牙膏%&quot;\r\n结构化检索：我想搜索商品分类为日化用品的商品都有哪些，select * from products where category_id=&#039;日化用品&#039;\r\n部分匹配、自动完成、搜索纠错、搜索推荐\r\n数据分析：我们分析每一个商品分类下有多少个商品，select category_id,count(*) from products group by category_id\r\n\r\n（3）对海量数据进行近实时的处理\r\n\r\n分布式：ES自动可以将海量数据分散到多台服务器上去存储和检索\r\n海联数据的处理：分布式以后，就可以采用大量的服务器去存储和检索数据，自然而然就可以实现海量数据的处理了\r\n近实时：检索个数据要花费1小时（这就不要近实时，离线批处理，batch-processing）；在秒级别对数据进行搜索和分析\r\n\r\n跟分布式/海量数据相反的：lucene，单机应用，只能在单台服务器上使用，最多只能处理单台服务器可以处理的数据量\r\n',8,0,0,1514603029,0,0,0),(6,1,'Elasticsearch的适用场景','','','国外\r\n\r\n（1）维基百科，类似百度百科，牙膏，牙膏的维基百科，全文检索，高亮，搜索推荐\r\n（2）The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击，浏览，收藏，评论）+社交网络数据（对某某新闻的相关看法），数据分析，给到每篇新闻文章的作者，让他知道他的文章的公众反馈（好，坏，热门，垃圾，鄙视，崇拜）\r\n（3）Stack Overflow（国外的程序异常讨论论坛），IT问题，程序的报错，提交上去，有人会跟你讨论和回答，全文检索，搜索相关问题和答案，程序报错了，就会将报错信息粘贴到里面去，搜索有没有对应的答案\r\n（4）GitHub（开源代码管理），搜索上千亿行代码\r\n（5）电商网站，检索商品\r\n（6）日志数据分析，logstash采集日志，ES进行复杂的数据分析（ELK技术，elasticsearch+logstash+kibana）\r\n（7）商品价格监控网站，用户设定某商品的价格阈值，当低于该阈值的时候，发送通知消息给用户，比如说订阅牙膏的监控，如果高露洁牙膏的家庭套装低于50块钱，就通知我，我就去买\r\n（8）BI系统，商业智能，Business Intelligence。比如说有个大型商场集团，BI，分析一下某某区域最近3年的用户消费金额的趋势以及用户群体的组成构成，产出相关的数张报表，**区，最近3年，每年消费金额呈现100%的增长，而且用户群体85%是高级白领，开一个新商场。ES执行数据分析和挖掘，Kibana进行数据可视化\r\n\r\n国内\r\n\r\n（9）国内：站内搜索（电商，招聘，门户，等等），IT系统搜索（OA，CRM，ERP，等等），数据分析（ES热门的一个使用场景）\r\n',8,0,0,1514603083,0,0,0),(7,1,'Elasticsearch的特点','','','（1）可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上，服务小公司\r\n（2）Elasticsearch不是什么新技术，主要是将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的ES；lucene（全文检索），商用的数据分析软件（也是有的），分布式数据库（mycat）\r\n（3）对用户而言，是开箱即用的，非常简单，作为中小型的应用，直接3分钟部署一下ES，就可以作为生产环境的系统来使用了，数据量不大，操作不是太复杂\r\n（4）数据库的功能面对很多领域是不够用的（事务，还有各种联机事务型的操作）；特殊的功能，比如全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理；Elasticsearch作为传统数据库的一个补充，提供了数据库所不不能提供的很多功能\r\n\r\n',8,0,0,1514603107,0,0,0),(8,1,'lucene和elasticsearch的前世今生','','','lucene，最先进、功能最强大的搜索库，直接基于lucene开发，非常复杂，api复杂（实现一些简单的功能，写大量的java代码），需要深入理解原理（各种索引结构）\r\n\r\nelasticsearch，基于lucene，隐藏复杂性，提供简单易用的restful api接口、java api接口（还有其他语言的api接口）\r\n（1）分布式的文档存储引擎\r\n（2）分布式的搜索引擎和分析引擎\r\n（3）分布式，支持PB级数据\r\n\r\n开箱即用，优秀的默认参数，不需要任何额外设置，完全开源\r\n\r\n关于elasticsearch的一个传说，有一个程序员失业了，陪着自己老婆去英国伦敦学习厨师课程。程序员在失业期间想给老婆写一个菜谱搜索引擎，觉得lucene实在太复杂了，就开发了一个封装了lucene的开源项目，compass。后来程序员找到了工作，是做分布式的高性能项目的，觉得compass不够，就写了elasticsearch，让lucene变成分布式的系统。\r\n',8,0,0,1514603267,0,0,0),(9,1,'elasticsearch的核心概念','','','（1）Near Realtime（NRT）：近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级\r\n\r\n（2）Cluster：集群，包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常\r\n（3）Node：节点，集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为“elasticsearch”的集群，如果直接启动一堆节点，那么它们会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群\r\n\r\n（4）Document&amp;field：文档，es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。一个document里面有多个field，每个field就是一个数据字段。\r\n\r\nproduct document\r\n\r\n{\r\n  &quot;product_id&quot;: &quot;1&quot;,\r\n  &quot;product_name&quot;: &quot;高露洁牙膏&quot;,\r\n  &quot;product_desc&quot;: &quot;高效美白&quot;,\r\n  &quot;category_id&quot;: &quot;2&quot;,\r\n  &quot;category_name&quot;: &quot;日化用品&quot;\r\n}\r\n\r\n（5）Index：索引，包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。\r\n（6）Type：类型，每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type。\r\n\r\n商品index，里面存放了所有的商品数据，商品document\r\n\r\n但是商品分很多种类，每个种类的document的field可能不太一样，比如说电器商品，可能还包含一些诸如售后时间范围这样的特殊field；生鲜商品，还包含一些诸如生鲜保质期之类的特殊field\r\n\r\ntype，日化商品type，电器商品type，生鲜商品type\r\n\r\n日化商品type：product_id，product_name，product_desc，category_id，category_name\r\n电器商品type：product_id，product_name，product_desc，category_id，category_name，service_period\r\n生鲜商品type：product_id，product_name，product_desc，category_id，category_name，eat_period\r\n\r\n每一个type里面，都会包含一堆document\r\n\r\n\r\n{\r\n  &quot;product_id&quot;: &quot;2&quot;,\r\n  &quot;product_name&quot;: &quot;长虹电视机&quot;,\r\n  &quot;product_desc&quot;: &quot;4k高清&quot;,\r\n  &quot;category_id&quot;: &quot;3&quot;,\r\n  &quot;category_name&quot;: &quot;电器&quot;,\r\n  &quot;service_period&quot;: &quot;1年&quot;\r\n}\r\n\r\n\r\n{\r\n  &quot;product_id&quot;: &quot;3&quot;,\r\n  &quot;product_name&quot;: &quot;基围虾&quot;,\r\n  &quot;product_desc&quot;: &quot;纯天然，冰岛产&quot;,\r\n  &quot;category_id&quot;: &quot;4&quot;,\r\n  &quot;category_name&quot;: &quot;生鲜&quot;,\r\n  &quot;eat_period&quot;: &quot;7天&quot;\r\n}\r\n\r\n（7）shard：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个shard都是一个lucene index。\r\n（8）replica：任何一个服务器随时可能故障或宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据不丢失，多个replica还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认5个），replica shard（随时修改数量，默认1个），默认每个索引10个shard，5个primary shard，5个replica shard，最小的高可用配置，是2台服务器。\r\n',8,0,0,1514603492,0,0,0),(10,1,'elasticsearch核心概念 vs. 数据库核心概念','','','Elasticsearch			数据库\r\n\r\n-----------------------------------------\r\n\r\nDocument			行\r\nType				表\r\nIndex				库',8,0,0,1514603531,0,0,0),(11,1,'Elasticsearch的安装','','','1、安装JDK，至少1.8.0以上版本，java -version\r\ncmd\r\nC:\\Users\\caopeng&gt;java -version\r\njava version &quot;1.8.0_151&quot;\r\nJava(TM) SE Runtime Environment (build 1.8.0_151-b12)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)\r\n\r\n2、下载和解压缩Elasticsearch安装包，目录结构\r\nbin 命令脚本\r\nconfig 配置文件,日志\r\nlib 依赖的jar (主要依赖lucene)\r\nmodules 功能模块\r\nplugins 插件\r\n\r\n3、启动Elasticsearch：bin\\elasticsearch.bat，es本身特点之一就是开箱即用，如果是中小型应用，数据量少，操作不是很复杂，直接启动就可以用了\r\ne:\r\ncd es\\elasticsearch-5.2.0\\bin\\elasticsearch.bat\r\n\r\n4、检查ES是否启动成功：http://localhost:9200/?pretty\r\n\r\nname: node名称\r\ncluster_name: 集群名称（默认的集群名称就是elasticsearch）\r\nversion.number: 5.2.0，es版本号\r\n\r\n{\r\n  &quot;name&quot; : &quot;4onsTYV&quot;,\r\n  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,\r\n  &quot;cluster_uuid&quot; : &quot;nKZ9VK_vQdSQ1J0Dx9gx1Q&quot;,\r\n  &quot;version&quot; : {\r\n    &quot;number&quot; : &quot;5.2.0&quot;,\r\n    &quot;build_hash&quot; : &quot;24e05b9&quot;,\r\n    &quot;build_date&quot; : &quot;2017-01-24T19:52:35.800Z&quot;,\r\n    &quot;build_snapshot&quot; : false,\r\n    &quot;lucene_version&quot; : &quot;6.4.0&quot;\r\n  },\r\n  &quot;tagline&quot; : &quot;You Know, for Search&quot;\r\n}\r\n\r\n5、修改集群名称：elasticsearch.yml\r\n6、下载和解压缩Kibana安装包，使用里面的开发界面，去操作elasticsearch，作为我们学习es知识点的一个主要的界面入口\r\n7、启动Kibana：bin\\kibana.bat\r\n8、进入Dev Tools界面\r\n9、GET _cluster/health (Console)\r\n{\r\n  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,\r\n  &quot;status&quot;: &quot;yellow&quot;,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;number_of_nodes&quot;: 1,\r\n  &quot;number_of_data_nodes&quot;: 1,\r\n  &quot;active_primary_shards&quot;: 1,\r\n  &quot;active_shards&quot;: 1,\r\n  &quot;relocating_shards&quot;: 0,\r\n  &quot;initializing_shards&quot;: 0,\r\n  &quot;unassigned_shards&quot;: 1,\r\n  &quot;delayed_unassigned_shards&quot;: 0,\r\n  &quot;number_of_pending_tasks&quot;: 0,\r\n  &quot;number_of_in_flight_fetch&quot;: 0,\r\n  &quot;task_max_waiting_in_queue_millis&quot;: 0,\r\n  &quot;active_shards_percent_as_number&quot;: 50\r\n}\r\n\r\n---------------------------------------\r\nlinux系统上安装\r\nelastic 安装\r\n-----------------------------------------------\r\nhttps://www.elastic.co/downloads/elasticsearch 下载 tar.gz包\r\n\r\nmkdir /opt/elastic\r\n将tar.gz包上传到/opt/elastic目录下\r\ncd  /opt/elastic\r\ntar -zxvf elasticsearch-5.1.1.tar.gz\r\ncd elasticsearch-5.1.1\r\n\r\n新建用户 (使用root用户会报错 can not run elasticsearch as root)\r\nuseradd apeng\r\npasswd apeng\r\n\r\n组目录添加用户权限 (不然会报错 could not register mbeans : access denied)\r\ncd /opt/elastic\r\nsudo chown -R apeng elasticsearch-5.1.1\r\nsudo chgrp -R apeng elasticsearch-5.1.1\r\ncd /opt/\r\nsudo chown -R apeng es\r\nsudo chgrp -R apeng es\r\n\r\nmax file descriptors[4096] for elasticsearch likely to low, increase to al least [65536]\r\n无法创建本地文件问题\r\n\r\nvi /etc/security/limits.conf #添加以下内容 (* 指所有用户)\r\n* soft nofile 65536\r\n* hard nofile 131072\r\n* soft nproc 2048\r\n* hard nproc 4096\r\n\r\nmax number of threads [1024] for user [es] likely too low, increase to at least[2048]\r\n无法创建本要线程问题\r\nvi /etc/security/limits.d/90-nproc.conf\r\n* soft nproc 1024 ---&gt; * soft nproc 2048\r\n\r\nmax virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]\r\n最大虚拟内存大小\r\nvi /etc/sysctl.conf #添加以下内容\r\nvm.max_map_count=655360\r\n修改后并执行命令\r\nsysctl -p\r\n\r\nunable to install syscall filter\r\n警告 (linux系统版本过低)\r\n\r\nsu apeng\r\ncd /opt/elastic/elasticsearch\r\nvim config/el.yml\r\npath.data: /opt/es/data\r\npath.logs: /opt/es/logs\r\nnetwork.host: 192.168.233.132\r\nhttp.port: 9200\r\n\r\nbin/elasticsearch\r\n页面访问 192.168.233.132:9200',8,0,0,1514603823,0,0,0),(12,1,'document数据格式','','','面向文档的搜索分析引擎\r\n\r\n（1）应用系统的数据结构都是面向对象的，复杂的\r\n（2）对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦\r\n（3）ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能\r\n（4）es的document用json数据格式来表达\r\n\r\npublic class Employee {\r\n\r\n  private String email;\r\n  private String firstName;\r\n  private String lastName;\r\n  private EmployeeInfo info;\r\n  private Date joinDate;\r\n\r\n}\r\n\r\nprivate class EmployeeInfo {\r\n  \r\n  private String bio; // 性格\r\n  private Integer age;\r\n  private String[] interests; // 兴趣爱好\r\n\r\n}\r\n\r\nEmployeeInfo info = new EmployeeInfo();\r\ninfo.setBio(&quot;curious and modest&quot;);\r\ninfo.setAge(30);\r\ninfo.setInterests(new String[]{&quot;bike&quot;, &quot;climb&quot;});\r\n\r\nEmployee employee = new Employee();\r\nemployee.setEmail(&quot;zhangsan@sina.com&quot;);\r\nemployee.setFirstName(&quot;san&quot;);\r\nemployee.setLastName(&quot;zhang&quot;);\r\nemployee.setInfo(info);\r\nemployee.setJoinDate(new Date());\r\n\r\nemployee对象：里面包含了Employee类自己的属性，还有一个EmployeeInfo对象\r\n\r\n两张表：employee表，employee_info表，将employee对象的数据重新拆开来，变成Employee数据和EmployeeInfo数据\r\nemployee表：email，first_name，last_name，join_date，4个字段\r\nemployee_info表：bio，age，interests，3个字段；此外还有一个外键字段，比如employee_id，关联着employee表\r\n\r\n{\r\n    &quot;email&quot;:      &quot;zhangsan@sina.com&quot;,\r\n    &quot;first_name&quot;: &quot;san&quot;,\r\n    &quot;last_name&quot;: &quot;zhang&quot;,\r\n    &quot;info&quot;: {\r\n        &quot;bio&quot;:         &quot;curious and modest&quot;,\r\n        &quot;age&quot;:         30,\r\n        &quot;interests&quot;: [ &quot;bike&quot;, &quot;climb&quot; ]\r\n    },\r\n    &quot;join_date&quot;: &quot;2017/01/01&quot;\r\n}\r\n\r\n我们就明白了es的document数据格式和数据库的关系型数据格式的区别',8,0,0,1514603992,0,0,0),(13,1,'电商网站商品管理案例背景介绍','','','有一个电商网站，需要为其基于ES构建一个后台系统，提供以下功能：\r\n\r\n（1）对商品信息进行CRUD（增删改查）操作\r\n（2）执行简单的结构化查询\r\n（3）可以执行简单的全文检索，以及复杂的phrase（短语）检索\r\n（4）对于全文检索的结果，可以进行高亮显示\r\n（5）对数据进行简单的聚合分析',8,0,0,1514604016,0,0,0),(14,1,'简单的集群管理','','','（1）快速检查集群的健康状况\r\n\r\nes提供了一套api，叫做cat api，可以查看es中各种各样的数据\r\n\r\nGET /_cat/health?v\r\n\r\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\r\n1488006741 15:12:21  elasticsearch yellow          1         1      1   1    0    0        1             0                  -                 50.0%\r\n\r\n再解压一个 启动之后会自动加入集群 会在9301端口\r\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\r\n1488007113 15:18:33  elasticsearch green           2         2      2   1    0    0        0             0                  -                100.0%\r\n\r\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\r\n1488007216 15:20:16  elasticsearch yellow          1         1      1   1    0    0        1             0                  -                 50.0%\r\n\r\n如何快速了解集群的健康状况？green、yellow、red？\r\n\r\ngreen：每个索引的primary shard和replica shard都是active状态的\r\nyellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态\r\nred：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了\r\n\r\n为什么现在会处于一个yellow状态？\r\n\r\n我们现在就一个笔记本电脑，就启动了一个es进程，相当于就只有一个node。现在es中有一个index，就是kibana自己内置建立的index。由于默认的配置是给每个index分配5个primary shard和5个replica shard，而且primary shard和replica shard不能在同一台机器上（为了容错）。现在kibana自己建立的index是1个primary shard和1个replica shard。当前就一个node，所以只有1个primary shard被分配了和启动了，但是一个replica shard没有第二台机器去启动。\r\n\r\n做一个小实验：此时只要启动第二个es进程，就会在es集群中有2个node，然后那1个replica shard就会自动分配过去，然后cluster status就会变成green状态。\r\n\r\n（2）快速查看集群中有哪些索引\r\n\r\nGET /_cat/indices?v\r\n\r\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\r\nyellow open   .kibana rUm9n9wMRQCCrRDEhqneBg   1   1          1            0      3.1kb          3.1kb\r\n\r\n（3）简单的索引操作\r\n\r\n创建索引：PUT /test_index?pretty\r\n\r\nhealth status index      uuid                   pri rep docs.count docs.deleted store.size pri.store.size\r\nyellow open   test_index XmS9DTAtSkSZSwWhhGEKkQ   5   1          0            0       650b           650b\r\nyellow open   .kibana    rUm9n9wMRQCCrRDEhqneBg   1   1          1            0      3.1kb          3.1kb\r\n\r\n删除索引：DELETE /test_index?pretty\r\n\r\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\r\nyellow open   .kibana rUm9n9wMRQCCrRDEhqneBg   1   1          1            0      3.1kb          3.1kb\r\n',8,0,0,1514604059,0,0,0),(15,1,'商品的CRUD操作','','','（1）新增商品：新增文档，建立索引\r\n\r\nPUT /index/type/id\r\n{\r\n  &quot;json数据&quot;\r\n}\r\n\r\nPUT /ecommerce/product/1\r\n{\r\n    &quot;name&quot; : &quot;gaolujie yagao&quot;,\r\n    &quot;desc&quot; :  &quot;gaoxiao meibai&quot;,\r\n    &quot;price&quot; :  30,\r\n    &quot;producer&quot; :      &quot;gaolujie producer&quot;,\r\n    &quot;tags&quot;: [ &quot;meibai&quot;, &quot;fangzhu&quot; ]\r\n}\r\n创建返回值\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,  #索引\r\n  &quot;_type&quot;: &quot;product&quot;,     #类型\r\n  &quot;_id&quot;: &quot;1&quot;,             #id\r\n  &quot;_version&quot;: 1,          #初始版本\r\n  &quot;result&quot;: &quot;created&quot;,    #结果类型\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nPUT /ecommerce/product/2\r\n{\r\n    &quot;name&quot; : &quot;jiajieshi yagao&quot;,\r\n    &quot;desc&quot; :  &quot;youxiao fangzhu&quot;,\r\n    &quot;price&quot; :  25,\r\n    &quot;producer&quot; :      &quot;jiajieshi producer&quot;,\r\n    &quot;tags&quot;: [ &quot;fangzhu&quot; ]\r\n}\r\n\r\nPUT /ecommerce/product/3\r\n{\r\n    &quot;name&quot; : &quot;zhonghua yagao&quot;,\r\n    &quot;desc&quot; :  &quot;caoben zhiwu&quot;,\r\n    &quot;price&quot; :  40,\r\n    &quot;producer&quot; :      &quot;zhonghua producer&quot;,\r\n    &quot;tags&quot;: [ &quot;qingxin&quot; ]\r\n}\r\n\r\nes会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索\r\n\r\n（2）查询商品：检索文档\r\n\r\nGET /index/type/id\r\nGET /ecommerce/product/1\r\n\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;name&quot;: &quot;gaolujie yagao&quot;,\r\n    &quot;desc&quot;: &quot;gaoxiao meibai&quot;,\r\n    &quot;price&quot;: 30,\r\n    &quot;producer&quot;: &quot;gaolujie producer&quot;,\r\n    &quot;tags&quot;: [\r\n      &quot;meibai&quot;,\r\n      &quot;fangzhu&quot;\r\n    ]\r\n  }\r\n}\r\n\r\n（3）修改商品：替换文档\r\n\r\nPUT /ecommerce/product/1\r\n{\r\n    &quot;name&quot; : &quot;jiaqiangban gaolujie yagao&quot;, #更新name\r\n    &quot;desc&quot; :  &quot;gaoxiao meibai&quot;,\r\n    &quot;price&quot; :  30,\r\n    &quot;producer&quot; :      &quot;gaolujie producer&quot;,\r\n    &quot;tags&quot;: [ &quot;meibai&quot;, &quot;fangzhu&quot; ]\r\n}\r\n// 初始返回值(第一次执行)\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n// 第二次执行返回值 _version有改变 result由created ---&gt; updated created:true ---&gt; false\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}\r\n\r\n\r\nPUT /ecommerce/product/1 #执行之后只有name字段\r\n{\r\n    &quot;name&quot; : &quot;jiaqiangban gaolujie yagao&quot;\r\n}\r\n\r\n替换方式有一个不好，即使必须带上所有的field，才能去进行信息的修改\r\n\r\n（4）修改商品：更新文档\r\n\r\nPOST /ecommerce/product/1/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;name&quot;: &quot;jiaqiangban gaolujie yagao1&quot;\r\n  }\r\n}\r\n注意有修改result的值是updated没修改返回noop\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 8,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\n我的风格，其实有选择的情况下，不太喜欢念ppt，或者照着文档做，或者直接粘贴写好的代码，尽量是纯手敲代码\r\n\r\n（5）删除商品：删除文档\r\n\r\nDELETE /ecommerce/product/1\r\n\r\n{\r\n  &quot;found&quot;: true,\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 9,\r\n  &quot;result&quot;: &quot;deleted&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;found&quot;: false\r\n}\r\n',8,0,0,1514604713,0,0,0),(16,1,'query string search','','','query string search GET /索引/类型/_search\r\n\r\n搜索全部商品：GET /ecommerce/product/_search\r\n\r\ntook：耗费了几毫秒\r\ntimed_out：是否超时，这里是没有\r\n_shards：数据拆成了5个分片，所以对于搜索请求，会打到所有的primary shard（或者是它的某个replica shard也可以）\r\nhits.total：查询结果的数量，3个document\r\nhits.max_score：score的含义，就是document对于一个search的相关度的匹配分数，越相关，就越匹配，分数也高\r\nhits.hits：包含了匹配搜索的document的详细数据\r\n\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;jiajieshi yagao&quot;,\r\n          &quot;desc&quot;: &quot;youxiao fangzhu&quot;,\r\n          &quot;price&quot;: 25,\r\n          &quot;producer&quot;: &quot;jiajieshi producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;fangzhu&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;gaolujie yagao&quot;,\r\n          &quot;desc&quot;: &quot;gaoxiao meibai&quot;,\r\n          &quot;price&quot;: 30,\r\n          &quot;producer&quot;: &quot;gaolujie producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;meibai&quot;,\r\n            &quot;fangzhu&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nquery string search的由来，因为search参数都是以http请求的query string来附带的\r\n\r\n搜索商品名称中包含yagao的商品，而且按照售价降序排序：GET /ecommerce/product/_search?q=name:yagao&amp;sort=price:desc\r\n\r\n适用于临时的在命令行使用一些工具，比如curl，快速的发出请求，来检索想要的信息；但是如果查询请求很复杂，是很难去构建的\r\n在生产环境中，几乎很少使用query string search',8,0,0,1514604958,0,0,0),(17,1,'query DSL','','','query DSL\r\n\r\nDSL：Domain Specified Language，特定领域的语言\r\nhttp request body：请求体，可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法，比query string search肯定强大多了\r\n\r\n查询所有的商品\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;query&quot;: { &quot;match_all&quot;: {} }\r\n}\r\n\r\n查询名称包含yagao的商品，同时按照价格降序排序\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match&quot; : {\r\n            &quot;name&quot; : &quot;yagao&quot;\r\n        }\r\n    },\r\n    &quot;sort&quot;: [\r\n        { &quot;price&quot;: &quot;desc&quot; }\r\n    ]\r\n}\r\n\r\n分页查询商品，总共3条商品，假设每页就显示1条商品，现在显示第2页，所以就查出来第2个商品\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;query&quot;: { &quot;match_all&quot;: {} },\r\n  &quot;from&quot;: 1,\r\n  &quot;size&quot;: 1\r\n}\r\n\r\n指定要查询出来商品的名称和价格就可以\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;query&quot;: { &quot;match_all&quot;: {} },\r\n  &quot;_source&quot;: [&quot;name&quot;, &quot;price&quot;]\r\n}\r\n\r\n更加适合生产环境的使用，可以构建复杂的查询',8,0,0,1514605008,0,0,0),(18,1,'query filter','','','query filter\r\n\r\n搜索商品名称包含yagao，而且售价大于25元的商品\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;bool&quot; : {\r\n            &quot;must&quot; : {\r\n                &quot;match&quot; : {\r\n                    &quot;name&quot; : &quot;yagao&quot; \r\n                }\r\n            },\r\n            &quot;filter&quot; : {\r\n                &quot;range&quot; : {\r\n                    &quot;price&quot; : { &quot;gt&quot; : 25 } \r\n                }\r\n            }\r\n        }\r\n    }\r\n}',8,0,0,1514605036,0,0,0),(19,1,'full-text search','','','full-text search（全文检索）\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match&quot; : {\r\n            &quot;producer&quot; : &quot;yagao producer&quot;\r\n        }\r\n    }\r\n}\r\n\r\n尽量，无论是学什么技术，比如说你当初学java，学linux，学shell，学javascript，学hadoop。。。。一定自己动手，特别是手工敲各种命令和代码，切记切记，减少复制粘贴的操作。只有自己动手手工敲，学习效果才最好。\r\n\r\nproducer这个字段，会先被拆解，建立倒排索引\r\n\r\nspecial		4\r\nyagao		4\r\nproducer	1,2,3,4\r\ngaolujie	1\r\nzhognhua	3\r\njiajieshi	2\r\n\r\nyagao producer ---&gt; yagao和producer\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 4,\r\n    &quot;max_score&quot;: 0.70293105,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 0.70293105,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;special yagao&quot;,\r\n          &quot;desc&quot;: &quot;special meibai&quot;,\r\n          &quot;price&quot;: 50,\r\n          &quot;producer&quot;: &quot;special yagao producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;meibai&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.25811607,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;gaolujie yagao&quot;,\r\n          &quot;desc&quot;: &quot;gaoxiao meibai&quot;,\r\n          &quot;price&quot;: 30,\r\n          &quot;producer&quot;: &quot;gaolujie producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;meibai&quot;,\r\n            &quot;fangzhu&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514605087,0,0,0),(20,1,'phrase search','','','phrase search\r\n\r\n跟全文检索相对应，相反，全文检索会将输入的搜索串拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回\r\nphrase search，要求输入的搜索串，必须在指定的字段文本中，完全包含一模一样的，才可以算匹配，才能作为结果返回\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match_phrase&quot; : {\r\n            &quot;producer&quot; : &quot;yagao producer&quot;\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 11,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 0.70293105,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 0.70293105,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;special yagao&quot;,\r\n          &quot;desc&quot;: &quot;special meibai&quot;,\r\n          &quot;price&quot;: 50,\r\n          &quot;producer&quot;: &quot;special yagao producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;meibai&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514605143,0,0,0),(21,1,'highlight search（高亮搜索结果）','','','GET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match&quot; : {\r\n            &quot;producer&quot; : &quot;producer&quot;\r\n        }\r\n    },\r\n    &quot;highlight&quot;: {\r\n        &quot;fields&quot; : {\r\n            &quot;producer&quot; : {}\r\n        }\r\n    }\r\n}',8,0,0,1514605160,0,0,0),(22,1,'聚合计算一','','','第一个分析需求：计算每个tag下的商品数量\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_tags&quot;: {\r\n      &quot;terms&quot;: { &quot;field&quot;: &quot;tags&quot; }\r\n    }\r\n  }\r\n}\r\n\r\n将文本field的fielddata属性设置为true\r\n\r\nPUT /ecommerce/_mapping/product\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;tags&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;fielddata&quot;: true\r\n    }\r\n  }\r\n}\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;all_tags&quot;: {\r\n      &quot;terms&quot;: { &quot;field&quot;: &quot;tags&quot; }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 20,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 4,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_tags&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;fangzhu&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;meibai&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;qingxin&quot;,\r\n          &quot;doc_count&quot;: 1\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\n第二个聚合分析的需求：对名称中包含yagao的商品，计算每个tag下的商品数量\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;name&quot;: &quot;yagao&quot;\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;all_tags&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;tags&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\n第三个聚合分析的需求：先分组，再算每组的平均值，计算每个tag下的商品的平均价格\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;size&quot;: 0,\r\n    &quot;aggs&quot; : {\r\n        &quot;group_by_tags&quot; : {\r\n            &quot;terms&quot; : { &quot;field&quot; : &quot;tags&quot; },\r\n            &quot;aggs&quot; : {\r\n                &quot;avg_price&quot; : {\r\n                    &quot;avg&quot; : { &quot;field&quot; : &quot;price&quot; }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 8,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 4,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_tags&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;fangzhu&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 27.5\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;meibai&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 40\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;qingxin&quot;,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 40\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\n第四个数据分析需求：计算每个tag下的商品的平均价格，并且按照平均价格降序排序\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;size&quot;: 0,\r\n    &quot;aggs&quot; : {\r\n        &quot;all_tags&quot; : {\r\n            &quot;terms&quot; : { &quot;field&quot; : &quot;tags&quot;, &quot;order&quot;: { &quot;avg_price&quot;: &quot;desc&quot; } },\r\n            &quot;aggs&quot; : {\r\n                &quot;avg_price&quot; : {\r\n                    &quot;avg&quot; : { &quot;field&quot; : &quot;price&quot; }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n我们现在全部都是用es的restful api在学习和讲解es的所欲知识点和功能点，但是没有使用一些编程语言去讲解（比如java），原因有以下：\r\n\r\n1、es最重要的api，让我们进行各种尝试、学习甚至在某些环境下进行使用的api，就是restful api。如果你学习不用es restful api，比如我上来就用java api来讲es，也是可以的，但是你根本就漏掉了es知识的一大块，你都不知道它最重要的restful api是怎么用的\r\n2、讲知识点，用es restful api，更加方便，快捷，不用每次都写大量的java代码，能加快讲课的效率和速度，更加易于同学们关注es本身的知识和功能的学习\r\n3、我们通常会讲完es知识点后，开始详细讲解java api，如何用java api执行各种操作\r\n4、我们每个篇章都会搭配一个项目实战，项目实战是完全基于java去开发的真实项目和系统\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\n第五个数据分析需求：按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_price&quot;: {\r\n      &quot;range&quot;: {\r\n        &quot;field&quot;: &quot;price&quot;,\r\n        &quot;ranges&quot;: [\r\n          {\r\n            &quot;from&quot;: 0,\r\n            &quot;to&quot;: 20\r\n          },\r\n          {\r\n            &quot;from&quot;: 20,\r\n            &quot;to&quot;: 40\r\n          },\r\n          {\r\n            &quot;from&quot;: 40,\r\n            &quot;to&quot;: 50\r\n          }\r\n        ]\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_tags&quot;: {\r\n          &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;tags&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;average_price&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;price&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514605416,0,0,0),(23,1,'Elasticsearch对复杂分布式机制的透明隐藏特性','','','Elasticsearch是一套分布式的系统，分布式是为了应对大数据量\r\n隐藏了复杂的分布式机制\r\n\r\n分片机制（我们之前随随便便就将一些document插入到es集群中去了，我们有没有care过数据怎么进行分片的，数据到哪个shard中去）\r\n\r\ncluster discovery（集群发现机制，我们之前在做那个集群status从yellow转green的实验里，直接启动了第二个es进程，那个进程作为一个node自动就发现了集群，并且加入了进去，还接受了部分数据，replica shard）\r\n\r\nshard负载均衡（举例，假设现在有3个节点，总共有25个shard要分配到3个节点上去，es会自动进行均匀分配，以保持每个节点的均衡的读写负载请求）\r\n\r\nshard副本，请求路由，集群扩容，shard重分配',8,0,0,1514605567,0,0,0),(24,1,'Elasticsearch的垂直扩容与水平扩容','','','垂直扩容：采购更强大的服务器，成本非常高昂，而且会有瓶颈，假设世界上最强大的服务器容量就是10T，但是当你的总数据量达到5000T的时候，你要采购多少台最强大的服务器啊\r\n\r\n水平扩容：业界经常采用的方案，采购越来越多的普通服务器，性能比较一般，但是很多普通服务器组织在一起，就能构成强大的计算和存储能力\r\n\r\n普通服务器：1T，1万，100万\r\n强大服务器：10T，50万，500万\r\n\r\n扩容对应用程序的透明性',8,0,0,1514605594,0,0,0),(25,1,'节点介绍','','','1.增减或减少节点时的数据rebalance\r\n\r\n保持负载均衡\r\n\r\n2.master节点\r\n\r\n（1）创建或删除索引\r\n（2）增加或删除节点\r\n\r\n3.节点平等的分布式架构\r\n\r\n（1）节点对等，每个节点都能接收所有的请求\r\n（2）自动请求路由\r\n（3）响应收集',8,0,0,1514605655,0,0,0),(26,1,'kibana安装','','','https://www.elastic.co/downloads/elasticsearch 下载 tar.gz包\r\n\r\nuname -a #查看linux的版本 (下载对应的版本kibana)\r\nmkdir /opt/kibana\r\n将tar.gz包上传到/opt/kibana\r\ncd /opt/kibana\r\ntar -zxvf kibana-5.1.1.tar.gz\r\ncd kibana-5.1.1\r\n\r\n修改配置\r\nvi config/kibana.yml\r\nserver.host 在windows中不用指定IP,访问使用 http://localhost:5601\r\n在vm虚拟机上，使用IP来访问，这时需要指定IP 访问使用http://IP:5601\r\nserver.host: &quot;192.168.233.132&quot;\r\nelasticsearch.url: &quot;http://192.168.233.132:9200&quot;\r\nkibana.index: &quot;.kibana&quot;\r\n\r\nbin/kibana\r\nhttp://192.168.233.132:5601/server.port: 5601',8,0,0,1514605909,0,0,0),(27,1,'shard&replica机制再次梳理','','','（1）index包含多个shard\r\n（2）每个shard都是一个最小工作单元，承载部分数据，lucene实例，完整的建立索引和处理请求的能力\r\n（3）增减节点时，shard会自动在nodes中负载均衡\r\n（4）primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard\r\n（5）replica shard是primary shard的副本，负责容错，以及承担读请求负载\r\n（6）primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改\r\n（7）primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard\r\n（8）primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上\r\n',8,0,0,1514605958,0,0,0),(28,1,'图解单node环境下创建index是什么样子的','','','（1）单node环境下，创建一个index，有3个primary shard，3个replica shard\r\n（2）集群status是yellow\r\n（3）这个时候，只会将3个primary shard分配到仅有的一个node上去，另外3个replica shard是无法分配的\r\n（4）集群可以正常工作，但是一旦出现节点宕机，数据全部丢失，而且集群不可用，无法承接任何请求\r\n\r\nPUT /test_index\r\n{\r\n   &quot;settings&quot; : {\r\n      &quot;number_of_shards&quot; : 3,\r\n      &quot;number_of_replicas&quot; : 1\r\n   }\r\n}',8,0,0,1514605992,0,0,0),(29,1,'图解2个node环境下replica shard是如何分配的','','','（1）replica shard分配：3个primary shard，3个replica shard，1 node\r\n（2）primary ---&gt; replica同步\r\n（3）读请求：primary/replica',8,0,0,1514606330,0,0,0),(30,1,'图解横向扩容过程，如何超出扩容极限，以及如何提升容错性','','','（1）primary&amp;replica自动负载均衡，6个shard，3 primary，3 replica\r\n（2）每个node有更少的shard，IO/CPU/Memory资源给每个shard分配更多，每个shard性能更好\r\n（3）扩容的极限，6个shard（3 primary，3 replica），最多扩容到6台机器，每个shard可以占用单台服务器的所有资源，性能最好\r\n（4）超出扩容极限，动态修改replica数量，9个shard（3primary，6 replica），扩容到9台机器，比3台机器时，拥有3倍的读吞吐量\r\n（5）3台机器下，9个shard（3 primary，6 replica），资源更少，但是容错性更好，最多容纳2台机器宕机，6个shard只能容纳0台机器宕机\r\n（6）这里的这些知识点，你综合起来看，就是说，一方面告诉你扩容的原理，怎么扩容，怎么提升系统整体吞吐量；另一方面要考虑到系统的容错性，怎么保证提高容错性，让尽可能多的服务器宕机，保证数据不丢失\r\n',8,0,0,1514606434,0,0,0),(31,1,'图解Elasticsearch容错机制：master选举，r','','','（1）9 shard，3 node\r\n（2）master node宕机，自动master选举，red\r\n（3）replica容错：新master将replica提升为primary shard，yellow\r\n（4）重启宕机node，master copy replica到该node，使用原有的shard并同步宕机后的修改，green\r\n',8,0,0,1514606508,0,0,0),(32,1,'es元数据','','','{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_content&quot;: &quot;test test&quot;\r\n  }\r\n}\r\n\r\n------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n1、_index元数据 (index如何创建的反例分析.png)\r\n\r\n（1）代表一个document存放在哪个index中\r\n（2）类似的数据放在一个索引，非类似的数据放不同索引：product index（包含了所有的商品），sales index（包含了所有的商品销售数据），inventory index（包含了所有库存相关的数据）。如果你把比如product，sales，human resource（employee），全都放在一个大的index里面，比如说company index，不合适的。\r\n（3）index中包含了很多类似的document：类似是什么意思，其实指的就是说，这些document的fields很大一部分是相同的，你说你放了3个document，每个document的fields都完全不一样，这就不是类似了，就不太适合放到一个index里面去了。\r\n（4）索引名称必须是小写的，不能用下划线开头，不能包含逗号：product，website，blog\r\n\r\n2、_type元数据\r\n\r\n（1）代表document属于index中的哪个类别（type）\r\n（2）一个索引通常会划分为多个type，逻辑上对index中有些许不同的几类数据进行分类：因为一批相同的数据，可能有很多相同的fields，但是还是可能会有一些轻微的不同，可能会有少数fields是不一样的，举个例子，就比如说，商品，可能划分为电子商品，生鲜商品，日化商品，等等。\r\n（3）type名称可以是大写或者小写，但是同时不能用下划线开头，不能包含逗号\r\n\r\n3、_id元数据\r\n\r\n（1）代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document\r\n（2）我们可以手动指定document的id（put /index/type/id），也可以不指定，由es自动为我们创建一个id\r\n',8,0,0,1514606589,0,0,0),(33,1,'指定document id','','','1、手动指定document id\r\n\r\n（1）根据应用情况来说，是否满足手动指定document id的前提：\r\n\r\n一般来说，是从某些其他的系统中，导入一些数据到es时，会采取这种方式，就是使用系统中已有数据的唯一标识，作为es中document的id。举个例子，比如说，我们现在在开发一个电商网站，做搜索功能，或者是OA系统，做员工检索功能。这个时候，数据首先会在网站系统或者IT系统内部的数据库中，会先有一份，此时就肯定会有一个数据库的primary key（自增长，UUID，或者是业务编号）。如果将数据导入到es中，此时就比较适合采用数据在数据库中已有的primary key。\r\n\r\n如果说，我们是在做一个系统，这个系统主要的数据存储就是es一种，也就是说，数据产生出来以后，可能就没有id，直接就放es一个存储，那么这个时候，可能就不太适合说手动指定document id的形式了，因为你也不知道id应该是什么，此时可以采取下面要讲解的让es自动生成id的方式。\r\n\r\n（2）put /index/type/id\r\n\r\nPUT /test_index/test_type/2\r\n{\r\n  &quot;test_content&quot;: &quot;my test&quot;\r\n}\r\n\r\n2、自动生成document id\r\n\r\n（1）post /index/type\r\n\r\nPOST /test_index/test_type\r\n{\r\n  &quot;test_content&quot;: &quot;my test&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;AVp4RN0bhjxldOOnBxaE&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n（2）自动生成的id，长度为20个字符，URL安全，base64编码，GUID，分布式系统并行生成时不可能会发生冲突\r\n',8,0,0,1514606739,0,0,0),(34,1,'es source','','','1、_source元数据\r\n\r\nput /test_index/test_type/1\r\n{\r\n  &quot;test_field1&quot;: &quot;test field1&quot;,\r\n  &quot;test_field2&quot;: &quot;test field2&quot;\r\n}\r\n\r\nget /test_index/test_type/1\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field1&quot;: &quot;test field1&quot;,\r\n    &quot;test_field2&quot;: &quot;test field2&quot;\r\n  }\r\n}\r\n\r\n_source元数据：就是说，我们在创建一个document的时候，使用的那个放在request body中的json串，默认情况下，在get的时候，会原封不动的给我们返回回来。\r\n\r\n------------------------------------------------------------------------------------------------------------------\r\n\r\n2、定制返回结果\r\n\r\n定制返回的结果，指定_source中，返回哪些field\r\n\r\nGET /test_index/test_type/1?_source=test_field1,test_field2\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field2&quot;: &quot;test field2&quot;\r\n  }\r\n}',8,0,0,1514606831,0,0,0),(35,1,'es document','','','1、document的全量替换\r\n\r\n（1）语法与创建文档是一样的，如果document id不存在，那么就是创建；如果document id已经存在，那么就是全量替换操作，替换document的json串内容\r\n（2）document是不可变的，如果要修改document的内容，第一种方式就是全量替换，直接对document重新建立索引，替换里面所有的内容\r\n（3）es会将老的document标记为deleted，然后新增我们给定的一个document，当我们创建越来越多的document的时候，es会在适当的时机在后台自动删除标记为deleted的document\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n\r\n2、document的强制创建\r\n\r\n（1）创建文档与全量替换的语法是一样的，有时我们只是想新建文档，不想替换文档，如果强制进行创建呢？\r\n（2）PUT /index/type/id?op_type=create，PUT /index/type/id/_create\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n\r\n3、document的删除\r\n\r\n（1）DELETE /index/type/id\r\n（2）不会理解物理删除，只会将其标记为deleted，当数据越来越多的时候，在后台自动删除\r\n',8,0,0,1514607024,0,0,0),(36,1,'深度图解剖析Elasticsearch并发冲突问题','','','深度图解剖析Elasticsearch并发冲突问题',8,0,0,1514607395,0,0,0),(37,1,'深度图解剖析悲观锁与乐观锁两种并发控制方案','','','深度图解剖析悲观锁与乐观锁两种并发控制方案',8,0,0,1514607438,0,0,0),(38,1,'图解Elasticsearch内部如何基于_version进','','','（1）_version元数据\r\n\r\nPUT /test_index/test_type/6\r\n{\r\n  &quot;test_field&quot;: &quot;test test&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;6&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n第一次创建一个document的时候，它的_version内部版本号就是1；以后，每次对这个document执行修改或者删除操作，都会对这个_version版本号自动加1；哪怕是删除，也会对这条数据的版本号加1\r\n\r\n{\r\n  &quot;found&quot;: true,\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;6&quot;,\r\n  &quot;_version&quot;: 4,\r\n  &quot;result&quot;: &quot;deleted&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\n我们会发现，在删除一个document之后，可以从一个侧面证明，它不是立即物理删除掉的，因为它的一些版本号等信息还是保留着的。先删除一条document，再重新创建这条document，其实会在delete version基础之上，再把version号加1\r\n',8,0,0,1514607485,0,0,0),(39,1,'上机动手实战演练基于_version进行乐观锁并发控制','','','1、上机动手实战演练基于_version进行乐观锁并发控制\r\n\r\n（1）先构造一条数据出来\r\n\r\nPUT /test_index/test_type/7\r\n{\r\n  &quot;test_field&quot;: &quot;test test&quot;\r\n}\r\n\r\n（2）模拟两个客户端，都获取到了同一条数据\r\n\r\nGET test_index/test_type/7\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;7&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field&quot;: &quot;test test&quot;\r\n  }\r\n}\r\n\r\n（3）其中一个客户端，先更新了一下这个数据\r\n\r\n同时带上数据的版本号，确保说，es中的数据的版本号，跟客户端中的数据的版本号是相同的，才能修改\r\n\r\nPUT /test_index/test_type/7?version=1 \r\n{\r\n  &quot;test_field&quot;: &quot;test client 1&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;7&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}\r\n\r\n（4）另外一个客户端，尝试基于version=1的数据去进行修改，同样带上version版本号，进行乐观锁的并发控制\r\n\r\nPUT /test_index/test_type/7?version=1 \r\n{\r\n  &quot;test_field&quot;: &quot;test client 2&quot;\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[test_type][7]: version conflict, current version [2] is different than the one provided [1]&quot;,\r\n        &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n        &quot;shard&quot;: &quot;3&quot;,\r\n        &quot;index&quot;: &quot;test_index&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[test_type][7]: version conflict, current version [2] is different than the one provided [1]&quot;,\r\n    &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n    &quot;shard&quot;: &quot;3&quot;,\r\n    &quot;index&quot;: &quot;test_index&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n（5）在乐观锁成功阻止并发问题之后，尝试正确的完成更新\r\n\r\nGET /test_index/test_type/7\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;7&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field&quot;: &quot;test client 1&quot;\r\n  }\r\n}\r\n\r\n基于最新的数据和版本号，去进行修改，修改后，带上最新的版本号，可能这个步骤会需要反复执行好几次，才能成功，特别是在多线程并发更新同一条数据很频繁的情况下\r\n\r\nPUT /test_index/test_type/7?version=2 \r\n{\r\n  &quot;test_field&quot;: &quot;test client 2&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;7&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}',8,0,0,1514607581,0,0,0),(40,1,'上机动手实战演练基于external version进行乐观','','','1、上机动手实战演练基于external version进行乐观锁并发控制\r\n\r\nexternal version\r\n\r\nes提供了一个feature，就是说，你可以不用它提供的内部_version版本号来进行并发控制，可以基于你自己维护的一个版本号来进行并发控制。举个列子，加入你的数据在mysql里也有一份，然后你的应用系统本身就维护了一个版本号，无论是什么自己生成的，程序控制的。这个时候，你进行乐观锁并发控制的时候，可能并不是想要用es内部的_version来进行控制，而是用你自己维护的那个version来进行控制。\r\n\r\n?version=1\r\n?version=1&amp;version_type=external\r\n\r\nversion_type=external，唯一的区别在于，_version，只有当你提供的version与es中的_version一模一样的时候，才可以进行修改，只要不一样，就报错；当version_type=external的时候，只有当你提供的version比es中的_version大的时候，才能完成修改\r\n\r\nes，_version=1，?version=1，才能更新成功\r\nes，_version=1，?version&gt;1&amp;version_type=external，才能成功，比如说?version=2&amp;version_type=external\r\n\r\n（1）先构造一条数据\r\n\r\nPUT /test_index/test_type/8\r\n{\r\n  &quot;test_field&quot;: &quot;test&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n（2）模拟两个客户端同时查询到这条数据\r\n\r\nGET /test_index/test_type/8\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field&quot;: &quot;test&quot;\r\n  }\r\n}\r\n\r\n（3）第一个客户端先进行修改，此时客户端程序是在自己的数据库中获取到了这条数据的最新版本号，比如说是2\r\n\r\nPUT /test_index/test_type/8?version=2&amp;version_type=external\r\n{\r\n  &quot;test_field&quot;: &quot;test client 1&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}\r\n\r\n（4）模拟第二个客户端，同时拿到了自己数据库中维护的那个版本号，也是2，同时基于version=2发起了修改\r\n\r\nPUT /test_index/test_type/8?version=2&amp;version_type=external\r\n{\r\n  &quot;test_field&quot;: &quot;test client 2&quot;\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[test_type][8]: version conflict, current version [2] is higher or equal to the one provided [2]&quot;,\r\n        &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n        &quot;shard&quot;: &quot;1&quot;,\r\n        &quot;index&quot;: &quot;test_index&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[test_type][8]: version conflict, current version [2] is higher or equal to the one provided [2]&quot;,\r\n    &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n    &quot;shard&quot;: &quot;1&quot;,\r\n    &quot;index&quot;: &quot;test_index&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n（5）在并发控制成功后，重新基于最新的版本号发起更新\r\n\r\nGET /test_index/test_type/8\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field&quot;: &quot;test client 1&quot;\r\n  }\r\n}\r\n\r\nPUT /test_index/test_type/8?version=3&amp;version_type=external\r\n{\r\n  &quot;test_field&quot;: &quot;test client 2&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}',8,0,0,1514607647,0,0,0),(41,1,'什么是partial update','','','1、什么是partial update？\r\n\r\nPUT /index/type/id，创建文档&amp;替换文档，就是一样的语法\r\n\r\n一般对应到应用程序中，每次的执行流程基本是这样的：\r\n\r\n（1）应用程序先发起一个get请求，获取到document，展示到前台界面，供用户查看和修改\r\n（2）用户在前台界面修改数据，发送到后台\r\n（3）后台代码，会将用户修改的数据在内存中进行执行，然后封装好修改后的全量数据\r\n（4）然后发送PUT请求，到es中，进行全量替换\r\n（5）es将老的document标记为deleted，然后重新创建一个新的document\r\n\r\npartial update\r\n\r\npost /index/type/id/_update \r\n{\r\n   &quot;doc&quot;: {\r\n      &quot;要修改的少数几个field即可，不需要全量的数据&quot;\r\n   }\r\n}\r\n\r\n看起来，好像就比较方便了，每次就传递少数几个发生修改的field即可，不需要将全量的document数据发送过去\r\n\r\n2、图解partial update实现原理以及其优点\r\n\r\npartial update，看起来很方便的操作，实际内部的原理是什么样子的，然后它的优点是什么\r\n\r\n3、上机动手实战演练partial update\r\n\r\nPUT /test_index/test_type/10\r\n{\r\n  &quot;test_field1&quot;: &quot;test1&quot;,\r\n  &quot;test_field2&quot;: &quot;test2&quot;\r\n}\r\n\r\nPOST /test_index/test_type/10/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;test_field2&quot;: &quot;updated test2&quot;\r\n  }\r\n}',8,0,0,1514607714,0,0,0),(42,1,'groovy脚本','','','es，其实是有个内置的脚本支持的，可以基于groovy脚本实现各种各样的复杂操作\r\n基于groovy脚本，如何执行partial update\r\nes scripting module，我们会在高手进阶篇去讲解，这里就只是初步讲解一下\r\n\r\nPUT /test_index/test_type/11\r\n{\r\n  &quot;num&quot;: 0,\r\n  &quot;tags&quot;: []\r\n}\r\n\r\n（1）内置脚本\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n   &quot;script&quot; : &quot;ctx._source.num+=1&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;11&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;num&quot;: 1,\r\n    &quot;tags&quot;: []\r\n  }\r\n}\r\n\r\n（2）外部脚本 (config/scripts)\r\n\r\nctx._source.tags+=new_tag\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;, \r\n    &quot;file&quot;: &quot;test-add-tags&quot;,\r\n    &quot;params&quot;: {\r\n      &quot;new_tag&quot;: &quot;tag1&quot;\r\n    }\r\n  }\r\n}\r\n\r\n（3）用脚本删除文档\r\n\r\nctx.op = ctx._source.num == count ? &#039;delete&#039; : &#039;none&#039;\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;test-delete-document&quot;,\r\n    &quot;params&quot;: {\r\n      &quot;count&quot;: 1\r\n    }\r\n  }\r\n}\r\n\r\n（4）upsert操作\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;num&quot;: 1\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;document_missing_exception&quot;,\r\n        &quot;reason&quot;: &quot;[test_type][11]: document missing&quot;,\r\n        &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n        &quot;shard&quot;: &quot;4&quot;,\r\n        &quot;index&quot;: &quot;test_index&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;document_missing_exception&quot;,\r\n    &quot;reason&quot;: &quot;[test_type][11]: document missing&quot;,\r\n    &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n    &quot;shard&quot;: &quot;4&quot;,\r\n    &quot;index&quot;: &quot;test_index&quot;\r\n  },\r\n  &quot;status&quot;: 404\r\n}\r\n\r\n如果指定的document不存在，就执行upsert中的初始化操作；如果指定的document存在，就执行doc或者script指定的partial update操作\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n   &quot;script&quot; : &quot;ctx._source.num+=1&quot;,\r\n   &quot;upsert&quot;: {\r\n       &quot;num&quot;: 0,\r\n       &quot;tags&quot;: []\r\n   }\r\n}',8,0,0,1514607816,0,0,0),(43,1,'partial update','','','（1）partial update内置乐观锁并发控制\r\n（2）retry_on_conflict\r\n（3）_version\r\n\r\npost /index/type/id/_update?retry_on_conflict=5&amp;version=6',8,0,0,1514607877,0,0,0),(44,1,'批量查询mget','','','1、批量查询的好处\r\n\r\n就是一条一条的查询，比如说要查询100条数据，那么就要发送100次网络请求，这个开销还是很大的\r\n如果进行批量查询的话，查询100条数据，就只要发送1次网络请求，网络请求的性能开销缩减100倍\r\n\r\n2、mget的语法\r\n\r\n（1）一条一条的查询\r\n\r\nGET /test_index/test_type/1\r\nGET /test_index/test_type/2\r\n\r\n（2）mget批量查询\r\n\r\nGET /_mget\r\n{\r\n   &quot;docs&quot; : [\r\n      {\r\n         &quot;_index&quot; : &quot;test_index&quot;,\r\n         &quot;_type&quot; :  &quot;test_type&quot;,\r\n         &quot;_id&quot; :    1\r\n      },\r\n      {\r\n         &quot;_index&quot; : &quot;test_index&quot;,\r\n         &quot;_type&quot; :  &quot;test_type&quot;,\r\n         &quot;_id&quot; :    2\r\n      }\r\n   ]\r\n}\r\n\r\n{\r\n  &quot;docs&quot;: [\r\n    {\r\n      &quot;_index&quot;: &quot;test_index&quot;,\r\n      &quot;_type&quot;: &quot;test_type&quot;,\r\n      &quot;_id&quot;: &quot;1&quot;,\r\n      &quot;_version&quot;: 2,\r\n      &quot;found&quot;: true,\r\n      &quot;_source&quot;: {\r\n        &quot;test_field1&quot;: &quot;test field1&quot;,\r\n        &quot;test_field2&quot;: &quot;test field2&quot;\r\n      }\r\n    },\r\n    {\r\n      &quot;_index&quot;: &quot;test_index&quot;,\r\n      &quot;_type&quot;: &quot;test_type&quot;,\r\n      &quot;_id&quot;: &quot;2&quot;,\r\n      &quot;_version&quot;: 1,\r\n      &quot;found&quot;: true,\r\n      &quot;_source&quot;: {\r\n        &quot;test_content&quot;: &quot;my test&quot;\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\n（3）如果查询的document是一个index下的不同type种的话\r\n\r\nGET /test_index/_mget\r\n{\r\n   &quot;docs&quot; : [\r\n      {\r\n         &quot;_type&quot; :  &quot;test_type&quot;,\r\n         &quot;_id&quot; :    1\r\n      },\r\n      {\r\n         &quot;_type&quot; :  &quot;test_type&quot;,\r\n         &quot;_id&quot; :    2\r\n      }\r\n   ]\r\n}\r\n\r\n（4）如果查询的数据都在同一个index下的同一个type下，最简单了\r\n\r\nGET /test_index/test_type/_mget\r\n{\r\n   &quot;ids&quot;: [1, 2]\r\n}\r\n\r\n3、mget的重要性\r\n\r\n可以说mget是很重要的，一般来说，在进行查询的时候，如果一次性要查询多条数据的话，那么一定要用batch批量操作的api\r\n尽可能减少网络开销次数，可能可以将性能提升数倍，甚至数十倍，非常非常之重要\r\n',8,0,0,1514607958,0,0,0),(45,1,'bulk语法','','','1、bulk语法\r\n\r\nPOST /_bulk\r\n{ &quot;delete&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;3&quot; }} \r\n{ &quot;create&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;12&quot; }}\r\n{ &quot;test_field&quot;:    &quot;test12&quot; }\r\n{ &quot;index&quot;:  { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;test_field&quot;:    &quot;replaced test2&quot; }\r\n{ &quot;update&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_retry_on_conflict&quot; : 3} }\r\n{ &quot;doc&quot; : {&quot;test_field2&quot; : &quot;bulk test1&quot;} }\r\n\r\n每一个操作要两个json串，语法如下：\r\n\r\n{&quot;action&quot;: {&quot;metadata&quot;}}\r\n{&quot;data&quot;}\r\n\r\n举例，比如你现在要创建一个文档，放bulk里面，看起来会是这样子的：\r\n\r\n{&quot;index&quot;: {&quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;, &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;}}\r\n{&quot;test_field1&quot;: &quot;test1&quot;, &quot;test_field2&quot;: &quot;test2&quot;}\r\n\r\n有哪些类型的操作可以执行呢？\r\n（1）delete：删除一个文档，只要1个json串就可以了\r\n（2）create：PUT /index/type/id/_create，强制创建\r\n（3）index：普通的put操作，可以是创建文档，也可以是全量替换文档\r\n（4）update：执行的partial update操作\r\n\r\nbulk api对json的语法，有严格的要求，每个json串不能换行，只能放一行，同时一个json串和一个json串之间，必须有一个换行\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;json_e_o_f_exception&quot;,\r\n        &quot;reason&quot;: &quot;Unexpected end-of-input: expected close marker for Object (start marker at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@5a5932cd; line: 1, column: 1])\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@5a5932cd; line: 1, column: 3]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;json_e_o_f_exception&quot;,\r\n    &quot;reason&quot;: &quot;Unexpected end-of-input: expected close marker for Object (start marker at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@5a5932cd; line: 1, column: 1])\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@5a5932cd; line: 1, column: 3]&quot;\r\n  },\r\n  &quot;status&quot;: 500\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 41,\r\n  &quot;errors&quot;: true,\r\n  &quot;items&quot;: [\r\n    {\r\n      &quot;delete&quot;: {\r\n        &quot;found&quot;: true,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;10&quot;,\r\n        &quot;_version&quot;: 3,\r\n        &quot;result&quot;: &quot;deleted&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;status&quot;: 200\r\n      }\r\n    },\r\n    {\r\n      &quot;create&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_version&quot;: 1,\r\n        &quot;result&quot;: &quot;created&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;created&quot;: true,\r\n        &quot;status&quot;: 201\r\n      }\r\n    },\r\n    {\r\n      &quot;create&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;status&quot;: 409,\r\n        &quot;error&quot;: {\r\n          &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n          &quot;reason&quot;: &quot;[test_type][2]: version conflict, document already exists (current version [1])&quot;,\r\n          &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n          &quot;shard&quot;: &quot;2&quot;,\r\n          &quot;index&quot;: &quot;test_index&quot;\r\n        }\r\n      }\r\n    },\r\n    {\r\n      &quot;index&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_version&quot;: 1,\r\n        &quot;result&quot;: &quot;created&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;created&quot;: true,\r\n        &quot;status&quot;: 201\r\n      }\r\n    },\r\n    {\r\n      &quot;index&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_version&quot;: 2,\r\n        &quot;result&quot;: &quot;updated&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;created&quot;: false,\r\n        &quot;status&quot;: 200\r\n      }\r\n    },\r\n    {\r\n      &quot;update&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_version&quot;: 3,\r\n        &quot;result&quot;: &quot;updated&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;status&quot;: 200\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\nbulk操作中，任意一个操作失败，是不会影响其他的操作的，但是在返回结果里，会告诉你异常日志\r\n\r\nPOST /test_index/_bulk\r\n{ &quot;delete&quot;: { &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;3&quot; }} \r\n{ &quot;create&quot;: { &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;12&quot; }}\r\n{ &quot;test_field&quot;:    &quot;test12&quot; }\r\n{ &quot;index&quot;:  { &quot;_type&quot;: &quot;test_type&quot; }}\r\n{ &quot;test_field&quot;:    &quot;auto-generate id test&quot; }\r\n{ &quot;index&quot;:  { &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;test_field&quot;:    &quot;replaced test2&quot; }\r\n{ &quot;update&quot;: { &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_retry_on_conflict&quot; : 3} }\r\n{ &quot;doc&quot; : {&quot;test_field2&quot; : &quot;bulk test1&quot;} }\r\n\r\nPOST /test_index/test_type/_bulk\r\n{ &quot;delete&quot;: { &quot;_id&quot;: &quot;3&quot; }} \r\n{ &quot;create&quot;: { &quot;_id&quot;: &quot;12&quot; }}\r\n{ &quot;test_field&quot;:    &quot;test12&quot; }\r\n{ &quot;index&quot;:  { }}\r\n{ &quot;test_field&quot;:    &quot;auto-generate id test&quot; }\r\n{ &quot;index&quot;:  { &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;test_field&quot;:    &quot;replaced test2&quot; }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;, &quot;_retry_on_conflict&quot; : 3} }\r\n{ &quot;doc&quot; : {&quot;test_field2&quot; : &quot;bulk test1&quot;} }\r\n\r\n2、bulk size最佳大小\r\n\r\nbulk request会加载到内存里，如果太大的话，性能反而会下降，因此需要反复尝试一个最佳的bulk size。一般从1000~5000条数据开始，尝试逐渐增加。另外，如果看大小的话，最好是在5~15MB之间。\r\n',8,0,0,1514608013,0,0,0),(46,1,'distributed document store','','','到目前为止，你觉得你在学什么东西，给大家一个直观的感觉，好像已经知道了es是分布式的，包括一些基本的原理，然后花了不少时间在学习document本身相关的操作，增删改查。一句话点出来，给大家归纳总结一下，其实我们应该思考一下，es的一个最最核心的功能，已经被我们相对完整的讲完了。\r\n\r\nElasticsearch在跑起来以后，其实起到的第一个最核心的功能，就是一个分布式的文档数据存储系统。ES是分布式的。文档数据存储系统。文档数据，存储系统。\r\n文档数据：es可以存储和操作json文档类型的数据，而且这也是es的核心数据结构。\r\n存储系统：es可以对json文档类型的数据进行存储，查询，创建，更新，删除，等等操作。其实已经起到了一个什么样的效果呢？其实ES满足了这些功能，就可以说已经是一个NoSQL的存储系统了。\r\n\r\n围绕着document在操作，其实就是把es当成了一个NoSQL存储引擎，一个可以存储文档类型数据的存储系统，在操作里面的document。\r\n\r\nes可以作为一个分布式的文档存储系统，所以说，我们的应用系统，是不是就可以基于这个概念，去进行相关的应用程序的开发了。\r\n\r\n什么类型的应用程序呢？\r\n\r\n（1）数据量较大，es的分布式本质，可以帮助你快速进行扩容，承载大量数据\r\n（2）数据结构灵活多变，随时可能会变化，而且数据结构之间的关系，非常复杂，如果我们用传统数据库，那是不是很坑，因为要面临大量的表\r\n（3）对数据的相关操作，较为简单，比如就是一些简单的增删改查，用我们之前讲解的那些document操作就可以搞定\r\n（4）NoSQL数据库，适用的也是类似于上面的这种场景\r\n\r\n举个例子，比如说像一些网站系统，或者是普通的电商系统，博客系统，面向对象概念比较复杂，但是作为终端网站来说，没什么太复杂的功能，就是一些简单的CRUD操作，而且数据量可能还比较大。这个时候选用ES这种NoSQL型的数据存储，比传统的复杂的功能务必强大的支持SQL的关系型数据库，更加合适一些。无论是性能，还是吞吐量，可能都会更好。\r\n',8,0,0,1514608228,0,0,0),(47,1,'路由','','','（1）document路由到shard上是什么意思？\r\n\r\n（2）路由算法：shard = hash(routing) % number_of_primary_shards\r\n\r\n举个例子，一个index有3个primary shard，P0，P1，P2\r\n\r\n每次增删改查一个document的时候，都会带过来一个routing number，默认就是这个document的_id（可能是手动指定，也可能是自动生成）\r\nrouting = _id，假设_id=1\r\n\r\n会将这个routing值，传入一个hash函数中，产出一个routing值的hash值，hash(routing) = 21\r\n然后将hash函数产出的值对这个index的primary shard的数量求余数，21 % 3 = 0\r\n就决定了，这个document就放在P0上。\r\n\r\n决定一个document在哪个shard上，最重要的一个值就是routing值，默认是_id，也可以手动指定，相同的routing值，每次过来，从hash函数中，产出的hash值一定是相同的\r\n\r\n无论hash值是几，无论是什么数字，对number_of_primary_shards求余数，结果一定是在0~number_of_primary_shards-1之间这个范围内的。0,1,2。\r\n\r\n（3）_id or custom routing value\r\n\r\n默认的routing就是_id\r\n也可以在发送请求的时候，手动指定一个routing value，比如说put /index/type/id?routing=user_id\r\n\r\n手动指定routing value是很有用的，可以保证说，某一类document一定被路由到一个shard上去，那么在后续进行应用级别的负载均衡，以及提升批量读取的性能的时候，是很有帮助的\r\n\r\n（4）primary shard数量不可变的谜底',8,0,0,1514608295,0,0,0),(48,1,'node','','','（1）客户端选择一个node发送请求过去，这个node就是coordinating node（协调节点）\r\n（2）coordinating node，对document进行路由，将请求转发给对应的node（有primary shard）\r\n（3）实际的node上的primary shard处理请求，然后将数据同步到replica node\r\n（4）coordinating node，如果发现primary node和所有replica node都搞定之后，就返回响应结果给客户端\r\n',8,0,0,1514608345,0,0,0),(49,1,'quorum','','','（1）consistency，one（primary shard），all（all shard），quorum（default）\r\n\r\n我们在发送任何一个增删改操作的时候，比如说put /index/type/id，都可以带上一个consistency参数，指明我们想要的写一致性是什么？\r\nput /index/type/id?consistency=quorum\r\n\r\none：要求我们这个写操作，只要有一个primary shard是active活跃可用的，就可以执行\r\nall：要求我们这个写操作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作\r\nquorum：默认的值，要求所有的shard中，必须是大部分的shard都是活跃的，可用的，才可以执行这个写操作\r\n\r\n（2）quorum机制，写之前必须确保大多数shard都可用，int( (primary + number_of_replicas) / 2 ) + 1，当number_of_replicas&gt;1时才生效\r\n\r\nquroum = int( (primary + number_of_replicas) / 2 ) + 1\r\n举个例子，3个primary shard，number_of_replicas=1，总共有3 + 3 * 1 = 6个shard\r\nquorum = int( (3 + 1) / 2 ) + 1 = 3\r\n所以，要求6个shard中至少有3个shard是active状态的，才可以执行这个写操作\r\n\r\n（3）如果节点数少于quorum数量，可能导致quorum不齐全，进而导致无法执行任何写操作\r\n\r\n3个primary shard，replica=1，要求至少3个shard是active，3个shard按照之前学习的shard&amp;replica机制，必须在不同的节点上，如果说只有2台机器的话，是不是有可能出现说，3个shard都没法分配齐全，此时就可能会出现写操作无法执行的情况\r\n\r\nes提供了一种特殊的处理场景，就是说当number_of_replicas&gt;1时才生效，因为假如说，你就一个primary shard，replica=1，此时就2个shard\r\n\r\n(1 + 1 / 2) + 1 = 2，要求必须有2个shard是活跃的，但是可能就1个node，此时就1个shard是活跃的，如果你不特殊处理的话，导致我们的单节点集群就无法工作\r\n\r\n（4）quorum不齐全时，wait，默认1分钟，timeout，100，30s\r\n\r\n等待期间，期望活跃的shard数量可以增加，最后实在不行，就会timeout\r\n我们其实可以在写操作的时候，加一个timeout参数，比如说put /index/type/id?timeout=30，这个就是说自己去设定quorum不齐全的时候，es的timeout时长，可以缩短，也可以增长\r\n\r\n',8,0,0,1514608428,0,0,0),(50,1,'coordinate node','','','1、客户端发送请求到任意一个node，成为coordinate node\r\n2、coordinate node对document进行路由，将请求转发到对应的node，此时会使用round-robin随机轮询算法，在primary shard以及其所有replica中随机选择一个，让读请求负载均衡\r\n3、接收请求的node返回document给coordinate node\r\n4、coordinate node返回document给客户端\r\n5、特殊情况：document如果还在建立索引过程中，可能只有primary shard有，任何一个replica shard都没有，此时可能会导致无法读取到document，但是document完成索引建立之后，primary shard和replica shard就都有了\r\n',8,0,0,1514608562,0,0,0),(51,1,'bulk api','','','bulk api奇特的json格式\r\n\r\n{&quot;action&quot;: {&quot;meta&quot;}}\\n\r\n{&quot;data&quot;}\\n\r\n{&quot;action&quot;: {&quot;meta&quot;}}\\n\r\n{&quot;data&quot;}\\n\r\n\r\n[{\r\n  &quot;action&quot;: {\r\n \r\n  },\r\n  &quot;data&quot;: {\r\n\r\n  }\r\n}]\r\n\r\n1、bulk中的每个操作都可能要转发到不同的node的shard去执行\r\n\r\n2、如果采用比较良好的json数组格式\r\n\r\n允许任意的换行，整个可读性非常棒，读起来很爽，es拿到那种标准格式的json串以后，要按照下述流程去进行处理\r\n\r\n（1）将json数组解析为JSONArray对象，这个时候，整个数据，就会在内存中出现一份一模一样的拷贝，一份数据是json文本，一份数据是JSONArray对象\r\n（2）解析json数组里的每个json，对每个请求中的document进行路由\r\n（3）为路由到同一个shard上的多个请求，创建一个请求数组\r\n（4）将这个请求数组序列化\r\n（5）将序列化后的请求数组发送到对应的节点上去\r\n\r\n3、耗费更多内存，更多的jvm gc开销\r\n\r\n我们之前提到过bulk size最佳大小的那个问题，一般建议说在几千条那样，然后大小在10MB左右，所以说，可怕的事情来了。假设说现在100个bulk请求发送到了一个节点上去，然后每个请求是10MB，100个请求，就是1000MB = 1GB，然后每个请求的json都copy一份为jsonarray对象，此时内存中的占用就会翻倍，就会占用2GB的内存，甚至还不止。因为弄成jsonarray之后，还可能会多搞一些其他的数据结构，2GB+的内存占用。\r\n\r\n占用更多的内存可能就会积压其他请求的内存使用量，比如说最重要的搜索请求，分析请求，等等，此时就可能会导致其他请求的性能急速下降\r\n另外的话，占用内存更多，就会导致java虚拟机的垃圾回收次数更多，跟频繁，每次要回收的垃圾对象更多，耗费的时间更多，导致es的java虚拟机停止工作线程的时间更多\r\n\r\n4、现在的奇特格式\r\n\r\n{&quot;action&quot;: {&quot;meta&quot;}}\\n\r\n{&quot;data&quot;}\\n\r\n{&quot;action&quot;: {&quot;meta&quot;}}\\n\r\n{&quot;data&quot;}\\n\r\n\r\n（1）不用将其转换为json对象，不会出现内存中的相同数据的拷贝，直接按照换行符切割json\r\n（2）对每两个一组的json，读取meta，进行document路由\r\n（3）直接将对应的json发送到node上去\r\n\r\n5、最大的优势在于，不需要将json数组解析为一个JSONArray对象，形成一份大数据的拷贝，浪费内存空间，尽可能地保证性能\r\n',8,0,0,1514608624,0,0,0),(52,1,'took','','','1、我们如果发出一个搜索请求的话，会拿到一堆搜索结果，本节课，我们来讲解一下，这个搜索结果里的各种数据，都代表了什么含义\r\n2、我们来讲解一下，搜索的timeout机制，底层的原理，画图讲解\r\n\r\nGET /_search\r\n\r\n{\r\n  &quot;took&quot;: 6,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 6,\r\n    &quot;successful&quot;: 6,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 10,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;.kibana&quot;,\r\n        &quot;_type&quot;: &quot;config&quot;,\r\n        &quot;_id&quot;: &quot;5.2.0&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;buildNum&quot;: 14695\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\ntook：整个搜索请求花费了多少毫秒\r\n\r\nhits.total：本次搜索，返回了几条结果\r\nhits.max_score：本次搜索的所有结果中，最大的相关度分数是多少，每一条document对于search的相关度，越相关，_score分数越大，排位越靠前\r\nhits.hits：默认查询前10条数据，完整数据，_score降序排序\r\n\r\nshards：shards fail的条件（primary和replica全部挂掉），不影响其他shard。默认情况下来说，一个搜索请求，会打到一个index的所有primary shard上去，当然了，每个primary shard都可能会有一个或多个replic shard，所以请求也可以到primary shard的其中一个replica shard上去。\r\n\r\ntimeout：默认无timeout，latency平衡completeness，手动指定timeout，timeout查询执行机制\r\n\r\ntimeout=10ms，timeout=1s，timeout=1m\r\nGET /_search?timeout=10m\r\n',8,0,0,1514608672,0,0,0),(53,1,'multi','','','1、multi-index和multi-type搜索模式\r\n\r\n告诉你如何一次性搜索多个index和多个type下的数据\r\n\r\n/_search：所有索引，所有type下的所有数据都搜索出来\r\n/index1/_search：指定一个index，搜索其下所有type的数据\r\n/index1,index2/_search：同时搜索两个index下的数据\r\n/*1,*2/_search：按照通配符去匹配多个索引\r\n/index1/type1/_search：搜索一个index下指定的type的数据\r\n/index1/type1,type2/_search：可以搜索一个index下多个type的数据\r\n/index1,index2/type1,type2/_search：搜索多个index下的多个type的数据\r\n/_all/type1,type2/_search：_all，可以代表搜索所有index下的指定type的数据\r\n\r\n2、初步图解一下简单的搜索原理\r\n\r\n搜索原理初步图解',8,0,0,1514608732,0,0,0),(54,1,'分页搜索','','','1、讲解如何使用es进行分页搜索的语法\r\n\r\nsize，from\r\n\r\nGET /_search?size=10\r\nGET /_search?size=10&amp;from=0\r\nGET /_search?size=10&amp;from=20\r\n\r\n分页的上机实验\r\n\r\nGET /test_index/test_type/_search\r\n\r\n&quot;hits&quot;: {\r\n    &quot;total&quot;: 9,\r\n    &quot;max_score&quot;: 1,\r\n\r\n我们假设将这9条数据分成3页，每一页是3条数据，来实验一下这个分页搜索的效果\r\n\r\nGET /test_index/test_type/_search?from=0&amp;size=3\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 9,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;8&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test client 2&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;6&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;tes test&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test4&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n第一页：id=8,6,4\r\n\r\nGET /test_index/test_type/_search?from=3&amp;size=3\r\n\r\n第二页：id=2,自动生成,7\r\n\r\nGET /test_index/test_type/_search?from=6&amp;size=3\r\n\r\n第三页：id=1,11,3\r\n\r\n2、什么是deep paging问题？为什么会产生这个问题，它的底层原理是什么？\r\n\r\ndeep paging性能问题，以及原理深度图解揭秘，很高级的知识点\r\n',8,0,0,1514608838,0,0,0),(55,1,'query string','','','1、query string基础语法\r\n\r\nGET /test_index/test_type/_search?q=test_field:test\r\nGET /test_index/test_type/_search?q=+test_field:test\r\nGET /test_index/test_type/_search?q=-test_field:test\r\n\r\n一个是掌握q=field:search content的语法，还有一个是掌握+和-的含义\r\n\r\n2、_all metadata的原理和作用\r\n\r\nGET /test_index/test_type/_search?q=test\r\n\r\n直接可以搜索所有的field，任意一个field包含指定的关键字就可以搜索出来。我们在进行中搜索的时候，难道是对document中的每一个field都进行一次搜索吗？不是的\r\n\r\nes中的_all元数据，在建立索引的时候，我们插入一条document，它里面包含了多个field，此时，es会自动将多个field的值，全部用字符串的方式串联起来，变成一个长的字符串，作为_all field的值，同时建立索引\r\n\r\n后面如果在搜索的时候，没有对某个field指定搜索，就默认搜索_all field，其中是包含了所有field的值的\r\n\r\n举个例子\r\n\r\n{\r\n  &quot;name&quot;: &quot;jack&quot;,\r\n  &quot;age&quot;: 26,\r\n  &quot;email&quot;: &quot;jack@sina.com&quot;,\r\n  &quot;address&quot;: &quot;guamgzhou&quot;\r\n}\r\n\r\n&quot;jack 26 jack@sina.com guangzhou&quot;，作为这一条document的_all field的值，同时进行分词后建立对应的倒排索引\r\n\r\n生产环境不使用',8,0,0,1514608884,0,0,0),(56,1,'mapping','','','插入几条数据，让es自动为我们建立一个索引\r\n\r\nPUT /website/article/1\r\n{\r\n  &quot;post_date&quot;: &quot;2017-01-01&quot;,\r\n  &quot;title&quot;: &quot;my first article&quot;,\r\n  &quot;content&quot;: &quot;this is my first article in this website&quot;,\r\n  &quot;author_id&quot;: 11400\r\n}\r\n\r\nPUT /website/article/2\r\n{\r\n  &quot;post_date&quot;: &quot;2017-01-02&quot;,\r\n  &quot;title&quot;: &quot;my second article&quot;,\r\n  &quot;content&quot;: &quot;this is my second article in this website&quot;,\r\n  &quot;author_id&quot;: 11400\r\n}\r\n\r\nPUT /website/article/3\r\n{\r\n  &quot;post_date&quot;: &quot;2017-01-03&quot;,\r\n  &quot;title&quot;: &quot;my third article&quot;,\r\n  &quot;content&quot;: &quot;this is my third article in this website&quot;,\r\n  &quot;author_id&quot;: 11400\r\n}\r\n\r\n尝试各种搜索\r\n\r\nGET /website/article/_search?q=2017			3条结果             \r\nGET /website/article/_search?q=2017-01-01        	3条结果\r\nGET /website/article/_search?q=post_date:2017-01-01   	1条结果\r\nGET /website/article/_search?q=post_date:2017         	1条结果\r\n\r\n查看es自动建立的mapping，带出什么是mapping的知识点\r\n自动或手动为index中的type建立的一种数据结构和相关配置，简称为mapping\r\ndynamic mapping，自动为我们建立index，创建type，以及type对应的mapping，mapping中包含了每个field对应的数据类型，以及如何分词等设置\r\n我们当然，后面会讲解，也可以手动在创建数据之前，先创建index和type，以及type对应的mapping\r\n\r\nGET /website/_mapping/article\r\n\r\n{\r\n  &quot;website&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;article&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;author_id&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          },\r\n          &quot;content&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          },\r\n          &quot;post_date&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          },\r\n          &quot;title&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n搜索结果为什么不一致，因为es自动建立mapping的时候，设置了不同的field不同的data type。不同的data type的分词、搜索等行为是不一样的。所以出现了_all field和post_date field的搜索表现完全不一样。\r\n',8,0,0,1514608990,0,0,0),(57,1,'exact value','','','1、exact value\r\n\r\n2017-01-01，exact value，搜索的时候，必须输入2017-01-01，才能搜索出来\r\n如果你输入一个01，是搜索不出来的\r\n\r\n2、full text\r\n（1）缩写 vs. 全程：cn vs. china\r\n（2）格式转化：like liked likes\r\n（3）大小写：Tom vs tom\r\n（4）同义词：like vs love\r\n\r\n2017-01-01，2017 01 01，搜索2017，或者01，都可以搜索出来\r\nchina，搜索cn，也可以将china搜索出来\r\nlikes，搜索like，也可以将likes搜索出来\r\nTom，搜索tom，也可以将Tom搜索出来\r\nlike，搜索love，同义词，也可以将like搜索出来\r\n\r\n就不是说单纯的只是匹配完整的一个值，而是可以对值进行拆分词语后（分词）进行匹配，也可以通过缩写、时态、大小写、同义词等进行匹配',8,0,0,1514609054,0,0,0),(58,1,'分词初识','','','doc1：I really liked my small dogs, and I think my mom also liked them.\r\ndoc2：He never liked any dogs, so I hope that my mom will not expect me to liked him.\r\n\r\n分词，初步的倒排索引的建立\r\n\r\nword		doc1			doc2\r\n\r\nI		*			*\r\nreally		*\r\nliked		*			*\r\nmy		*			*\r\nsmall		*	\r\ndogs		*\r\nand		*\r\nthink		*\r\nmom		*			*\r\nalso		*\r\nthem		*	\r\nHe					*\r\nnever					*\r\nany					*\r\nso					*\r\nhope					*\r\nthat					*\r\nwill					*\r\nnot					*\r\nexpect					*\r\nme					*\r\nto					*\r\nhim					*\r\n\r\n演示了一下倒排索引最简单的建立的一个过程\r\n\r\n搜索\r\n\r\nmother like little dog，不可能有任何结果\r\n\r\nmother\r\nlike\r\nlittle\r\ndog\r\n\r\n这个是不是我们想要的搜索结果？？？绝对不是，因为在我们看来，mother和mom有区别吗？同义词，都是妈妈的意思。like和liked有区别吗？没有，都是喜欢的意思，只不过一个是现在时，一个是过去时。little和small有区别吗？同义词，都是小小的。dog和dogs有区别吗？狗，只不过一个是单数，一个是复数。\r\n\r\nnormalization，建立倒排索引的时候，会执行一个操作，也就是说对拆分出的各个单词进行相应的处理，以提升后面搜索的时候能够搜索到相关联的文档的概率\r\n\r\n时态的转换，单复数的转换，同义词的转换，大小写的转换\r\n\r\nmom —&gt; mother\r\nliked —&gt; like\r\nsmall —&gt; little\r\ndogs —&gt; dog\r\n\r\n重新建立倒排索引，加入normalization，再次用mother liked little dog搜索，就可以搜索到了\r\n\r\nword		doc1			doc2\r\n\r\nI		*			*\r\nreally		*\r\nlike		*			*			liked --&gt; like\r\nmy		*			*\r\nlittle		*						small --&gt; little\r\ndog		*			*			dogs --&gt; dog						\r\nand		*\r\nthink		*\r\nmom		*			*\r\nalso		*\r\nthem		*	\r\nHe					*\r\nnever					*\r\nany					*\r\nso					*\r\nhope					*\r\nthat					*\r\nwill					*\r\nnot					*\r\nexpect					*\r\nme					*\r\nto					*\r\nhim					*\r\n\r\nmother like little dog，分词，normalization\r\n\r\nmother	--&gt; mom\r\nlike	--&gt; like\r\nlittle	--&gt; little\r\ndog	--&gt; dog\r\n\r\ndoc1和doc2都会搜索出来\r\n\r\ndoc1：I really liked my small dogs, and I think my mom also liked them.\r\ndoc2：He never liked any dogs, so I hope that my mom will not expect me to liked him.\r\n',8,0,0,1514609108,0,0,0),(59,1,'分词器','','','1、什么是分词器\r\n\r\n切分词语，normalization（提升recall召回率）\r\n\r\n给你一段句子，然后将这段句子拆分成一个一个的单个的单词，同时对每个单词进行normalization（时态转换，单复数转换），分瓷器\r\nrecall，召回率：搜索的时候，增加能够搜索到的结果的数量\r\n\r\ncharacter filter：在一段文本进行分词之前，先进行预处理，比如说最常见的就是，过滤html标签（&lt;span&gt;hello&lt;span&gt; --&gt; hello），&amp; --&gt; and（I&amp;you --&gt; I and you）\r\ntokenizer：分词，hello you and me --&gt; hello, you, and, me\r\ntoken filter：lowercase，stop word，synonymom，dogs --&gt; dog，liked --&gt; like，Tom --&gt; tom，a/the/an --&gt; 干掉，mother --&gt; mom，small --&gt; little\r\n\r\n一个分词器，很重要，将一段文本进行各种处理，最后处理好的结果才会拿去建立倒排索引\r\n\r\n2、内置分词器的介绍\r\n\r\nSet the shape to semi-transparent by calling set_trans(5)\r\n\r\nstandard analyzer：set, the, shape, to, semi, transparent, by, calling, set_trans, 5（默认的是standard）\r\nsimple analyzer：set, the, shape, to, semi, transparent, by, calling, set, trans\r\nwhitespace analyzer：Set, the, shape, to, semi-transparent, by, calling, set_trans(5)\r\nlanguage analyzer（特定的语言的分词器，比如说，english，英语分词器）：set, shape, semi, transpar, call, set_tran, 5\r\n',8,0,0,1514609194,0,0,0),(60,1,'query string分词','','','1、query string分词\r\n\r\nquery string必须以和index建立时相同的analyzer进行分词\r\nquery string对exact value和full text的区别对待\r\n\r\ndate：exact value\r\n_all：full text\r\n\r\n比如我们有一个document，其中有一个field，包含的value是：hello you and me，建立倒排索引\r\n我们要搜索这个document对应的index，搜索文本是hell me，这个搜索文本就是query string\r\nquery string，默认情况下，es会使用它对应的field建立倒排索引时相同的分词器去进行分词，分词和normalization，只有这样，才能实现正确的搜索\r\n\r\n我们建立倒排索引的时候，将dogs --&gt; dog，结果你搜索的时候，还是一个dogs，那不就搜索不到了吗？所以搜索的时候，那个dogs也必须变成dog才行。才能搜索到。\r\n\r\n知识点：不同类型的field，可能有的就是full text，有的就是exact value\r\n\r\npost_date，date：exact value\r\n_all：full text，分词，normalization\r\n\r\n2、mapping引入案例遗留问题大揭秘\r\n\r\nGET /_search?q=2017\r\n\r\n搜索的是_all field，document所有的field都会拼接成一个大串，进行分词\r\n\r\n2017-01-02 my second article this is my second article in this website 11400\r\n\r\n		doc1		doc2		doc3\r\n2017		*		*		*\r\n01		* 		\r\n02				*\r\n03						*\r\n\r\n_all，2017，自然会搜索到3个docuemnt\r\n\r\nGET /_search?q=2017-01-01\r\n\r\n_all，2017-01-01，query string会用跟建立倒排索引一样的分词器去进行分词\r\n\r\n2017\r\n01\r\n01\r\n\r\nGET /_search?q=post_date:2017-01-01\r\n\r\ndate，会作为exact value去建立索引\r\n\r\n		doc1		doc2		doc3\r\n2017-01-01	*		\r\n2017-01-02			* 		\r\n2017-01-03					*\r\n\r\npost_date:2017-01-01，2017-01-01，doc1一条document\r\n\r\nGET /_search?q=post_date:2017，这个在这里不讲解，因为是es 5.2以后做的一个优化\r\n\r\n\r\n\r\n3、测试分词器\r\n\r\nGET /_analyze\r\n{\r\n  &quot;analyzer&quot;: &quot;standard&quot;,\r\n  &quot;text&quot;: &quot;Text to analyze&quot;\r\n}',8,0,0,1514609247,0,0,0),(61,1,'mapping 介绍','','','（1）往es里面直接插入数据，es会自动建立索引，同时建立type以及对应的mapping\r\n（2）mapping中就自动定义了每个field的数据类型\r\n（3）不同的数据类型（比如说text和date），可能有的是exact value，有的是full text\r\n（4）exact value，在建立倒排索引的时候，分词的时候，是将整个值一起作为一个关键词建立到倒排索引中的；full text，会经历各种各样的处理，分词，normaliztion（时态转换，同义词转换，大小写转换），才会建立到倒排索引中\r\n（5）同时呢，exact value和full text类型的field就决定了，在一个搜索过来的时候，对exact value field或者是full text field进行搜索的行为也是不一样的，会跟建立倒排索引的行为保持一致；比如说exact value搜索的时候，就是直接按照整个值进行匹配，full text query string，也会进行分词和normalization再去倒排索引中去搜索\r\n（6）可以用es的dynamic mapping，让其自动建立mapping，包括自动设置数据类型；也可以提前手动创建index和type的mapping，自己对各个field进行设置，包括数据类型，包括索引行为，包括分词器，等等\r\n\r\nmapping，就是index的type的元数据，每个type都有一个自己的mapping，决定了数据类型，建立倒排索引的行为，还有进行搜索的行为',8,0,0,1514609361,0,0,0),(62,1,'数据类型','','','1、核心的数据类型\r\n\r\nstring\r\nbyte，short，integer，long\r\nfloat，double\r\nboolean\r\ndate\r\n\r\n2、dynamic mapping\r\n\r\ntrue or false	--&gt;	boolean\r\n123		--&gt;	long\r\n123.45		--&gt;	double\r\n2017-01-01	--&gt;	date\r\n&quot;hello world&quot;	--&gt;	string/text\r\n\r\n3、查看mapping\r\n\r\nGET /index/_mapping/type',8,0,0,1514609399,0,0,0),(63,1,'如何建立索引','','','1、如何建立索引\r\n\r\nanalyzed\r\nnot_analyzed\r\nno\r\n\r\n2、修改mapping\r\n\r\n只能创建index时手动建立mapping，或者新增field mapping，但是不能update field mapping\r\n\r\nPUT /website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;article&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;author_id&quot;: {\r\n          &quot;type&quot;: &quot;long&quot;\r\n        },\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;english&quot;\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        },\r\n        &quot;post_date&quot;: {\r\n          &quot;type&quot;: &quot;date&quot;\r\n        },\r\n        &quot;publisher_id&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;index&quot;: &quot;not_analyzed&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;article&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;author_id&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;index_already_exists_exception&quot;,\r\n        &quot;reason&quot;: &quot;index [website/co1dgJ-uTYGBEEOOL8GsQQ] already exists&quot;,\r\n        &quot;index_uuid&quot;: &quot;co1dgJ-uTYGBEEOOL8GsQQ&quot;,\r\n        &quot;index&quot;: &quot;website&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;index_already_exists_exception&quot;,\r\n    &quot;reason&quot;: &quot;index [website/co1dgJ-uTYGBEEOOL8GsQQ] already exists&quot;,\r\n    &quot;index_uuid&quot;: &quot;co1dgJ-uTYGBEEOOL8GsQQ&quot;,\r\n    &quot;index&quot;: &quot;website&quot;\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\nPUT /website/_mapping/article\r\n{\r\n  &quot;properties&quot; : {\r\n    &quot;new_field&quot; : {\r\n      &quot;type&quot; :    &quot;string&quot;,\r\n      &quot;index&quot;:    &quot;not_analyzed&quot;\r\n    }\r\n  }\r\n}\r\n\r\n3、测试mapping\r\n\r\nGET /website/_analyze\r\n{\r\n  &quot;field&quot;: &quot;content&quot;,\r\n  &quot;text&quot;: &quot;my-dogs&quot; \r\n}\r\n\r\nGET website/_analyze\r\n{\r\n  &quot;field&quot;: &quot;new_field&quot;,\r\n  &quot;text&quot;: &quot;my dogs&quot;\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;remote_transport_exception&quot;,\r\n        &quot;reason&quot;: &quot;[4onsTYV][127.0.0.1:9300][indices:admin/analyze[s]]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n    &quot;reason&quot;: &quot;Can&#039;t process field [new_field], Analysis requests are only supported on tokenized fields&quot;\r\n  },\r\n  &quot;status&quot;: 400\r\n}',8,0,0,1514609462,0,0,0),(64,1,'multivalue field','','','1、multivalue field\r\n\r\n{ &quot;tags&quot;: [ &quot;tag1&quot;, &quot;tag2&quot; ]}\r\n\r\n建立索引时与string是一样的，数据类型不能混\r\n\r\n2、empty field\r\n\r\nnull，[]，[null]\r\n\r\n3、object field\r\n\r\nPUT /company/employee/1\r\n{\r\n  &quot;address&quot;: {\r\n    &quot;country&quot;: &quot;china&quot;,\r\n    &quot;province&quot;: &quot;guangdong&quot;,\r\n    &quot;city&quot;: &quot;guangzhou&quot;\r\n  },\r\n  &quot;name&quot;: &quot;jack&quot;,\r\n  &quot;age&quot;: 27,\r\n  &quot;join_date&quot;: &quot;2017-01-01&quot;\r\n}\r\n\r\naddress：object类型\r\n\r\n{\r\n  &quot;company&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;employee&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;address&quot;: {\r\n            &quot;properties&quot;: {\r\n              &quot;city&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              },\r\n              &quot;country&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              },\r\n              &quot;province&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          },\r\n          &quot;age&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          },\r\n          &quot;join_date&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          },\r\n          &quot;name&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;address&quot;: {\r\n    &quot;country&quot;: &quot;china&quot;,\r\n    &quot;province&quot;: &quot;guangdong&quot;,\r\n    &quot;city&quot;: &quot;guangzhou&quot;\r\n  },\r\n  &quot;name&quot;: &quot;jack&quot;,\r\n  &quot;age&quot;: 27,\r\n  &quot;join_date&quot;: &quot;2017-01-01&quot;\r\n}\r\n\r\n{\r\n    &quot;name&quot;:            [jack],\r\n    &quot;age&quot;:          [27],\r\n    &quot;join_date&quot;:      [2017-01-01],\r\n    &quot;address.country&quot;:         [china],\r\n    &quot;address.province&quot;:   [guangdong],\r\n    &quot;address.city&quot;:  [guangzhou]\r\n}\r\n\r\n{\r\n    &quot;authors&quot;: [\r\n        { &quot;age&quot;: 26, &quot;name&quot;: &quot;Jack White&quot;},\r\n        { &quot;age&quot;: 55, &quot;name&quot;: &quot;Tom Jones&quot;},\r\n        { &quot;age&quot;: 39, &quot;name&quot;: &quot;Kitty Smith&quot;}\r\n    ]\r\n}\r\n\r\n{\r\n    &quot;authors.age&quot;:    [26, 55, 39],\r\n    &quot;authors.name&quot;:   [jack, white, tom, jones, kitty, smith]\r\n}',8,0,0,1514609501,0,0,0),(65,1,'search api的基本语法','','','1、search api的基本语法\r\n\r\nGET /search\r\n{}\r\n\r\nGET /index1,index2/type1,type2/search\r\n{}\r\n\r\nGET /_search\r\n{\r\n  &quot;from&quot;: 0,\r\n  &quot;size&quot;: 10\r\n}\r\n\r\n2、http协议中get是否可以带上request body\r\n\r\nHTTP协议，一般不允许get请求带上request body，但是因为get更加适合描述查询数据的操作，因此还是这么用了\r\n\r\nGET /_search?from=0&amp;size=10\r\n\r\nPOST /_search\r\n{\r\n  &quot;from&quot;:0,\r\n  &quot;size&quot;:10\r\n}\r\n\r\n碰巧，很多浏览器，或者是服务器，也都支持GET+request body模式\r\n\r\n如果遇到不支持的场景，也可以用POST /_search',8,0,0,1514609537,0,0,0),(66,1,'Query DSL的基本语法','','','1、一个例子让你明白什么是Query DSL\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_all&quot;: {}\r\n    }\r\n}\r\n\r\n2、Query DSL的基本语法\r\n\r\n{\r\n    QUERY_NAME: {\r\n        ARGUMENT: VALUE,\r\n        ARGUMENT: VALUE,...\r\n    }\r\n}\r\n\r\n{\r\n    QUERY_NAME: {\r\n        FIELD_NAME: {\r\n            ARGUMENT: VALUE,\r\n            ARGUMENT: VALUE,...\r\n        }\r\n    }\r\n}\r\n\r\n示例：\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;test_field&quot;: &quot;test&quot;\r\n    }\r\n  }\r\n}\r\n\r\n3、如何组合多个搜索条件\r\n\r\n搜索需求：title必须包含elasticsearch，content可以包含elasticsearch也可以不包含，author_id必须不为111\r\n\r\n{\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;my hadoop article&quot;,\r\n          &quot;content&quot;: &quot;hadoop is very bad&quot;,\r\n          &quot;author_id&quot;: 111\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;my elasticsearch article&quot;,\r\n          &quot;content&quot;: &quot;es is very bad&quot;,\r\n          &quot;author_id&quot;: 110\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;my elasticsearch article&quot;,\r\n          &quot;content&quot;: &quot;es is very goods&quot;,\r\n          &quot;author_id&quot;: 111\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /website/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;must_not&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;author_id&quot;: 111\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nGET /test_index/_search\r\n{\r\n    &quot;query&quot;: {\r\n            &quot;bool&quot;: {\r\n                &quot;must&quot;: { &quot;match&quot;:   { &quot;name&quot;: &quot;tom&quot; }},\r\n                &quot;should&quot;: [\r\n                    { &quot;match&quot;:       { &quot;hired&quot;: true }},\r\n                    { &quot;bool&quot;: {\r\n                        &quot;must&quot;:      { &quot;match&quot;: { &quot;personality&quot;: &quot;good&quot; }},\r\n                        &quot;must_not&quot;:  { &quot;match&quot;: { &quot;rude&quot;: true }}\r\n                    }}\r\n                ],\r\n                &quot;minimum_should_match&quot;: 1\r\n            }\r\n    }\r\n}',8,0,0,1514609591,0,0,0),(67,1,'filter与query示例','','','1、filter与query示例\r\n\r\nPUT /company/employee/2\r\n{\r\n  &quot;address&quot;: {\r\n    &quot;country&quot;: &quot;china&quot;,\r\n    &quot;province&quot;: &quot;jiangsu&quot;,\r\n    &quot;city&quot;: &quot;nanjing&quot;\r\n  },\r\n  &quot;name&quot;: &quot;tom&quot;,\r\n  &quot;age&quot;: 30,\r\n  &quot;join_date&quot;: &quot;2016-01-01&quot;\r\n}\r\n\r\nPUT /company/employee/3\r\n{\r\n  &quot;address&quot;: {\r\n    &quot;country&quot;: &quot;china&quot;,\r\n    &quot;province&quot;: &quot;shanxi&quot;,\r\n    &quot;city&quot;: &quot;xian&quot;\r\n  },\r\n  &quot;name&quot;: &quot;marry&quot;,\r\n  &quot;age&quot;: 35,\r\n  &quot;join_date&quot;: &quot;2015-01-01&quot;\r\n}\r\n\r\n搜索请求：年龄必须大于等于30，同时join_date必须是2016-01-01\r\n\r\nGET /company/employee/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;join_date&quot;: &quot;2016-01-01&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;gte&quot;: 30\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2、filter与query对比大解密\r\n\r\nfilter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响\r\nquery，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序\r\n\r\n一般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter\r\n除非是你的这些搜索条件，你希望越符合这些搜索条件的document越排在前面返回，那么这些搜索条件要放在query中；如果你不希望一些搜索条件来影响你的document排序，那么就放在filter中即可\r\n\r\n3、filter与query性能\r\n\r\nfilter，不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据\r\nquery，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果\r\n\r\n',8,0,0,1514609622,0,0,0),(68,1,'match all','','','1、match all\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_all&quot;: {}\r\n    }\r\n}\r\n\r\n2、match\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;my elasticsearch article&quot; }}\r\n}\r\n\r\n3、multi match\r\n\r\nGET /test_index/test_type/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;multi_match&quot;: {\r\n      &quot;query&quot;: &quot;test&quot;,\r\n      &quot;fields&quot;: [&quot;test_field&quot;, &quot;test_field1&quot;]\r\n    }\r\n  }\r\n}\r\n\r\n4、range query\r\n\r\nGET /company/employee/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;range&quot;: {\r\n      &quot;age&quot;: {\r\n        &quot;gte&quot;: 30\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n5、term query\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;test_field&quot;: &quot;test hello&quot;\r\n    }\r\n  }\r\n}\r\n\r\n6、terms query\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot;: { &quot;terms&quot;: { &quot;tag&quot;: [ &quot;search&quot;, &quot;full_text&quot;, &quot;nosql&quot; ] }}\r\n}\r\n\r\n7、exist query（2.x中的查询，现在已经不提供了）\r\n',8,0,0,1514609677,0,0,0),(69,1,'bool','','','GET /website/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;must_not&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;author_id&quot;: 111\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n    &quot;bool&quot;: {\r\n        &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }},\r\n        &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;:   &quot;spam&quot; }},\r\n        &quot;should&quot;: [\r\n            { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }}\r\n        ],\r\n        &quot;filter&quot;: {\r\n          &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }} \r\n        }\r\n    }\r\n}\r\n\r\nbool\r\nmust，must_not，should，filter\r\n\r\n每个子查询都会计算一个document针对它的相关度分数，然后bool综合所有分数，合并为一个分数，当然filter是不会计算分数的\r\n\r\n{\r\n    &quot;bool&quot;: {\r\n        &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }},\r\n        &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;:   &quot;spam&quot; }},\r\n        &quot;should&quot;: [\r\n            { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }}\r\n        ],\r\n        &quot;filter&quot;: {\r\n          &quot;bool&quot;: { \r\n              &quot;must&quot;: [\r\n                  { &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }}},\r\n                  { &quot;range&quot;: { &quot;price&quot;: { &quot;lte&quot;: 29.99 }}}\r\n              ],\r\n              &quot;must_not&quot;: [\r\n                  { &quot;term&quot;: { &quot;category&quot;: &quot;ebooks&quot; }}\r\n              ]\r\n          }\r\n        }\r\n    }\r\n}\r\n\r\nGET /company/employee/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;gte&quot;: 30\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514609865,0,0,0),(70,1,'explain','','','GET /test_index/test_type/_validate/query?explain\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;math&quot;: {\r\n      &quot;test_field&quot;: &quot;test&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;valid&quot;: false,\r\n  &quot;error&quot;: &quot;org.elasticsearch.common.ParsingException: no [query] registered for [math]&quot;\r\n}\r\n\r\nGET /test_index/test_type/_validate/query?explain\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;test_field&quot;: &quot;test&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;valid&quot;: true,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;explanations&quot;: [\r\n    {\r\n      &quot;index&quot;: &quot;test_index&quot;,\r\n      &quot;valid&quot;: true,\r\n      &quot;explanation&quot;: &quot;+test_field:test #(#_type:test_type)&quot;\r\n    }\r\n  ]\r\n}\r\n\r\n一般用在那种特别复杂庞大的搜索下，比如你一下子写了上百行的搜索，这个时候可以先用validate api去验证一下，搜索是否合法\r\n',8,0,0,1514609964,0,0,0),(71,1,'排序规则','','','1、默认排序规则\r\n\r\n默认情况下，是按照_score降序排序的\r\n\r\n然而，某些情况下，可能没有有用的_score，比如说filter\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;bool&quot; : {\r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : {\r\n                    &quot;author_id&quot; : 1\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n当然，也可以是constant_score\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : {\r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : {\r\n                    &quot;author_id&quot; : 1\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n2、定制排序规则\r\n\r\nGET /company/employee/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;gte&quot;: 30\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;sort&quot;: [\r\n    {\r\n      &quot;join_date&quot;: {\r\n        &quot;order&quot;: &quot;asc&quot;\r\n      }\r\n    }\r\n  ]\r\n}',8,0,0,1514610176,0,0,0),(72,1,'string field','','','如果对一个string field进行排序，结果往往不准确，因为分词后是多个单词，再排序就不是我们想要的结果了\r\n\r\n通常解决方案是，将一个string field建立两次索引，一个分词，用来进行搜索；一个不分词，用来进行排序\r\n\r\nPUT /website \r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;article&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;fields&quot;: {\r\n            &quot;raw&quot;: {\r\n              &quot;type&quot;: &quot;string&quot;,\r\n              &quot;index&quot;: &quot;not_analyzed&quot;\r\n            }\r\n          },\r\n          &quot;fielddata&quot;: true\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        },\r\n        &quot;post_date&quot;: {\r\n          &quot;type&quot;: &quot;date&quot;\r\n        },\r\n        &quot;author_id&quot;: {\r\n          &quot;type&quot;: &quot;long&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /website/article/1\r\n{\r\n  &quot;title&quot;: &quot;first article&quot;,\r\n  &quot;content&quot;: &quot;this is my second article&quot;,\r\n  &quot;post_date&quot;: &quot;2017-01-01&quot;,\r\n  &quot;author_id&quot;: 110\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;first article&quot;,\r\n          &quot;content&quot;: &quot;this is my first article&quot;,\r\n          &quot;post_date&quot;: &quot;2017-02-01&quot;,\r\n          &quot;author_id&quot;: 110\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;second article&quot;,\r\n          &quot;content&quot;: &quot;this is my second article&quot;,\r\n          &quot;post_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;author_id&quot;: 110\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;third article&quot;,\r\n          &quot;content&quot;: &quot;this is my third article&quot;,\r\n          &quot;post_date&quot;: &quot;2017-03-01&quot;,\r\n          &quot;author_id&quot;: 110\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /website/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_all&quot;: {}\r\n  },\r\n  &quot;sort&quot;: [\r\n    {\r\n      &quot;title.raw&quot;: {\r\n        &quot;order&quot;: &quot;desc&quot;\r\n      }\r\n    }\r\n  ]\r\n}',8,0,0,1514610229,0,0,0),(73,1,'算法','','','1、算法介绍\r\n\r\nrelevance score算法，简单来说，就是计算出，一个索引中的文本，与搜索文本，他们之间的关联匹配程度\r\n\r\nElasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法\r\n\r\nTerm frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关\r\n\r\n搜索请求：hello world\r\n\r\ndoc1：hello you, and world is very good\r\ndoc2：hello, how are you\r\n\r\nInverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关\r\n\r\n搜索请求：hello world\r\n\r\ndoc1：hello, today is very good\r\ndoc2：hi world, how are you\r\n\r\n比如说，在index中有1万条document，hello这个单词在所有的document中，一共出现了1000次；world这个单词在所有的document中，一共出现了100次\r\n\r\ndoc2更相关\r\n\r\nField-length norm：field长度，field越长，相关度越弱\r\n\r\n搜索请求：hello world\r\n\r\ndoc1：{ &quot;title&quot;: &quot;hello article&quot;, &quot;content&quot;: &quot;babaaba 1万个单词&quot; }\r\ndoc2：{ &quot;title&quot;: &quot;my article&quot;, &quot;content&quot;: &quot;blablabala 1万个单词，hi world&quot; }\r\n\r\nhello world在整个index中出现的次数是一样多的\r\n\r\ndoc1更相关，title field更短\r\n\r\n2、_score是如何被计算出来的\r\n\r\nGET /test_index/test_type/_search?explain\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;test_field&quot;: &quot;test hello&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 6,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 4,\r\n    &quot;max_score&quot;: 1.595089,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_shard&quot;: &quot;[test_index][2]&quot;,\r\n        &quot;_node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;20&quot;,\r\n        &quot;_score&quot;: 1.595089,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test hello&quot;\r\n        },\r\n        &quot;_explanation&quot;: {\r\n          &quot;value&quot;: 1.595089,\r\n          &quot;description&quot;: &quot;sum of:&quot;,\r\n          &quot;details&quot;: [\r\n            {\r\n              &quot;value&quot;: 1.595089,\r\n              &quot;description&quot;: &quot;sum of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0.58279467,\r\n                  &quot;description&quot;: &quot;weight(test_field:test in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 0.58279467,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 0.6931472,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 2,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.840795,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.75,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2.56,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1.0122943,\r\n                  &quot;description&quot;: &quot;weight(test_field:hello in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1.0122943,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 1.2039728,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.840795,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.75,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2.56,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            {\r\n              &quot;value&quot;: 0,\r\n              &quot;description&quot;: &quot;match on required clause, product of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0,\r\n                  &quot;description&quot;: &quot;# clause&quot;,\r\n                  &quot;details&quot;: []\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1,\r\n                  &quot;description&quot;: &quot;*:*, product of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;boost&quot;,\r\n                      &quot;details&quot;: []\r\n                    },\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;queryNorm&quot;,\r\n                      &quot;details&quot;: []\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_shard&quot;: &quot;[test_index][2]&quot;,\r\n        &quot;_node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;6&quot;,\r\n        &quot;_score&quot;: 0.58279467,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;tes test&quot;\r\n        },\r\n        &quot;_explanation&quot;: {\r\n          &quot;value&quot;: 0.58279467,\r\n          &quot;description&quot;: &quot;sum of:&quot;,\r\n          &quot;details&quot;: [\r\n            {\r\n              &quot;value&quot;: 0.58279467,\r\n              &quot;description&quot;: &quot;sum of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0.58279467,\r\n                  &quot;description&quot;: &quot;weight(test_field:test in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 0.58279467,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 0.6931472,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 2,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.840795,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.75,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2.56,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            {\r\n              &quot;value&quot;: 0,\r\n              &quot;description&quot;: &quot;match on required clause, product of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0,\r\n                  &quot;description&quot;: &quot;# clause&quot;,\r\n                  &quot;details&quot;: []\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1,\r\n                  &quot;description&quot;: &quot;*:*, product of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;boost&quot;,\r\n                      &quot;details&quot;: []\r\n                    },\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;queryNorm&quot;,\r\n                      &quot;details&quot;: []\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_shard&quot;: &quot;[test_index][3]&quot;,\r\n        &quot;_node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;7&quot;,\r\n        &quot;_score&quot;: 0.5565415,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test client 2&quot;\r\n        },\r\n        &quot;_explanation&quot;: {\r\n          &quot;value&quot;: 0.5565415,\r\n          &quot;description&quot;: &quot;sum of:&quot;,\r\n          &quot;details&quot;: [\r\n            {\r\n              &quot;value&quot;: 0.5565415,\r\n              &quot;description&quot;: &quot;sum of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0.5565415,\r\n                  &quot;description&quot;: &quot;weight(test_field:test in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 0.5565415,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 0.6931472,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.8029196,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2.5,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            {\r\n              &quot;value&quot;: 0,\r\n              &quot;description&quot;: &quot;match on required clause, product of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0,\r\n                  &quot;description&quot;: &quot;# clause&quot;,\r\n                  &quot;details&quot;: []\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1,\r\n                  &quot;description&quot;: &quot;_type:test_type, product of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;boost&quot;,\r\n                      &quot;details&quot;: []\r\n                    },\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;queryNorm&quot;,\r\n                      &quot;details&quot;: []\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_shard&quot;: &quot;[test_index][1]&quot;,\r\n        &quot;_node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;8&quot;,\r\n        &quot;_score&quot;: 0.25316024,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test client 2&quot;\r\n        },\r\n        &quot;_explanation&quot;: {\r\n          &quot;value&quot;: 0.25316024,\r\n          &quot;description&quot;: &quot;sum of:&quot;,\r\n          &quot;details&quot;: [\r\n            {\r\n              &quot;value&quot;: 0.25316024,\r\n              &quot;description&quot;: &quot;sum of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0.25316024,\r\n                  &quot;description&quot;: &quot;weight(test_field:test in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 0.25316024,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 0.2876821,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.88,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 3,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            {\r\n              &quot;value&quot;: 0,\r\n              &quot;description&quot;: &quot;match on required clause, product of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0,\r\n                  &quot;description&quot;: &quot;# clause&quot;,\r\n                  &quot;details&quot;: []\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1,\r\n                  &quot;description&quot;: &quot;*:*, product of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;boost&quot;,\r\n                      &quot;details&quot;: []\r\n                    },\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;queryNorm&quot;,\r\n                      &quot;details&quot;: []\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n3、分析一个document是如何被匹配上的\r\n\r\nGET /test_index/test_type/6/_explain\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;test_field&quot;: &quot;test hello&quot;\r\n    }\r\n  }\r\n}',8,0,0,1514610289,0,0,0),(74,1,'排序','','','搜索的时候，要依靠倒排索引；排序的时候，需要依靠正排索引，看到每个document的每个field，然后进行排序，所谓的正排索引，其实就是doc values\r\n\r\n在建立索引的时候，一方面会建立倒排索引，以供搜索用；一方面会建立正排索引，也就是doc values，以供排序，聚合，过滤等操作使用\r\n\r\ndoc values是被保存在磁盘上的，此时如果内存足够，os会自动将其缓存在内存中，性能还是会很高；如果内存不足够，os会将其写入磁盘上\r\n\r\n\r\ndoc1: hello world you and me\r\ndoc2: hi, world, how are you\r\n\r\nword		doc1		doc2\r\n\r\nhello		*\r\nworld		*		*\r\nyou		*		*\r\nand 		*\r\nme		*\r\nhi				*\r\nhow				*\r\nare				*\r\n\r\nhello you --&gt; hello, you\r\n\r\nhello --&gt; doc1\r\nyou --&gt; doc1,doc2\r\n\r\ndoc1: hello world you and me\r\ndoc2: hi, world, how are you\r\n\r\nsort by age\r\n\r\n\r\ndoc1: { &quot;name&quot;: &quot;jack&quot;, &quot;age&quot;: 27 }\r\ndoc2: { &quot;name&quot;: &quot;tom&quot;, &quot;age&quot;: 30 }\r\n\r\ndocument	name		age\r\n\r\ndoc1		jack		27\r\ndoc2		tom		30	',8,0,0,1514610564,0,0,0),(75,1,'query phase','','','1、query phase\r\n\r\n（1）搜索请求发送到某一个coordinate node，构构建一个priority queue，长度以paging操作from和size为准，默认为10\r\n（2）coordinate node将请求转发到所有shard，每个shard本地搜索，并构建一个本地的priority queue\r\n（3）各个shard将自己的priority queue返回给coordinate node，并构建一个全局的priority queue\r\n\r\n2、replica shard如何提升搜索吞吐量\r\n\r\n一次请求要打到所有shard的一个replica/primary上去，如果每个shard都有多个replica，那么同时并发过来的搜索请求可以同时打到其他的replica上去\r\n',8,0,0,1514610599,0,0,0),(76,1,'fetch phbase','','','1、fetch phbase工作流程\r\n\r\n（1）coordinate node构建完priority queue之后，就发送mget请求去所有shard上获取对应的document\r\n（2）各个shard将document返回给coordinate node\r\n（3）coordinate node将合并后的document结果返回给client客户端\r\n\r\n2、一般搜索，如果不加from和size，就默认搜索前10条，按照_score排序\r\n\r\n',8,0,0,1514610631,0,0,0),(77,1,'preference','','','1、preference\r\n\r\n决定了哪些shard会被用来执行搜索操作\r\n\r\n_primary, _primary_first, _local, _only_node:xyz, _prefer_node:xyz, _shards:2,3\r\n\r\nbouncing results问题，两个document排序，field值相同；不同的shard上，可能排序不同；每次请求轮询打到不同的replica shard上；每次页面上看到的搜索结果的排序都不一样。这就是bouncing result，也就是跳跃的结果。\r\n\r\n搜索的时候，是轮询将搜索请求发送到每一个replica shard（primary shard），但是在不同的shard上，可能document的排序不同\r\n\r\n解决方案就是将preference设置为一个字符串，比如说user_id，让每个user每次搜索的时候，都使用同一个replica shard去执行，就不会看到bouncing results了\r\n\r\n2、timeout，已经讲解过原理了，主要就是限定在一定时间内，将部分获取到的数据直接返回，避免查询耗时过长\r\n\r\n3、routing，document文档路由，_id路由，routing=user_id，这样的话可以让同一个user对应的数据到一个shard上去\r\n\r\n4、search_type\r\n\r\ndefault：query_then_fetch\r\ndfs_query_then_fetch，可以提升revelance sort精准度\r\n',8,0,0,1514610669,0,0,0),(78,1,'scroll','','','如果一次性要查出来比如10万条数据，那么性能会很差，此时一般会采取用scroll滚动查询，一批一批的查，直到所有数据都查询完处理完\r\n\r\n使用scroll滚动搜索，可以先搜索一批数据，然后下次再搜索一批数据，以此类推，直到搜索出全部的数据来\r\nscroll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的\r\n采用基于_doc进行排序的方式，性能较高\r\n每次发送scroll请求，我们还需要指定一个scoll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了\r\n\r\nGET /test_index/test_type/_search?scroll=1m\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_all&quot;: {}\r\n  },\r\n  &quot;sort&quot;: [ &quot;_doc&quot; ],\r\n  &quot;size&quot;: 3\r\n}\r\n\r\n{\r\n  &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACxeFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAALF8WNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACxhFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYhY0b25zVFlWWlRqR3ZJajlfc3BXejJ3&quot;,\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 10,\r\n    &quot;max_score&quot;: null,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;8&quot;,\r\n        &quot;_score&quot;: null,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test client 2&quot;\r\n        },\r\n        &quot;sort&quot;: [\r\n          0\r\n        ]\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;6&quot;,\r\n        &quot;_score&quot;: null,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;tes test&quot;\r\n        },\r\n        &quot;sort&quot;: [\r\n          0\r\n        ]\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;AVp4RN0bhjxldOOnBxaE&quot;,\r\n        &quot;_score&quot;: null,\r\n        &quot;_source&quot;: {\r\n          &quot;test_content&quot;: &quot;my test&quot;\r\n        },\r\n        &quot;sort&quot;: [\r\n          0\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n获得的结果会有一个scoll_id，下一次再发送scoll请求的时候，必须带上这个scoll_id\r\n\r\nGET /_search/scroll\r\n{\r\n    &quot;scroll&quot;: &quot;1m&quot;, \r\n    &quot;scroll_id&quot; : &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACxeFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAALF8WNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACxhFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYhY0b25zVFlWWlRqR3ZJajlfc3BXejJ3&quot;\r\n}\r\n\r\n11,4,7\r\n3,2,1\r\n20\r\n\r\nscoll，看起来挺像分页的，但是其实使用场景不一样。分页主要是用来一页一页搜索，给用户看的；scoll主要是用来一批一批检索数据，让系统进行处理的\r\n',8,0,0,1514610727,0,0,0),(79,1,'创建索引','','','1、为什么我们要手动创建索引？\r\n\r\n2、创建索引\r\n\r\n创建索引的语法\r\n\r\nPUT /my_index\r\n{\r\n    &quot;settings&quot;: { ... any settings ... },\r\n    &quot;mappings&quot;: {\r\n        &quot;type_one&quot;: { ... any mappings ... },\r\n        &quot;type_two&quot;: { ... any mappings ... },\r\n        ...\r\n    }\r\n}\r\n\r\n创建索引的示例\r\n\r\nPUT /my_index\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;number_of_shards&quot;: 1,\r\n    &quot;number_of_replicas&quot;: 0\r\n  },\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;my_field&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n3、修改索引\r\n\r\nPUT /my_index/_settings\r\n{\r\n    &quot;number_of_replicas&quot;: 1\r\n}\r\n\r\n4、删除索引\r\n\r\nDELETE /my_index\r\nDELETE /index_one,index_two\r\nDELETE /index_*\r\nDELETE /_all\r\n\r\nelasticsearch.yml\r\naction.destructive_requires_name: true\r\n\r\n',8,0,0,1514610832,0,0,0),(80,1,'分词器二','','','1、默认的分词器\r\n\r\nstandard\r\n\r\nstandard tokenizer：以单词边界进行切分\r\nstandard token filter：什么都不做\r\nlowercase token filter：将所有字母转换为小写\r\nstop token filer（默认被禁用）：移除停用词，比如a the it等等\r\n\r\n2、修改分词器的设置\r\n\r\n启用english停用词token filter\r\n\r\nPUT /my_index\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;analysis&quot;: {\r\n      &quot;analyzer&quot;: {\r\n        &quot;es_std&quot;: {\r\n          &quot;type&quot;: &quot;standard&quot;,\r\n          &quot;stopwords&quot;: &quot;_english_&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;analyzer&quot;: &quot;standard&quot;, \r\n  &quot;text&quot;: &quot;a dog is in the house&quot;\r\n}\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;analyzer&quot;: &quot;es_std&quot;,\r\n  &quot;text&quot;:&quot;a dog is in the house&quot;\r\n}\r\n\r\n3、定制化自己的分词器\r\n\r\nPUT /my_index\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;analysis&quot;: {\r\n      &quot;char_filter&quot;: {\r\n        &quot;&amp;_to_and&quot;: {\r\n          &quot;type&quot;: &quot;mapping&quot;,\r\n          &quot;mappings&quot;: [&quot;&amp;=&gt; and&quot;]\r\n        }\r\n      },\r\n      &quot;filter&quot;: {\r\n        &quot;my_stopwords&quot;: {\r\n          &quot;type&quot;: &quot;stop&quot;,\r\n          &quot;stopwords&quot;: [&quot;the&quot;, &quot;a&quot;]\r\n        }\r\n      },\r\n      &quot;analyzer&quot;: {\r\n        &quot;my_analyzer&quot;: {\r\n          &quot;type&quot;: &quot;custom&quot;,\r\n          &quot;char_filter&quot;: [&quot;html_strip&quot;, &quot;&amp;_to_and&quot;],\r\n          &quot;tokenizer&quot;: &quot;standard&quot;,\r\n          &quot;filter&quot;: [&quot;lowercase&quot;, &quot;my_stopwords&quot;]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;text&quot;: &quot;tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!&quot;,\r\n  &quot;analyzer&quot;: &quot;my_analyzer&quot;\r\n}\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;content&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;analyzer&quot;: &quot;my_analyzer&quot;\r\n    }\r\n  }\r\n}',8,0,0,1514610880,0,0,0),(81,1,'type','','','type，是一个index中用来区分类似的数据的，类似的数据，但是可能有不同的fields，而且有不同的属性来控制索引建立、分词器\r\nfield的value，在底层的lucene中建立索引的时候，全部是opaque bytes类型，不区分类型的\r\nlucene是没有type的概念的，在document中，实际上将type作为一个document的field来存储，即_type，es通过_type来进行type的过滤和筛选\r\n一个index中的多个type，实际上是放在一起存储的，因此一个index下，不能有多个type重名，而类型或者其他设置不同的，因为那样是无法处理的\r\n\r\n{\r\n   &quot;ecommerce&quot;: {\r\n      &quot;mappings&quot;: {\r\n         &quot;elactronic_goods&quot;: {\r\n            &quot;properties&quot;: {\r\n               &quot;name&quot;: {\r\n                  &quot;type&quot;: &quot;string&quot;,\r\n               },\r\n               &quot;price&quot;: {\r\n                  &quot;type&quot;: &quot;double&quot;\r\n               },\r\n	       &quot;service_period&quot;: {\r\n		  &quot;type&quot;: &quot;string&quot;\r\n	       }			\r\n            }\r\n         },\r\n         &quot;fresh_goods&quot;: {\r\n            &quot;properties&quot;: {\r\n               &quot;name&quot;: {\r\n                  &quot;type&quot;: &quot;string&quot;,\r\n               },\r\n               &quot;price&quot;: {\r\n                  &quot;type&quot;: &quot;double&quot;\r\n               },\r\n	       &quot;eat_period&quot;: {\r\n		  &quot;type&quot;: &quot;string&quot;\r\n	       }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n{\r\n  &quot;name&quot;: &quot;geli kongtiao&quot;,\r\n  &quot;price&quot;: 1999.0,\r\n  &quot;service_period&quot;: &quot;one year&quot;\r\n}\r\n\r\n{\r\n  &quot;name&quot;: &quot;aozhou dalongxia&quot;,\r\n  &quot;price&quot;: 199.0,\r\n  &quot;eat_period&quot;: &quot;one week&quot;\r\n}\r\n\r\n在底层的存储是这样子的。。。。\r\n\r\n{\r\n   &quot;ecommerce&quot;: {\r\n      &quot;mappings&quot;: {\r\n        &quot;_type&quot;: {\r\n          &quot;type&quot;: &quot;string&quot;,\r\n          &quot;index&quot;: &quot;not_analyzed&quot;\r\n        },\r\n        &quot;name&quot;: {\r\n          &quot;type&quot;: &quot;string&quot;\r\n        }\r\n        &quot;price&quot;: {\r\n          &quot;type&quot;: &quot;double&quot;\r\n        }\r\n        &quot;service_period&quot;: {\r\n          &quot;type&quot;: &quot;string&quot;\r\n        }\r\n        &quot;eat_period&quot;: {\r\n          &quot;type&quot;: &quot;string&quot;\r\n        }\r\n      }\r\n   }\r\n}\r\n\r\n{\r\n  &quot;_type&quot;: &quot;elactronic_goods&quot;,\r\n  &quot;name&quot;: &quot;geli kongtiao&quot;,\r\n  &quot;price&quot;: 1999.0,\r\n  &quot;service_period&quot;: &quot;one year&quot;,\r\n  &quot;eat_period&quot;: &quot;&quot;\r\n}\r\n\r\n{\r\n  &quot;_type&quot;: &quot;fresh_goods&quot;,\r\n  &quot;name&quot;: &quot;aozhou dalongxia&quot;,\r\n  &quot;price&quot;: 199.0,\r\n  &quot;service_period&quot;: &quot;&quot;,\r\n  &quot;eat_period&quot;: &quot;one week&quot;\r\n}\r\n\r\n\r\n最佳实践，将类似结构的type放在一个index下，这些type应该有多个field是相同的\r\n假如说，你将两个type的field完全不同，放在一个index下，那么就每条数据都至少有一半的field在底层的lucene中是空值，会有严重的性能问题\r\n\r\n',8,0,0,1514610919,0,0,0),(82,1,'root object','','','1、root object\r\n\r\n就是某个type对应的mapping json，包括了properties，metadata（_id，_source，_type），settings（analyzer），其他settings（比如include_in_all）\r\n\r\nPUT /my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {}\r\n    }\r\n  }\r\n}\r\n\r\n2、properties\r\n\r\ntype，index，analyzer\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;title&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;\r\n    }\r\n  }\r\n}\r\n\r\n3、_source\r\n\r\n好处\r\n\r\n（1）查询的时候，直接可以拿到完整的document，不需要先拿document id，再发送一次请求拿document\r\n（2）partial update基于_source实现\r\n（3）reindex时，直接基于_source实现，不需要从数据库（或者其他外部存储）查询数据再修改\r\n（4）可以基于_source定制返回field\r\n（5）debug query更容易，因为可以直接看到_source\r\n\r\n如果不需要上述好处，可以禁用_source\r\n\r\nPUT /my_index/_mapping/my_type2\r\n{\r\n  &quot;_source&quot;: {&quot;enabled&quot;: false}\r\n}\r\n\r\n4、_all\r\n\r\n将所有field打包在一起，作为一个_all field，建立索引。没指定任何field进行搜索时，就是使用_all field在搜索。\r\n\r\nPUT /my_index/_mapping/my_type3\r\n{\r\n  &quot;_all&quot;: {&quot;enabled&quot;: false}\r\n}\r\n\r\n也可以在field级别设置include_in_all field，设置是否要将field的值包含在_all field中\r\n\r\nPUT /my_index/_mapping/my_type4\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;my_field&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;include_in_all&quot;: false\r\n    }\r\n  }\r\n}\r\n\r\n5、标识性metadata\r\n\r\n_index，_type，_id\r\n',8,0,0,1514610967,0,0,0),(83,1,'dynamic策略','','','1、定制dynamic策略\r\n\r\ntrue：遇到陌生字段，就进行dynamic mapping\r\nfalse：遇到陌生字段，就忽略\r\nstrict：遇到陌生字段，就报错\r\n\r\nPUT /my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;dynamic&quot;: &quot;strict&quot;,\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        },\r\n        &quot;address&quot;: {\r\n          &quot;type&quot;: &quot;object&quot;,\r\n          &quot;dynamic&quot;: &quot;true&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;title&quot;: &quot;my article&quot;,\r\n  &quot;content&quot;: &quot;this is my article&quot;,\r\n  &quot;address&quot;: {\r\n    &quot;province&quot;: &quot;guangdong&quot;,\r\n    &quot;city&quot;: &quot;guangzhou&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;,\r\n        &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;,\r\n    &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed&quot;\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;title&quot;: &quot;my article&quot;,\r\n  &quot;address&quot;: {\r\n    &quot;province&quot;: &quot;guangdong&quot;,\r\n    &quot;city&quot;: &quot;guangzhou&quot;\r\n  }\r\n}\r\n\r\nGET /my_index/_mapping/my_type\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;dynamic&quot;: &quot;strict&quot;,\r\n        &quot;properties&quot;: {\r\n          &quot;address&quot;: {\r\n            &quot;dynamic&quot;: &quot;true&quot;,\r\n            &quot;properties&quot;: {\r\n              &quot;city&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              },\r\n              &quot;province&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          },\r\n          &quot;title&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2、定制dynamic mapping策略\r\n\r\n（1）date_detection\r\n\r\n默认会按照一定格式识别date，比如yyyy-MM-dd。但是如果某个field先过来一个2017-01-01的值，就会被自动dynamic mapping成date，后面如果再来一个&quot;hello world&quot;之类的值，就会报错。可以手动关闭某个type的date_detection，如果有需要，自己手动指定某个field为date类型。\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n    &quot;date_detection&quot;: false\r\n}\r\n\r\n（2）定制自己的dynamic mapping template（type level）\r\n\r\nPUT /my_index\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;my_type&quot;: {\r\n            &quot;dynamic_templates&quot;: [\r\n                { &quot;en&quot;: {\r\n                      &quot;match&quot;:              &quot;*_en&quot;, \r\n                      &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n                      &quot;mapping&quot;: {\r\n                          &quot;type&quot;:           &quot;string&quot;,\r\n                          &quot;analyzer&quot;:       &quot;english&quot;\r\n                      }\r\n                }}\r\n            ]\r\n}}}\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;title&quot;: &quot;this is my first article&quot;\r\n}\r\n\r\nPUT /my_index/my_type/2\r\n{\r\n  &quot;title_en&quot;: &quot;this is my first article&quot;\r\n}\r\n\r\ntitle没有匹配到任何的dynamic模板，默认就是standard分词器，不会过滤停用词，is会进入倒排索引，用is来搜索是可以搜索到的\r\ntitle_en匹配到了dynamic模板，就是english分词器，会过滤停用词，is这种停用词就会被过滤掉，用is来搜索就搜索不到了\r\n\r\n（3）定制自己的default mapping template（index level）\r\n\r\nPUT /my_index\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;_default_&quot;: {\r\n            &quot;_all&quot;: { &quot;enabled&quot;:  false }\r\n        },\r\n        &quot;blog&quot;: {\r\n            &quot;_all&quot;: { &quot;enabled&quot;:  true  }\r\n        }\r\n    }\r\n}',8,0,0,1514611015,0,0,0),(84,1,'重建索引','','','1、重建索引\r\n\r\n一个field的设置是不能被修改的，如果要修改一个Field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入index中\r\n\r\n批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scoll就查询指定日期的一段数据，交给一个线程即可\r\n\r\n（1）一开始，依靠dynamic mapping，插入数据，但是不小心有些数据是2017-01-01这种日期格式的，所以title这种field被自动映射为了date类型，实际上它应该是string类型的\r\n\r\nPUT /my_index/my_type/3\r\n{\r\n  &quot;title&quot;: &quot;2017-01-03&quot;\r\n}\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;title&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n（2）当后期向索引中加入string类型的title值的时候，就会报错\r\n\r\nPUT /my_index/my_type/4\r\n{\r\n  &quot;title&quot;: &quot;my first article&quot;\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;mapper_parsing_exception&quot;,\r\n        &quot;reason&quot;: &quot;failed to parse [title]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;mapper_parsing_exception&quot;,\r\n    &quot;reason&quot;: &quot;failed to parse [title]&quot;,\r\n    &quot;caused_by&quot;: {\r\n      &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n      &quot;reason&quot;: &quot;Invalid format: \\&quot;my first article\\&quot;&quot;\r\n    }\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\n（3）如果此时想修改title的类型，是不可能的\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;title&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n        &quot;reason&quot;: &quot;mapper [title] of different type, current_type [date], merged_type [text]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n    &quot;reason&quot;: &quot;mapper [title] of different type, current_type [date], merged_type [text]&quot;\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\n（4）此时，唯一的办法，就是进行reindex，也就是说，重新建立一个索引，将旧索引的数据查询出来，再导入新索引\r\n\r\n（5）如果说旧索引的名字，是old_index，新索引的名字是new_index，终端java应用，已经在使用old_index在操作了，难道还要去停止java应用，修改使用的index为new_index，才重新启动java应用吗？这个过程中，就会导致java应用停机，可用性降低\r\n\r\n（6）所以说，给java应用一个别名，这个别名是指向旧索引的，java应用先用着，java应用先用goods_index alias来操作，此时实际指向的是旧的my_index\r\n\r\nPUT /my_index/_alias/goods_index\r\n\r\n（7）新建一个index，调整其title的类型为string\r\n\r\nPUT /my_index_new\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n（8）使用scroll api将数据批量查询出来\r\n\r\nGET /my_index/_search?scroll=1m\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_all&quot;: {}\r\n    },\r\n    &quot;sort&quot;: [&quot;_doc&quot;],\r\n    &quot;size&quot;:  1\r\n}\r\n\r\n{\r\n  &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAADpAFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAA6QRY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAAOkIWNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAADpDFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAA6RBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3&quot;,\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: null,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;my_index&quot;,\r\n        &quot;_type&quot;: &quot;my_type&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: null,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;2017-01-02&quot;\r\n        },\r\n        &quot;sort&quot;: [\r\n          0\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n（9）采用bulk api将scoll查出来的一批数据，批量写入新索引\r\n\r\nPOST /_bulk\r\n{ &quot;index&quot;:  { &quot;_index&quot;: &quot;my_index_new&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;title&quot;:    &quot;2017-01-02&quot; }\r\n\r\n（10）反复循环8~9，查询一批又一批的数据出来，采取bulk api将每一批数据批量写入新索引\r\n\r\n（11）将goods_index alias切换到my_index_new上去，java应用会直接通过index别名使用新的索引中的数据，java应用程序不需要停机，零提交，高可用\r\n\r\nPOST /_aliases\r\n{\r\n    &quot;actions&quot;: [\r\n        { &quot;remove&quot;: { &quot;index&quot;: &quot;my_index&quot;, &quot;alias&quot;: &quot;goods_index&quot; }},\r\n        { &quot;add&quot;:    { &quot;index&quot;: &quot;my_index_new&quot;, &quot;alias&quot;: &quot;goods_index&quot; }}\r\n    ]\r\n}\r\n\r\n（12）直接通过goods_index别名来查询，是否ok\r\n\r\nGET /goods_index/my_type/_search\r\n\r\n2、基于alias对client透明切换index\r\n\r\nPUT /my_index_v1/_alias/my_index\r\n\r\nclient对my_index进行操作\r\n\r\nreindex操作，完成之后，切换v1到v2\r\n\r\nPOST /_aliases\r\n{\r\n    &quot;actions&quot;: [\r\n        { &quot;remove&quot;: { &quot;index&quot;: &quot;my_index_v1&quot;, &quot;alias&quot;: &quot;my_index&quot; }},\r\n        { &quot;add&quot;:    { &quot;index&quot;: &quot;my_index_v2&quot;, &quot;alias&quot;: &quot;my_index&quot; }}\r\n    ]\r\n}',8,0,0,1514611132,0,0,0),(85,1,'倒排索引','','','倒排索引，是适合用于进行搜索的\r\n\r\n倒排索引的结构\r\n\r\n（1）包含这个关键词的document list\r\n（2）包含这个关键词的所有document的数量：IDF（inverse document frequency）\r\n（3）这个关键词在每个document中出现的次数：TF（term frequency）\r\n（4）这个关键词在这个document中的次序\r\n（5）每个document的长度：length norm\r\n（6）包含这个关键词的所有document的平均长度\r\n\r\nword		doc1		doc2\r\n\r\ndog		*		*\r\nhello		*\r\nyou				*\r\n\r\n倒排索引不可变的好处\r\n\r\n（1）不需要锁，提升并发能力，避免锁的问题\r\n（2）数据不变，一直保存在os cache中，只要cache内存足够\r\n（3）filter cache一直驻留在内存，因为数据不变\r\n（4）可以压缩，节省cpu和io开销\r\n\r\n倒排索引不可变的坏处：每次都要重新构建整个索引',8,0,0,1514611208,0,0,0),(86,1,'commit point','','','（1）数据写入buffer\r\n（2）commit point\r\n（3）buffer中的数据写入新的index segment\r\n（4）等待在os cache中的index segment被fsync强制刷到磁盘上\r\n（5）新的index sgement被打开，供search使用\r\n（6）buffer被清空\r\n\r\n每次commit point时，会有一个.del文件，标记了哪些segment中的哪些document被标记为deleted了\r\n搜索的时候，会依次查询所有的segment，从旧的到新的，比如被修改过的document，在旧的segment中，会标记为deleted，在新的segment中会有其新的数据\r\n',8,0,0,1514611241,0,0,0),(87,1,'写入流程','','','现有流程的问题，每次都必须等待fsync将segment刷入磁盘，才能将segment打开供search使用，这样的话，从一个document写入，到它可以被搜索，可能会超过1分钟！！！这就不是近实时的搜索了！！！主要瓶颈在于fsync实际发生磁盘IO写数据进磁盘，是很耗时的。\r\n\r\n写入流程别改进如下：\r\n\r\n（1）数据写入buffer\r\n（2）每隔一定时间，buffer中的数据被写入segment文件，但是先写入os cache\r\n（3）只要segment写入os cache，那就直接打开供search使用，不立即执行commit\r\n\r\n数据写入os cache，并被打开供搜索的过程，叫做refresh，默认是每隔1秒refresh一次。也就是说，每隔一秒就会将buffer中的数据写入一个新的index segment file，先写入os cache中。所以，es是近实时的，数据写入到可以被搜索，默认是1秒。\r\n\r\nPOST /my_index/_refresh，可以手动refresh，一般不需要手动执行，没必要，让es自己搞就可以了\r\n\r\n比如说，我们现在的时效性要求，比较低，只要求一条数据写入es，一分钟以后才让我们搜索到就可以了，那么就可以调整refresh interval\r\n\r\nPUT /my_index\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;refresh_interval&quot;: &quot;30s&quot; \r\n  }\r\n}\r\n\r\ncommit。。。稍后就会讲。。。',8,0,0,1514611275,0,0,0),(88,1,'优化的写入流程','','','再次优化的写入流程\r\n\r\n（1）数据写入buffer缓冲和translog日志文件\r\n（2）每隔一秒钟，buffer中的数据被写入新的segment file，并进入os cache，此时segment被打开并供search使用\r\n（3）buffer被清空\r\n（4）重复1~3，新的segment不断添加，buffer不断被清空，而translog中的数据不断累加\r\n（5）当translog长度达到一定程度的时候，commit操作发生\r\n  （5-1）buffer中的所有数据写入一个新的segment，并写入os cache，打开供使用\r\n  （5-2）buffer被清空\r\n  （5-3）一个commit ponit被写入磁盘，标明了所有的index segment\r\n  （5-4）filesystem cache中的所有index segment file缓存数据，被fsync强行刷到磁盘上\r\n  （5-5）现有的translog被清空，创建一个新的translog\r\n\r\n基于translog和commit point，如何进行数据恢复\r\n\r\nfsync+清空translog，就是flush，默认每隔30分钟flush一次，或者当translog过大的时候，也会flush\r\n\r\nPOST /my_index/_flush，一般来说别手动flush，让它自动执行就可以了\r\n\r\ntranslog，每隔5秒被fsync一次到磁盘上。在一次增删改操作之后，当fsync在primary shard和replica shard都成功之后，那次增删改操作才会成功\r\n\r\n但是这种在一次增删改时强行fsync translog可能会导致部分操作比较耗时，也可以允许部分数据丢失，设置异步fsync translog\r\n\r\nPUT /my_index/_settings\r\n{\r\n    &quot;index.translog.durability&quot;: &quot;async&quot;,\r\n    &quot;index.translog.sync_interval&quot;: &quot;5s&quot;\r\n}',8,0,0,1514611329,0,0,0),(89,1,'segment file','','','每秒一个segment file，文件过多，而且每次search都要搜索所有的segment，很耗时\r\n\r\n默认会在后台执行segment merge操作，在merge的时候，被标记为deleted的document也会被彻底物理删除\r\n\r\n每次merge操作的执行流程\r\n\r\n（1）选择一些有相似大小的segment，merge成一个大的segment\r\n（2）将新的segment flush到磁盘上去\r\n（3）写一个新的commit point，包括了新的segment，并且排除旧的那些segment\r\n（4）将新的segment打开供搜索\r\n（5）将旧的segment删除\r\n\r\nPOST /my_index/_optimize?max_num_segments=1，尽量不要手动执行，让它自动默认执行就可以了',8,0,0,1514611461,0,0,0),(90,1,'java api 操作document','','','强调一下，我们的es讲课的风格\r\n\r\n1、es这门技术有点特殊，跟比如其他的像纯java的课程，比如分布式课程，或者大数据类的课程，比如hadoop，spark，storm等。不太一样\r\n\r\n2、es非常重要的一个api，是它的restful api，你自己思考一下，掌握这个es的restful api，可以让你执行一些核心的运维管理的操作，比如说创建索引，维护索引，执行各种refresh、flush、optimize操作，查看集群的健康状况，比如还有其他的一些操作，就不在这里枚举了。或者说探查一些数据，可能用java api并不方便。\r\n\r\n3、es的学习，首先，你必须学好restful api，然后才是你自己的熟悉语言的api，java api。\r\n\r\n这个《核心知识篇（上半季）》，其实主要还是打基础，包括核心的原理，还有核心的操作，还有部分高级的技术和操作，大量的实验，大量的画图，最后初步讲解怎么使用java api\r\n\r\n《核心知识篇（下半季）》，包括深度讲解搜索这块技术，还有聚合分析这块技术，包括数据建模，包括java api的复杂使用，有一个项目实战s\r\n\r\n员工信息\r\n\r\n姓名\r\n年龄\r\n职位\r\n国家\r\n入职日期\r\n薪水\r\n\r\n我是默认大家至少有java基础的，如果你java一点都不会，请先自己补一下\r\n\r\n1、maven依赖\r\n\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\r\n    &lt;artifactId&gt;transport&lt;/artifactId&gt;\r\n    &lt;version&gt;5.2.2&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\r\n    &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;\r\n    &lt;version&gt;2.7&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\r\n    &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;\r\n    &lt;version&gt;2.7&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n\r\nlog4j2.properties\r\n\r\nappender.console.type = Console\r\nappender.console.name = console\r\nappender.console.layout.type = PatternLayout\r\n\r\nrootLogger.level = info\r\nrootLogger.appenderRef.console.ref = console\r\n\r\n2、构建client\r\n\r\nSettings settings = Settings.builder()\r\n        .put(&quot;cluster.name&quot;, &quot;myClusterName&quot;).build();\r\nTransportClient client = new PreBuiltTransportClient(settings);\r\n\r\nTransportClient client = new PreBuiltTransportClient(Settings.EMPTY)\r\n        .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;host1&quot;), 9300))\r\n        .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;host2&quot;), 9300));\r\n\r\nclient.close();\r\n\r\n3、创建document\r\n\r\nIndexResponse response = client.prepareIndex(&quot;index&quot;, &quot;type&quot;, &quot;1&quot;)\r\n        .setSource(jsonBuilder()\r\n                    .startObject()\r\n                        .field(&quot;user&quot;, &quot;kimchy&quot;)\r\n                        .field(&quot;postDate&quot;, new Date())\r\n                        .field(&quot;message&quot;, &quot;trying out Elasticsearch&quot;)\r\n                    .endObject()\r\n                  )\r\n        .get();\r\n\r\n4、查询document\r\n\r\nGetResponse response = client.prepareGet(&quot;index&quot;, &quot;type&quot;, &quot;1&quot;).get();\r\n\r\n5、修改document\r\n\r\nclient.prepareUpdate(&quot;index&quot;, &quot;type&quot;, &quot;1&quot;)\r\n        .setDoc(jsonBuilder()               \r\n            .startObject()\r\n                .field(&quot;gender&quot;, &quot;male&quot;)\r\n            .endObject())\r\n        .get();\r\n\r\n6、删除document\r\n\r\nDeleteResponse response = client.prepareDelete(&quot;index&quot;, &quot;type&quot;, &quot;1&quot;).get();\r\n',8,0,0,1514611530,0,0,0),(91,1,'java api 分页','','','SearchResponse response = client.prepareSearch(&quot;index1&quot;, &quot;index2&quot;)\r\n        .setTypes(&quot;type1&quot;, &quot;type2&quot;)\r\n        .setQuery(QueryBuilders.termQuery(&quot;multi&quot;, &quot;test&quot;))                 // Query\r\n        .setPostFilter(QueryBuilders.rangeQuery(&quot;age&quot;).from(12).to(18))     // Filter\r\n        .setFrom(0).setSize(60)\r\n        .get();\r\n\r\n需求：\r\n\r\n（1）搜索职位中包含technique的员工\r\n（2）同时要求age在30到40岁之间\r\n（3）分页查询，查找第一页\r\n\r\nGET /company/employee/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;position&quot;: &quot;technique&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;gte&quot;: 30,\r\n            &quot;lte&quot;: 40\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;from&quot;: 0,\r\n  &quot;size&quot;: 1\r\n}\r\n\r\n告诉大家，为什么刚才一边运行创建document，一边搜索什么都没搜索到？？？？\r\n\r\n近实时！！！\r\n\r\n默认是1秒以后，写入es的数据，才能被搜索到。很明显刚才，写入数据不到一秒，我门就在搜索。\r\n\r\n',8,0,0,1514611581,0,0,0),(92,1,'java api 分组','','','SearchResponse sr = node.client().prepareSearch()\r\n    .addAggregation(\r\n        AggregationBuilders.terms(&quot;by_country&quot;).field(&quot;country&quot;)\r\n        .subAggregation(AggregationBuilders.dateHistogram(&quot;by_year&quot;)\r\n            .field(&quot;dateOfBirth&quot;)\r\n            .dateHistogramInterval(DateHistogramInterval.YEAR)\r\n            .subAggregation(AggregationBuilders.avg(&quot;avg_children&quot;).field(&quot;children&quot;))\r\n        )\r\n    )\r\n    .execute().actionGet();\r\n\r\n我们先给个需求：\r\n\r\n（1）首先按照country国家来进行分组\r\n（2）然后在每个country分组内，再按照入职年限进行分组\r\n（3）最后计算每个分组内的平均薪资\r\n\r\nPUT /company\r\n{\r\n  &quot;mappings&quot;: {\r\n      &quot;employee&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          },\r\n          &quot;country&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            },\r\n            &quot;fielddata&quot;: true\r\n          },\r\n          &quot;join_date&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          },\r\n          &quot;name&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          },\r\n          &quot;position&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          },\r\n          &quot;salary&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n}\r\n\r\nGET /company/employee/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_country&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;country&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_join_date&quot;: {\r\n          &quot;date_histogram&quot;: {\r\n            &quot;field&quot;: &quot;join_date&quot;,\r\n            &quot;interval&quot;: &quot;year&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;avg_salary&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;salary&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nMap&lt;String, Aggregation&gt; aggrMap = searchResponse.getAggregations().asMap();\r\n		StringTerms groupByCountry = (StringTerms) aggrMap.get(&quot;group_by_country&quot;);\r\n		Iterator&lt;Bucket&gt; groupByCountryBucketIterator = groupByCountry.getBuckets().iterator();\r\n		\r\n		while(groupByCountryBucketIterator.hasNext()) {\r\n			Bucket groupByCountryBucket = groupByCountryBucketIterator.next();\r\n			\r\n			System.out.println(groupByCountryBucket.getKey() + &quot;\\t&quot; + groupByCountryBucket.getDocCount()); \r\n			\r\n			Histogram groupByJoinDate = (Histogram) groupByCountryBucket.getAggregations().asMap().get(&quot;group_by_join_date&quot;); \r\n			Iterator&lt;org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket&gt; groupByJoinDateBucketIterator = groupByJoinDate.getBuckets().iterator();\r\n			 \r\n			while(groupByJoinDateBucketIterator.hasNext()) {\r\n				org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket groupByJoinDateBucket = groupByJoinDateBucketIterator.next();\r\n				\r\n				System.out.println(groupByJoinDateBucket.getKey() + &quot;\\t&quot; + groupByJoinDateBucket.getDocCount()); \r\n				\r\n				Avg avgSalary = (Avg) groupByJoinDateBucket.getAggregations().asMap().get(&quot;avg_salary&quot;);\r\n				System.out.println(avgSalary.getValue()); \r\n			}\r\n		}\r\n		\r\n		client.close();\r\n	}',8,0,0,1514611634,0,0,0),(93,1,'搜索帖子','','','1、根据用户ID、是否隐藏、帖子ID、发帖日期来搜索帖子\r\n\r\n（1）插入一些测试帖子数据\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 1 }}\r\n{ &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 2 }}\r\n{ &quot;articleID&quot; : &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 3 }}\r\n{ &quot;articleID&quot; : &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 4 }}\r\n{ &quot;articleID&quot; : &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot; }\r\n\r\n初步来说，就先搞4个字段，因为整个es是支持json document格式的，所以说扩展性和灵活性非常之好。如果后续随着业务需求的增加，要在document中增加更多的field，那么我们可以很方便的随时添加field。但是如果是在关系型数据库中，比如mysql，我们建立了一个表，现在要给表中新增一些column，那就很坑爹了，必须用复杂的修改表结构的语法去执行。而且可能对系统代码还有一定的影响。\r\n\r\nGET /forum/_mapping/article\r\n\r\n{\r\n  &quot;forum&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;article&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;articleID&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          },\r\n          &quot;hidden&quot;: {\r\n            &quot;type&quot;: &quot;boolean&quot;\r\n          },\r\n          &quot;postDate&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          },\r\n          &quot;userID&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n现在es 5.2版本，type=text，默认会设置两个field，一个是field本身，比如articleID，就是分词的；还有一个的话，就是field.keyword，articleID.keyword，默认不分词，会最多保留256个字符\r\n\r\n（2）根据用户ID搜索帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;userID&quot; : 1\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nterm filter/query：对搜索文本不分词，直接拿去倒排索引中匹配，你输入的是什么，就去匹配什么\r\n比如说，如果对搜索文本进行分词的话，“helle world” --&gt; “hello”和“world”，两个词分别去倒排索引中匹配\r\nterm，“hello world” --&gt; “hello world”，直接去倒排索引中匹配“hello world”\r\n\r\n（3）搜索没有隐藏的帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;hidden&quot; : false\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n（4）根据发帖日期搜索帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;postDate&quot; : &quot;2017-01-01&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n（5）根据帖子ID搜索帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 0,\r\n    &quot;max_score&quot;: null,\r\n    &quot;hits&quot;: []\r\n  }\r\n}\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;articleID.keyword&quot; : &quot;XHDK-A-1293-#fJ3&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\narticleID.keyword，是es最新版本内置建立的field，就是不分词的。所以一个articleID过来的时候，会建立两次索引，一次是自己本身，是要分词的，分词后放入倒排索引；另外一次是基于articleID.keyword，不分词，保留256个字符最多，直接一个字符串放入倒排索引中。\r\n\r\n所以term filter，对text过滤，可以考虑使用内置的field.keyword来进行匹配。但是有个问题，默认就保留256个字符。所以尽可能还是自己去手动建立索引，指定not_analyzed吧。在最新版本的es中，不需要指定not_analyzed也可以，将type=keyword即可。\r\n\r\n（6）查看分词\r\n\r\nGET /forum/_analyze\r\n{\r\n  &quot;field&quot;: &quot;articleID&quot;,\r\n  &quot;text&quot;: &quot;XHDK-A-1293-#fJ3&quot;\r\n}\r\n\r\n默认是analyzed的text类型的field，建立倒排索引的时候，就会对所有的articleID分词，分词以后，原本的articleID就没有了，只有分词后的各个word存在于倒排索引中。\r\nterm，是不对搜索文本分词的，XHDK-A-1293-#fJ3 --&gt; XHDK-A-1293-#fJ3；但是articleID建立索引的时候，XHDK-A-1293-#fJ3 --&gt; xhdk，a，1293，fj3\r\n\r\n（7）重建索引\r\n\r\nDELETE /forum\r\n\r\nPUT /forum\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;article&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;articleID&quot;: {\r\n          &quot;type&quot;: &quot;keyword&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 1 }}\r\n{ &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 2 }}\r\n{ &quot;articleID&quot; : &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 3 }}\r\n{ &quot;articleID&quot; : &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 4 }}\r\n{ &quot;articleID&quot; : &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot; }\r\n\r\n（8）重新根据帖子ID和发帖日期进行搜索\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n2、梳理学到的知识点\r\n\r\n（1）term filter：根据exact value进行搜索，数字、boolean、date天然支持\r\n（2）text需要建索引时指定为not_analyzed，才能用term query\r\n（3）相当于SQL中的单个where条件\r\n\r\nselect *\r\nfrom forum.article\r\nwhere articleID=&#039;XHDK-A-1293-#fJ3&#039;',8,0,0,1514611855,0,0,0),(94,1,'倒排索引中查找搜索串，获取document list','','','（1）在倒排索引中查找搜索串，获取document list\r\n\r\ndate来举例\r\n\r\nword		doc1		doc2		doc3\r\n\r\n2017-01-01	*		*\r\n2017-02-02			*		*\r\n2017-03-03	*		*		*\r\n\r\nfilter：2017-02-02\r\n\r\n到倒排索引中一找，发现2017-02-02对应的document list是doc2,doc3\r\n\r\n（2）为每个在倒排索引中搜索到的结果，构建一个bitset，[0, 0, 0, 1, 0, 1]\r\n\r\n非常重要\r\n\r\n使用找到的doc list，构建一个bitset，就是一个二进制的数组，数组每个元素都是0或1，用来标识一个doc对一个filter条件是否匹配，如果匹配就是1，不匹配就是0\r\n\r\n[0, 1, 1]\r\n\r\ndoc1：不匹配这个filter的\r\ndoc2和do3：是匹配这个filter的\r\n\r\n尽可能用简单的数据结构去实现复杂的功能，可以节省内存空间，提升性能\r\n\r\n（3）遍历每个过滤条件对应的bitset，优先从最稀疏的开始搜索，查找满足所有条件的document\r\n\r\n后面会讲解，一次性其实可以在一个search请求中，发出多个filter条件，每个filter条件都会对应一个bitset\r\n遍历每个filter条件对应的bitset，先从最稀疏的开始遍历\r\n\r\n[0, 0, 0, 1, 0, 0]：比较稀疏\r\n[0, 1, 0, 1, 0, 1]\r\n\r\n先遍历比较稀疏的bitset，就可以先过滤掉尽可能多的数据\r\n\r\n遍历所有的bitset，找到匹配所有filter条件的doc\r\n\r\n请求：filter，postDate=2017-01-01，userID=1\r\n\r\npostDate: [0, 0, 1, 1, 0, 0]\r\nuserID:   [0, 1, 0, 1, 0, 1]\r\n\r\n遍历完两个bitset之后，找到的匹配所有条件的doc，就是doc4\r\n\r\n就可以将document作为结果返回给client了\r\n\r\n（4）caching bitset，跟踪query，在最近256个query中超过一定次数的过滤条件，缓存其bitset。对于小segment（&lt;1000，或&lt;3%），不缓存bitset。\r\n\r\n比如postDate=2017-01-01，[0, 0, 1, 1, 0, 0]，可以缓存在内存中，这样下次如果再有这个条件过来的时候，就不用重新扫描倒排索引，反复生成bitset，可以大幅度提升性能。\r\n\r\n在最近的256个filter中，有某个filter超过了一定的次数，次数不固定，就会自动缓存这个filter对应的bitset\r\n\r\nsegment（上半季），filter针对小segment获取到的结果，可以不缓存，segment记录数&lt;1000，或者segment大小&lt;index总大小的3%\r\n\r\nsegment数据量很小，此时哪怕是扫描也很快；segment会在后台自动合并，小segment很快就会跟其他小segment合并成大segment，此时就缓存也没有什么意义，segment很快就消失了\r\n\r\n针对一个小segment的bitset，[0, 0, 1, 0]\r\n\r\nfilter比query的好处就在于会caching，但是之前不知道caching的是什么东西，实际上并不是一个filter返回的完整的doc list数据结果。而是filter bitset缓存起来。下次不用扫描倒排索引了。\r\n\r\n（5）filter大部分情况下来说，在query之前执行，先尽量过滤掉尽可能多的数据\r\n\r\nquery：是会计算doc对搜索条件的relevance score，还会根据这个score去排序\r\nfilter：只是简单过滤出想要的数据，不计算relevance score，也不排序\r\n\r\n（6）如果document有新增或修改，那么cached bitset会被自动更新\r\n\r\npostDate=2017-01-01，[0, 0, 1, 0]\r\ndocument，id=5，postDate=2017-01-01，会自动更新到postDate=2017-01-01这个filter的bitset中，全自动，缓存会自动更新。postDate=2017-01-01的bitset，[0, 0, 1, 0, 1]\r\ndocument，id=1，postDate=2016-12-30，修改为postDate-2017-01-01，此时也会自动更新bitset，[1, 0, 1, 0, 1]\r\n\r\n（7）以后只要是有相同的filter条件的，会直接来使用这个过滤条件对应的cached bitset\r\n',8,0,0,1514611981,0,0,0),(95,1,'搜索发帖','','','1、搜索发帖日期为2017-01-01，或者帖子ID为XHDK-A-1293-#fJ3的帖子，同时要求帖子的发帖日期绝对不为2017-01-02\r\n\r\nselect *\r\nfrom forum.article\r\nwhere (post_date=&#039;2017-01-01&#039; or article_id=&#039;XHDK-A-1293-#fJ3&#039;)\r\nand post_date!=&#039;2017-01-02&#039;\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;bool&quot;: {\r\n          &quot;should&quot;: [\r\n            {&quot;term&quot;: { &quot;postDate&quot;: &quot;2017-01-01&quot; }},\r\n            {&quot;term&quot;: {&quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;}}\r\n          ],\r\n          &quot;must_not&quot;: {\r\n            &quot;term&quot;: {\r\n              &quot;postDate&quot;: &quot;2017-01-02&quot;\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nmust，should，must_not，filter：必须匹配，可以匹配其中任意一个即可，必须不匹配\r\n\r\n2、搜索帖子ID为XHDK-A-1293-#fJ3，或者是帖子ID为JODL-X-1937-#pV7而且发帖日期为2017-01-01的帖子\r\n\r\nselect *\r\nfrom forum.article\r\nwhere article_id=&#039;XHDK-A-1293-#fJ3&#039;\r\nor (article_id=&#039;JODL-X-1937-#pV7&#039; and post_date=&#039;2017-01-01&#039;)\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;bool&quot;: {\r\n          &quot;should&quot;: [\r\n            {\r\n              &quot;term&quot;: {\r\n                &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;\r\n              }\r\n            },\r\n            {\r\n              &quot;bool&quot;: {\r\n                &quot;must&quot;: [\r\n                  {\r\n                    &quot;term&quot;:{\r\n                      &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;\r\n                    }\r\n                  },\r\n                  {\r\n                    &quot;term&quot;: {\r\n                      &quot;postDate&quot;: &quot;2017-01-01&quot;\r\n                    }\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n3、梳理学到的知识点\r\n\r\n（1）bool：must，must_not，should，组合多个过滤条件\r\n（2）bool可以嵌套\r\n（3）相当于SQL中的多个and条件：当你把搜索语法学好了以后，基本可以实现部分常用的sql语法对应的功能\r\n\r\n',8,0,0,1514612021,0,0,0),(96,1,'sql in','','','term: {&quot;field&quot;: &quot;value&quot;}\r\nterms: {&quot;field&quot;: [&quot;value1&quot;, &quot;value2&quot;]}\r\n\r\nsql中的in\r\n\r\nselect * from tbl where col in (&quot;value1&quot;, &quot;value2&quot;)\r\n\r\n1、为帖子数据增加tag字段\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag&quot; : [&quot;java&quot;, &quot;hadoop&quot;]} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag&quot; : [&quot;java&quot;]} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag&quot; : [&quot;hadoop&quot;]} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag&quot; : [&quot;java&quot;, &quot;elasticsearch&quot;]} }\r\n\r\n2、搜索articleID为KDKE-B-9947-#kL5或QQPX-R-3956-#aD8的帖子，搜索tag中包含java的帖子\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;terms&quot;: {\r\n          &quot;articleID&quot;: [\r\n            &quot;KDKE-B-9947-#kL5&quot;,\r\n            &quot;QQPX-R-3956-#aD8&quot;\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : {\r\n            &quot;filter&quot; : {\r\n                &quot;terms&quot; : { \r\n                    &quot;tag&quot; : [&quot;java&quot;]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;,\r\n          &quot;userID&quot;: 2,\r\n          &quot;hidden&quot;: true,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;elasticsearch&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n3、优化搜索结果，仅仅搜索tag只包含java的帖子\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag_cnt&quot; : 2} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag_cnt&quot; : 1} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag_cnt&quot; : 1} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag_cnt&quot; : 2} }\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;bool&quot;: {\r\n          &quot;must&quot;: [\r\n            {\r\n              &quot;term&quot;: {\r\n                &quot;tag_cnt&quot;: 1\r\n              }\r\n            },\r\n            {\r\n              &quot;terms&quot;: {\r\n                &quot;tag&quot;: [&quot;java&quot;]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n[&quot;java&quot;, &quot;hadoop&quot;, &quot;elasticsearch&quot;]\r\n\r\n4、学到的知识点梳理\r\n\r\n（1）terms多值搜索\r\n（2）优化terms多值搜索的结果\r\n（3）相当于SQL中的in语句',8,0,0,1514612093,0,0,0),(97,1,'range','','','1、为帖子数据增加浏览量的字段\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;view_cnt&quot; : 30} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;view_cnt&quot; : 50} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;view_cnt&quot; : 100} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;view_cnt&quot; : 80} }\r\n\r\n2、搜索浏览量在30~60之间的帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;view_cnt&quot;: {\r\n            &quot;gt&quot;: 30,\r\n            &quot;lt&quot;: 60\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\ngte\r\nlte\r\n\r\n3、搜索发帖日期在最近1个月的帖子\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 5 }}\r\n{ &quot;articleID&quot; : &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot; : 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-03-01&quot;, &quot;tag&quot;: [&quot;elasticsearch&quot;], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10 }\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;postDate&quot;: {\r\n            &quot;gt&quot;: &quot;2017-03-10||-30d&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;postDate&quot;: {\r\n            &quot;gt&quot;: &quot;now-30d&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n4、梳理一下学到的知识点\r\n\r\n（1）range，sql中的between，或者是&gt;=1，&lt;=1\r\n（2）range做范围过滤',8,0,0,1514612128,0,0,0),(98,1,'搜索帖子标题','','','1、为帖子数据增加标题字段\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is java and elasticsearch blog&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is java blog&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is elasticsearch blog&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is java, elasticsearch, hadoop blog&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is spark blog&quot;} }\r\n\r\n2、搜索标题中包含java或elasticsearch的blog\r\n\r\n这个，就跟之前的那个term query，不一样了。不是搜索exact value，是进行full text全文检索。\r\nmatch query，是负责进行全文检索的。当然，如果要检索的field，是not_analyzed类型的，那么match query也相当于term query。\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;java elasticsearch&quot;\r\n        }\r\n    }\r\n}\r\n\r\n3、搜索标题中包含java和elasticsearch的blog\r\n\r\n搜索结果精准控制的第一步：灵活使用and关键字，如果你是希望所有的搜索关键字都要匹配的，那么就用and，可以实现单纯match query无法实现的效果\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n		&quot;query&quot;: &quot;java elasticsearch&quot;,\r\n		&quot;operator&quot;: &quot;and&quot;\r\n   	    }\r\n        }\r\n    }\r\n}\r\n\r\n4、搜索包含java，elasticsearch，spark，hadoop，4个关键字中，至少3个的blog\r\n\r\n控制搜索结果的精准度的第二步：指定一些关键字中，必须至少匹配其中的多少个关键字，才能作为结果返回\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;title&quot;: {\r\n        &quot;query&quot;: &quot;java elasticsearch spark hadoop&quot;,\r\n        &quot;minimum_should_match&quot;: &quot;75%&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n5、用bool组合多个搜索条件，来搜索title\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n      &quot;must_not&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;spark&quot;  }},\r\n      &quot;should&quot;: [\r\n                  { &quot;match&quot;: { &quot;title&quot;: &quot;hadoop&quot; }},\r\n                  { &quot;match&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }}\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n6、bool组合多个搜索条件，如何计算relevance score\r\n\r\nmust和should搜索对应的分数，加起来，除以must和should的总数\r\n\r\n排名第一：java，同时包含should中所有的关键字，hadoop，elasticsearch\r\n排名第二：java，同时包含should中的elasticsearch\r\n排名第三：java，不包含should中的任何关键字\r\n\r\nshould是可以影响相关度分数的\r\n\r\nmust是确保说，谁必须有这个关键字，同时会根据这个must的条件去计算出document对这个搜索条件的relevance score\r\n在满足must的基础之上，should中的条件，不匹配也可以，但是如果匹配的更多，那么document的relevance score就会更高\r\n\r\n{\r\n  &quot;took&quot;: 6,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1.3375794,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 1.3375794,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;,\r\n          &quot;userID&quot;: 2,\r\n          &quot;hidden&quot;: true,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 80,\r\n          &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.53484553,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.19856805,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n7、搜索java，hadoop，spark，elasticsearch，至少包含其中3个关键字\r\n\r\n默认情况下，should是可以不匹配任何一个的，比如上面的搜索中，this is java blog，就不匹配任何一个should条件\r\n但是有个例外的情况，如果没有must的话，那么should中必须至少匹配一个才可以\r\n比如下面的搜索，should中有4个条件，默认情况下，只要满足其中一个条件，就可以匹配作为结果返回\r\n\r\n但是可以精准控制，should的4个条件中，至少匹配几个才能作为结果返回\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n        { &quot;match&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }},\r\n        { &quot;match&quot;: { &quot;title&quot;: &quot;hadoop&quot;   }},\r\n	{ &quot;match&quot;: { &quot;title&quot;: &quot;spark&quot;   }}\r\n      ],\r\n      &quot;minimum_should_match&quot;: 3 \r\n    }\r\n  }\r\n}\r\n\r\n梳理一下学到的知识点\r\n\r\n1、全文检索的时候，进行多个值的检索，有两种做法，match query；should\r\n2、控制搜索结果精准度：and operator，minimum_should_match\r\n\r\n',8,0,0,1514612202,0,0,0),(99,1,'bool should','','','1、普通match如何转换为term+should\r\n\r\n{\r\n    &quot;match&quot;: { &quot;title&quot;: &quot;java elasticsearch&quot;}\r\n}\r\n\r\n使用诸如上面的match query进行多值搜索的时候，es会在底层自动将这个match query转换为bool的语法\r\nbool should，指定多个搜索词，同时使用term query\r\n\r\n{\r\n  &quot;bool&quot;: {\r\n    &quot;should&quot;: [\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }}\r\n    ]\r\n  }\r\n}\r\n\r\n2、and match如何转换为term+must\r\n\r\n{\r\n    &quot;match&quot;: {\r\n        &quot;title&quot;: {\r\n            &quot;query&quot;:    &quot;java elasticsearch&quot;,\r\n            &quot;operator&quot;: &quot;and&quot;\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;bool&quot;: {\r\n    &quot;must&quot;: [\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }}\r\n    ]\r\n  }\r\n}\r\n\r\n3、minimum_should_match如何转换\r\n\r\n{\r\n    &quot;match&quot;: {\r\n        &quot;title&quot;: {\r\n            &quot;query&quot;:                &quot;java elasticsearch hadoop spark&quot;,\r\n            &quot;minimum_should_match&quot;: &quot;75%&quot;\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;bool&quot;: {\r\n    &quot;should&quot;: [\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;hadoop&quot; }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;spark&quot; }}\r\n    ],\r\n    &quot;minimum_should_match&quot;: 3 \r\n  }\r\n}\r\n\r\n上一讲，为啥要讲解两种实现multi-value搜索的方式呢？实际上，就是给这一讲进行铺垫的。match query --&gt; bool + term。\r\n\r\n',8,0,0,1514612290,0,0,0),(100,1,'搜索标题中包含java的帖子','','','需求：搜索标题中包含java的帖子，同时呢，如果标题中包含hadoop或elasticsearch就优先搜索出来，同时呢，如果一个帖子包含java hadoop，一个帖子包含java elasticsearch，包含hadoop的帖子要比elasticsearch优先搜索出来\r\n\r\n知识点，搜索条件的权重，boost，可以将某个搜索条件的权重加大，此时当匹配这个搜索条件和匹配另一个搜索条件的document，计算relevance score时，匹配权重更大的搜索条件的document，relevance score会更高，当然也就会优先被返回回来\r\n\r\n默认情况下，搜索条件的权重都是一样的，都是1\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;blog&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;java&quot;\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;hadoop&quot;\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;elasticsearch&quot;\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;spark&quot;,\r\n              &quot;boost&quot;: 5\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514612368,0,0,0),(101,1,'多shard场景','','','1、多shard场景下relevance score不准确问题大揭秘\r\n\r\n如果你的一个index有多个shard的话，可能搜索结果会不准确\r\n\r\n图解\r\n\r\n2、如何解决该问题？\r\n\r\n（1）生产环境下，数据量大，尽可能实现均匀分配\r\n\r\n数据量很大的话，其实一般情况下，在概率学的背景下，es都是在多个shard中均匀路由数据的，路由的时候根据_id，负载均衡\r\n比如说有10个document，title都包含java，一共有5个shard，那么在概率学的背景下，如果负载均衡的话，其实每个shard都应该有2个doc，title包含java\r\n如果说数据分布均匀的话，其实就没有刚才说的那个问题了\r\n\r\n（2）测试环境下，将索引的primary shard设置为1个，number_of_shards=1，index settings\r\n\r\n如果说只有一个shard，那么当然，所有的document都在这个shard里面，就没有这个问题了\r\n\r\n（3）测试环境下，搜索附带search_type=dfs_query_then_fetch参数，会将local IDF取出来计算global IDF\r\n\r\n计算一个doc的相关度分数的时候，就会将所有shard对的local IDF计算一下，获取出来，在本地进行global IDF分数的计算，会将所有shard的doc作为上下文来进行计算，也能确保准确性。但是production生产环境下，不推荐这个参数，因为性能很差。\r\n',8,0,0,1514612451,0,0,0),(102,1,'为帖子数据增加content字段','','','1、为帖子数据增加content字段\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;i like to write best elasticsearch article&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;i think java is the best programming language&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;i am only an elasticsearch beginner&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;elasticsearch and hadoop are all very good solution, i am a beginner&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;} }\r\n\r\n2、搜索title或content中包含java或solution的帖子\r\n\r\n下面这个就是multi-field搜索，多字段搜索\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;bool&quot;: {\r\n            &quot;should&quot;: [\r\n                { &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }},\r\n                { &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}\r\n            ]\r\n        }\r\n    }\r\n}\r\n\r\n3、结果分析\r\n\r\n期望的是doc5，结果是doc2,doc4排在了前面\r\n\r\n计算每个document的relevance score：每个query的分数，乘以matched query数量，除以总query数量\r\n\r\n算一下doc4的分数\r\n\r\n{ &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }}，针对doc4，是有一个分数的\r\n{ &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}，针对doc4，也是有一个分数的\r\n\r\n所以是两个分数加起来，比如说，1.1 + 1.2 = 2.3\r\nmatched query数量 = 2\r\n总query数量 = 2\r\n\r\n2.3 * 2 / 2 = 2.3\r\n\r\n算一下doc5的分数\r\n\r\n{ &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }}，针对doc5，是没有分数的\r\n{ &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}，针对doc5，是有一个分数的\r\n\r\n所以说，只有一个query是有分数的，比如2.3\r\nmatched query数量 = 1\r\n总query数量 = 2\r\n\r\n2.3 * 1 / 2 = 1.15\r\n\r\ndoc5的分数 = 1.15 &lt; doc4的分数 = 2.3\r\n\r\n4、best fields策略，dis_max\r\n\r\nbest fields策略，就是说，搜索到的结果，应该是某一个field中匹配到了尽可能多的关键词，被排在前面；而不是尽可能多的field匹配到了少数的关键词，排在了前面\r\n\r\ndis_max语法，直接取多个query中，分数最高的那一个query的分数即可\r\n\r\n{ &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }}，针对doc4，是有一个分数的，1.1\r\n{ &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}，针对doc4，也是有一个分数的，1.2\r\n取最大分数，1.2\r\n\r\n{ &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }}，针对doc5，是没有分数的\r\n{ &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}，针对doc5，是有一个分数的，2.3\r\n取最大分数，2.3\r\n\r\n然后doc4的分数 = 1.2 &lt; doc5的分数 = 2.3，所以doc5就可以排在更前面的地方，符合我们的需要\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;dis_max&quot;: {\r\n            &quot;queries&quot;: [\r\n                { &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }},\r\n                { &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}\r\n            ]\r\n        }\r\n    }\r\n}',8,0,0,1514612556,0,0,0),(103,1,'搜索title或content中包含java beginne','','','1、搜索title或content中包含java beginner的帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;dis_max&quot;: {\r\n            &quot;queries&quot;: [\r\n                { &quot;match&quot;: { &quot;title&quot;: &quot;java beginner&quot; }},\r\n                { &quot;match&quot;: { &quot;body&quot;:  &quot;java beginner&quot; }}\r\n            ]\r\n        }\r\n    }\r\n}\r\n\r\n有些场景不是太好复现的，因为是这样，你需要尝试去构造不同的文本，然后去构造一些搜索出来，去达到你要的一个效果\r\n\r\n可能在实际场景中出现的一个情况是这样的：\r\n\r\n（1）某个帖子，doc1，title中包含java，content不包含java beginner任何一个关键词\r\n（2）某个帖子，doc2，content中包含beginner，title中不包含任何一个关键词\r\n（3）某个帖子，doc3，title中包含java，content中包含beginner\r\n（4）最终搜索，可能出来的结果是，doc1和doc2排在doc3的前面，而不是我们期望的doc3排在最前面\r\n\r\ndis_max，只是取分数最高的那个query的分数而已。\r\n\r\n2、dis_max只取某一个query最大的分数，完全不考虑其他query的分数\r\n\r\n3、使用tie_breaker将其他query的分数也考虑进去\r\n\r\ntie_breaker参数的意义，在于说，将其他query的分数，乘以tie_breaker，然后综合与最高分数的那个query的分数，综合在一起进行计算\r\n除了取最高分以外，还会考虑其他的query的分数\r\ntie_breaker的值，在0~1之间，是个小数，就ok\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;dis_max&quot;: {\r\n            &quot;queries&quot;: [\r\n                { &quot;match&quot;: { &quot;title&quot;: &quot;java beginner&quot; }},\r\n                { &quot;match&quot;: { &quot;body&quot;:  &quot;java beginner&quot; }}\r\n            ],\r\n            &quot;tie_breaker&quot;: 0.3\r\n        }\r\n    }\r\n}',8,0,0,1514612600,0,0,0),(104,1,'minimum_should_match','','','GET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;multi_match&quot;: {\r\n        &quot;query&quot;:                &quot;java solution&quot;,\r\n        &quot;type&quot;:                 &quot;best_fields&quot;, \r\n        &quot;fields&quot;:               [ &quot;title^2&quot;, &quot;content&quot; ],\r\n        &quot;tie_breaker&quot;:          0.3,\r\n        &quot;minimum_should_match&quot;: &quot;50%&quot; \r\n    }\r\n  } \r\n}\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;dis_max&quot;: {\r\n      &quot;queries&quot;:  [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;java beginner&quot;,\r\n              &quot;minimum_should_match&quot;: &quot;50%&quot;,\r\n	      &quot;boost&quot;: 2\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;body&quot;: {\r\n              &quot;query&quot;: &quot;java beginner&quot;,\r\n              &quot;minimum_should_match&quot;: &quot;30%&quot;\r\n            }\r\n          }\r\n        }\r\n      ],\r\n      &quot;tie_breaker&quot;: 0.3\r\n    }\r\n  } \r\n}\r\n\r\nminimum_should_match，主要是用来干嘛的？\r\n去长尾，long tail\r\n长尾，比如你搜索5个关键词，但是很多结果是只匹配1个关键词的，其实跟你想要的结果相差甚远，这些结果就是长尾\r\nminimum_should_match，控制搜索结果的精准度，只有匹配一定数量的关键词的数据，才能返回\r\n',8,0,0,1514612668,0,0,0),(105,1,'从best-fields换成most-fields策略','','','从best-fields换成most-fields策略\r\nbest-fields策略，主要是说将某一个field匹配尽可能多的关键词的doc优先返回回来\r\nmost-fields策略，主要是说尽可能返回更多field匹配到某个关键词的doc，优先返回回来\r\n\r\nPOST /forum/_mapping/article\r\n{\r\n  &quot;properties&quot;: {\r\n      &quot;sub_title&quot;: { \r\n          &quot;type&quot;:     &quot;string&quot;,\r\n          &quot;analyzer&quot;: &quot;english&quot;,\r\n          &quot;fields&quot;: {\r\n              &quot;std&quot;:   { \r\n                  &quot;type&quot;:     &quot;string&quot;,\r\n                  &quot;analyzer&quot;: &quot;standard&quot;\r\n              }\r\n          }\r\n      }\r\n  }\r\n}\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;learning more courses&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;learned a lot of course&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;we have a lot of fun&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;both of them are good&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;haha, hello world&quot;} }\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;sub_title&quot;: &quot;learning courses&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 3,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1.219939,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1.219939,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.5063205,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;,\r\n          &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;,\r\n          &quot;sub_title&quot;: &quot;learning more courses&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nsub_title用的是enligsh analyzer，所以还原了单词\r\n\r\n为什么，因为如果我们用的是类似于english analyzer这种分词器的话，就会将单词还原为其最基本的形态，stemmer\r\nlearning --&gt; learn\r\nlearned --&gt; learn\r\ncourses --&gt; course\r\n\r\nsub_titile: learning coureses --&gt; learn course\r\n\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;learned a lot of course&quot;} }，就排在了{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;learning more courses&quot;} }的前面\r\n\r\nGET /forum/article/_search\r\n{\r\n   &quot;query&quot;: {\r\n        &quot;match&quot;: {\r\n            &quot;sub_title&quot;: &quot;learning courses&quot;\r\n        }\r\n    }\r\n}\r\n\r\n\r\n很绕。。。。我自己都觉得很绕\r\n\r\n很多东西，你看文字就觉得很绕，然后用语言去表述，也很绕，但是我觉得，用语言去说，相对来说会好一点点\r\n\r\nGET /forum/article/_search\r\n{\r\n   &quot;query&quot;: {\r\n        &quot;multi_match&quot;: {\r\n            &quot;query&quot;:  &quot;learning courses&quot;,\r\n            &quot;type&quot;:   &quot;most_fields&quot;, \r\n            &quot;fields&quot;: [ &quot;sub_title&quot;, &quot;sub_title.std&quot; ]\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1.219939,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1.219939,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1.012641,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;,\r\n          &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;,\r\n          &quot;sub_title&quot;: &quot;learning more courses&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n你问我，具体的分数怎么算出来的，很难说，因为这个东西很复杂， 还不只是TF/IDF算法。因为不同的query，不同的语法，都有不同的计算score的细节。\r\n\r\n与best_fields的区别\r\n\r\n（1）best_fields，是对多个field进行搜索，挑选某个field匹配度最高的那个分数，同时在多个query最高分相同的情况下，在一定程度上考虑其他query的分数。简单来说，你对多个field进行搜索，就想搜索到某一个field尽可能包含更多关键字的数据\r\n\r\n优点：通过best_fields策略，以及综合考虑其他field，还有minimum_should_match支持，可以尽可能精准地将匹配的结果推送到最前面\r\n缺点：除了那些精准匹配的结果，其他差不多大的结果，排序结果不是太均匀，没有什么区分度了\r\n\r\n实际的例子：百度之类的搜索引擎，最匹配的到最前面，但是其他的就没什么区分度了\r\n\r\n（2）most_fields，综合多个field一起进行搜索，尽可能多地让所有field的query参与到总分数的计算中来，此时就会是个大杂烩，出现类似best_fields案例最开始的那个结果，结果不一定精准，某一个document的一个field包含更多的关键字，但是因为其他document有更多field匹配到了，所以排在了前面；所以需要建立类似sub_title.std这样的field，尽可能让某一个field精准匹配query string，贡献更高的分数，将更精准匹配的数据排到前面\r\n\r\n优点：将尽可能匹配更多field的结果推送到最前面，整个排序结果是比较均匀的\r\n缺点：可能那些精准匹配的结果，无法推送到最前面\r\n\r\n实际的例子：wiki，明显的most_fields策略，搜索结果比较均匀，但是的确要翻好几页才能找到最匹配的结果\r\n',8,0,0,1514612809,0,0,0),(106,1,'cross-fields搜索','','','cross-fields搜索，一个唯一标识，跨了多个field。比如一个人，标识，是姓名；一个建筑，它的标识是地址。姓名可以散落在多个field中，比如first_name和last_name中，地址可以散落在country，province，city中。\r\n\r\n跨多个field搜索一个标识，比如搜索一个人名，或者一个地址，就是cross-fields搜索\r\n\r\n初步来说，如果要实现，可能用most_fields比较合适。因为best_fields是优先搜索单个field最匹配的结果，cross-fields本身就不是一个field的问题了。\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Peter&quot;, &quot;author_last_name&quot; : &quot;Smith&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Smith&quot;, &quot;author_last_name&quot; : &quot;Williams&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Jack&quot;, &quot;author_last_name&quot; : &quot;Ma&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Robbin&quot;, &quot;author_last_name&quot; : &quot;Li&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Tonny&quot;, &quot;author_last_name&quot; : &quot;Peter Smith&quot;} }\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;multi_match&quot;: {\r\n      &quot;query&quot;:       &quot;Peter Smith&quot;,\r\n      &quot;type&quot;:        &quot;most_fields&quot;,\r\n      &quot;fields&quot;:      [ &quot;author_first_name&quot;, &quot;author_last_name&quot; ]\r\n    }\r\n  }\r\n}\r\n\r\nPeter Smith，匹配author_first_name，匹配到了Smith，这时候它的分数很高，为什么啊？？？\r\n因为IDF分数高，IDF分数要高，那么这个匹配到的term（Smith），在所有doc中的出现频率要低，author_first_name field中，Smith就出现过1次\r\nPeter Smith这个人，doc 1，Smith在author_last_name中，但是author_last_name出现了两次Smith，所以导致doc 1的IDF分数较低\r\n\r\n不要有过多的疑问，一定是这样吗？\r\n\r\n\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 0.6931472,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.6931472,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.5753642,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;,\r\n          &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;,\r\n          &quot;sub_title&quot;: &quot;learning more courses&quot;,\r\n          &quot;author_first_name&quot;: &quot;Peter&quot;,\r\n          &quot;author_last_name&quot;: &quot;Smith&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 0.51623213,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n问题1：只是找到尽可能多的field匹配的doc，而不是某个field完全匹配的doc\r\n\r\n问题2：most_fields，没办法用minimum_should_match去掉长尾数据，就是匹配的特别少的结果\r\n\r\n问题3：TF/IDF算法，比如Peter Smith和Smith Williams，搜索Peter Smith的时候，由于first_name中很少有Smith的，所以query在所有document中的频率很低，得到的分数很高，可能Smith Williams反而会排在Peter Smith前面\r\n',8,0,0,1514612846,0,0,0),(107,1,'多个field组合成一个field','','','上一讲，我们其实说了，用most_fields策略，去实现cross-fields搜索，有3大弊端，而且搜索结果也显示出了这3大弊端\r\n\r\n第一个办法：用copy_to，将多个field组合成一个field\r\n\r\n问题其实就出在有多个field，有多个field以后，就很尴尬，我们只要想办法将一个标识跨在多个field的情况，合并成一个field即可。比如说，一个人名，本来是first_name，last_name，现在合并成一个full_name，不就ok了吗。。。。。\r\n\r\nPUT /forum/_mapping/article\r\n{\r\n  &quot;properties&quot;: {\r\n      &quot;new_author_first_name&quot;: {\r\n          &quot;type&quot;:     &quot;string&quot;,\r\n          &quot;copy_to&quot;:  &quot;new_author_full_name&quot; \r\n      },\r\n      &quot;new_author_last_name&quot;: {\r\n          &quot;type&quot;:     &quot;string&quot;,\r\n          &quot;copy_to&quot;:  &quot;new_author_full_name&quot; \r\n      },\r\n      &quot;new_author_full_name&quot;: {\r\n          &quot;type&quot;:     &quot;string&quot;\r\n      }\r\n  }\r\n}\r\n\r\n用了这个copy_to语法之后，就可以将多个字段的值拷贝到一个字段中，并建立倒排索引\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Peter&quot;, &quot;new_author_last_name&quot; : &quot;Smith&quot;} }		--&gt; Peter Smith\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }	\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Smith&quot;, &quot;new_author_last_name&quot; : &quot;Williams&quot;} }		--&gt; Smith Williams\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Jack&quot;, &quot;new_author_last_name&quot; : &quot;Ma&quot;} }			--&gt; Jack Ma\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Robbin&quot;, &quot;new_author_last_name&quot; : &quot;Li&quot;} }			--&gt; Robbin Li\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Tonny&quot;, &quot;new_author_last_name&quot; : &quot;Peter Smith&quot;} }		--&gt; Tonny Peter Smith\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;new_author_full_name&quot;:       &quot;Peter Smith&quot;\r\n    }\r\n  }\r\n}\r\n\r\n很无奈，很多时候，我们很难复现。比如官网也会给一些例子，说用什么什么文本，怎么怎么搜索，是怎么怎么样的效果。es版本在不断迭代，这个打分的算法也在不断的迭代。所以我们其实很难说，对类似这几讲讲解的best_fields，most_fields，cross_fields，完全复现出来应有的场景和效果。\r\n\r\n更多的把原理和知识点给大家讲解清楚，带着大家演练一遍怎么操作的，做一下实验\r\n\r\n期望的是说，比如大家自己在开发搜索应用的时候，碰到需要best_fields的场景，知道怎么做，知道best_fields的原理，可以达到什么效果；碰到most_fields的场景，知道怎么做，以及原理；碰到搜搜cross_fields标识的场景，知道怎么做，知道原理是什么，效果是什么。。。。\r\n\r\n\r\n\r\n问题1：只是找到尽可能多的field匹配的doc，而不是某个field完全匹配的doc --&gt; 解决，最匹配的document被最先返回\r\n\r\n问题2：most_fields，没办法用minimum_should_match去掉长尾数据，就是匹配的特别少的结果 --&gt; 解决，可以使用minimum_should_match去掉长尾数据\r\n\r\n问题3：TF/IDF算法，比如Peter Smith和Smith Williams，搜索Peter Smith的时候，由于first_name中很少有Smith的，所以query在所有document中的频率很低，得到的分数很高，可能Smith Williams反而会排在Peter Smith前面 --&gt; 解决，Smith和Peter在一个field了，所以在所有document中出现的次数是均匀的，不会有极端的偏差\r\n\r\n',8,0,0,1514613241,0,0,0),(108,1,'field匹配的doc','','','GET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;multi_match&quot;: {\r\n      &quot;query&quot;: &quot;Peter Smith&quot;,\r\n      &quot;type&quot;: &quot;cross_fields&quot;, \r\n      &quot;operator&quot;: &quot;and&quot;,\r\n      &quot;fields&quot;: [&quot;author_first_name&quot;, &quot;author_last_name&quot;]\r\n    }\r\n  }\r\n}\r\n\r\n问题1：只是找到尽可能多的field匹配的doc，而不是某个field完全匹配的doc --&gt; 解决，要求每个term都必须在任何一个field中出现\r\n\r\nPeter，Smith\r\n\r\n要求Peter必须在author_first_name或author_last_name中出现\r\n要求Smith必须在author_first_name或author_last_name中出现\r\n\r\nPeter Smith可能是横跨在多个field中的，所以必须要求每个term都在某个field中出现，组合起来才能组成我们想要的标识，完整的人名\r\n\r\n原来most_fiels，可能像Smith Williams也可能会出现，因为most_fields要求只是任何一个field匹配了就可以，匹配的field越多，分数越高\r\n\r\n问题2：most_fields，没办法用minimum_should_match去掉长尾数据，就是匹配的特别少的结果 --&gt; 解决，既然每个term都要求出现，长尾肯定被去除掉了\r\n\r\njava hadoop spark --&gt; 这3个term都必须在任何一个field出现了\r\n\r\n比如有的document，只有一个field中包含一个java，那就被干掉了，作为长尾就没了\r\n\r\n问题3：TF/IDF算法，比如Peter Smith和Smith Williams，搜索Peter Smith的时候，由于first_name中很少有Smith的，所以query在所有document中的频率很低，得到的分数很高，可能Smith Williams反而会排在Peter Smith前面 --&gt; 计算IDF的时候，将每个query在每个field中的IDF都取出来，取最小值，就不会出现极端情况下的极大值了\r\n\r\nPeter Smith\r\n\r\nPeter\r\nSmith\r\n\r\nSmith，在author_first_name这个field中，在所有doc的这个Field中，出现的频率很低，导致IDF分数很高；Smith在所有doc的author_last_name field中的频率算出一个IDF分数，因为一般来说last_name中的Smith频率都较高，所以IDF分数是正常的，不会太高；然后对于Smith来说，会取两个IDF分数中，较小的那个分数。就不会出现IDF分过高的情况。\r\n',8,0,0,1514613285,0,0,0),(109,1,'近似匹配','','','近似匹配\r\n\r\n1、什么是近似匹配\r\n\r\n两个句子\r\n\r\njava is my favourite programming language, and I also think spark is a very good big data system.\r\njava spark are very related, because scala is spark&#039;s programming language and scala is also based on jvm like java.\r\n\r\nmatch query，搜索java spark\r\n\r\n{\r\n	&quot;match&quot;: {\r\n		&quot;content&quot;: &quot;java spark&quot;\r\n	}\r\n}\r\n\r\nmatch query，只能搜索到包含java和spark的document，但是不知道java和spark是不是离的很近\r\n\r\n包含java或包含spark，或包含java和spark的doc，都会被返回回来。我们其实并不知道哪个doc，java和spark距离的比较近。如果我们就是希望搜索java spark，中间不能插入任何其他的字符，那这个时候match去做全文检索，能搞定我们的需求吗？答案是，搞不定。\r\n\r\n如果我们要尽量让java和spark离的很近的document优先返回，要给它一个更高的relevance score，这就涉及到了proximity match，近似匹配\r\n\r\n如果说，要实现两个需求：\r\n\r\n1、java spark，就靠在一起，中间不能插入任何其他字符，就要搜索出来这种doc\r\n2、java spark，但是要求，java和spark两个单词靠的越近，doc的分数越高，排名越靠前\r\n\r\n要实现上述两个需求，用match做全文检索，是搞不定的，必须得用proximity match，近似匹配\r\n\r\nphrase match，proximity match：短语匹配，近似匹配\r\n\r\n这一讲，要学习的是phrase match，就是仅仅搜索出java和spark靠在一起的那些doc，比如有个doc，是java use&#039;d spark，不行。必须是比如java spark are very good friends，是可以搜索出来的。\r\n\r\nphrase match，就是要去将多个term作为一个短语，一起去搜索，只有包含这个短语的doc才会作为结果返回。不像是match，java spark，java的doc也会返回，spark的doc也会返回。\r\n\r\n2、match_phrase\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;java spark&quot;\r\n    }\r\n  }\r\n}\r\n\r\n单单包含java的doc也返回了，不是我们想要的结果\r\n\r\nPOST /forum/article/5/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;\r\n  }\r\n}\r\n\r\n将一个doc的content设置为恰巧包含java spark这个短语\r\n\r\nmatch_phrase语法\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_phrase&quot;: {\r\n            &quot;content&quot;: &quot;java spark&quot;\r\n        }\r\n    }\r\n}\r\n\r\n成功了，只有包含java spark这个短语的doc才返回了，只包含java的doc不会返回\r\n\r\n3、term position\r\n\r\nhello world, java spark		doc1\r\nhi, spark java				doc2\r\n\r\nhello 		doc1(0)		\r\nwolrd		doc1(1)\r\njava		doc1(2) doc2(2)\r\nspark		doc1(3) doc2(1)\r\n\r\n了解什么是分词后的position\r\n\r\nGET _analyze\r\n{\r\n  &quot;text&quot;: &quot;hello world, java spark&quot;,\r\n  &quot;analyzer&quot;: &quot;standard&quot;\r\n}\r\n4、match_phrase的基本原理\r\n\r\n索引中的position，match_phrase\r\n\r\nhello world, java spark		doc1\r\nhi, spark java				doc2\r\n\r\nhello 		doc1(0)		\r\nwolrd		doc1(1)\r\njava		doc1(2) doc2(2)\r\nspark		doc1(3) doc2(1)\r\n\r\njava spark --&gt; match phrase\r\n\r\njava spark --&gt; java和spark\r\n\r\njava --&gt; doc1(2) doc2(2)\r\nspark --&gt; doc1(3) doc2(1)\r\n\r\n要找到每个term都在的一个共有的那些doc，就是要求一个doc，必须包含每个term，才能拿出来继续计算\r\n\r\ndoc1 --&gt; java和spark --&gt; spark position恰巧比java大1 --&gt; java的position是2，spark的position是3，恰好满足条件\r\n\r\ndoc1符合条件\r\n\r\ndoc2 --&gt; java和spark --&gt; java position是2，spark position是1，spark position比java position小1，而不是大1 --&gt; 光是position就不满足，那么doc2不匹配\r\n\r\n必须理解这块原理！！！！\r\n\r\n因为后面的proximity match就是原理跟这个一模一样！！！\r\n',8,0,0,1514613327,0,0,0),(110,1,'slop','','','GET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_phrase&quot;: {\r\n            &quot;title&quot;: {\r\n                &quot;query&quot;: &quot;java spark&quot;,\r\n                &quot;slop&quot;:  1\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nslop的含义是什么？\r\n\r\nquery string，搜索文本，中的几个term，要经过几次移动才能与一个document匹配，这个移动的次数，就是slop\r\n\r\n实际举例，一个query string经过几次移动之后可以匹配到一个document，然后设置slop\r\n\r\nhello world, java is very good, spark is also very good.\r\n\r\njava spark，match phrase，搜不到\r\n\r\n如果我们指定了slop，那么就允许java spark进行移动，来尝试与doc进行匹配\r\n\r\njava		is		very		good		spark		is\r\n\r\njava		spark\r\njava		--&gt;		spark\r\njava				--&gt;			spark\r\njava							--&gt;			spark\r\n\r\n这里的slop，就是3，因为java spark这个短语，spark移动了3次，就可以跟一个doc匹配上了\r\n\r\nslop的含义，不仅仅是说一个query string terms移动几次，跟一个doc匹配上。一个query string terms，最多可以移动几次去尝试跟一个doc匹配上\r\n\r\nslop，设置的是3，那么就ok\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_phrase&quot;: {\r\n            &quot;title&quot;: {\r\n                &quot;query&quot;: &quot;java spark&quot;,\r\n                &quot;slop&quot;:  3\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n就可以把刚才那个doc匹配上，那个doc会作为结果返回\r\n\r\n但是如果slop设置的是2，那么java spark，spark最多只能移动2次，此时跟doc是匹配不上的，那个doc是不会作为结果返回的\r\n\r\n做实验，验证slop的含义\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;query&quot;: &quot;spark data&quot;,\r\n        &quot;slop&quot;: 3\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nspark is best big data solution based on scala ,an programming language similar to java spark\r\n\r\nspark data\r\n	  --&gt; data\r\n	      --&gt; data\r\nspark		  --&gt; data\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;query&quot;: &quot;data spark&quot;,\r\n        &quot;slop&quot;: 5\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nspark		is				best		big			data\r\n\r\ndata		spark\r\n--&gt;			data/spark\r\nspark		&lt;--data\r\nspark		--&gt;				data\r\nspark						--&gt;			data\r\nspark									--&gt;			data\r\n\r\nslop搜索下，关键词离的越近，relevance score就会越高，做实验说明。。。\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1.3728157,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1.3728157,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Smith&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 0.5753642,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Tonny&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.28582606,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;,\r\n          &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;,\r\n          &quot;sub_title&quot;: &quot;learning more courses&quot;,\r\n          &quot;author_first_name&quot;: &quot;Peter&quot;,\r\n          &quot;author_last_name&quot;: &quot;Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Peter&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;query&quot;: &quot;java best&quot;,\r\n        &quot;slop&quot;: 15\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 3,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0.65380025,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.65380025,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Smith&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 0.07111243,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Tonny&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n其实，加了slop的phrase match，就是proximity match，近似匹配\r\n\r\n1、java spark，短语，doc，phrase match\r\n2、java spark，可以有一定的距离，但是靠的越近，越先搜索出来，proximity match\r\n',8,0,0,1514613382,0,0,0),(111,1,'召回率','','','召回率\r\n\r\n比如你搜索一个java spark，总共有100个doc，能返回多少个doc作为结果，就是召回率，recall\r\n\r\n精准度\r\n\r\n比如你搜索一个java spark，能不能尽可能让包含java spark，或者是java和spark离的很近的doc，排在最前面，precision\r\n\r\n直接用match_phrase短语搜索，会导致必须所有term都在doc field中出现，而且距离在slop限定范围内，才能匹配上\r\n\r\nmatch phrase，proximity match，要求doc必须包含所有的term，才能作为结果返回；如果某一个doc可能就是有某个term没有包含，那么就无法作为结果返回\r\n\r\njava spark --&gt; hello world java --&gt; 就不能返回了\r\njava spark --&gt; hello world, java spark --&gt; 才可以返回\r\n\r\n近似匹配的时候，召回率比较低，精准度太高了\r\n\r\n但是有时可能我们希望的是匹配到几个term中的部分，就可以作为结果出来，这样可以提高召回率。同时我们也希望用上match_phrase根据距离提升分数的功能，让几个term距离越近分数就越高，优先返回\r\n\r\n就是优先满足召回率，意思，java spark，包含java的也返回，包含spark的也返回，包含java和spark的也返回；同时兼顾精准度，就是包含java和spark，同时java和spark离的越近的doc排在最前面\r\n\r\n此时可以用bool组合match query和match_phrase query一起，来实现上述效果\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: {\r\n        &quot;match&quot;: { \r\n          &quot;title&quot;: {\r\n            &quot;query&quot;:                &quot;java spark&quot; --&gt; java或spark或java spark，java和spark靠前，但是没法区分java和spark的距离，也许java和spark靠的很近，但是没法排在最前面\r\n          }\r\n        }\r\n      },\r\n      &quot;should&quot;: {\r\n        &quot;match_phrase&quot;: { --&gt; 在slop以内，如果java spark能匹配上一个doc，那么就会对doc贡献自己的relevance score，如果java和spark靠的越近，那么就分数越高\r\n          &quot;title&quot;: {\r\n            &quot;query&quot;: &quot;java spark&quot;,\r\n            &quot;slop&quot;:  50\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java spark&quot;\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0.68640786,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.68640786,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;followers&quot;: [\r\n            &quot;Tom&quot;,\r\n            &quot;Jack&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 0.68324494,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;followers&quot;: [\r\n            &quot;Jack&quot;,\r\n            &quot;Robbin Li&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java spark&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match_phrase&quot;: {\r\n            &quot;content&quot;: {\r\n              &quot;query&quot;: &quot;java spark&quot;,\r\n              &quot;slop&quot;: 50\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1.258609,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 1.258609,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;followers&quot;: [\r\n            &quot;Jack&quot;,\r\n            &quot;Robbin Li&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.68640786,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;followers&quot;: [\r\n            &quot;Tom&quot;,\r\n            &quot;Jack&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514613536,0,0,0),(112,1,'match和phrase match(proximity m','','','match和phrase match(proximity match)区别\r\n\r\nmatch --&gt; 只要简单的匹配到了一个term，就可以理解将term对应的doc作为结果返回，扫描倒排索引，扫描到了就ok\r\n\r\nphrase match --&gt; 首先扫描到所有term的doc list; 找到包含所有term的doc list; 然后对每个doc都计算每个term的position，是否符合指定的范围; slop，需要进行复杂的运算，来判断能否通过slop移动，匹配一个doc\r\n\r\nmatch query的性能比phrase match和proximity match（有slop）要高很多。因为后两者都要计算position的距离。\r\nmatch query比phrase match的性能要高10倍，比proximity match的性能要高20倍。\r\n\r\n但是别太担心，因为es的性能一般都在毫秒级别，match query一般就在几毫秒，或者几十毫秒，而phrase match和proximity match的性能在几十毫秒到几百毫秒之间，所以也是可以接受的。\r\n\r\n优化proximity match的性能，一般就是减少要进行proximity match搜索的document数量。主要思路就是，用match query先过滤出需要的数据，然后再用proximity match来根据term距离提高doc的分数，同时proximity match只针对每个shard的分数排名前n个doc起作用，来重新调整它们的分数，这个过程称之为rescoring，重计分。因为一般用户会分页查询，只会看到前几页的数据，所以不需要对所有结果进行proximity match操作。\r\n\r\n用我们刚才的说法，match + proximity match同时实现召回率和精准度\r\n\r\n默认情况下，match也许匹配了1000个doc，proximity match全都需要对每个doc进行一遍运算，判断能否slop移动匹配上，然后去贡献自己的分数\r\n但是很多情况下，match出来也许1000个doc，其实用户大部分情况下是分页查询的，所以可能最多只会看前几页，比如一页是10条，最多也许就看5页，就是50条\r\nproximity match只要对前50个doc进行slop移动去匹配，去贡献自己的分数即可，不需要对全部1000个doc都去进行计算和贡献分数\r\n\r\nrescore：重打分\r\n\r\nmatch：1000个doc，其实这时候每个doc都有一个分数了; proximity match，前50个doc，进行rescore，重打分，即可; 让前50个doc，term举例越近的，排在越前面\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;java spark&quot;\r\n    }\r\n  },\r\n  &quot;rescore&quot;: {\r\n    &quot;window_size&quot;: 50,\r\n    &quot;query&quot;: {\r\n      &quot;rescore_query&quot;: {\r\n        &quot;match_phrase&quot;: {\r\n          &quot;content&quot;: {\r\n            &quot;query&quot;: &quot;java spark&quot;,\r\n            &quot;slop&quot;: 50\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514613593,0,0,0),(113,1,'前缀搜索','','','1、前缀搜索\r\n\r\nC3D0-KD345\r\nC3K5-DFG65\r\nC4I8-UI365\r\n\r\nC3 --&gt; 上面这两个都搜索出来 --&gt; 根据字符串的前缀去搜索\r\n\r\n不用帖子的案例背景，因为比较简单，直接用自己手动建的新索引，给大家演示一下就可以了\r\n\r\nPUT my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;keyword&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET my_index/my_type/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;prefix&quot;: {\r\n      &quot;title&quot;: {\r\n        &quot;value&quot;: &quot;C3&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2、前缀搜索的原理\r\n\r\nprefix query不计算relevance score，与prefix filter唯一的区别就是，filter会cache bitset\r\n\r\n扫描整个倒排索引，举例说明\r\n\r\n前缀越短，要处理的doc越多，性能越差，尽可能用长前缀搜索\r\n\r\n前缀搜索，它是怎么执行的？性能为什么差呢？\r\n\r\nmatch\r\n\r\nC3-D0-KD345\r\nC3-K5-DFG65\r\nC4-I8-UI365\r\n\r\n全文检索\r\n\r\n每个字符串都需要被分词\r\n\r\nc3			doc1,doc2\r\nd0\r\nkd345\r\nk5\r\ndfg65\r\nc4\r\ni8\r\nui365\r\n\r\nc3 --&gt; 扫描倒排索引 --&gt; 一旦扫描到c3，就可以停了，因为带c3的就2个doc，已经找到了 --&gt; 没有必要继续去搜索其他的term了\r\n\r\nmatch性能往往是很高的\r\n\r\n不分词\r\n\r\nC3-D0-KD345\r\nC3-K5-DFG65\r\nC4-I8-UI365\r\n\r\nc3 --&gt; 先扫描到了C3-D0-KD345，很棒，找到了一个前缀带c3的字符串 --&gt; 还是要继续搜索的，因为后面还有一个C3-K5-DFG65，也许还有其他很多的前缀带c3的字符串 --&gt; 你扫描到了一个前缀匹配的term，不能停，必须继续搜索 --&gt; 直到扫描完整个的倒排索引，才能结束\r\n\r\n因为实际场景中，可能有些场景是全文检索解决不了的\r\n\r\nC3D0-KD345\r\nC3K5-DFG65\r\nC4I8-UI365\r\n\r\nc3d0\r\nkd345\r\n\r\nc3 --&gt; match --&gt; 扫描整个倒排索引，能找到吗\r\n\r\nc3 --&gt; 只能用prefix\r\n\r\nprefix性能很差\r\n\r\n3、通配符搜索\r\n\r\n跟前缀搜索类似，功能更加强大\r\n\r\nC3D0-KD345\r\nC3K5-DFG65\r\nC4I8-UI365\r\n\r\n5字符-D任意个字符5\r\n\r\n5?-*5：通配符去表达更加复杂的模糊搜索的语义\r\n\r\nGET my_index/my_type/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;wildcard&quot;: {\r\n      &quot;title&quot;: {\r\n        &quot;value&quot;: &quot;C?K*5&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n?：任意字符\r\n*：0个或任意多个字符\r\n\r\n性能一样差，必须扫描整个倒排索引，才ok\r\n\r\n4、正则搜索\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;regexp&quot;: {\r\n      &quot;title&quot;: &quot;C[0-9].+&quot;\r\n    }\r\n  }\r\n}\r\n\r\nC[0-9].+\r\n\r\n[0-9]：指定范围内的数字\r\n[a-z]：指定范围内的字母\r\n.：一个字符\r\n+：前面的正则表达式可以出现一次或多次\r\n\r\nwildcard和regexp，与prefix原理一致，都会扫描整个索引，性能很差\r\n\r\n主要是给大家介绍一些高级的搜索语法。在实际应用中，能不用尽量别用。性能太差了。\r\n',8,0,0,1514613629,0,0,0),(114,1,'搜索推荐，search as you type','','','搜索推荐，search as you type，搜索提示，解释一下什么意思\r\n\r\nhello w --&gt; 搜索\r\n\r\nhello world\r\nhello we\r\nhello win\r\nhello wind\r\nhello dog\r\nhello cat\r\n\r\nhello w --&gt;\r\n\r\nhello world\r\nhello we\r\nhello win\r\nhello wind\r\n\r\n搜索推荐的功能\r\n\r\n百度 --&gt; elas --&gt; elasticsearch --&gt; elasticsearch权威指南\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase_prefix&quot;: {\r\n      &quot;title&quot;: &quot;hello d&quot;\r\n    }\r\n  }\r\n}\r\n\r\n原理跟match_phrase类似，唯一的区别，就是把最后一个term作为前缀去搜索\r\n\r\nhello就是去进行match，搜索对应的doc\r\nw，会作为前缀，去扫描整个倒排索引，找到所有w开头的doc\r\n然后找到所有doc中，即包含hello，又包含w开头的字符的doc\r\n根据你的slop去计算，看在slop范围内，能不能让hello w，正好跟doc中的hello和w开头的单词的position相匹配\r\n\r\n也可以指定slop，但是只有最后一个term会作为前缀\r\n\r\nmax_expansions：指定prefix最多匹配多少个term，超过这个数量就不继续匹配了，限定性能\r\n\r\n默认情况下，前缀要扫描所有的倒排索引中的term，去查找w打头的单词，但是这样性能太差。可以用max_expansions限定，w前缀最多匹配多少个term，就不再继续搜索倒排索引了。\r\n\r\n尽量不要用，因为，最后一个前缀始终要去扫描大量的索引，性能可能会很差\r\n',8,0,0,1514613705,0,0,0),(115,1,'ngram和index-time搜索推荐原理','','','1、ngram和index-time搜索推荐原理\r\n\r\n什么是ngram\r\n\r\nquick，5种长度下的ngram\r\n\r\nngram length=1，q u i c k\r\nngram length=2，qu ui ic ck\r\nngram length=3，qui uic ick\r\nngram length=4，quic uick\r\nngram length=5，quick\r\n\r\n什么是edge ngram\r\n\r\nquick，anchor首字母后进行ngram\r\n\r\nq\r\nqu\r\nqui\r\nquic\r\nquick\r\n\r\n使用edge ngram将每个单词都进行进一步的分词切分，用切分后的ngram来实现前缀搜索推荐功能\r\n\r\nhello world\r\nhello we\r\n\r\nh\r\nhe\r\nhel\r\nhell\r\nhello		doc1,doc2\r\n\r\nw			doc1,doc2\r\nwo\r\nwor\r\nworl\r\nworld\r\ne			doc2\r\n\r\nhelloworld\r\n\r\nmin ngram = 1\r\nmax ngram = 3\r\n\r\nh\r\nhe\r\nhel\r\n\r\nhello w\r\n\r\nhello --&gt; hello，doc1\r\nw --&gt; w，doc1\r\n\r\ndoc1，hello和w，而且position也匹配，所以，ok，doc1返回，hello world\r\n\r\n搜索的时候，不用再根据一个前缀，然后扫描整个倒排索引了; 简单的拿前缀去倒排索引中匹配即可，如果匹配上了，那么就好了; match，全文检索\r\n\r\n2、实验一下ngram\r\n\r\nPUT /my_index\r\n{\r\n    &quot;settings&quot;: {\r\n        &quot;analysis&quot;: {\r\n            &quot;filter&quot;: {\r\n                &quot;autocomplete_filter&quot;: { \r\n                    &quot;type&quot;:     &quot;edge_ngram&quot;,\r\n                    &quot;min_gram&quot;: 1,\r\n                    &quot;max_gram&quot;: 20\r\n                }\r\n            },\r\n            &quot;analyzer&quot;: {\r\n                &quot;autocomplete&quot;: {\r\n                    &quot;type&quot;:      &quot;custom&quot;,\r\n                    &quot;tokenizer&quot;: &quot;standard&quot;,\r\n                    &quot;filter&quot;: [\r\n                        &quot;lowercase&quot;,\r\n                        &quot;autocomplete_filter&quot; \r\n                    ]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;analyzer&quot;: &quot;autocomplete&quot;,\r\n  &quot;text&quot;: &quot;quick brown&quot;\r\n}\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n      &quot;title&quot;: {\r\n          &quot;type&quot;:     &quot;string&quot;,\r\n          &quot;analyzer&quot;: &quot;autocomplete&quot;,\r\n          &quot;search_analyzer&quot;: &quot;standard&quot;\r\n      }\r\n  }\r\n}\r\n\r\nhello world\r\n\r\nh\r\nhe\r\nhel\r\nhell\r\nhello		\r\n\r\nw			\r\nwo\r\nwor\r\nworl\r\nworld\r\n\r\nhello w\r\n\r\nh\r\nhe\r\nhel\r\nhell\r\nhello	\r\n\r\nw\r\n\r\nhello w --&gt; hello --&gt; w\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase&quot;: {\r\n      &quot;title&quot;: &quot;hello w&quot;\r\n    }\r\n  }\r\n}\r\n\r\n如果用match，只有hello的也会出来，全文检索，只是分数比较低\r\n推荐使用match_phrase，要求每个term都有，而且position刚好靠着1位，符合我们的期望的\r\n\r\n\r\n',8,0,0,1514613746,0,0,0),(116,1,'boolean model','','','1、boolean model\r\n\r\n类似and这种逻辑操作符，先过滤出包含指定term的doc\r\n\r\nquery &quot;hello world&quot; --&gt; 过滤 --&gt; hello / world / hello &amp; world\r\nbool --&gt; must/must not/should --&gt; 过滤 --&gt; 包含 / 不包含 / 可能包含\r\ndoc --&gt; 不打分数 --&gt; 正或反 true or false --&gt; 为了减少后续要计算的doc的数量，提升性能\r\n\r\n2、TF/IDF\r\n\r\n单个term在doc中的分数\r\n\r\nquery: hello world --&gt; doc.content\r\ndoc1: java is my favourite programming language, hello world !!!\r\ndoc2: hello java, you are very good, oh hello world!!!\r\n\r\nhello对doc1的评分\r\n\r\nTF: term frequency \r\n\r\n找到hello在doc1中出现了几次，1次，会根据出现的次数给个分数\r\n一个term在一个doc中，出现的次数越多，那么最后给的相关度评分就会越高\r\n\r\nIDF：inversed document frequency\r\n\r\n找到hello在所有的doc中出现的次数，3次\r\n一个term在所有的doc中，出现的次数越多，那么最后给的相关度评分就会越低\r\n\r\nlength norm\r\n\r\nhello搜索的那个field的长度，field长度越长，给的相关度评分越低; field长度越短，给的相关度评分越高\r\n\r\n最后，会将hello这个term，对doc1的分数，综合TF，IDF，length norm，计算出来一个综合性的分数\r\n\r\nhello world --&gt; doc1 --&gt; hello对doc1的分数，world对doc1的分数 --&gt; 但是最后hello world query要对doc1有一个总的分数 --&gt; vector space model\r\n\r\n3、vector space model\r\n\r\n多个term对一个doc的总分数\r\n\r\nhello world --&gt; es会根据hello world在所有doc中的评分情况，计算出一个query vector，query向量\r\n\r\nhello这个term，给的基于所有doc的一个评分就是2\r\nworld这个term，给的基于所有doc的一个评分就是5\r\n\r\n[2, 5]\r\n\r\nquery vector\r\n\r\ndoc vector，3个doc，一个包含1个term，一个包含另一个term，一个包含2个term\r\n\r\n3个doc\r\n\r\ndoc1：包含hello --&gt; [2, 0]\r\ndoc2：包含world --&gt; [0, 5]\r\ndoc3：包含hello, world --&gt; [2, 5]\r\n\r\n会给每一个doc，拿每个term计算出一个分数来，hello有一个分数，world有一个分数，再拿所有term的分数组成一个doc vector\r\n\r\n画在一个图中，取每个doc vector对query vector的弧度，给出每个doc对多个term的总分数\r\n\r\n每个doc vector计算出对query vector的弧度，最后基于这个弧度给出一个doc相对于query中多个term的总分数\r\n弧度越大，分数月底; 弧度越小，分数越高\r\n\r\n如果是多个term，那么就是线性代数来计算，无法用图表示\r\n',8,0,0,1514613799,0,0,0),(117,1,'boolean mode','','','我们boolean model、TF/IDF、vector space model\r\n\r\n深入讲解TF/IDF算法，在lucene中，底层，到底进行TF/IDF算法计算的一个完整的公式是什么？\r\n\r\n0、boolean model\r\n\r\nquery: hello world\r\n\r\n&quot;match&quot;: {\r\n	&quot;title&quot;: &quot;hello world&quot;\r\n}\r\n\r\n&quot;bool&quot;: {\r\n	&quot;should&quot;: [\r\n		{\r\n			&quot;match&quot;: {\r\n				&quot;title&quot;: &quot;hello&quot;\r\n			}\r\n		},\r\n		{\r\n			&quot;natch&quot;: {\r\n				&quot;title&quot;: &quot;world&quot;\r\n			}\r\n		}\r\n	]\r\n}\r\n\r\n普通multivalue搜索，转换为bool搜索，boolean model\r\n\r\n1、lucene practical scoring function\r\n\r\npractical scoring function，来计算一个query对一个doc的分数的公式，该函数会使用一个公式来计算\r\n\r\nscore(q,d)  =  \r\n            queryNorm(q)  \r\n          · coord(q,d)    \r\n          · ∑ (           \r\n                tf(t in d)   \r\n              · idf(t)2      \r\n              · t.getBoost() \r\n              · norm(t,d)    \r\n            ) (t in q) \r\n\r\nscore(q,d) score(q,d) is the relevance score of document d for query q.\r\n\r\n这个公式的最终结果，就是说是一个query（叫做q），对一个doc（叫做d）的最终的总评分\r\n\r\nqueryNorm(q) is the query normalization factor (new).\r\n\r\nqueryNorm，是用来让一个doc的分数处于一个合理的区间内，不要太离谱，举个例子，一个doc分数是10000，一个doc分数是0.1，你们说好不好，肯定不好\r\n\r\ncoord(q,d) is the coordination factor (new).\r\n\r\n简单来说，就是对更加匹配的doc，进行一些分数上的成倍的奖励\r\n\r\nThe sum of the weights for each term t in the query q for document d.\r\n\r\n∑：求和的符号\r\n\r\n∑ (t in q)：query中每个term，query = hello world，query中的term就包含了hello和world\r\n\r\nquery中每个term对doc的分数，进行求和，多个term对一个doc的分数，组成一个vector space，然后计算吗，就在这一步\r\n\r\ntf(t in d) is the term frequency for term t in document d.\r\n\r\n计算每一个term对doc的分数的时候，就是TF/IDF算法\r\n\r\nidf(t) is the inverse document frequency for term t.\r\n\r\nt.getBoost() is the boost that has been applied to the query (new).\r\n\r\nnorm(t,d) is the field-length norm, combined with the index-time field-level boost, if any. (new).\r\n\r\n2、query normalization factor\r\n\r\nqueryNorm = 1 / √sumOfSquaredWeights\r\n\r\nsumOfSquaredWeights = 所有term的IDF分数之和，开一个平方根，然后做一个平方根分之1\r\n主要是为了将分数进行规范化 --&gt; 开平方根，首先数据就变小了 --&gt; 然后还用1去除以这个平方根，分数就会很小 --&gt; 1.几 / 零点几\r\n分数就不会出现几万，几十万，那样的离谱的分数\r\n\r\n3、query coodination\r\n\r\n奖励那些匹配更多字符的doc更多的分数\r\n\r\nDocument 1 with hello → score: 1.5\r\nDocument 2 with hello world → score: 3.0\r\nDocument 3 with hello world java → score: 4.5\r\n\r\nDocument 1 with hello → score: 1.5 * 1 / 3 = 0.5\r\nDocument 2 with hello world → score: 3.0 * 2 / 3 = 2.0\r\nDocument 3 with hello world java → score: 4.5 * 3 / 3 = 4.5\r\n\r\n把计算出来的总分数 * 匹配上的term数量 / 总的term数量，让匹配不同term/query数量的doc，分数之间拉开差距\r\n\r\n4、field level boost',8,0,0,1514613877,0,0,0),(118,1,'query-time boost','','','之前两节课，我觉得已经很了解整个es的相关度评分的算法了，算法思想，TF/IDF，vector model，boolean model; 实际的公式，query norm，query coordination，boost\r\n\r\n对相关度评分进行调节和优化的常见的4种方法\r\n\r\n1、query-time boost\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;java spark&quot;,\r\n              &quot;boost&quot;: 2\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java spark&quot;\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n2、重构查询结构\r\n\r\n重构查询结果，在es新版本中，影响越来越小了。一般情况下，没什么必要的话，大家不用也行。\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;spark&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;bool&quot;: {\r\n            &quot;should&quot;: [\r\n              {\r\n                &quot;match&quot;: {\r\n                  &quot;content&quot;: &quot;solution&quot;\r\n                }\r\n              },\r\n              {\r\n                &quot;match&quot;: {\r\n                  &quot;content&quot;: &quot;beginner&quot;\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n3、negative boost\r\n\r\n搜索包含java，不包含spark的doc，但是这样子很死板\r\n搜索包含java，尽量不包含spark的doc，如果包含了spark，不会说排除掉这个doc，而是说将这个doc的分数降低\r\n包含了negative term的doc，分数乘以negative boost，分数降低\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;must_not&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;spark&quot;\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;boosting&quot;: {\r\n      &quot;positive&quot;: {\r\n        &quot;match&quot;: {\r\n          &quot;content&quot;: &quot;java&quot;\r\n        }\r\n      },\r\n      &quot;negative&quot;: {\r\n        &quot;match&quot;: {\r\n          &quot;content&quot;: &quot;spark&quot;\r\n        }\r\n      },\r\n      &quot;negative_boost&quot;: 0.2\r\n    }\r\n  }\r\n}\r\n\r\nnegative的doc，会乘以negative_boost，降低分数\r\n\r\n4、constant_score\r\n\r\n如果你压根儿不需要相关度评分，直接走constant_score加filter，所有的doc分数都是1，没有评分的概念了\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;constant_score&quot;: {\r\n            &quot;query&quot;: {\r\n              &quot;match&quot;: {\r\n                &quot;title&quot;: &quot;java&quot;\r\n              }\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;constant_score&quot;: {\r\n            &quot;query&quot;: {\r\n              &quot;match&quot;: {\r\n                &quot;title&quot;: &quot;spark&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514613937,0,0,0),(119,1,'帖子数据增加follower数量','','','我们可以做到自定义一个function_score函数，自己将某个field的值，跟es内置算出来的分数进行运算，然后由自己指定的field来进行分数的增强\r\n\r\n给所有的帖子数据增加follower数量\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 5} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 10} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 25} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 3} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 60} }\r\n\r\n将对帖子搜索得到的分数，跟follower_num进行运算，由follower_num在一定程度上增强帖子的分数\r\n看帖子的人越多，那么帖子的分数就越高\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;function_score&quot;: {\r\n      &quot;query&quot;: {\r\n        &quot;multi_match&quot;: {\r\n          &quot;query&quot;: &quot;java spark&quot;,\r\n          &quot;fields&quot;: [&quot;tile&quot;, &quot;content&quot;]\r\n        }\r\n      },\r\n      &quot;field_value_factor&quot;: {\r\n        &quot;field&quot;: &quot;follower_num&quot;,\r\n        &quot;modifier&quot;: &quot;log1p&quot;,\r\n        &quot;factor&quot;: 0.5\r\n      },\r\n      &quot;boost_mode&quot;: &quot;sum&quot;,\r\n      &quot;max_boost&quot;: 2\r\n    }\r\n  }\r\n}\r\n\r\n如果只有field，那么会将每个doc的分数都乘以follower_num，如果有的doc follower是0，那么分数就会变为0，效果很不好。因此一般会加个log1p函数，公式会变为，new_score = old_score * log(1 + number_of_votes)，这样出来的分数会比较合理\r\n再加个factor，可以进一步影响分数，new_score = old_score * log(1 + factor * number_of_votes)\r\nboost_mode，可以决定分数与指定字段的值如何计算，multiply，sum，min，max，replace\r\nmax_boost，限制计算出来的分数不要超过max_boost指定的值\r\n',8,0,0,1514613993,0,0,0),(120,1,' fuzzy搜索技术','','','搜索的时候，可能输入的搜索文本会出现误拼写的情况\r\n\r\ndoc1: hello world\r\ndoc2: hello java\r\n\r\n搜索：hallo world\r\n\r\nfuzzy搜索技术 --&gt; 自动将拼写错误的搜索文本，进行纠正，纠正以后去尝试匹配索引中的数据\r\n\r\nPOST /my_index/my_type/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 1 }}\r\n{ &quot;text&quot;: &quot;Surprise me!&quot;}\r\n{ &quot;index&quot;: { &quot;_id&quot;: 2 }}\r\n{ &quot;text&quot;: &quot;That was surprising.&quot;}\r\n{ &quot;index&quot;: { &quot;_id&quot;: 3 }}\r\n{ &quot;text&quot;: &quot;I wasn&#039;t surprised.&quot;}\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;fuzzy&quot;: {\r\n      &quot;text&quot;: {\r\n        &quot;value&quot;: &quot;surprize&quot;,\r\n        &quot;fuzziness&quot;: 2\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nsurprize --&gt; 拼写错误 --&gt; surprise --&gt; s -&gt; z\r\n\r\nsurprize --&gt; surprise -&gt; z -&gt; s，纠正一个字母，就可以匹配上，所以在fuziness指定的2范围内\r\nsurprize --&gt; surprised -&gt; z -&gt; s，末尾加个d，纠正了2次，也可以匹配上，在fuziness指定的2范围内\r\nsurprize --&gt; surprising -&gt; z -&gt; s，去掉e，ing，3次，总共要5次，才可以匹配上，始终纠正不了\r\n\r\nfuzzy搜索以后，会自动尝试将你的搜索文本进行纠错，然后去跟文本进行匹配\r\nfuzziness，你的搜索文本最多可以纠正几个字母去跟你的数据进行匹配，默认如果不设置，就是2\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;text&quot;: {\r\n        &quot;query&quot;: &quot;SURPIZE ME&quot;,\r\n        &quot;fuzziness&quot;: &quot;AUTO&quot;,\r\n        &quot;operator&quot;: &quot;and&quot;\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514614036,0,0,0),(121,1,'中文分词','','','之前大家会发现，我们全部是用英文在玩儿。。。好玩儿不好玩儿。。。不好玩儿\r\n\r\n中国人，其实我们用来进行搜索的，绝大多数，都是中文应用，很少做英文的\r\nstandard：没有办法对中文进行合理分词的，只是将每个中文字符一个一个的切割开来，比如说中国人 --&gt; 中 国 人\r\n\r\n英语的也要学：所以说，我们利用核心知识篇的相关的知识，来把es这种英文原生的搜索引擎，先学一下; 因为有些知识点，可能用英文讲更靠谱，因为比如说analyzed，palyed，students --&gt; stemmer，analyze，play，student。有些知识点，仅仅适用于英文，不太适用于中文\r\n\r\n从这一讲开始，大家就会觉得很爽，因为全部都是我们熟悉的中文了，没有英文了，高阶知识点，搜索，聚合，全部是中文了\r\n\r\n在搜索引擎领域，比较成熟和流行的，就是ik分词器\r\n\r\n中国人很喜欢吃油条\r\n\r\nstandard：中 国 人 很 喜 欢 吃 油 条\r\nik：中国人 很 喜欢 吃 油条\r\n\r\n1、在elasticsearch中安装ik中文分词器\r\n\r\n（1）git clone https://github.com/medcl/elasticsearch-analysis-ik\r\n（2）git checkout tags/v5.2.0\r\n（3）mvn package\r\n（4）将target/releases/elasticsearch-analysis-ik-5.2.0.zip拷贝到es/plugins/ik目录下\r\n（5）在es/plugins/ik下对elasticsearch-analysis-ik-5.2.0.zip进行解压缩\r\n在plugins目录下创建ik目录 --- 然后进行解压缩 --- 会出现以下内容\r\nconfig\r\ncommons-codec-1.9.jar\r\ncommons-logging-1.2.jar\r\nelasticsearch-analysis-ik-5.2.0.jar\r\nhttpclient-4.5.2.jar\r\nhttpcore-4.4.4jar\r\n（6）重启es\r\n\r\n2、ik分词器基础知识\r\n\r\n两种analyzer，你根据自己的需要自己选吧，但是一般是选用ik_max_word\r\n\r\nik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合；\r\n\r\nik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。\r\n\r\n共和国 --&gt; 中华人民共和国和国歌，搜到吗？？？？\r\n\r\n3、ik分词器的使用\r\n\r\nPUT /my_index \r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;text&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPOST /my_index/my_type/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;text&quot;: &quot;男子偷上万元发红包求交女友 被抓获时仍然单身&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;text&quot;: &quot;16岁少女为结婚“变”22岁 7年后想离婚被法院拒绝&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;text&quot;: &quot;深圳女孩骑车逆行撞奔驰 遭索赔被吓哭(图)&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;text&quot;: &quot;女人对护肤品比对男票好？网友神怼&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;text&quot;: &quot;为什么国内的街道招牌用的都是红黄配？&quot; }\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;text&quot;: &quot;男子偷上万元发红包求交女友 被抓获时仍然单身&quot;,\r\n  &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n}\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;text&quot;: &quot;16岁少女结婚好还是单身好？&quot;\r\n    }\r\n  }\r\n}',8,0,0,1514614361,0,0,0),(122,1,'ik配置文件','','','1、ik配置文件\r\n\r\nik配置文件地址：es/plugins/ik/config目录\r\n\r\nIKAnalyzer.cfg.xml：用来配置自定义词库\r\nmain.dic：ik原生内置的中文词库，总共有27万多条，只要是这些单词，都会被分在一起\r\nquantifier.dic：放了一些单位相关的词\r\nsuffix.dic：放了一些后缀\r\nsurname.dic：中国的姓氏\r\nstopword.dic：英文停用词\r\n\r\nik原生最重要的两个配置文件\r\n\r\nmain.dic：包含了原生的中文词语，会按照这个里面的词语去分词\r\nstopword.dic：包含了英文的停用词\r\n\r\n停用词，stopword\r\n\r\na the and at but\r\n\r\n一般，像停用词，会在分词的时候，直接被干掉，不会建立在倒排索引中\r\n\r\n2、自定义词库\r\n\r\n（1）自己建立词库：每年都会涌现一些特殊的流行词，网红，蓝瘦香菇，喊麦，鬼畜，一般不会在ik的原生词典里\r\n\r\n自己补充自己的最新的词语，到ik的词库里面去\r\n\r\nIKAnalyzer.cfg.xml：ext_dict，custom/mydict.dic\r\n\r\n补充自己的词语，然后需要重启es，才能生效\r\n\r\n（2）自己建立停用词库：比如了，的，啥，么，我们可能并不想去建立索引，让人家搜索\r\n\r\ncustom/ext_stopword.dic，已经有了常用的中文停用词，可以补充自己的停用词，然后重启es\r\n',8,0,0,1514614473,0,0,0),(123,1,'热更新','','','热更新\r\n\r\n每次都是在es的扩展词典中，手动添加新词语，很坑\r\n（1）每次添加完，都要重启es才能生效，非常麻烦\r\n（2）es是分布式的，可能有数百个节点，你不能每次都一个一个节点上面去修改\r\n\r\nes不停机，直接我们在外部某个地方添加新的词语，es中立即热加载到这些新词语\r\n\r\n热更新的方案\r\n\r\n（1）修改ik分词器源码，然后手动支持从mysql中每隔一定时间，自动加载新的词库\r\n（2）基于ik分词器原生支持的热更新方案，部署一个web服务器，提供一个http接口，通过modified和tag两个http响应头，来提供词语的热更新\r\n\r\n用第一种方案，第二种，ik git社区官方都不建议采用，觉得不太稳定\r\n\r\n1、下载源码\r\n\r\nhttps://github.com/medcl/elasticsearch-analysis-ik/tree/v5.2.0\r\n\r\nik分词器，是个标准的java maven工程，直接导入eclipse就可以看到源码\r\n\r\n2、修改源码\r\n\r\nDictionary类，169行：Dictionary单例类的初始化方法，在这里需要创建一个我们自定义的线程，并且启动它\r\nHotDictReloadThread类：就是死循环，不断调用Dictionary.getSingleton().reLoadMainDict()，去重新加载词典\r\nDictionary类，389行：this.loadMySQLExtDict();\r\nDictionary类，683行：this.loadMySQLStopwordDict();\r\n\r\n3、mvn package打包代码\r\n\r\ntarget\\releases\\elasticsearch-analysis-ik-5.2.0.zip\r\n\r\n4、解压缩ik压缩包\r\n\r\n将mysql驱动jar，放入ik的目录下\r\n\r\n5、修改jdbc相关配置\r\ncreate table hot_words(\r\n    id int unsigned not null auto_increment primary key,\r\n	word varchar(50) not null default &#039;&#039; comment &#039;热词&#039;\r\n)engine=innodb default charset=utf8 comment=&#039;热词表&#039;;\r\n\r\ncreate table hot_stopwords(\r\n    id int unsigned not null auto_increment primary key,\r\n	stopword varchar(50) not null default &#039;&#039; comment &#039;停用词&#039;\r\n)engine=innodb default charset=utf8 comment=&#039;停用词表&#039;;\r\n\r\n\r\n6、重启es\r\n\r\n观察日志，日志中就会显示我们打印的那些东西，比如加载了什么配置，加载了什么词语，什么停用词\r\n\r\n7、在mysql中添加词库与停用词\r\n\r\n8、分词实验，验证热更新生效',8,0,0,1514614518,0,0,0),(124,1,'文本编辑器介绍','','','1、文本编辑器介绍\r\n\r\n（1）windows操作系统，原生的txt文本编辑器，一些json格式，不太方便去调整\r\n（2）notepad++，功能不是太丰富\r\n（3）sublime，整个功能也比较丰富，比较好，自己可以上网去下载，官网，免费的\r\n\r\n2、两个核心概念：bucket和metric\r\n\r\nbucket：一个数据分组\r\n\r\ncity name\r\n\r\n北京 小李\r\n北京 小王\r\n上海 小张\r\n上海 小丽\r\n上海 小陈\r\n\r\n基于city划分buckets\r\n\r\n划分出来两个bucket，一个是北京bucket，一个是上海bucket\r\n\r\n北京bucket：包含了2个人，小李，小王\r\n上海bucket：包含了3个人，小张，小丽，小陈\r\n\r\n按照某个字段进行bucket划分，那个字段的值相同的那些数据，就会被划分到一个bucket中\r\n\r\n有一些mysql的sql知识的话，聚合，首先第一步就是分组，对每个组内的数据进行聚合分析，分组，就是我们的bucket\r\n\r\nmetric：对一个数据分组执行的统计\r\n\r\n当我们有了一堆bucket之后，就可以对每个bucket中的数据进行聚合分词了，比如说计算一个bucket内所有数据的数量，或者计算一个bucket内所有数据的平均值，最大值，最小值\r\n\r\nmetric，就是对一个bucket执行的某种聚合分析的操作，比如说求平均值，求最大值，求最小值\r\n\r\nselect count(*)\r\nfrom access_log\r\ngroup by user_id\r\n\r\nbucket：group by user_id --&gt; 那些user_id相同的数据，就会被划分到一个bucket中\r\nmetric：count(*)，对每个user_id bucket中所有的数据，计算一个数量\r\n',8,0,0,1514614564,0,0,0),(125,1,'家电卖场案例背景','','','1、家电卖场案例背景\r\n\r\n以一个家电卖场中的电视销售数据为背景，来对各种品牌，各种颜色的电视的销量和销售额，进行各种各样角度的分析\r\n\r\nPUT /tvs\r\n{\r\n	&quot;mappings&quot;: {\r\n		&quot;sales&quot;: {\r\n			&quot;properties&quot;: {\r\n				&quot;price&quot;: {\r\n					&quot;type&quot;: &quot;long&quot;\r\n				},\r\n				&quot;color&quot;: {\r\n					&quot;type&quot;: &quot;keyword&quot;\r\n				},\r\n				&quot;brand&quot;: {\r\n					&quot;type&quot;: &quot;keyword&quot;\r\n				},\r\n				&quot;sold_date&quot;: {\r\n					&quot;type&quot;: &quot;date&quot;\r\n				}\r\n			}\r\n		}\r\n	}\r\n}\r\n\r\nPOST /tvs/sales/_bulk\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 1000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 3000, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2016-05-18&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 1500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-07-02&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 1200, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-08-19&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 8000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;三星&quot;, &quot;sold_date&quot; : &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 2500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2017-02-12&quot; }\r\n\r\n2、统计哪种颜色的电视销量最高\r\n\r\nGET /tvs/sales/_search\r\n{\r\n    &quot;size&quot; : 0,\r\n    &quot;aggs&quot; : { \r\n        &quot;popular_colors&quot; : { \r\n            &quot;terms&quot; : { \r\n              &quot;field&quot; : &quot;color&quot;\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nsize：只获取聚合结果，而不要执行聚合的原始数据\r\naggs：固定语法，要对一份数据执行分组聚合操作\r\npopular_colors：就是对每个aggs，都要起一个名字，这个名字是随机的，你随便取什么都ok\r\nterms：根据字段的值进行分组\r\nfield：根据指定的字段的值进行分组\r\n\r\n{\r\n  &quot;took&quot;: 61,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;popular_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nhits.hits：我们指定了size是0，所以hits.hits就是空的，否则会把执行聚合的那些原始数据给你返回回来\r\naggregations：聚合结果\r\npopular_color：我们指定的某个聚合的名称\r\nbuckets：根据我们指定的field划分出的buckets\r\nkey：每个bucket对应的那个值\r\ndoc_count：这个bucket分组内，有多少个数据\r\n数量，其实就是这种颜色的销量\r\n\r\n每种颜色对应的bucket中的数据的\r\n默认的排序规则：按照doc_count降序排序',8,0,0,1514614642,0,0,0),(126,1,'按照color去分bucket','','','GET /tvs/sales/_search\r\n{\r\n   &quot;size&quot; : 0,\r\n   &quot;aggs&quot;: {\r\n      &quot;colors&quot;: {\r\n         &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;color&quot;\r\n         },\r\n         &quot;aggs&quot;: { \r\n            &quot;avg_price&quot;: { \r\n               &quot;avg&quot;: {\r\n                  &quot;field&quot;: &quot;price&quot; \r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n按照color去分bucket，可以拿到每个color bucket中的数量，这个仅仅只是一个bucket操作，doc_count其实只是es的bucket操作默认执行的一个内置metric\r\n\r\n这一讲，就是除了bucket操作，分组，还要对每个bucket执行一个metric聚合统计操作\r\n\r\n在一个aggs执行的bucket操作（terms），平级的json结构下，再加一个aggs，这个第二个aggs内部，同样取个名字，执行一个metric操作，avg，对之前的每个bucket中的数据的指定的field，price field，求一个平均值\r\n\r\n&quot;aggs&quot;: { \r\n   &quot;avg_price&quot;: { \r\n      &quot;avg&quot;: {\r\n         &quot;field&quot;: &quot;price&quot; \r\n      }\r\n   }\r\n}\r\n\r\n就是一个metric，就是一个对一个bucket分组操作之后，对每个bucket都要执行的一个metric\r\n\r\n第一个metric，avg，求指定字段的平均值\r\n\r\n{\r\n  &quot;took&quot;: 28,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 3250\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2100\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2000\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nbuckets，除了key和doc_count\r\navg_price：我们自己取的metric aggs的名字\r\nvalue：我们的metric计算的结果，每个bucket中的数据的price字段求平均值后的结果\r\n\r\nselect avg(price)\r\nfrom tvs.sales\r\ngroup by color',8,0,0,1514614743,0,0,0),(127,1,'从颜色到品牌进行下钻分析','','','从颜色到品牌进行下钻分析，每种颜色的平均价格，以及找到每种颜色每个品牌的平均价格\r\n\r\n我们可以进行多层次的下钻\r\n\r\n比如说，现在红色的电视有4台，同时这4台电视中，有3台是属于长虹的，1台是属于小米的\r\n\r\n红色电视中的3台长虹的平均价格是多少？\r\n红色电视中的1台小米的平均价格是多少？\r\n\r\n下钻的意思是，已经分了一个组了，比如说颜色的分组，然后还要继续对这个分组内的数据，再分组，比如一个颜色内，还可以分成多个不同的品牌的组，最后对每个最小粒度的分组执行聚合分析操作，这就叫做下钻分析\r\n\r\nes，下钻分析，就要对bucket进行多层嵌套，多次分组\r\n\r\n按照多个维度（颜色+品牌）多层下钻分析，而且学会了每个下钻维度（颜色，颜色+品牌），都可以对每个维度分别执行一次metric聚合操作\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;color_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        },\r\n        &quot;group_by_brand&quot;: {\r\n          &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;brand&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;brand_avg_price&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;price&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 8,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;color_avg_price&quot;: {\r\n            &quot;value&quot;: 3250\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;长虹&quot;,\r\n                &quot;doc_count&quot;: 3,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 1666.6666666666667\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;三星&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 8000\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;color_avg_price&quot;: {\r\n            &quot;value&quot;: 2100\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 1200\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 3000\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;color_avg_price&quot;: {\r\n            &quot;value&quot;: 2000\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 1500\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 2500\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514614857,0,0,0),(128,1,'要学更多的metric','','','要学更多的metric\r\n\r\ncount，avg\r\n\r\ncount：bucket，terms，自动就会有一个doc_count，就相当于是count\r\navg：avg aggs，求平均值\r\nmax：求一个bucket内，指定field值最大的那个数据\r\nmin：求一个bucket内，指定field值最小的那个数据\r\nsum：求一个bucket内，指定field值的总和\r\n\r\n一般来说，90%的常见的数据分析的操作，metric，无非就是count，avg，max，min，sum\r\nm\r\nGET /tvs/sales/_search\r\n{\r\n   &quot;size&quot; : 0,\r\n   &quot;aggs&quot;: {\r\n      &quot;colors&quot;: {\r\n         &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;color&quot;\r\n         },\r\n         &quot;aggs&quot;: {\r\n            &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } },\r\n            &quot;min_price&quot; : { &quot;min&quot;: { &quot;field&quot;: &quot;price&quot;} }, \r\n            &quot;max_price&quot; : { &quot;max&quot;: { &quot;field&quot;: &quot;price&quot;} },\r\n            &quot;sum_price&quot; : { &quot;sum&quot;: { &quot;field&quot;: &quot;price&quot; } } \r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n求总和，就可以拿到一个颜色下的所有电视的销售总额\r\n\r\n{\r\n  &quot;took&quot;: 16,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;max_price&quot;: {\r\n            &quot;value&quot;: 8000\r\n          },\r\n          &quot;min_price&quot;: {\r\n            &quot;value&quot;: 1000\r\n          },\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 3250\r\n          },\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 13000\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;max_price&quot;: {\r\n            &quot;value&quot;: 3000\r\n          },\r\n          &quot;min_price&quot;: {\r\n            &quot;value&quot;:\r\n          }, 1200\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2100\r\n          },\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 4200\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;max_price&quot;: {\r\n            &quot;value&quot;: 2500\r\n          },\r\n          &quot;min_price&quot;: {\r\n            &quot;value&quot;: 1500\r\n          },\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2000\r\n          },\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 4000\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514614927,0,0,0),(129,1,'histogram','','','histogram：类似于terms，也是进行bucket分组操作，接收一个field，按照这个field的值的各个范围区间，进行bucket分组操作\r\n\r\n&quot;histogram&quot;:{ \r\n  &quot;field&quot;: &quot;price&quot;,\r\n  &quot;interval&quot;: 2000\r\n},\r\n\r\ninterval：2000，划分范围，0~2000，2000~4000，4000~6000，6000~8000，8000~10000，buckets\r\n\r\n去根据price的值，比如2500，看落在哪个区间内，比如2000~4000，此时就会将这条数据放入2000~4000对应的那个bucket中\r\n\r\nbucket划分的方法，terms，将field值相同的数据划分到一个bucket中\r\n\r\nbucket有了之后，一样的，去对每个bucket执行avg，count，sum，max，min，等各种metric操作，聚合分析\r\n\r\nGET /tvs/sales/_search\r\n{\r\n   &quot;size&quot; : 0,\r\n   &quot;aggs&quot;:{\r\n      &quot;price&quot;:{\r\n         &quot;histogram&quot;:{ \r\n            &quot;field&quot;: &quot;price&quot;,\r\n            &quot;interval&quot;: 2000\r\n         },\r\n         &quot;aggs&quot;:{\r\n            &quot;revenue&quot;: {\r\n               &quot;sum&quot;: { \r\n                 &quot;field&quot; : &quot;price&quot;\r\n               }\r\n             }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 13,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_price&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: 0,\r\n          &quot;doc_count&quot;: 3,\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 3700\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: 2000,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 9500\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: 4000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: 6000,\r\n          &quot;doc_count: {\r\n            &quot;value&quot;:&quot;: 0,\r\n          &quot;sum_price&quot; 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: 8000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 8000\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514614998,0,0,0),(130,1,'bucket，分组操作','','','bucket，分组操作，histogram，按照某个值指定的interval，划分一个一个的bucket\r\n\r\ndate histogram，按照我们指定的某个date类型的日期field，以及日期interval，按照一定的日期间隔，去划分bucket\r\n\r\ndate interval = 1m，\r\n\r\n2017-01-01~2017-01-31，就是一个bucket\r\n2017-02-01~2017-02-28，就是一个bucket\r\n\r\n然后会去扫描每个数据的date field，判断date落在哪个bucket中，就将其放入那个bucket\r\n\r\n2017-01-05，就将其放入2017-01-01~2017-01-31，就是一个bucket\r\n\r\nmin_doc_count：即使某个日期interval，2017-01-01~2017-01-31中，一条数据都没有，那么这个区间也是要返回的，不然默认是会过滤掉这个区间的\r\nextended_bounds，min，max：划分bucket的时候，会限定在这个起始日期，和截止日期内\r\n\r\nGET /tvs/sales/_search\r\n{\r\n   &quot;size&quot; : 0,\r\n   &quot;aggs&quot;: {\r\n      &quot;sales&quot;: {\r\n         &quot;date_histogram&quot;: {\r\n            &quot;field&quot;: &quot;sold_date&quot;,\r\n            &quot;interval&quot;: &quot;month&quot;, \r\n            &quot;format&quot;: &quot;yyyy-MM-dd&quot;,\r\n            &quot;min_doc_count&quot; : 0, \r\n            &quot;extended_bounds&quot; : { \r\n                &quot;min&quot; : &quot;2016-01-01&quot;,\r\n                &quot;max&quot; : &quot;2017-12-31&quot;\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 16,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_sold_date&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-01-01&quot;,\r\n          &quot;key&quot;: 1451606400000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-02-01&quot;,\r\n          &quot;key&quot;: 1454284800000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-03-01&quot;,\r\n          &quot;key&quot;: 1456790400000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-04-01&quot;,\r\n          &quot;key&quot;: 1459468800000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-05-01&quot;,\r\n          &quot;key&quot;: 1462060800000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-06-01&quot;,\r\n          &quot;key&quot;: 1464739200000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-07-01&quot;,\r\n          &quot;key&quot;: 1467331200000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key_as_strin\r\n          &quot;key_as_string&quot;: &quot;2016-09-01&quot;,\r\n          &quot;key&quot;: 1472688000000,\r\n          &quot;doc_count&quot;: 0\r\n        },g&quot;: &quot;2016-08-01&quot;,\r\n          &quot;key&quot;: 1470009600000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-10-01&quot;,\r\n          &quot;key&quot;: 1475280000000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-11-01&quot;,\r\n          &quot;key&quot;: 1477958400000,\r\n          &quot;doc_count&quot;: 2\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-12-01&quot;,\r\n          &quot;key&quot;: 1480550400000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-01-01&quot;,\r\n          &quot;key&quot;: 1483228800000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-02-01&quot;,\r\n          &quot;key&quot;: 1485907200000,\r\n          &quot;doc_count&quot;: 1\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615043,0,0,0),(131,1,'aggs','','','GET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_sold_date&quot;: {\r\n      &quot;date_histogram&quot;: {\r\n        &quot;field&quot;: &quot;sold_date&quot;,\r\n        &quot;interval&quot;: &quot;quarter&quot;,\r\n        &quot;format&quot;: &quot;yyyy-MM-dd&quot;,\r\n        &quot;min_doc_count&quot;: 0,\r\n        &quot;extended_bounds&quot;: {\r\n          &quot;min&quot;: &quot;2016-01-01&quot;,\r\n          &quot;max&quot;: &quot;2017-12-31&quot;\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_brand&quot;: {\r\n          &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;brand&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;sum_price&quot;: {\r\n              &quot;sum&quot;: {\r\n                &quot;field&quot;: &quot;price&quot;\r\n              }\r\n            }\r\n          }\r\n        },\r\n        &quot;total_sum_price&quot;: {\r\n          &quot;sum&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 10,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_sold_date&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-01-01&quot;,\r\n          &quot;key&quot;: 1451606400000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: []\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-04-01&quot;,\r\n          &quot;key&quot;: 1459468800000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 3000\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 3000\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-07-01&quot;,\r\n          &quot;key&quot;: 1467331200000,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 2700\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 2,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 2700\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-10-01&quot;,\r\n          &quot;key&quot;: 1475280000000,\r\n          &quot;doc_count&quot;: 3,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 5000\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;长虹&quot;,\r\n                &quot;doc_count&quot;: 3,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 5000\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-01-01&quot;,\r\n          &quot;key&quot;: 1483228800000,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 10500\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;三星&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 8000\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 2500\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-04-01&quot;,\r\n          &quot;key&quot;: 1491004800000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: []\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-07-01&quot;,\r\n          &quot;key&quot;: 1498867200000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: []\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-10-01&quot;,\r\n          &quot;key&quot;: 1506816000000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: []\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615215,0,0,0),(132,1,'es aggregation，scope','','','实际上来说，我们之前学习的搜索相关的知识，完全可以和聚合组合起来使用\r\n\r\nselect count(*)\r\nfrom tvs.sales\r\nwhere brand like &quot;%长%&quot;\r\ngroup by price\r\n\r\nes aggregation，scope，任何的聚合，都必须在搜索出来的结果数据中之行，搜索结果，就是聚合分析操作的scope\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;brand&quot;: {\r\n        &quot;value&quot;: &quot;小米&quot;\r\n      }\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 1\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615302,0,0,0),(133,1,'基于query搜索结果','','','aggregation，scope，一个聚合操作，必须在query的搜索结果范围内执行\r\n\r\n出来两个结果，一个结果，是基于query搜索结果来聚合的; 一个结果，是对所有数据执行聚合的\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0, \r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;brand&quot;: {\r\n        &quot;value&quot;: &quot;长虹&quot;\r\n      }\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;single_brand_avg_price&quot;: {\r\n      &quot;avg&quot;: {\r\n        &quot;field&quot;: &quot;price&quot;\r\n      }\r\n    },\r\n    &quot;all&quot;: {\r\n      &quot;global&quot;: {},\r\n      &quot;aggs&quot;: {\r\n        &quot;all_brand_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nglobal：就是global bucket，就是将所有数据纳入聚合的scope，而不管之前的query\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;all&quot;: {\r\n      &quot;doc_count&quot;: 8,\r\n      &quot;all_brand_avg_price&quot;: {\r\n        &quot;value&quot;: 2650\r\n      }\r\n    },\r\n    &quot;single_brand_avg_price&quot;: {\r\n      &quot;value&quot;: 1666.6666666666667\r\n    }\r\n  }\r\n}\r\n\r\nsingle_brand_avg_price：就是针对query搜索结果，执行的，拿到的，就是长虹品牌的平均价格\r\nall.all_brand_avg_price：拿到所有品牌的平均价格\r\n',8,0,0,1514615361,0,0,0),(134,1,'搜索+聚合 过滤+聚合','','','搜索+聚合\r\n过滤+聚合\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;price&quot;: {\r\n            &quot;gte&quot;: 1200\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;avg_price&quot;: {\r\n      &quot;avg&quot;: {\r\n        &quot;field&quot;: &quot;price&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 41,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 7,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;avg_price&quot;: {\r\n      &quot;value&quot;: 2885.714285714286\r\n    }\r\n  }\r\n}',8,0,0,1514615400,0,0,0),(135,1,'aggs.filter','','','GET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;brand&quot;: {\r\n        &quot;value&quot;: &quot;长虹&quot;\r\n      }\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;recent_150d&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;sold_date&quot;: {\r\n            &quot;gte&quot;: &quot;now-150d&quot;\r\n          }\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;recent_150d_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    },\r\n    &quot;recent_140d&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;sold_date&quot;: {\r\n            &quot;gte&quot;: &quot;now-140d&quot;\r\n          }\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;recent_140d_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    },\r\n    &quot;recent_130d&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;sold_date&quot;: {\r\n            &quot;gte&quot;: &quot;now-130d&quot;\r\n          }\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;recent_130d_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\naggs.filter，针对的是聚合去做的\r\n\r\n如果放query里面的filter，是全局的，会对所有的数据都有影响\r\n\r\n但是，如果，比如说，你要统计，长虹电视，最近1个月的平均值; 最近3个月的平均值; 最近6个月的平均值\r\n\r\nbucket filter：对不同的bucket下的aggs，进行filter\r\n',8,0,0,1514615485,0,0,0),(136,1,'bucket的doc_count降序','','','之前的话，排序，是按照每个bucket的doc_count降序来排的\r\n\r\n但是假如说，我们现在统计出来每个颜色的电视的销售额，需要按照销售额降序排序？？？？\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 3250\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2100\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2000\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;,\r\n        &quot;order&quot;: {\r\n          &quot;avg_price&quot;: &quot;asc&quot;\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514615589,0,0,0),(137,1,'聚合计算二','','','GET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_brand&quot;: {\r\n          &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;brand&quot;,\r\n            &quot;order&quot;: {\r\n              &quot;avg_price&quot;: &quot;desc&quot;\r\n            }\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;avg_price&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;price&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;三星&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 8000\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;长虹&quot;,\r\n                &quot;doc_count&quot;: 3,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 1666.6666666666667\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 3000\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 1200\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 2500\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 1500\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615673,0,0,0),(138,1,'聚合算法：max','','','1、画图讲解易并行聚合算法：max\r\n\r\n有些聚合分析的算法，是很容易就可以并行的，比如说max\r\n\r\n有些聚合分析的算法，是不好并行的，比如说，count(distinct)，并不是说，在每个node上，直接就出一些distinct value，就可以的，因为数据可能会很多\r\n\r\nes会采取近似聚合的方式，就是采用在每个node上进行近估计的方式，得到最终的结论，cuont(distcint)，100万，1050万/95万 --&gt; 5%左右的错误率\r\n近似估计后的结果，不完全准确，但是速度会很快，一般会达到完全精准的算法的性能的数十倍\r\n\r\n2、三角选择原则\r\n\r\n精准+实时+大数据 --&gt; 选择2个\r\n\r\n（1）精准+实时: 没有大数据，数据量很小，那么一般就是单击跑，随便你则么玩儿就可以\r\n（2）精准+大数据：hadoop，批处理，非实时，可以处理海量数据，保证精准，可能会跑几个小时\r\n（3）大数据+实时：es，不精准，近似估计，可能会有百分之几的错误率\r\n\r\n3、近似聚合算法\r\n\r\n如果采取近似估计的算法：延时在100ms左右，0.5%错误\r\n如果采取100%精准的算法：延时一般在5s~几十s，甚至几十分钟，几小时， 0%错误\r\n',8,0,0,1514615714,0,0,0),(139,1,'去重','','','es，去重，cartinality metric，对每个bucket中的指定的field进行去重，取去重后的count，类似于count(distcint)\r\n\r\nGET /tvs/sales/_search\r\n{\r\n  &quot;size&quot; : 0,\r\n  &quot;aggs&quot; : {\r\n      &quot;months&quot; : {\r\n        &quot;date_histogram&quot;: {\r\n          &quot;field&quot;: &quot;sold_date&quot;,\r\n          &quot;interval&quot;: &quot;month&quot;\r\n        },\r\n        &quot;aggs&quot;: {\r\n          &quot;distinct_colors&quot; : {\r\n              &quot;cardinality&quot; : {\r\n                &quot;field&quot; : &quot;brand&quot;\r\n              }\r\n          }\r\n        }\r\n      }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 70,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_sold_date&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-05-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1462060800000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-06-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1464739200000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-07-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1467331200000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-08-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1470009600000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-09-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1472688000000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-10-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1475280000000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-11-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1477958400000,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-12-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1480550400000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-01-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1483228800000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-02-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1485907200000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615764,0,0,0),(140,1,'cardinality，count(distinct)','','','cardinality，count(distinct)，5%的错误率，性能在100ms左右\r\n\r\n1、precision_threshold优化准确率和内存开销\r\n\r\nGET /tvs/sales/_search\r\n{\r\n    &quot;size&quot; : 0,\r\n    &quot;aggs&quot; : {\r\n        &quot;distinct_brand&quot; : {\r\n            &quot;cardinality&quot; : {\r\n              &quot;field&quot; : &quot;brand&quot;,\r\n              &quot;precision_threshold&quot; : 100 \r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nbrand去重，如果brand的unique value，在100个以内，小米，长虹，三星，TCL，HTL。。。\r\n\r\n在多少个unique value以内，cardinality，几乎保证100%准确\r\ncardinality算法，会占用precision_threshold * 8 byte 内存消耗，100 * 8 = 800个字节\r\n占用内存很小。。。而且unique value如果的确在值以内，那么可以确保100%准确\r\n100，数百万的unique value，错误率在5%以内\r\n\r\nprecision_threshold，值设置的越大，占用内存越大，1000 * 8 = 8000 / 1000 = 8KB，可以确保更多unique value的场景下，100%的准确\r\n\r\nfield，去重，count，这时候，unique value，10000，precision_threshold=10000，10000 * 8 = 80000个byte，80KB\r\n\r\n2、HyperLogLog++ (HLL)算法性能优化\r\n\r\ncardinality底层算法：HLL算法，HLL算法的性能\r\n\r\n会对所有的uqniue value取hash值，通过hash值近似去求distcint count，误差\r\n\r\n默认情况下，发送一个cardinality请求的时候，会动态地对所有的field value，取hash值; 将取hash值的操作，前移到建立索引的时候\r\n\r\nPUT /tvs/\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;sales&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;brand&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;fields&quot;: {\r\n            &quot;hash&quot;: {\r\n              &quot;type&quot;: &quot;murmur3&quot; \r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /tvs/sales/_search\r\n{\r\n    &quot;size&quot; : 0,\r\n    &quot;aggs&quot; : {\r\n        &quot;distinct_brand&quot; : {\r\n            &quot;cardinality&quot; : {\r\n              &quot;field&quot; : &quot;brand.hash&quot;,\r\n              &quot;precision_threshold&quot; : 100 \r\n            }\r\n        }\r\n    }\r\n}',8,0,0,1514615837,0,0,0),(141,1,'统计tp50','','','需求：比如有一个网站，记录下了每次请求的访问的耗时，需要统计tp50，tp90，tp99\r\n\r\ntp50：50%的请求的耗时最长在多长时间\r\ntp90：90%的请求的耗时最长在多长时间\r\ntp99：99%的请求的耗时最长在多长时间\r\n\r\nPUT /website\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;logs&quot;: {\r\n            &quot;properties&quot;: {\r\n                &quot;latency&quot;: {\r\n                    &quot;type&quot;: &quot;long&quot;\r\n                },\r\n                &quot;province&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;\r\n                },\r\n                &quot;timestamp&quot;: {\r\n                    &quot;type&quot;: &quot;date&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nPOST /website/logs/_bulk\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 105, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 83, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 92, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 112, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 68, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 76, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 101, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 275, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 166, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 654, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 389, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 302, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n\r\npencentiles\r\n\r\nGET /website/logs/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;latency_percentiles&quot;: {\r\n      &quot;percentiles&quot;: {\r\n        &quot;field&quot;: &quot;latency&quot;,\r\n        &quot;percents&quot;: [\r\n          50,\r\n          95,\r\n          99\r\n        ]\r\n      }\r\n    },\r\n    &quot;latency_avg&quot;: {\r\n      &quot;avg&quot;: {\r\n        &quot;field&quot;: &quot;latency&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 31,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 12,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;latency_avg&quot;: {\r\n      &quot;value&quot;: 201.91666666666666\r\n    },\r\n    &quot;latency_percentiles&quot;: {\r\n      &quot;values&quot;: {\r\n        &quot;50.0&quot;: 108.5,\r\n        &quot;95.0&quot;: 508.24999999999983,\r\n        &quot;99.0&quot;: 624.8500000000001\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n50%的请求，数值的最大的值是多少，不是完全准确的\r\n\r\nGET /website/logs/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_province&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;province&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;latency_percentiles&quot;: {\r\n          &quot;percentiles&quot;: {\r\n            &quot;field&quot;: &quot;latency&quot;,\r\n            &quot;percents&quot;: [\r\n              50,\r\n              95,\r\n              99\r\n            ]\r\n          }\r\n        },\r\n        &quot;latency_avg&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;latency&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 33,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 12,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_province&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;新疆&quot;,\r\n          &quot;doc_count&quot;: 6,\r\n          &quot;latency_avg&quot;: {\r\n            &quot;value&quot;: 314.5\r\n          },\r\n          &quot;latency_percentiles&quot;: {\r\n            &quot;values&quot;: {\r\n              &quot;50.0&quot;: 288.5,\r\n              &quot;95.0&quot;: 587.75,\r\n              &quot;99.0&quot;: 640.75\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;江苏&quot;,\r\n          &quot;doc_count&quot;: 6,\r\n          &quot;latency_avg&quot;: {\r\n            &quot;value&quot;: 89.33333333333333\r\n          },\r\n          &quot;latency_percentiles&quot;: {\r\n            &quot;values&quot;: {\r\n              &quot;50.0&quot;: 87.5,\r\n              &quot;95.0&quot;: 110.25,\r\n              &quot;99.0&quot;: 111.65\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514616114,0,0,0),(142,1,'SLA','','','SLA：就是你提供的服务的标准\r\n\r\n我们的网站的提供的访问延时的SLA，确保所有的请求100%，都必须在200ms以内，大公司内，一般都是要求100%在200ms以内\r\n\r\n如果超过1s，则需要升级到A级故障，代表网站的访问性能和用户体验急剧下降\r\n\r\n需求：在200ms以内的，有百分之多少，在1000毫秒以内的有百分之多少，percentile ranks metric\r\n\r\n这个percentile ranks，其实比pencentile还要常用\r\n\r\n按照品牌分组，计算，电视机，售价在1000占比，2000占比，3000占比\r\n\r\nGET /website/logs/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_province&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;province&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;latency_percentile_ranks&quot;: {\r\n          &quot;percentile_ranks&quot;: {\r\n            &quot;field&quot;: &quot;latency&quot;,\r\n            &quot;values&quot;: [\r\n              200,\r\n              1000\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 38,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 12,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_province&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;新疆&quot;,\r\n          &quot;doc_count&quot;: 6,\r\n          &quot;latency_percentile_ranks&quot;: {\r\n            &quot;values&quot;: {\r\n              &quot;200.0&quot;: 29.40613026819923,\r\n              &quot;1000.0&quot;: 100\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;江苏&quot;,\r\n          &quot;doc_count&quot;: 6,\r\n          &quot;latency_percentile_ranks&quot;: {\r\n            &quot;values&quot;: {\r\n              &quot;200.0&quot;: 100,\r\n              &quot;1000.0&quot;: 100\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\npercentile的优化\r\n\r\nTDigest算法，用很多节点来执行百分比的计算，近似估计，有误差，节点越多，越精准\r\n\r\ncompression\r\n\r\n限制节点数量最多 compression * 20 = 2000个node去计算\r\n\r\n默认100\r\n\r\n越大，占用内存越多，越精准，性能越差\r\n\r\n一个节点占用32字节，100 * 20 * 32 = 64KB\r\n\r\n如果你想要percentile算法越精准，compression可以设置的越大\r\n',8,0,0,1514616727,0,0,0),(143,1,'聚合分析','','','聚合分析的内部原理是什么？？？？aggs，term，metric avg max，执行一个聚合操作的时候，内部原理是怎样的呢？用了什么样的数据结构去执行聚合？是不是用的倒排索引？\r\n\r\n搜索+聚合，写个示例\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n	&quot;query&quot;: {\r\n		&quot;match&quot;: {\r\n			&quot;search_field&quot;: &quot;test&quot;\r\n		}\r\n	},\r\n	&quot;aggs&quot;: {\r\n		&quot;group_by_agg_field&quot;: {\r\n			&quot;terms&quot;: {\r\n				&quot;field&quot;: &quot;agg_field&quot;\r\n			}\r\n		}\r\n	}\r\n}\r\n\r\n纯用倒排索引来实现的弊端\r\n\r\nes肯定不是纯用倒排索引来实现聚合+搜索的\r\n\r\nsearch_field\r\n\r\ndoc1: hello world test1, test2\r\ndoc2: hello test\r\ndoc3: world	test\r\n\r\nhello	doc1,doc2\r\nworld	doc1,doc3\r\ntest1	doc1\r\ntest2	doc1\r\ntest 	doc2,doc3\r\n\r\n&quot;query&quot;: {\r\n	&quot;match&quot;: {\r\n		&quot;search_field&quot;: &quot;test&quot;\r\n	}\r\n}\r\n\r\ntest --&gt; doc2,doc3 --&gt; search result, doc2,doc3\r\n\r\nagg_field\r\n\r\ndoc2: agg1\r\ndoc3: agg2\r\n\r\n\r\n100万个值\r\n...\r\n...\r\n...\r\n...\r\nagg1	doc2\r\nagg2	doc3\r\n\r\ndoc2, doc3, search result --&gt; 实际上，要搜索到doc2的agg_field的值是多少，doc3的agg_field的值是多少\r\n\r\ndoc2和doc3的agg_field的值之后，就可以根据值进行分组，实现terms bucket操作\r\n\r\ndoc2的agg_field的值是多少，这个时候，如果你手上只有一个倒排索引，你该怎么办？？？你要扫描整个倒排索引，去一个一个的搜，拿到每个值，比如说agg1，看一下，它是不是doc2的值，拿到agg2,看一下，是不是doc2的值，直到找到doc2的agg_field的值，在倒排索引中\r\n\r\n如果用纯倒排索引去实现聚合，现实不现实啊？？？性能是很低下的。。。搜索，search，搜倒排索引，搜那个term，就结束了。。。聚合，搜索出了1万个doc，每个doc都要在倒排索引中搜索出它的那个聚合field的值\r\n\r\n倒排索引+正排索引（doc value）的原理和优势\r\n\r\ndoc value：正排索引\r\n\r\nsearch_field\r\n\r\ndoc1: hello world test1, test2\r\ndoc2: hello test\r\ndoc3: world	test\r\n\r\nhello	doc1,doc2\r\nworld	doc1,doc3\r\ntest1	doc1\r\ntest2	doc1\r\ntest 	doc2,doc3\r\n\r\n&quot;query&quot;: {\r\n	&quot;match&quot;: {\r\n		&quot;search_field&quot;: &quot;test&quot;\r\n	}\r\n}\r\n\r\ntest --&gt; doc2,doc3 --&gt; search result, doc2,doc3\r\n\r\ndoc value数据结构，正排索引\r\n\r\n\r\n\r\n...\r\n...\r\n...\r\n100万个\r\ndoc2: agg1\r\ndoc3: agg2\r\n\r\n倒排索引的话，必须遍历完整个倒排索引才可以。。。。\r\n\r\n因为可能你要聚合的那个field的值，是分词的，比如说hello world my name --&gt; 一个doc的聚合field的值可能在倒排索引中对应多个value\r\n\r\n所以说，当你在倒排索引中找到一个值，发现它是属于某个doc的时候，还不能停，必须遍历完整个倒排索引，才能说确保找到了每个doc对应的所有terms，然后进行分组聚合\r\n\r\n...\r\n...\r\n...\r\n100万个\r\ndoc2: agg1 hello world\r\ndoc3: agg2 test hello\r\n\r\n我们有没有必要搜索完整个正排索引啊？？1万个doc --&gt; 搜 -&gt; 可能跟搜索到15000次，就搜索完了，就找到了1万个doc的聚合field的所有值了，然后就可以执行分组聚合操作了\r\n',8,0,0,1514616775,0,0,0),(144,1,'doc value原理','','','1、doc value原理\r\n\r\n（1）index-time生成\r\n\r\nPUT/POST的时候，就会生成doc value数据，也就是正排索引\r\n\r\n（2）核心原理与倒排索引类似\r\n\r\n正排索引，也会写入磁盘文件中，然后呢，os cache先进行缓存，以提升访问doc value正排索引的性能\r\n如果os cache内存大小不足够放得下整个正排索引，doc value，就会将doc value的数据写入磁盘文件中\r\n\r\n（3）性能问题：给jvm更少内存，64g服务器，给jvm最多16g\r\n\r\nes官方是建议，es大量是基于os cache来进行缓存和提升性能的，不建议用jvm内存来进行缓存，那样会导致一定的gc开销和oom问题\r\n给jvm更少的内存，给os cache更大的内存\r\n64g服务器，给jvm最多16g，几十个g的内存给os cache\r\nos cache可以提升doc value和倒排索引的缓存和查询效率\r\n\r\n2、column压缩\r\n\r\ndoc1: 550\r\ndoc2: 550\r\ndoc3: 500\r\n\r\n合并相同值，550，doc1和doc2都保留一个550的标识即可\r\n\r\n（1）所有值相同，直接保留单值\r\n（2）少于256个值，使用table encoding模式：一种压缩方式\r\n（3）大于256个值，看有没有最大公约数，有就除以最大公约数，然后保留这个最大公约数\r\n\r\ndoc1: 36\r\ndoc2: 24\r\n\r\n6 --&gt; doc1: 6, doc2: 4 --&gt; 保留一个最大公约数6的标识，6也保存起来\r\n\r\n（4）如果没有最大公约数，采取offset结合压缩的方式：\r\n\r\n3、disable doc value\r\n\r\n如果的确不需要doc value，比如聚合等操作，那么可以禁用，减少磁盘空间占用\r\n\r\nPUT my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;my_field&quot;: {\r\n          &quot;type&quot;:       &quot;keyword&quot;\r\n          &quot;doc_values&quot;: false \r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514616821,0,0,0),(145,1,'分词的field执行aggregation','','','1、对于分词的field执行aggregation，发现报错。。。\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;test_field&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n        &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;search_phase_execution_exception&quot;,\r\n    &quot;reason&quot;: &quot;all shards failed&quot;,\r\n    &quot;phase&quot;: &quot;query&quot;,\r\n    &quot;grouped&quot;: true,\r\n    &quot;failed_shards&quot;: [\r\n      {\r\n        &quot;shard&quot;: 0,\r\n        &quot;index&quot;: &quot;test_index&quot;,\r\n        &quot;node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;reason&quot;: {\r\n          &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n          &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot;\r\n        }\r\n      }\r\n    ],\r\n    &quot;caused_by&quot;: {\r\n      &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n      &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot;\r\n    }\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\n对分词的field，直接执行聚合操作，会报错，大概意思是说，你必须要打开fielddata，然后将正排索引数据加载到内存中，才可以对分词的field执行聚合操作，而且会消耗很大的内存\r\n\r\n2、给分词的field，设置fielddata=true，发现可以执行，但是结果却。。。\r\n\r\nPOST /test_index/_mapping/test_type \r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;test_field&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;fielddata&quot;: true\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;test_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;test_type&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;test_field&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            },\r\n            &quot;fielddata&quot;: true\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;size&quot;: 0, \r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;test_field&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 23,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;test&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n如果要对分词的field执行聚合操作，必须将fielddata设置为true\r\n\r\n3、使用内置field不分词，对string field进行聚合\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;test_field.keyword&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 3,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;test&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n如果对不分词的field执行聚合操作，直接就可以执行，不需要设置fieldata=true\r\n\r\n4、分词field+fielddata的工作原理\r\n\r\ndoc value --&gt; 不分词的所有field，可以执行聚合操作 --&gt; 如果你的某个field不分词，那么在index-time，就会自动生成doc value --&gt; 针对这些不分词的field执行聚合操作的时候，自动就会用doc value来执行\r\n\r\n分词field，是没有doc value的。。。在index-time，如果某个field是分词的，那么是不会给它建立doc value正排索引的，因为分词后，占用的空间过于大，所以默认是不支持分词field进行聚合的\r\n\r\n分词field默认没有doc value，所以直接对分词field执行聚合操作，是会报错的\r\n\r\n对于分词field，必须打开和使用fielddata，完全存在于纯内存中。。。结构和doc value类似。。。如果是ngram或者是大量term，那么必将占用大量的内存。。。\r\n\r\n如果一定要对分词的field执行聚合，那么必须将fielddata=true，然后es就会在执行聚合操作的时候，现场将field对应的数据，建立一份fielddata正排索引，fielddata正排索引的结构跟doc value是类似的，但是只会讲fielddata正排索引加载到内存中来，然后基于内存中的fielddata正排索引执行分词field的聚合操作\r\n\r\n如果直接对分词field执行聚合，报错，才会让我们开启fielddata=true，告诉我们，会将fielddata uninverted index，正排索引，加载到内存，会耗费内存空间\r\n\r\n为什么fielddata必须在内存？因为大家自己思考一下，分词的字符串，需要按照term进行聚合，需要执行更加复杂的算法和操作，如果基于磁盘和os cache，那么性能会很差\r\n',8,0,0,1514616864,0,0,0),(146,1,'fielddata','','','1、fielddata核心原理\r\n\r\nfielddata加载到内存的过程是lazy加载的，对一个analzyed field执行聚合时，才会加载，而且是field-level加载的\r\n一个index的一个field，所有doc都会被加载，而不是少数doc\r\n不是index-time创建，是query-time创建\r\n\r\n2、fielddata内存限制\r\n\r\nindices.fielddata.cache.size: 20%，超出限制，清除内存已有fielddata数据\r\nfielddata占用的内存超出了这个比例的限制，那么就清除掉内存中已有的fielddata数据\r\n默认无限制，限制内存使用，但是会导致频繁evict和reload，大量IO性能损耗，以及内存碎片和gc\r\n\r\n3、监控fielddata内存使用\r\n\r\nGET /_stats/fielddata?fields=*\r\nGET /_nodes/stats/indices/fielddata?fields=*\r\nGET /_nodes/stats/indices/fielddata?level=indices&amp;fields=*\r\n\r\n4、circuit breaker\r\n\r\n如果一次query load的feilddata超过总内存，就会oom --&gt; 内存溢出\r\n\r\ncircuit breaker会估算query要加载的fielddata大小，如果超出总内存，就短路，query直接失败\r\n\r\nindices.breaker.fielddata.limit：fielddata的内存限制，默认60%\r\nindices.breaker.request.limit：执行聚合的内存限制，默认40%\r\nindices.breaker.total.limit：综合上面两个，限制在70%以内\r\n\r\n',8,0,0,1514616950,0,0,0),(147,1,'fielddata实例','','','POST /test_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;my_field&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;fielddata&quot;: { \r\n        &quot;filter&quot;: {\r\n          &quot;frequency&quot;: { \r\n            &quot;min&quot;:              0.01, \r\n            &quot;min_segment_size&quot;: 500  \r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nmin：仅仅加载至少在1%的doc中出现过的term对应的fielddata\r\n\r\n比如说某个值，hello，总共有1000个doc，hello必须在10个doc中出现，那么这个hello对应的fielddata才会加载到内存中来\r\n\r\nmin_segment_size：少于500 doc的segment不加载fielddata\r\n\r\n加载fielddata的时候，也是按照segment去进行加载的，某个segment里面的doc数量少于500个，那么这个segment的fielddata就不加载\r\n\r\n这个，就我的经验来看，有点底层了，一般不会去设置它，大家知道就好\r\n',8,0,0,1514616998,0,0,0),(148,1,'query-time现场','','','如果真的要对分词的field执行聚合，那么每次都在query-time现场生产fielddata并加载到内存中来，速度可能会比较慢\r\n\r\n我们是不是可以预先生成加载fielddata到内存中来？？？\r\n\r\n1、fielddata预加载\r\n\r\nPOST /test_index/_mapping/test_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;test_field&quot;: {\r\n      &quot;type&quot;: &quot;string&quot;,\r\n      &quot;fielddata&quot;: {\r\n        &quot;loading&quot; : &quot;eager&quot; \r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nquery-time的fielddata生成和加载到内存，变为index-time，建立倒排索引的时候，会同步生成fielddata并且加载到内存中来，这样的话，对分词field的聚合性能当然会大幅度增强\r\n\r\n2、序号标记预加载\r\n\r\nglobal ordinal原理解释\r\n\r\ndoc1: status1\r\ndoc2: status2\r\ndoc3: status2\r\ndoc4: status1\r\n\r\n有很多重复值的情况，会进行global ordinal标记\r\n\r\nstatus1 --&gt; 0\r\nstatus2 --&gt; 1\r\n\r\ndoc1: 0\r\ndoc2: 1\r\ndoc3: 1\r\ndoc4: 0\r\n\r\n建立的fielddata也会是这个样子的，这样的好处就是减少重复字符串的出现的次数，减少内存的消耗\r\n\r\nPOST /test_index/_mapping/test_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;test_field&quot;: {\r\n      &quot;type&quot;: &quot;string&quot;,\r\n      &quot;fielddata&quot;: {\r\n        &quot;loading&quot; : &quot;eager_global_ordinals&quot; \r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514617047,0,0,0),(149,1,'深度优先和广度优先','','','当buckets数量特别多的时候，深度优先和广度优先的原理，图解\r\n\r\n我们的数据，是每个演员的每个电影的评论\r\n\r\n每个演员的评论的数量 --&gt; 每个演员的每个电影的评论的数量\r\n\r\n评论数量排名前10个的演员 --&gt; 每个演员的电影取到评论数量排名前5的电影\r\n\r\n{\r\n  &quot;aggs&quot; : {\r\n    &quot;actors&quot; : {\r\n      &quot;terms&quot; : {\r\n         &quot;field&quot; :        &quot;actors&quot;,\r\n         &quot;size&quot; :         10,\r\n         &quot;collect_mode&quot; : &quot;breadth_first&quot; \r\n      },\r\n      &quot;aggs&quot; : {\r\n        &quot;costars&quot; : {\r\n          &quot;terms&quot; : {\r\n            &quot;field&quot; : &quot;films&quot;,\r\n            &quot;size&quot; :  5\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n深度优先的方式去执行聚合操作的\r\n\r\n    actor1            actor2            .... actor\r\nfilm1 film2 film3   film1 film2 film3   ...film\r\n\r\n比如说，我们有10万个actor，最后其实是主要10个actor就可以了\r\n\r\n但是我们已经深度优先的方式，构建了一整颗完整的树出来了，10万个actor，每个actor平均有10部电影，10万 + 100万 --&gt; 110万的数据量的一颗树\r\n\r\n裁剪掉10万个actor中的99990 actor，99990 * 10 = film，剩下10个actor，每个actor的10个film裁剪掉5个，110万 --&gt; 10 * 5 = 50个\r\n\r\n构建了大量的数据，然后裁剪掉了99.99%的数据，浪费了\r\n\r\n广度优先的方式去执行聚合\r\n\r\nactor1    actor2    actor3    ..... n个actor\r\n\r\n10万个actor，不去构建它下面的film数据，10万 --&gt; 99990，10个actor，构建出film，裁剪出其中的5个film即可，10万 -&gt; 50个\r\n\r\n10倍',8,0,0,1514617089,0,0,0),(150,1,'es的document数据模型','','','关系型数据库的数据模型\r\n\r\nes的document数据模型\r\n\r\npublic class Department {\r\n	\r\n	private Integer deptId;\r\n	private String name;\r\n	private String desc;\r\n	private List&lt;Employee&gt; employees;\r\n\r\n}\r\n\r\npublic class Employee {\r\n	\r\n	private Integer empId;\r\n	private String name;\r\n	private Integer age;\r\n	private String gender;\r\n	private Department dept;\r\n\r\n}\r\n\r\n关系型数据库中\r\n\r\ndepartment表\r\n\r\ndept_id\r\nname\r\ndesc\r\n\r\nemployee表\r\n\r\nemp_id\r\nname\r\nage\r\ngender\r\ndept_id\r\n\r\n三范式 --&gt; 将每个数据实体拆分为一个独立的数据表，同时使用主外键关联关系将多个数据表关联起来 --&gt; 确保没有任何冗余的数据\r\n\r\n一份数据，只会放在一个数据表中 --&gt; dept name，部门名称，就只会放在department表中，不会在employee表中也放一个dept name，如果说你要查看某个员工的部门名称，那么必须通过员工表中的外键，dept_id，找到在部门表中对应的记录，然后找到部门名称\r\n\r\nes文档数据模型\r\n\r\n{\r\n	&quot;deptId&quot;: &quot;1&quot;,\r\n	&quot;name&quot;: &quot;研发部门&quot;,\r\n	&quot;desc&quot;: &quot;负责公司的所有研发项目&quot;,\r\n	&quot;employees&quot;: [\r\n		{\r\n			&quot;empId&quot;: &quot;1&quot;,\r\n			&quot;name&quot;: &quot;张三&quot;,\r\n			&quot;age&quot;: 28,\r\n			&quot;gender&quot;: &quot;男&quot;\r\n		},\r\n		{\r\n			&quot;empId&quot;: &quot;2&quot;,\r\n			&quot;name&quot;: &quot;王兰&quot;,\r\n			&quot;age&quot;: 25,\r\n			&quot;gender&quot;: &quot;女&quot;\r\n		},\r\n		{\r\n			&quot;empId&quot;: &quot;3&quot;,\r\n			&quot;name&quot;: &quot;李四&quot;,\r\n			&quot;age&quot;: 34,\r\n			&quot;gender&quot;: &quot;男&quot;\r\n		}\r\n	]\r\n}\r\n\r\nes，更加类似于面向对象的数据模型，将所有由关联关系的数据，放在一个doc json类型数据中，整个数据的关系，还有完整的数据，都放在了一起\r\n\r\n',8,0,0,1514617123,0,0,0),(151,1,'构造用户与博客数据','','','DELETE /website #删除website索引\r\n\r\n1、构造用户与博客数据\r\n\r\n在构造数据模型的时候，还是将有关联关系的数据，然后分割为不同的实体，类似于关系型数据库中的模型\r\n\r\n案例背景：博客网站， 我们会模拟各种用户发表各种博客，然后针对用户和博客之间的关系进行数据建模，同时针对建模好的数据执行各种搜索/聚合的操作\r\n\r\nPUT /website/users/1 \r\n{\r\n  &quot;name&quot;:     &quot;小鱼儿&quot;,\r\n  &quot;email&quot;:    &quot;xiaoyuer@sina.com&quot;,\r\n  &quot;birthday&quot;:      &quot;1980-01-01&quot;\r\n}\r\n\r\nPUT /website/blogs/1\r\n{\r\n  &quot;title&quot;:    &quot;我的第一篇博客&quot;,\r\n  &quot;content&quot;:     &quot;这是我的第一篇博客，开通啦！！！&quot;\r\n  &quot;userId&quot;:     1 \r\n}\r\n\r\n一个用户对应多个博客，一对多的关系，做了建模\r\n\r\n建模方式，分割实体，类似三范式的方式，用主外键关联关系，将多个实体关联起来\r\n\r\n2、搜索小鱼儿发表的所有博客\r\n\r\nGET /website/users/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;name.keyword&quot;: {\r\n        &quot;value&quot;: &quot;小鱼儿&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 91,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 0.2876821,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;users&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.2876821,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;小鱼儿&quot;,\r\n          &quot;email&quot;: &quot;xiaoyuer@sina.com&quot;,\r\n          &quot;birthday&quot;: &quot;1980-01-01&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n比如这里搜索的是，1万个用户的博客，可能第一次搜索，会得到1万个userId\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;terms&quot;: {\r\n          &quot;userId&quot;: [\r\n            1\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n第二次搜索的时候，要放入terms中1万个userId，才能进行搜索，这个时候性能比较差了\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;blogs&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot;,\r\n          &quot;content&quot;: &quot;大家好，我是小鱼儿，这是我写的第一篇博客！&quot;,\r\n          &quot;userId&quot;: 1\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n上面的操作，就属于应用层的join，在应用层先查出一份数据，然后再查出一份数据，进行关联\r\n\r\n3、优点和缺点\r\n\r\n优点：数据不冗余，维护方便\r\n缺点：应用层join，如果关联数据过多，导致查询过大，性能很差\r\n\r\n',8,0,0,1514617249,0,0,0),(152,1,'构造冗余的用户和博客数据','','','1、构造冗余的用户和博客数据\r\n\r\n第二种建模方式：用冗余数据，采用文档数据模型，进行数据建模，实现用户和博客的关联\r\n\r\nPUT /website/users/1\r\n{\r\n  &quot;name&quot;:     &quot;小鱼儿&quot;,\r\n  &quot;email&quot;:    &quot;xiaoyuer@sina.com&quot;,\r\n  &quot;birthday&quot;:      &quot;1980-01-01&quot;\r\n}\r\n\r\nPUT /website/blogs/1\r\n{\r\n  &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot;,\r\n  &quot;content&quot;: &quot;大家好，我是小鱼儿。。。&quot;,\r\n  &quot;userInfo&quot;: {\r\n    &quot;userId&quot;: 1,\r\n    &quot;username&quot;: &quot;小鱼儿&quot;\r\n  }\r\n}\r\n\r\n冗余数据，就是说，将可能会进行搜索的条件和要搜索的数据，放在一个doc中\r\n\r\n2、基于冗余用户数据搜索博客\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;userInfo.username.keyword&quot;: {\r\n        &quot;value&quot;: &quot;小鱼儿&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n就不需要走应用层的join，先搜一个数据，找到id，再去搜另一份数据\r\n\r\n直接走一个有冗余数据的type即可，指定要的搜索条件，即可搜索出自己想要的数据来\r\n\r\n3、优点和缺点\r\n\r\n优点：性能高，不需要执行两次搜索\r\n缺点：数据冗余，维护成本高 --&gt; 每次如果你的username变化了，同时要更新user type和blog type\r\n\r\n一般来说，对于es这种NoSQL类型的数据存储来讲，都是冗余模式....\r\n\r\n当然，你要去维护数据的关联关系，也是很有必要的，所以一旦出现冗余数据的修改，必须记得将所有关联的数据全部更新\r\n',8,0,0,1514618911,0,0,0),(153,1,'构造更多测试数据','','','1、构造更多测试数据\r\n\r\nPUT /website/users/3\r\n{\r\n  &quot;name&quot;: &quot;黄药师&quot;,\r\n  &quot;email&quot;: &quot;huangyaoshi@sina.com&quot;,\r\n  &quot;birthday&quot;: &quot;1970-10-24&quot;\r\n}\r\n\r\nPUT /website/blogs/3\r\n{\r\n  &quot;title&quot;: &quot;我是黄药师&quot;,\r\n  &quot;content&quot;: &quot;我是黄药师啊，各位同学们！！！&quot;,\r\n  &quot;userInfo&quot;: {\r\n    &quot;userId&quot;: 1,\r\n    &quot;userName&quot;: &quot;黄药师&quot;\r\n  }\r\n}\r\n\r\nPUT /website/users/2\r\n{\r\n  &quot;name&quot;: &quot;花无缺&quot;,\r\n  &quot;email&quot;: &quot;huawuque@sina.com&quot;,\r\n  &quot;birthday&quot;: &quot;1980-02-02&quot;\r\n}\r\n\r\nPUT /website/blogs/4\r\n{\r\n  &quot;title&quot;: &quot;花无缺的身世揭秘&quot;,\r\n  &quot;content&quot;: &quot;大家好，我是花无缺，所以我的身世是。。。&quot;,\r\n  &quot;userInfo&quot;: {\r\n    &quot;userId&quot;: 2,\r\n    &quot;userName&quot;: &quot;花无缺&quot;\r\n  }\r\n}\r\n\r\n2、对每个用户发表的博客进行分组\r\n\r\n比如说，小鱼儿发表的那些博客，花无缺发表了哪些博客，黄药师发表了哪些博客\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;size&quot;: 0, \r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_username&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;userInfo.username.keyword&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;top_blogs&quot;: {\r\n          &quot;top_hits&quot;: {\r\n            &quot;_source&quot;: {\r\n              &quot;include&quot;: &quot;title&quot;\r\n            }, \r\n            &quot;size&quot;: 5\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514618962,0,0,0),(154,1,'文件系统数据建模','','','数据建模，对类似文件系统这种的有多层级关系的数据进行建模\r\n\r\n1、文件系统数据构造\r\n\r\nPUT /fs\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;analysis&quot;: {\r\n      &quot;analyzer&quot;: {\r\n        &quot;paths&quot;: { \r\n          &quot;tokenizer&quot;: &quot;path_hierarchy&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\npath_hierarchy tokenizer讲解\r\n\r\n/a/b/c/d --&gt; path_hierarchy -&gt; /a/b/c/d, /a/b/c, /a/b, /a\r\n\r\nfs: filesystem\r\n\r\nPUT /fs/_mapping/file\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;name&quot;: { \r\n      &quot;type&quot;:  &quot;keyword&quot;\r\n    },\r\n    &quot;path&quot;: { \r\n      &quot;type&quot;:  &quot;keyword&quot;,\r\n      &quot;fields&quot;: {\r\n        &quot;tree&quot;: { \r\n          &quot;type&quot;:     &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;paths&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /fs/file/1\r\n{\r\n  &quot;name&quot;:     &quot;README.txt&quot;, \r\n  &quot;path&quot;:     &quot;/workspace/projects/helloworld&quot;, \r\n  &quot;contents&quot;: &quot;这是我的第一个elasticsearch程序&quot;\r\n}\r\n\r\n2、对文件系统执行搜索\r\n\r\n文件搜索需求：查找一份，内容包括elasticsearch，在/workspace/projects/hellworld这个目录下的文件\r\n\r\nGET /fs/file/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;contents&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;constant_score&quot;: {\r\n            &quot;filter&quot;: {\r\n              &quot;term&quot;: {\r\n                &quot;path&quot;: &quot;/workspace/projects/helloworld&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1.284885,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;fs&quot;,\r\n        &quot;_type&quot;: &quot;file&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1.284885,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;README.txt&quot;,\r\n          &quot;path&quot;: &quot;/workspace/projects/helloworld&quot;,\r\n          &quot;contents&quot;: &quot;这是我的第一个elasticsearch程序&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n搜索需求2：搜索/workspace目录下，内容包含elasticsearch的所有的文件\r\n\r\n/workspace/projects/helloworld    doc1\r\n/workspace/projects               doc1\r\n/workspace                        doc1\r\n\r\nGET /fs/file/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;contents&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;constant_score&quot;: {\r\n            &quot;filter&quot;: {\r\n              &quot;term&quot;: {\r\n                &quot;path.tree&quot;: &quot;/workspace&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514619049,0,0,0),(155,1,'悲观锁的简要说明','','','1、悲观锁的简要说明\r\n\r\n基于version的乐观锁并发控制\r\n\r\n在数据建模，结合文件系统建模的这个案例，把悲观锁的并发控制，3种锁粒度，都给大家仔细讲解一下\r\n\r\n最粗的一个粒度，全局锁\r\n\r\n/workspace/projects/helloworld\r\n\r\n如果多个线程，都过来，要并发地给/workspace/projects/helloworld下的README.txt修改文件名\r\n\r\n实际上要进行并发的控制，避免出现多线程的并发安全问题，比如多个线程修改，纯并发，先执行的修改操作被后执行的修改操作给覆盖了\r\n\r\nget current version\r\n\r\n带着这个current version去执行修改，如果一旦发现数据已经被别人给修改了，version号跟之前自己获取的已经不一样了; 那么必须重新获取新的version号再次尝试修改\r\n\r\n上来就尝试给这条数据加个锁，然后呢，此时就只有你能执行各种各样的操作了，其他人不能执行操作\r\n\r\n第一种锁：全局锁，直接锁掉整个fs index\r\n\r\n2、全局锁的上锁实验\r\n\r\nPUT /fs/lock/global/_create\r\n{}\r\n\r\nfs: 你要上锁的那个index\r\nlock: 就是你指定的一个对这个index上全局锁的一个type\r\nglobal: 就是你上的全局锁对应的这个doc的id\r\n_create：强制必须是创建，如果/fs/lock/global这个doc已经存在，那么创建失败，报错\r\n\r\n利用了doc来进行上锁\r\n\r\n/fs/lock/global /index/type/id --&gt; doc\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;global&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n另外一个线程同时尝试上锁\r\n\r\nPUT /fs/lock/global/_create\r\n{}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[lock][global]: version conflict, document already exists (current version [1])&quot;,\r\n        &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n        &quot;shard&quot;: &quot;2&quot;,\r\n        &quot;index&quot;: &quot;fs&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[lock][global]: version conflict, document already exists (current version [1])&quot;,\r\n    &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n    &quot;shard&quot;: &quot;2&quot;,\r\n    &quot;index&quot;: &quot;fs&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n如果失败，就再次重复尝试上锁\r\n\r\n执行各种操作。。。\r\n\r\nPOST /fs/file/1/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;name&quot;: &quot;README1.txt&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;file&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nDELETE /fs/lock/global\r\n\r\n{\r\n  &quot;found&quot;: true,\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;global&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;deleted&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\n另外一个线程，因为之前发现上锁失败，反复尝试重新上锁，终于上锁成功了，因为之前获取到全局锁的那个线程已经delete /fs/lock/global全局锁了\r\n\r\nPUT /fs/lock/global/_create\r\n{}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;global&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nPOST /fs/file/1/_update \r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;name&quot;: &quot;README.txt&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;file&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nDELETE /fs/lock/global\r\n\r\n3、全局锁的优点和缺点\r\n\r\n优点：操作非常简单，非常容易使用，成本低\r\n缺点：你直接就把整个index给上锁了，这个时候对index中所有的doc的操作，都会被block住，导致整个系统的并发能力很低\r\n\r\n上锁解锁的操作不是频繁，然后每次上锁之后，执行的操作的耗时不会太长，用这种方式，方便\r\n\r\n',8,0,0,1514619107,0,0,0),(156,1,'document level锁','','','1、对document level锁，详细的讲解\r\n\r\n全局锁，一次性就锁整个index，对这个index的所有增删改操作都会被block住，如果上锁不频繁，还可以，比较简单\r\n\r\n细粒度的一个锁，document锁，顾名思义，每次就锁你要操作的，你要执行增删改的那些doc，doc锁了，其他线程就不能对这些doc执行增删改操作了\r\n但是你只是锁了部分doc，其他线程对其他的doc还是可以上锁和执行增删改操作的\r\n\r\ndocument锁，是用脚本进行上锁\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: &quot;if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;&quot;\r\n  &quot;params&quot;: {\r\n    &quot;process_id&quot;: 123\r\n  }\r\n}\r\n\r\n/fs/lock，是固定的，就是说fs下的lock type，专门用于进行上锁\r\n/fs/lock/id，比如1，id其实就是你要上锁的那个doc的id，代表了某个doc数据对应的lock（也是一个doc）\r\n_update + upsert：执行upsert操作\r\n\r\nparams，里面有个process_id，process_id，是你的要执行增删改操作的进程的唯一id，比如说可以在java系统，启动的时候，给你的每个线程都用UUID自动生成一个thread id，你的系统进程启动的时候给整个进程也分配一个UUID。process_id + thread_id就代表了某一个进程下的某个线程的唯一标识。可以自己用UUID生成一个唯一ID\r\n\r\nprocess_id很重要，会在lock中，设置对对应的doc加锁的进程的id，这样其他进程过来的时候，才知道，这条数据已经被别人给锁了\r\n\r\nassert false，不是当前进程加锁的话，则抛出异常\r\nctx.op=&#039;noop&#039;，不做任何修改\r\n\r\n如果该document之前没有被锁，/fs/lock/1之前不存在，也就是doc id=1没有被别人上过锁; upsert的语法，那么执行index操作，创建一个/fs/lock/id这条数据，而且用params中的数据作为这个lock的数据。process_id被设置为123，script不执行。这个时候象征着process_id=123的进程已经锁了一个doc了。\r\n\r\n如果document被锁了，就是说/fs/lock/1已经存在了，代表doc id=1已经被某个进程给锁了。那么执行update操作，script，此时会比对process_id，如果相同，就是说，某个进程，之前锁了这个doc，然后这次又过来，就可以直接对这个doc执行操作，说明是该进程之前锁的doc，则不报错，不执行任何操作，返回success; 如果process_id比对不上，说明doc被其他doc给锁了，此时报错\r\n\r\n/fs/lock/1\r\n{\r\n  &quot;process_id&quot;: 123\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: &quot;if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;&quot;\r\n  &quot;params&quot;: {\r\n    &quot;process_id&quot;: 123\r\n  }\r\n}\r\n\r\n\r\nscript：ctx._source.process_id，123\r\nprocess_id：加锁的upsert请求中带过来额proess_id\r\n\r\n如果两个process_id相同，说明是一个进程先加锁，然后又过来尝试加锁，可能是要执行另外一个操作，此时就不会block，对同一个process_id是不会block，ctx.op= &#039;noop&#039;，什么都不做，返回一个success\r\n\r\n如果说已经有一个进程加了锁了\r\n\r\n/fs/lock/1\r\n{\r\n  &quot;process_id&quot;: 123\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: &quot;if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;&quot;\r\n  &quot;params&quot;: {\r\n    &quot;process_id&quot;: 234\r\n  }\r\n}\r\n\r\n&quot;script&quot;: &quot;if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;&quot;\r\n\r\nctx._source.process_id：123\r\nprocess_id: 234\r\n\r\nprocess_id不相等，说明这个doc之前已经被别人上锁了，process_id=123上锁了; process_id=234过来再次尝试上锁，失败，assert false，就会报错\r\n\r\n此时遇到报错的process，就应该尝试重新上锁，直到上锁成功\r\n\r\n有报错的话，如果有些doc被锁了，那么需要重试\r\n\r\n直到所有锁定都成功，执行自己的操作。。。\r\n\r\n释放所有的锁\r\n\r\n2、上document锁的完整实验过程\r\n\r\nscripts/judge-lock.groovy: if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;judge-lock&quot;, \r\n    &quot;params&quot;: {\r\n      &quot;process_id&quot;: 123\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nGET /fs/lock/1\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;process_id&quot;: 123\r\n  }\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 234 },\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;judge-lock&quot;, \r\n    &quot;params&quot;: {\r\n      &quot;process_id&quot;: 234\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;remote_transport_exception&quot;,\r\n        &quot;reason&quot;: &quot;[4onsTYV][127.0.0.1:9300][indices:data/write/update[s]]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n    &quot;reason&quot;: &quot;failed to execute script&quot;,\r\n    &quot;caused_by&quot;: {\r\n      &quot;type&quot;: &quot;script_exception&quot;,\r\n      &quot;reason&quot;: &quot;error evaluating judge-lock&quot;,\r\n      &quot;caused_by&quot;: {\r\n        &quot;type&quot;: &quot;power_assertion_error&quot;,\r\n        &quot;reason&quot;: &quot;assert false\\n&quot;\r\n      },\r\n      &quot;script_stack&quot;: [],\r\n      &quot;script&quot;: &quot;&quot;,\r\n      &quot;lang&quot;: &quot;groovy&quot;\r\n    }\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;judge-lock&quot;, \r\n    &quot;params&quot;: {\r\n      &quot;process_id&quot;: 123\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;noop&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 0,\r\n    &quot;successful&quot;: 0,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nPOST /fs/file/1/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;name&quot;: &quot;README1.txt&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;file&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 4,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nPOST /fs/_refresh \r\n\r\nGET /fs/lock/_search?scroll=1m\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;process_id&quot;: 123\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACPkFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAj5RY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAAI-YWNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACPnFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAj6BY0b25zVFlWWlRqR3ZJajlfc3BXejJ3&quot;,\r\n  &quot;took&quot;: 51,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;fs&quot;,\r\n        &quot;_type&quot;: &quot;lock&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;process_id&quot;: 123\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nPUT /fs/lock/_bulk\r\n{ &quot;delete&quot;: { &quot;_id&quot;: 1}}\r\n\r\n{\r\n  &quot;took&quot;: 20,\r\n  &quot;errors&quot;: false,\r\n  &quot;items&quot;: [\r\n    {\r\n      &quot;delete&quot;: {\r\n        &quot;found&quot;: true,\r\n        &quot;_index&quot;: &quot;fs&quot;,\r\n        &quot;_type&quot;: &quot;lock&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_version&quot;: 2,\r\n        &quot;result&quot;: &quot;deleted&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;status&quot;: 200\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 234 },\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;judge-lock&quot;, \r\n    &quot;params&quot;: {\r\n      &quot;process_id&quot;: 234\r\n    }\r\n  }\r\n}\r\n\r\nprocess_id=234上锁就成功了\r\n',8,0,0,1514619187,0,0,0),(157,1,'共享锁和排他锁的说明','','','1、共享锁和排他锁的说明\r\n\r\n共享锁：这份数据是共享的，然后多个线程过来，都可以获取同一个数据的共享锁，然后对这个数据执行读操作\r\n排他锁：是排他的操作，只能一个线程获取排他锁，然后执行增删改操作\r\n\r\n读写锁的分离\r\n\r\n如果只是要读取数据的话，那么任意个线程都可以同时进来然后读取数据，每个线程都可以上一个共享锁\r\n但是这个时候，如果有线程要过来修改数据，那么会尝试上排他锁，排他锁会跟共享锁互斥，也就是说，如果有人已经上了共享锁了，那么排他锁就不能上，就得等\r\n\r\n如果有人在读数据，就不允许别人来修改数据\r\n\r\n反之，也是一样的\r\n\r\n如果有人在修改数据，就是加了排他锁\r\n那么其他线程过来要修改数据，也会尝试加排他锁，此时会失败，锁冲突，必须等待，同时只能有一个线程修改数据\r\n如果有人过来同时要读取数据，那么会尝试加共享锁，此时会失败，因为共享锁和排他锁是冲突的\r\n\r\n如果有在修改数据，就不允许别人来修改数据，也不允许别人来读取数据\r\n\r\n2、共享锁和排他锁的实验\r\n\r\n第一步：有人在读数据，其他人也能过来读数据\r\n\r\njudge-lock-2.groovy: if (ctx._source.lock_type == &#039;exclusive&#039;) { assert false }; ctx._source.lock_count++\r\n\r\nPOST /fs/lock/1/_update \r\n{\r\n  &quot;upsert&quot;: { \r\n    &quot;lock_type&quot;:  &quot;shared&quot;,\r\n    &quot;lock_count&quot;: 1\r\n  },\r\n  &quot;script&quot;: {\r\n  	&quot;lang&quot;: &quot;groovy&quot;,\r\n  	&quot;file&quot;: &quot;judge-lock-2&quot;\r\n  }\r\n}\r\n\r\nPOST /fs/lock/1/_update \r\n{\r\n  &quot;upsert&quot;: { \r\n    &quot;lock_type&quot;:  &quot;shared&quot;,\r\n    &quot;lock_count&quot;: 1\r\n  },\r\n  &quot;script&quot;: {\r\n  	&quot;lang&quot;: &quot;groovy&quot;,\r\n  	&quot;file&quot;: &quot;judge-lock-2&quot;\r\n  }\r\n}\r\n\r\nGET /fs/lock/1\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;lock_type&quot;: &quot;shared&quot;,\r\n    &quot;lock_count&quot;: 3\r\n  }\r\n}\r\n\r\n就给大家模拟了，有人上了共享锁，你还是要上共享锁，直接上就行了，没问题，只是lock_count加1\r\n\r\n2、已经有人上了共享锁，然后有人要上排他锁\r\n\r\nPUT /fs/lock/1/_create\r\n{ &quot;lock_type&quot;: &quot;exclusive&quot; }\r\n\r\n排他锁用的不是upsert语法，create语法，要求lock必须不能存在，直接自己是第一个上锁的人，上的是排他锁\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [3])&quot;,\r\n        &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n        &quot;shard&quot;: &quot;3&quot;,\r\n        &quot;index&quot;: &quot;fs&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [3])&quot;,\r\n    &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n    &quot;shard&quot;: &quot;3&quot;,\r\n    &quot;index&quot;: &quot;fs&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n如果已经有人上了共享锁，明显/fs/lock/1是存在的，create语法去上排他锁，肯定会报错\r\n\r\n3、对共享锁进行解锁\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;script&quot;: {\r\n  	&quot;lang&quot;: &quot;groovy&quot;,\r\n  	&quot;file&quot;: &quot;unlock-shared&quot;\r\n  }\r\n}\r\n\r\n连续解锁3次，此时共享锁就彻底没了\r\n\r\n每次解锁一个共享锁，就对lock_count先减1，如果减了1之后，是0，那么说明所有的共享锁都解锁完了，此时就就将/fs/lock/1删除，就彻底解锁所有的共享锁\r\n\r\n3、上排他锁，再上排他锁\r\n\r\nPUT /fs/lock/1/_create\r\n{ &quot;lock_type&quot;: &quot;exclusive&quot; }\r\n\r\n其他线程\r\n\r\nPUT /fs/lock/1/_create\r\n{ &quot;lock_type&quot;: &quot;exclusive&quot; }\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [7])&quot;,\r\n        &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n        &quot;shard&quot;: &quot;3&quot;,\r\n        &quot;index&quot;: &quot;fs&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [7])&quot;,\r\n    &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n    &quot;shard&quot;: &quot;3&quot;,\r\n    &quot;index&quot;: &quot;fs&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n4、上排他锁，上共享锁\r\n\r\nPOST /fs/lock/1/_update \r\n{\r\n  &quot;upsert&quot;: { \r\n    &quot;lock_type&quot;:  &quot;shared&quot;,\r\n    &quot;lock_count&quot;: 1\r\n  },\r\n  &quot;script&quot;: {\r\n  	&quot;lang&quot;: &quot;groovy&quot;,\r\n  	&quot;file&quot;: &quot;judge-lock-2&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;remote_transport_exception&quot;,\r\n        &quot;reason&quot;: &quot;[4onsTYV][127.0.0.1:9300][indices:data/write/update[s]]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n    &quot;reason&quot;: &quot;failed to execute script&quot;,\r\n    &quot;caused_by&quot;: {\r\n      &quot;type&quot;: &quot;script_exception&quot;,\r\n      &quot;reason&quot;: &quot;error evaluating judge-lock-2&quot;,\r\n      &quot;caused_by&quot;: {\r\n        &quot;type&quot;: &quot;power_assertion_error&quot;,\r\n        &quot;reason&quot;: &quot;assert false\\n&quot;\r\n      },\r\n      &quot;script_stack&quot;: [],\r\n      &quot;script&quot;: &quot;&quot;,\r\n      &quot;lang&quot;: &quot;groovy&quot;\r\n    }\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\n5、解锁排他锁\r\n\r\nDELETE /fs/lock/1',8,0,0,1514619254,0,0,0),(158,1,'nested object','','','1、做一个实验，引出来为什么需要nested object\r\n\r\n冗余数据方式的来建模，其实用的就是object类型，我们这里又要引入一种新的object类型，nested object类型\r\n\r\n博客，评论，做的这种数据模型\r\n\r\nPUT /website/blogs/6\r\n{\r\n  &quot;title&quot;: &quot;花无缺发表的一篇帖子&quot;,\r\n  &quot;content&quot;:  &quot;我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。&quot;,\r\n  &quot;tags&quot;:  [ &quot;投资&quot;, &quot;理财&quot; ],\r\n  &quot;comments&quot;: [ \r\n    {\r\n      &quot;name&quot;:    &quot;小鱼儿&quot;,\r\n      &quot;comment&quot;: &quot;什么股票啊？推荐一下呗&quot;,\r\n      &quot;age&quot;:     28,\r\n      &quot;stars&quot;:   4,\r\n      &quot;date&quot;:    &quot;2016-09-01&quot;\r\n    },\r\n    {\r\n      &quot;name&quot;:    &quot;黄药师&quot;,\r\n      &quot;comment&quot;: &quot;我喜欢投资房产，风，险大收益也大&quot;,\r\n      &quot;age&quot;:     31,\r\n      &quot;stars&quot;:   5,\r\n      &quot;date&quot;:    &quot;2016-10-22&quot;\r\n    }\r\n  ]\r\n}\r\n\r\n被年龄是28岁的黄药师评论过的博客，搜索\r\n\r\nGET /website/blogs/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        { &quot;match&quot;: { &quot;comments.name&quot;: &quot;黄药师&quot; }},\r\n        { &quot;match&quot;: { &quot;comments.age&quot;:  28      }} \r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 102,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1.8022683,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;blogs&quot;,\r\n        &quot;_id&quot;: &quot;6&quot;,\r\n        &quot;_score&quot;: 1.8022683,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;花无缺发表的一篇帖子&quot;,\r\n          &quot;content&quot;: &quot;我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;投资&quot;,\r\n            &quot;理财&quot;\r\n          ],\r\n          &quot;comments&quot;: [\r\n            {\r\n              &quot;name&quot;: &quot;小鱼儿&quot;,\r\n              &quot;comment&quot;: &quot;什么股票啊？推荐一下呗&quot;,\r\n              &quot;age&quot;: 28,\r\n              &quot;stars&quot;: 4,\r\n              &quot;date&quot;: &quot;2016-09-01&quot;\r\n            },\r\n            {\r\n              &quot;name&quot;: &quot;黄药师&quot;,\r\n              &quot;comment&quot;: &quot;我喜欢投资房产，风，险大收益也大&quot;,\r\n              &quot;age&quot;: 31,\r\n              &quot;stars&quot;: 5,\r\n              &quot;date&quot;: &quot;2016-10-22&quot;\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n结果是。。。好像不太对啊？？？\r\n\r\nobject类型数据结构的底层存储。。。\r\n\r\n{\r\n  &quot;title&quot;:            [ &quot;花无缺&quot;, &quot;发表&quot;, &quot;一篇&quot;, &quot;帖子&quot; ],\r\n  &quot;content&quot;:             [ &quot;我&quot;, &quot;是&quot;, &quot;花无缺&quot;, &quot;大家&quot;, &quot;要不要&quot;, &quot;考虑&quot;, &quot;一下&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;买&quot;, &quot;股票&quot;, &quot;事情&quot; ],\r\n  &quot;tags&quot;:             [ &quot;投资&quot;, &quot;理财&quot; ],\r\n  &quot;comments.name&quot;:    [ &quot;小鱼儿&quot;, &quot;黄药师&quot; ],\r\n  &quot;comments.comment&quot;: [ &quot;什么&quot;, &quot;股票&quot;, &quot;推荐&quot;, &quot;我&quot;, &quot;喜欢&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;风险&quot;, &quot;收益&quot;, &quot;大&quot; ],\r\n  &quot;comments.age&quot;:     [ 28, 31 ],\r\n  &quot;comments.stars&quot;:   [ 4, 5 ],\r\n  &quot;comments.date&quot;:    [ 2016-09-01, 2016-10-22 ]\r\n}\r\n\r\nobject类型底层数据结构，会将一个json数组中的数据，进行扁平化\r\n\r\n所以，直接命中了这个document，name=黄药师，age=28，正好符合\r\n\r\n2、引入nested object类型，来解决object类型底层数据结构导致的问题\r\n\r\n修改mapping，将comments的类型从object设置为nested\r\n\r\nPUT /website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;blogs&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;comments&quot;: {\r\n          &quot;type&quot;: &quot;nested&quot;, \r\n          &quot;properties&quot;: {\r\n            &quot;name&quot;:    { &quot;type&quot;: &quot;string&quot;  },\r\n            &quot;comment&quot;: { &quot;type&quot;: &quot;string&quot;  },\r\n            &quot;age&quot;:     { &quot;type&quot;: &quot;short&quot;   },\r\n            &quot;stars&quot;:   { &quot;type&quot;: &quot;short&quot;   },\r\n            &quot;date&quot;:    { &quot;type&quot;: &quot;date&quot;    }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{ \r\n  &quot;comments.name&quot;:    [ &quot;小鱼儿&quot; ],\r\n  &quot;comments.comment&quot;: [ &quot;什么&quot;, &quot;股票&quot;, &quot;推荐&quot; ],\r\n  &quot;comments.age&quot;:     [ 28 ],\r\n  &quot;comments.stars&quot;:   [ 4 ],\r\n  &quot;comments.date&quot;:    [ 2014-09-01 ]\r\n}\r\n{ \r\n  &quot;comments.name&quot;:    [ &quot;黄药师&quot; ],\r\n  &quot;comments.comment&quot;: [ &quot;我&quot;, &quot;喜欢&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;风险&quot;, &quot;收益&quot;, &quot;大&quot; ],\r\n  &quot;comments.age&quot;:     [ 31 ],\r\n  &quot;comments.stars&quot;:   [ 5 ],\r\n  &quot;comments.date&quot;:    [ 2014-10-22 ]\r\n}\r\n{ \r\n  &quot;title&quot;:            [ &quot;花无缺&quot;, &quot;发表&quot;, &quot;一篇&quot;, &quot;帖子&quot; ],\r\n  &quot;body&quot;:             [ &quot;我&quot;, &quot;是&quot;, &quot;花无缺&quot;, &quot;大家&quot;, &quot;要不要&quot;, &quot;考虑&quot;, &quot;一下&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;买&quot;, &quot;股票&quot;, &quot;事情&quot; ],\r\n  &quot;tags&quot;:             [ &quot;投资&quot;, &quot;理财&quot; ]\r\n}\r\n\r\n再次搜索，成功了。。。\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;花无缺&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;nested&quot;: {\r\n            &quot;path&quot;: &quot;comments&quot;,\r\n            &quot;query&quot;: {\r\n              &quot;bool&quot;: {\r\n                &quot;must&quot;: [\r\n                  {\r\n                    &quot;match&quot;: {\r\n                      &quot;comments.name&quot;: &quot;黄药师&quot;\r\n                    }\r\n                  },\r\n                  {\r\n                    &quot;match&quot;: {\r\n                      &quot;comments.age&quot;: 28\r\n                    }\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nscore_mode：max，min，avg，none，默认是avg\r\n\r\n如果搜索命中了多个nested document，如何讲个多个nested document的分数合并为一个分数\r\n',8,0,0,1514619301,0,0,0),(159,1,'基于nested object中的数据进行聚合分析','','','我们讲解一下基于nested object中的数据进行聚合分析\r\n\r\n聚合数据分析的需求1：按照评论日期进行bucket划分，然后拿到每个月的评论的评分的平均值\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;size&quot;: 0, \r\n  &quot;aggs&quot;: {\r\n    &quot;comments_path&quot;: {\r\n      &quot;nested&quot;: {\r\n        &quot;path&quot;: &quot;comments&quot;\r\n      }, \r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_comments_date&quot;: {\r\n          &quot;date_histogram&quot;: {\r\n            &quot;field&quot;: &quot;comments.date&quot;,\r\n            &quot;interval&quot;: &quot;month&quot;,\r\n            &quot;format&quot;: &quot;yyyy-MM&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;avg_stars&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;comments.stars&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 52,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;comments_path&quot;: {\r\n      &quot;doc_count&quot;: 4,\r\n      &quot;group_by_comments_date&quot;: {\r\n        &quot;buckets&quot;: [\r\n          {\r\n            &quot;key_as_string&quot;: &quot;2016-08&quot;,\r\n            &quot;key&quot;: 1470009600000,\r\n            &quot;doc_count&quot;: 1,\r\n            &quot;avg_stars&quot;: {\r\n              &quot;value&quot;: 3\r\n            }\r\n          },\r\n          {\r\n            &quot;key_as_string&quot;: &quot;2016-09&quot;,\r\n            &quot;key&quot;: 1472688000000,\r\n            &quot;doc_count&quot;: 2,\r\n            &quot;avg_stars&quot;: {\r\n              &quot;value&quot;: 4.5\r\n            }\r\n          },\r\n          {\r\n            &quot;key_as_string&quot;: &quot;2016-10&quot;,\r\n            &quot;key&quot;: 1475280000000,\r\n            &quot;doc_count&quot;: 1,\r\n            &quot;avg_stars&quot;: {\r\n              &quot;value&quot;: 5\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;comments_path&quot;: {\r\n      &quot;nested&quot;: {\r\n        &quot;path&quot;: &quot;comments&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_comments_age&quot;: {\r\n          &quot;histogram&quot;: {\r\n            &quot;field&quot;: &quot;comments.age&quot;,\r\n            &quot;interval&quot;: 10\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;reverse_path&quot;: {\r\n              &quot;reverse_nested&quot;: {}, \r\n              &quot;aggs&quot;: {\r\n                &quot;group_by_tags&quot;: {\r\n                  &quot;terms&quot;: {\r\n                    &quot;field&quot;: &quot;tags.keyword&quot;\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;comments_path&quot;: {\r\n      &quot;doc_count&quot;: 4,\r\n      &quot;group_by_comments_age&quot;: {\r\n        &quot;buckets&quot;: [\r\n          {\r\n            &quot;key&quot;: 20,\r\n            &quot;doc_count&quot;: 1,\r\n            &quot;reverse_path&quot;: {\r\n              &quot;doc_count&quot;: 1,\r\n              &quot;group_by_tags&quot;: {\r\n                &quot;doc_count_error_upper_bound&quot;: 0,\r\n                &quot;sum_other_doc_count&quot;: 0,\r\n                &quot;buckets&quot;: [\r\n                  {\r\n                    &quot;key&quot;: &quot;投资&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  },\r\n                  {\r\n                    &quot;key&quot;: &quot;理财&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          },\r\n          {\r\n            &quot;key&quot;: 30,\r\n            &quot;doc_count&quot;: 3,\r\n            &quot;reverse_path&quot;: {\r\n              &quot;doc_count&quot;: 2,\r\n              &quot;group_by_tags&quot;: {\r\n                &quot;doc_count_error_upper_bound&quot;: 0,\r\n                &quot;sum_other_doc_count&quot;: 0,\r\n                &quot;buckets&quot;: [\r\n                  {\r\n                    &quot;key&quot;: &quot;大侠&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  },\r\n                  {\r\n                    &quot;key&quot;: &quot;投资&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  },\r\n                  {\r\n                    &quot;key&quot;: &quot;理财&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  },\r\n                  {\r\n                    &quot;key&quot;: &quot;练功&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514619351,0,0,0),(160,1,'nested object的建模','','','nested object的建模，有个不好的地方，就是采取的是类似冗余数据的方式，将多个数据都放在一起了，维护成本就比较高\r\n\r\nparent child建模方式，采取的是类似于关系型数据库的三范式类的建模，多个实体都分割开来，每个实体之间都通过一些关联方式，进行了父子关系的关联，各种数据不需要都放在一起，父doc和子doc分别在进行更新的时候，都不会影响对方\r\n\r\n一对多关系的建模，维护起来比较方便，而且我们之前说过，类似关系型数据库的建模方式，应用层join的方式，会导致性能比较差，因为做多次搜索。父子关系的数据模型，不会，性能很好。因为虽然数据实体之间分割开来，但是我们在搜索的时候，由es自动为我们处理底层的关联关系，并且通过一些手段保证搜索性能。\r\n\r\n父子关系数据模型，相对于nested数据模型来说，优点是父doc和子doc互相之间不会影响\r\n\r\n要点：父子关系元数据映射，用于确保查询时候的高性能，但是有一个限制，就是父子数据必须存在于一个shard中\r\n\r\n父子关系数据存在一个shard中，而且还有映射其关联关系的元数据，那么搜索父子关系数据的时候，不用跨分片，一个分片本地自己就搞定了，性能当然高咯\r\n\r\n案例背景：研发中心员工管理案例，一个IT公司有多个研发中心，每个研发中心有多个员工\r\n\r\nPUT /company\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;rd_center&quot;: {},\r\n    &quot;employee&quot;: {\r\n      &quot;_parent&quot;: {\r\n        &quot;type&quot;: &quot;rd_center&quot; \r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n父子关系建模的核心，多个type之间有父子关系，用_parent指定父type\r\n\r\nPOST /company/rd_center/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;name&quot;: &quot;上海研发中心&quot;, &quot;city&quot;: &quot;上海&quot;, &quot;country&quot;: &quot;中国&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;3&quot; }}\r\n{ &quot;name&quot;: &quot;硅谷人工智能实验室&quot;, &quot;city&quot;: &quot;硅谷&quot;, &quot;country&quot;: &quot;美国&quot; }\r\n\r\nshard路由的时候，id=1的rd_center doc，默认会根据id进行路由，到某一个shard\r\n\r\nPUT /company/employee/1?parent=1 \r\n{\r\n  &quot;name&quot;:  &quot;张三&quot;,\r\n  &quot;birthday&quot;:   &quot;1970-10-24&quot;,\r\n  &quot;hobby&quot;: &quot;爬山&quot;\r\n}\r\n\r\n维护父子关系的核心，parent=1，指定了这个数据的父doc的id\r\n\r\n此时，parent-child关系，就确保了说，父doc和子doc都是保存在一个shard上的。内部原理还是doc routing，employee和rd_center的数据，都会用parent id作为routing，这样就会到一个shard\r\n\r\n就不会根据id=1的employee doc的id进行路由了，而是根据parent=1进行路由，会根据父doc的id进行路由，那么就可以通过底层的路由机制，保证父子数据存在于一个shard中\r\n\r\nPOST /company/employee/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 2, &quot;parent&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;李四&quot;, &quot;birthday&quot;: &quot;1982-05-16&quot;, &quot;hobby&quot;: &quot;游泳&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 3, &quot;parent&quot;: &quot;2&quot; }}\r\n{ &quot;name&quot;: &quot;王二&quot;, &quot;birthday&quot;: &quot;1979-04-01&quot;, &quot;hobby&quot;: &quot;爬山&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 4, &quot;parent&quot;: &quot;3&quot; }}\r\n{ &quot;name&quot;: &quot;赵五&quot;, &quot;birthday&quot;: &quot;1987-05-11&quot;, &quot;hobby&quot;: &quot;骑马&quot; }\r\n\r\n',8,0,0,1514619392,0,0,0),(161,1,'父子关系的数据模型','','','我们已经建立了父子关系的数据模型之后，就要基于这个模型进行各种搜索和聚合了\r\n\r\n1、搜索有1980年以后出生的员工的研发中心\r\n\r\nGET /company/rd_center/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_child&quot;: {\r\n      &quot;type&quot;: &quot;employee&quot;,\r\n      &quot;query&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;birthday&quot;: {\r\n            &quot;gte&quot;: &quot;1980-01-01&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 33,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;rd_center&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;北京研发总部&quot;,\r\n          &quot;city&quot;: &quot;北京&quot;,\r\n          &quot;country&quot;: &quot;中国&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;rd_center&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;硅谷人工智能实验室&quot;,\r\n          &quot;city&quot;: &quot;硅谷&quot;,\r\n          &quot;country&quot;: &quot;美国&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n2、搜索有名叫张三的员工的研发中心\r\n\r\nGET /company/rd_center/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_child&quot;: {\r\n      &quot;type&quot;:       &quot;employee&quot;,\r\n      &quot;query&quot;: {\r\n        &quot;match&quot;: {\r\n          &quot;name&quot;: &quot;张三&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;rd_center&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;北京研发总部&quot;,\r\n          &quot;city&quot;: &quot;北京&quot;,\r\n          &quot;country&quot;: &quot;中国&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n3、搜索有至少2个以上员工的研发中心\r\n\r\nGET /company/rd_center/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_child&quot;: {\r\n      &quot;type&quot;:         &quot;employee&quot;,\r\n      &quot;min_children&quot;: 2, \r\n      &quot;query&quot;: {\r\n        &quot;match_all&quot;: {}\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;rd_center&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;北京研发总部&quot;,\r\n          &quot;city&quot;: &quot;北京&quot;,\r\n          &quot;country&quot;: &quot;中国&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n4、搜索在中国的研发中心的员工\r\n\r\nGET /company/employee/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_parent&quot;: {\r\n      &quot;parent_type&quot;: &quot;rd_center&quot;,\r\n      &quot;query&quot;: {\r\n        &quot;term&quot;: {\r\n          &quot;country.keyword&quot;: &quot;中国&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;employee&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_routing&quot;: &quot;2&quot;,\r\n        &quot;_parent&quot;: &quot;2&quot;,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;王二&quot;,\r\n          &quot;birthday&quot;: &quot;1979-04-01&quot;,\r\n          &quot;hobby&quot;: &quot;爬山&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;employee&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_routing&quot;: &quot;1&quot;,\r\n        &quot;_parent&quot;: &quot;1&quot;,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;张三&quot;,\r\n          &quot;birthday&quot;: &quot;1970-10-24&quot;,\r\n          &quot;hobby&quot;: &quot;爬山&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;employee&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_routing&quot;: &quot;1&quot;,\r\n        &quot;_parent&quot;: &quot;1&quot;,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;李四&quot;,\r\n          &quot;birthday&quot;: &quot;1982-05-16&quot;,\r\n          &quot;hobby&quot;: &quot;游泳&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514620472,0,0,0),(162,1,'统计国家员工爱好','','','统计每个国家的喜欢每种爱好的员工有多少个\r\n\r\nGET /company/rd_center/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_country&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;country.keyword&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_child_employee&quot;: {\r\n          &quot;children&quot;: {\r\n            &quot;type&quot;: &quot;employee&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;group_by_hobby&quot;: {\r\n              &quot;terms&quot;: {\r\n                &quot;field&quot;: &quot;hobby.keyword&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 15,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_country&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;中国&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;group_by_child_employee&quot;: {\r\n            &quot;doc_count&quot;: 3,\r\n            &quot;group_by_hobby&quot;: {\r\n              &quot;doc_count_error_upper_bound&quot;: 0,\r\n              &quot;sum_other_doc_count&quot;: 0,\r\n              &quot;buckets&quot;: [\r\n                {\r\n                  &quot;key&quot;: &quot;爬山&quot;,\r\n                  &quot;doc_count&quot;: 2\r\n                },\r\n                {\r\n                  &quot;key&quot;: &quot;游泳&quot;,\r\n                  &quot;doc_count&quot;: 1\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;美国&quot;,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;group_by_child_employee&quot;: {\r\n            &quot;doc_count&quot;: 1,\r\n            &quot;group_by_hobby&quot;: {\r\n              &quot;doc_count_error_upper_bound&quot;: 0,\r\n              &quot;sum_other_doc_count&quot;: 0,\r\n              &quot;buckets&quot;: [\r\n                {\r\n                  &quot;key&quot;: &quot;骑马&quot;,\r\n                  &quot;doc_count&quot;: 1\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514620525,0,0,0),(163,1,'父子关系，祖孙三层关系','','','父子关系，祖孙三层关系的数据建模，搜索\r\n\r\nPUT /company\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;country&quot;: {},\r\n    &quot;rd_center&quot;: {\r\n      &quot;_parent&quot;: {\r\n        &quot;type&quot;: &quot;country&quot; \r\n      }\r\n    },\r\n    &quot;employee&quot;: {\r\n      &quot;_parent&quot;: {\r\n        &quot;type&quot;: &quot;rd_center&quot; \r\n      }\r\n    }\r\n  }\r\n}\r\n\r\ncountry -&gt; rd_center -&gt; employee，祖孙三层数据模型\r\n\r\nPOST /company/country/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;中国&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;name&quot;: &quot;美国&quot; }\r\n\r\nPOST /company/rd_center/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;1&quot;, &quot;parent&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;北京研发总部&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;2&quot;, &quot;parent&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;上海研发中心&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;3&quot;, &quot;parent&quot;: &quot;2&quot; }}\r\n{ &quot;name&quot;: &quot;硅谷人工智能实验室&quot; }\r\n\r\nPUT /company/employee/1?parent=1&amp;routing=1\r\n{\r\n  &quot;name&quot;:  &quot;张三&quot;,\r\n  &quot;dob&quot;:   &quot;1970-10-24&quot;,\r\n  &quot;hobby&quot;: &quot;爬山&quot;\r\n}\r\n\r\nrouting参数的讲解，必须跟grandparent相同，否则有问题\r\n\r\ncountry，用的是自己的id去路由; rd_center，parent，用的是country的id去路由; employee，如果也是仅仅指定一个parent，那么用的是rd_center的id去路由，这就导致祖孙三层数据不会在一个shard上\r\n\r\n孙子辈儿，要手动指定routing，指定为爷爷辈儿的数据的id\r\n\r\n搜索有爬山爱好的员工所在的国家\r\n\r\nGET /company/country/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_child&quot;: {\r\n      &quot;type&quot;: &quot;rd_center&quot;,\r\n      &quot;query&quot;: {\r\n        &quot;has_child&quot;: {\r\n          &quot;type&quot;: &quot;employee&quot;,\r\n          &quot;query&quot;: {\r\n            &quot;match&quot;: {\r\n              &quot;hobby&quot;: &quot;爬山&quot;\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 10,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;country&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;中国&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514620568,0,0,0),(164,1,'term vector介绍','','','1、term vector介绍\r\n\r\n获取document中的某个field内的各个term的统计信息\r\n\r\nterm information: term frequency in the field, term positions, start and end offsets, term payloads\r\n\r\nterm statistics: 设置term_statistics=true; total term frequency, 一个term在所有document中出现的频率; document frequency，有多少document包含这个term\r\n\r\nfield statistics: document count，有多少document包含这个field; sum of document frequency，一个field中所有term的df之和; sum of total term frequency，一个field中的所有term的tf之和\r\n\r\nGET /twitter/tweet/1/_termvectors\r\nGET /twitter/tweet/1/_termvectors?fields=text\r\n\r\nterm statistics和field statistics并不精准，不会被考虑有的doc可能被删除了\r\n\r\n我告诉大家，其实很少用，用的时候，一般来说，就是你需要对一些数据做探查的时候。比如说，你想要看到某个term，某个词条，大话西游，这个词条，在多少个document中出现了。或者说某个field，film_desc，电影的说明信息，有多少个doc包含了这个说明信息。\r\n\r\n2、index-iime term vector实验\r\n\r\nterm vector，涉及了很多的term和field相关的统计信息，有两种方式可以采集到这个统计信息\r\n\r\n（1）index-time，你在mapping里配置一下，然后建立索引的时候，就直接给你生成这些term和field的统计信息了\r\n（2）query-time，你之前没有生成过任何的Term vector信息，然后在查看term vector的时候，直接就可以看到了，会on the fly，现场计算出各种统计信息，然后返回给你\r\n\r\n这一讲，不会手敲任何命令，直接copy我做好的命令，因为这一讲的重点，不是掌握什么搜索或者聚合的语法，而是说，掌握，如何采集term vector信息，然后如何看懂term vector信息，你能掌握利用term vector进行数据探查\r\n\r\nPUT /my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;text&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;term_vector&quot;: &quot;with_positions_offsets_payloads&quot;,\r\n            &quot;store&quot; : true,\r\n            &quot;analyzer&quot; : &quot;fulltext_analyzer&quot;\r\n         },\r\n         &quot;fullname&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;analyzer&quot; : &quot;fulltext_analyzer&quot;\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;settings&quot; : {\r\n    &quot;index&quot; : {\r\n      &quot;number_of_shards&quot; : 1,\r\n      &quot;number_of_replicas&quot; : 0\r\n    },\r\n    &quot;analysis&quot;: {\r\n      &quot;analyzer&quot;: {\r\n        &quot;fulltext_analyzer&quot;: {\r\n          &quot;type&quot;: &quot;custom&quot;,\r\n          &quot;tokenizer&quot;: &quot;whitespace&quot;,\r\n          &quot;filter&quot;: [\r\n            &quot;lowercase&quot;,\r\n            &quot;type_as_payload&quot;\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n  &quot;text&quot; : &quot;hello test test test &quot;\r\n}\r\n\r\nPUT /my_index/my_type/2\r\n{\r\n  &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n  &quot;text&quot; : &quot;other hello test ...&quot;\r\n}\r\n\r\nGET /my_index/my_type/1/_termvectors\r\n{\r\n  &quot;fields&quot; : [&quot;text&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;payloads&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;my_index&quot;,\r\n  &quot;_type&quot;: &quot;my_type&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;took&quot;: 10,\r\n  &quot;term_vectors&quot;: {\r\n    &quot;text&quot;: {\r\n      &quot;field_statistics&quot;: {\r\n        &quot;sum_doc_freq&quot;: 6,\r\n        &quot;doc_count&quot;: 2,\r\n        &quot;sum_ttf&quot;: 8\r\n      },\r\n      &quot;terms&quot;: {\r\n        &quot;hello&quot;: {\r\n          &quot;doc_freq&quot;: 2,\r\n          &quot;ttf&quot;: 2,\r\n          &quot;term_freq&quot;: 1,\r\n          &quot;tokens&quot;: [\r\n            {\r\n              &quot;position&quot;: 0,\r\n              &quot;start_offset&quot;: 0,\r\n              &quot;end_offset&quot;: 5,\r\n              &quot;payload&quot;: &quot;d29yZA==&quot;\r\n            }\r\n          ]\r\n        },\r\n        &quot;test&quot;: {\r\n          &quot;doc_freq&quot;: 2,\r\n          &quot;ttf&quot;: 4,\r\n          &quot;term_freq&quot;: 3,\r\n          &quot;tokens&quot;: [\r\n            {\r\n              &quot;position&quot;: 1,\r\n              &quot;start_offset&quot;: 6,\r\n              &quot;end_offset&quot;: 10,\r\n              &quot;payload&quot;: &quot;d29yZA==&quot;\r\n            },\r\n            {\r\n              &quot;position&quot;: 2,\r\n              &quot;start_offset&quot;: 11,\r\n              &quot;end_offset&quot;: 15,\r\n              &quot;payload&quot;: &quot;d29yZA==&quot;\r\n            },\r\n            {\r\n              &quot;position&quot;: 3,\r\n              &quot;start_offset&quot;: 16,\r\n              &quot;end_offset&quot;: 20,\r\n              &quot;payload&quot;: &quot;d29yZA==&quot;\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n3、query-time term vector实验\r\n\r\nGET /my_index/my_type/1/_termvectors\r\n{\r\n  &quot;fields&quot; : [&quot;fullname&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true\r\n}\r\n\r\n一般来说，如果条件允许，你就用query time的term vector就可以了，你要探查什么数据，现场去探查一下就好了\r\n\r\n4、手动指定doc的term vector\r\n\r\nGET /my_index/my_type/_termvectors\r\n{\r\n  &quot;doc&quot; : {\r\n    &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n    &quot;text&quot; : &quot;hello test test test&quot;\r\n  },\r\n  &quot;fields&quot; : [&quot;text&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;payloads&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true\r\n}\r\n\r\n手动指定一个doc，实际上不是要指定doc，而是要指定你想要安插的词条，hello test，那么就可以放在一个field中\r\n\r\n将这些term分词，然后对每个term，都去计算它在现有的所有doc中的一些统计信息\r\n\r\n这个挺有用的，可以让你手动指定要探查的term的数据情况，你就可以指定探查“大话西游”这个词条的统计信息\r\n\r\n5、手动指定analyzer来生成term vector\r\n\r\nGET /my_index/my_type/_termvectors\r\n{\r\n  &quot;doc&quot; : {\r\n    &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n    &quot;text&quot; : &quot;hello test test test&quot;\r\n  },\r\n  &quot;fields&quot; : [&quot;text&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;payloads&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true,\r\n  &quot;per_field_analyzer&quot; : {\r\n    &quot;text&quot;: &quot;standard&quot;\r\n  }\r\n}\r\n\r\n6、terms filter\r\n\r\nGET /my_index/my_type/_termvectors\r\n{\r\n  &quot;doc&quot; : {\r\n    &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n    &quot;text&quot; : &quot;hello test test test&quot;\r\n  },\r\n  &quot;fields&quot; : [&quot;text&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;payloads&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true,\r\n  &quot;filter&quot; : {\r\n      &quot;max_num_terms&quot; : 3,\r\n      &quot;min_term_freq&quot; : 1,\r\n      &quot;min_doc_freq&quot; : 1\r\n    }\r\n}\r\n\r\n这个就是说，根据term统计信息，过滤出你想要看到的term vector统计结果\r\n也挺有用的，比如你探查数据把，可以过滤掉一些出现频率过低的term，就不考虑了\r\n\r\n7、multi term vector\r\n\r\nGET _mtermvectors\r\n{\r\n   &quot;docs&quot;: [\r\n      {\r\n         &quot;_index&quot;: &quot;my_index&quot;,\r\n         &quot;_type&quot;: &quot;my_type&quot;,\r\n         &quot;_id&quot;: &quot;2&quot;,\r\n         &quot;term_statistics&quot;: true\r\n      },\r\n      {\r\n         &quot;_index&quot;: &quot;my_index&quot;,\r\n         &quot;_type&quot;: &quot;my_type&quot;,\r\n         &quot;_id&quot;: &quot;1&quot;,\r\n         &quot;fields&quot;: [\r\n            &quot;text&quot;\r\n         ]\r\n      }\r\n   ]\r\n}\r\n\r\nGET /my_index/_mtermvectors\r\n{\r\n   &quot;docs&quot;: [\r\n      {\r\n         &quot;_type&quot;: &quot;test&quot;,\r\n         &quot;_id&quot;: &quot;2&quot;,\r\n         &quot;fields&quot;: [\r\n            &quot;text&quot;\r\n         ],\r\n         &quot;term_statistics&quot;: true\r\n      },\r\n      {\r\n         &quot;_type&quot;: &quot;test&quot;,\r\n         &quot;_id&quot;: &quot;1&quot;\r\n      }\r\n   ]\r\n}\r\n\r\nGET /my_index/my_type/_mtermvectors\r\n{\r\n   &quot;docs&quot;: [\r\n      {\r\n         &quot;_id&quot;: &quot;2&quot;,\r\n         &quot;fields&quot;: [\r\n            &quot;text&quot;\r\n         ],\r\n         &quot;term_statistics&quot;: true\r\n      },\r\n      {\r\n         &quot;_id&quot;: &quot;1&quot;\r\n      }\r\n   ]\r\n}\r\n\r\nGET /_mtermvectors\r\n{\r\n   &quot;docs&quot;: [\r\n      {\r\n         &quot;_index&quot;: &quot;my_index&quot;,\r\n         &quot;_type&quot;: &quot;my_type&quot;,\r\n         &quot;doc&quot; : {\r\n            &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n            &quot;text&quot; : &quot;hello test test test&quot;\r\n         }\r\n      },\r\n      {\r\n         &quot;_index&quot;: &quot;my_index&quot;,\r\n         &quot;_type&quot;: &quot;my_type&quot;,\r\n         &quot;doc&quot; : {\r\n           &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n           &quot;text&quot; : &quot;other hello test ...&quot;\r\n         }\r\n      }\r\n   ]\r\n}',8,0,0,1514620612,0,0,0),(165,1,'最基本的高亮例子','','','1、一个最基本的高亮例子\r\n\r\nPUT /blog_website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;blogs&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /blog_website/blogs/1\r\n{\r\n  &quot;title&quot;: &quot;我的第一篇博客&quot;,\r\n  &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot;\r\n}\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;title&quot;: &quot;博客&quot;\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;fields&quot;: {\r\n      &quot;title&quot;: {}\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 103,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 0.28582606,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;blog_website&quot;,\r\n        &quot;_type&quot;: &quot;blogs&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.28582606,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;我的第一篇博客&quot;,\r\n          &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot;\r\n        },\r\n        &quot;highlight&quot;: {\r\n          &quot;title&quot;: [\r\n            &quot;我的第一篇&lt;em&gt;博客&lt;/em&gt;&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n&lt;em&gt;&lt;/em&gt;表现，会变成红色，所以说你的指定的field中，如果包含了那个搜索词的话，就会在那个field的文本中，对搜索词进行红色的高亮显示\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;博客&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;博客&quot;\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;fields&quot;: {\r\n      &quot;title&quot;: {},\r\n      &quot;content&quot;: {}\r\n    }\r\n  }\r\n}\r\n\r\nhighlight中的field，必须跟query中的field一一对齐的\r\n\r\n2、三种highlight介绍\r\n\r\nplain highlight，lucene highlight，默认\r\n\r\nposting highlight，index_options=offsets\r\n\r\n（1）性能比plain highlight要高，因为不需要重新对高亮文本进行分词\r\n（2）对磁盘的消耗更少\r\n（3）将文本切割为句子，并且对句子进行高亮，效果更好\r\n\r\nPUT /blog_website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;blogs&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n          &quot;index_options&quot;: &quot;offsets&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /blog_website/blogs/1\r\n{\r\n  &quot;title&quot;: &quot;我的第一篇博客&quot;,\r\n  &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot;\r\n}\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;博客&quot;\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;fields&quot;: {\r\n      &quot;content&quot;: {}\r\n    }\r\n  }\r\n}\r\n\r\nfast vector highlight\r\n\r\nindex-time term vector设置在mapping中，就会用fast verctor highlight\r\n\r\n（1）对大field而言（大于1mb），性能更高\r\n\r\nPUT /blog_website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;blogs&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n          &quot;term_vector&quot; : &quot;with_positions_offsets&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n强制使用某种highlighter，比如对于开启了term vector的field而言，可以强制使用plain highlight\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;博客&quot;\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;fields&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;type&quot;: &quot;plain&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n总结一下，其实可以根据你的实际情况去考虑，一般情况下，用plain highlight也就足够了，不需要做其他额外的设置\r\n如果对高亮的性能要求很高，可以尝试启用posting highlight\r\n如果field的值特别大，超过了1M，那么可以用fast vector highlight\r\n\r\n3、设置高亮html标签，默认是&lt;em&gt;标签\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;博客&quot;\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;pre_tags&quot;: [&quot;&lt;tag1&gt;&quot;],\r\n    &quot;post_tags&quot;: [&quot;&lt;/tag2&gt;&quot;], \r\n    &quot;fields&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;type&quot;: &quot;plain&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n4、高亮片段fragment的设置\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match&quot;: { &quot;user&quot;: &quot;kimchy&quot; }\r\n    },\r\n    &quot;highlight&quot; : {\r\n        &quot;fields&quot; : {\r\n            &quot;content&quot; : {&quot;fragment_size&quot; : 150, &quot;number_of_fragments&quot; : 3, &quot;no_match_size&quot;: 150 }\r\n        }\r\n    }\r\n}\r\n\r\nfragment_size: 你一个Field的值，比如有长度是1万，但是你不可能在页面上显示这么长啊。。。设置要显示出来的fragment文本判断的长度，默认是100\r\nnumber_of_fragments：你可能你的高亮的fragment文本片段有多个片段，你可以指定就显示几个片段\r\n',8,0,0,1514620732,0,0,0),(166,1,'搜索模板','','','搜索模板，search template，高级功能，就可以将我们的一些搜索进行模板化，然后的话，每次执行这个搜索，就直接调用模板，给传入一些参数就可以了\r\n\r\n越高级的功能，越少使用，可能只有在你真的遇到特别合适的场景的时候，才会去使用某个高级功能。但是，这些高级功能你是否掌握，其实就是普通的es开发人员，和es高手之间的一个区别。高手，一般来说，会把一个技术掌握的特别好，特别全面，特别深入，也许他平时用不到这个技术，但是当真的遇到一定的场景的时候，高手可以基于自己的深厚的技术储备，立即反应过来，找到一个合适的解决方案。\r\n\r\n如果是一个普通的技术人员，一般只会学习跟自己当前工作相关的一些知识和技术，只要求自己掌握的技术可以解决工作目前遇到的问题就可以了，就满足了，就会止步不前了，然后就不会更加深入的去学习一个技术。但是，当你的项目真正遇到问题的时候，遇到了一些难题，你之前的那一点技术储备已经没法去应付这些更加困难的问题了，此时，普通的技术人员就会扎耳挠腮，没有任何办法。\r\n\r\n高手，对技术是很有追求，能够精通很多自己遇到过的技术，但是也许自己学的很多东西，自己都没用过，但是不要紧，这是你的一种技术储备。\r\n\r\n1、search template入门\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot; : {\r\n    &quot;query&quot;: { \r\n      &quot;match&quot; : { \r\n        &quot;{{field}}&quot; : &quot;{{value}}&quot; \r\n      } \r\n    }\r\n  },\r\n  &quot;params&quot; : {\r\n      &quot;field&quot; : &quot;title&quot;,\r\n      &quot;value&quot; : &quot;博客&quot;\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: { \r\n    &quot;match&quot; : { \r\n      &quot;title&quot; : &quot;博客&quot; \r\n    } \r\n  }\r\n}\r\n\r\nsearch template：&quot;{{field}}&quot; : &quot;{{value}}&quot; \r\n\r\n2、toJson\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot;: &quot;{\\&quot;query\\&quot;: {\\&quot;match\\&quot;: {{#toJson}}matchCondition{{/toJson}}}}&quot;,\r\n  &quot;params&quot;: {\r\n    &quot;matchCondition&quot;: {\r\n      &quot;title&quot;: &quot;博客&quot;\r\n    }\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: { \r\n    &quot;match&quot; : { \r\n      &quot;title&quot; : &quot;博客&quot; \r\n    } \r\n  }\r\n}\r\n\r\n3、join\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot;: {\r\n    &quot;query&quot;: {\r\n      &quot;match&quot;: {\r\n        &quot;title&quot;: &quot;{{#join delimiter=&#039; &#039;}}titles{{/join delimiter=&#039; &#039;}}&quot;\r\n      }\r\n    }\r\n  },\r\n  &quot;params&quot;: {\r\n    &quot;titles&quot;: [&quot;博客&quot;, &quot;网站&quot;]\r\n  }\r\n}\r\n\r\n博客,网站\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: { \r\n    &quot;match&quot; : { \r\n      &quot;title&quot; : &quot;博客 网站&quot; \r\n    } \r\n  }\r\n}\r\n\r\n4、default value\r\n\r\nPOST /blog_website/blogs/1/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;views&quot;: 5\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot;: {\r\n    &quot;query&quot;: {\r\n      &quot;range&quot;: {\r\n        &quot;views&quot;: {\r\n          &quot;gte&quot;: &quot;{{start}}&quot;,\r\n          &quot;lte&quot;: &quot;{{end}}{{^end}}20{{/end}}&quot;\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;params&quot;: {\r\n    &quot;start&quot;: 1,\r\n    &quot;end&quot;: 10\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;range&quot;: {\r\n      &quot;views&quot;: {\r\n        &quot;gte&quot;: 1,\r\n        &quot;lte&quot;: 10\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot;: {\r\n    &quot;query&quot;: {\r\n      &quot;range&quot;: {\r\n        &quot;views&quot;: {\r\n          &quot;gte&quot;: &quot;{{start}}&quot;,\r\n          &quot;lte&quot;: &quot;{{end}}{{^end}}20{{/end}}&quot;\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;params&quot;: {\r\n    &quot;start&quot;: 1\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;range&quot;: {\r\n      &quot;views&quot;: {\r\n        &quot;gte&quot;: 1,\r\n        &quot;lte&quot;: 20\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\n5、conditional\r\n\r\nes的config/scripts目录下，预先保存这个复杂的模板，后缀名是.mustache，文件名是conditonal\r\n\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: {\r\n        &quot;match&quot;: {\r\n          &quot;line&quot;: &quot;{{text}}&quot; \r\n        }\r\n      },\r\n      &quot;filter&quot;: {\r\n        {{#line_no}} \r\n          &quot;range&quot;: {\r\n            &quot;line_no&quot;: {\r\n              {{#start}} \r\n                &quot;gte&quot;: &quot;{{start}}&quot; \r\n                {{#end}},{{/end}} \r\n              {{/start}} \r\n              {{#end}} \r\n                &quot;lte&quot;: &quot;{{end}}&quot; \r\n              {{/end}} \r\n            }\r\n          }\r\n        {{/line_no}} \r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /my_index/my_type/_search \r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;my_index&quot;,\r\n        &quot;_type&quot;: &quot;my_type&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;line&quot;: &quot;我的博客&quot;,\r\n          &quot;line_no&quot;: 5\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /my_index/my_type/_search/template\r\n{\r\n  &quot;file&quot;: &quot;conditional&quot;,\r\n  &quot;params&quot;: {\r\n    &quot;text&quot;: &quot;博客&quot;,\r\n    &quot;line_no&quot;: true,\r\n    &quot;start&quot;: 1,\r\n    &quot;end&quot;: 10\r\n  }\r\n}\r\n\r\n6、保存search template\r\n\r\nconfig/scripts，.mustache \r\n\r\n提供一个思路\r\n\r\n比如说，一般在大型的团队中，可能不同的人，都会想要执行一些类似的搜索操作\r\n这个时候，有一些负责底层运维的一些同学，就可以基于search template，封装一些模板出来，然后是放在各个es进程的scripts目录下的\r\n其他的团队，其实就不用各个团队自己反复手写复杂的通用的查询语句了，直接调用某个搜索模板，传入一些参数就好了\r\n\r\n',8,0,0,1514620781,0,0,0),(167,1,'completion suggest','','','suggest，completion suggest，自动完成，搜索推荐，搜索提示 --&gt; 自动完成，auto completion\r\n\r\nauto completion\r\n\r\n比如说我们在百度，搜索，你现在搜索“大话西游” --&gt; \r\n百度，自动给你提示，“大话西游电影”，“大话西游小说”， “大话西游手游”\r\n\r\n不用你把所有你想要输入的文本都输入完，搜索引擎会自动提示你可能是你想要搜索的那个文本\r\n\r\nPUT /news_website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;news&quot; : {\r\n      &quot;properties&quot; : {\r\n        &quot;title&quot; : {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n          &quot;fields&quot;: {\r\n            &quot;suggest&quot; : {\r\n              &quot;type&quot; : &quot;completion&quot;,\r\n              &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n            }\r\n          }\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\ncompletion，es实现的时候，是非常高性能的，会构建不是倒排索引，也不是正拍索引，就是纯的用于进行前缀搜索的一种特殊的数据结构，而且会全部放在内存中，所以auto completion进行的前缀搜索提示，性能是非常高的\r\n\r\n大话西游\r\n\r\nPUT /news_website/news/1\r\n{\r\n  &quot;title&quot;: &quot;大话西游电影&quot;,\r\n  &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot;\r\n}\r\nPUT /news_website/news/2\r\n{\r\n  &quot;title&quot;: &quot;大话西游小说&quot;,\r\n  &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;\r\n}\r\nPUT /news_website/news/3\r\n{\r\n  &quot;title&quot;: &quot;大话西游手游&quot;,\r\n  &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot;\r\n}\r\n\r\nGET /news_website/news/_search\r\n{\r\n  &quot;suggest&quot;: {\r\n    &quot;my-suggest&quot; : {\r\n      &quot;prefix&quot; : &quot;大话西游&quot;,\r\n      &quot;completion&quot; : {\r\n        &quot;field&quot; : &quot;title.suggest&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 6,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 0,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;suggest&quot;: {\r\n    &quot;my-suggest&quot;: [\r\n      {\r\n        &quot;text&quot;: &quot;大话西游&quot;,\r\n        &quot;offset&quot;: 0,\r\n        &quot;length&quot;: 4,\r\n        &quot;options&quot;: [\r\n          {\r\n            &quot;text&quot;: &quot;大话西游小说&quot;,\r\n            &quot;_index&quot;: &quot;news_website&quot;,\r\n            &quot;_type&quot;: &quot;news&quot;,\r\n            &quot;_id&quot;: &quot;2&quot;,\r\n            &quot;_score&quot;: 1,\r\n            &quot;_source&quot;: {\r\n              &quot;title&quot;: &quot;大话西游小说&quot;,\r\n              &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;\r\n            }\r\n          },\r\n          {\r\n            &quot;text&quot;: &quot;大话西游手游&quot;,\r\n            &quot;_index&quot;: &quot;news_website&quot;,\r\n            &quot;_type&quot;: &quot;news&quot;,\r\n            &quot;_id&quot;: &quot;3&quot;,\r\n            &quot;_score&quot;: 1,\r\n            &quot;_source&quot;: {\r\n              &quot;title&quot;: &quot;大话西游手游&quot;,\r\n              &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot;\r\n            }\r\n          },\r\n          {\r\n            &quot;text&quot;: &quot;大话西游电影&quot;,\r\n            &quot;_index&quot;: &quot;news_website&quot;,\r\n            &quot;_type&quot;: &quot;news&quot;,\r\n            &quot;_id&quot;: &quot;1&quot;,\r\n            &quot;_score&quot;: 1,\r\n            &quot;_source&quot;: {\r\n              &quot;title&quot;: &quot;大话西游电影&quot;,\r\n              &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot;\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /news_website/news/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;大话西游电影&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 9,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1.3495269,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;news_website&quot;,\r\n        &quot;_type&quot;: &quot;news&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1.3495269,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;大话西游电影&quot;,\r\n          &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;news_website&quot;,\r\n        &quot;_type&quot;: &quot;news&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1.217097,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;大话西游手游&quot;,\r\n          &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;news_website&quot;,\r\n        &quot;_type&quot;: &quot;news&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1.1299736,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;大话西游小说&quot;,\r\n          &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514620827,0,0,0),(168,1,'高级的用法','','','高级的用法\r\n\r\n比如说，我们本来没有某个type，或者没有某个field，但是希望在插入数据的时候，es自动为我们做一个识别，动态映射出这个type的mapping，包括每个field的数据类型，一般用的动态映射，dynamic mapping\r\n\r\n这里有个问题，如果说，我们其实对dynamic mapping有一些自己独特的需求，比如说，es默认来说，如经过识别到一个数字，field: 10，默认是搞成这个field的数据类型是long，再比如说，如果我们弄了一个field : &quot;10&quot;，默认就是text，还会带一个keyword的内置field。我们没法改变。\r\n\r\n但是我们现在就是希望动态映射的时候，根据我们的需求去映射，而不是让es自己按照默认的规则去玩儿\r\n\r\ndyanmic mapping template，动态映射模板\r\n\r\n我们自己预先定义一个模板，然后插入数据的时候，相关的field，如果能够根据我们预先定义的规则，匹配上某个我们预定义的模板，那么就会根据我们的模板来进行mapping，决定这个Field的数据类型\r\n\r\n0、默认的动态映射的效果咋样\r\n\r\nDELETE /my_index\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;test_string&quot;: &quot;hello world&quot;,\r\n  &quot;test_number&quot;: 10\r\n}\r\n\r\nes的自动的默认的，动态映射是咋样的。。。\r\n\r\nGET /my_index/_mapping/my_type\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;test_number&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          },\r\n          &quot;test_string&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n这个就是es的默认的动态映射规则，可能就不是我们想要的。。。\r\n\r\n我们比如说，现在想要的效果是啥。。。\r\n\r\ntest_number，如果是个数字，我们希望默认就是integer类型的\r\ntest_string，如果是字符串，我们希望默认是个text，这个没问题，但是内置的field名字，叫做raw，不叫座keyword，类型还是keyword，保留500个字符\r\n\r\n1、根据类型匹配映射模板\r\n\r\n动态映射模板，有两种方式，第一种，是根据新加入的field的默认的数据类型，来进行匹配，匹配上某个预定义的模板；第二种，是根据新加入的field的名字，去匹配预定义的名字，或者去匹配一个预定义的通配符，然后匹配上某个预定义的模板\r\n\r\nPUT my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;dynamic_templates&quot;: [\r\n        {\r\n          &quot;integers&quot;: {\r\n            &quot;match_mapping_type&quot;: &quot;long&quot;,\r\n            &quot;mapping&quot;: {\r\n              &quot;type&quot;: &quot;integer&quot;\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;strings&quot;: {\r\n            &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n            &quot;mapping&quot;: {\r\n              &quot;type&quot;: &quot;text&quot;,\r\n              &quot;fields&quot;: {\r\n                &quot;raw&quot;: {\r\n                  &quot;type&quot;: &quot;keyword&quot;,\r\n                  &quot;ignore_above&quot;: 500\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;test_long&quot;: 1,\r\n  &quot;test_string&quot;: &quot;hello world&quot;\r\n}\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;dynamic_templates&quot;: [\r\n          {\r\n            &quot;integers&quot;: {\r\n              &quot;match_mapping_type&quot;: &quot;long&quot;,\r\n              &quot;mapping&quot;: {\r\n                &quot;type&quot;: &quot;integer&quot;\r\n              }\r\n            }\r\n          },\r\n          {\r\n            &quot;strings&quot;: {\r\n              &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n              &quot;mapping&quot;: {\r\n                &quot;fields&quot;: {\r\n                  &quot;raw&quot;: {\r\n                    &quot;ignore_above&quot;: 500,\r\n                    &quot;type&quot;: &quot;keyword&quot;\r\n                  }\r\n                },\r\n                &quot;type&quot;: &quot;text&quot;\r\n              }\r\n            }\r\n          }\r\n        ],\r\n        &quot;properties&quot;: {\r\n          &quot;test_number&quot;: {\r\n            &quot;type&quot;: &quot;integer&quot;\r\n          },\r\n          &quot;test_string&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;raw&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 500\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n\r\n2、根据字段名配映射模板\r\n\r\nPUT /my_index \r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;dynamic_templates&quot;: [\r\n        {\r\n          &quot;string_as_integer&quot;: {\r\n            &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n            &quot;match&quot;: &quot;long_*&quot;,\r\n            &quot;unmatch&quot;: &quot;*_text&quot;,\r\n            &quot;mapping&quot;: {\r\n              &quot;type&quot;: &quot;integer&quot;\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n举个例子，field : &quot;10&quot;，把类似这种field，弄成long型\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;dynamic_templates&quot;: [\r\n          {\r\n            &quot;string_as_integer&quot;: {\r\n              &quot;match&quot;: &quot;long_*&quot;,\r\n              &quot;unmatch&quot;: &quot;*_text&quot;,\r\n              &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n              &quot;mapping&quot;: {\r\n                &quot;type&quot;: &quot;integer&quot;\r\n              }\r\n            }\r\n          }\r\n        ],\r\n        &quot;properties&quot;: {\r\n          &quot;long_field&quot;: {\r\n            &quot;type&quot;: &quot;integer&quot;\r\n          },\r\n          &quot;long_field_text&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n场景，有些时候，dynamic mapping + template，每天有一堆日志，每天有一堆数据\r\n\r\n这些数据，每天的数据都放一个新的type中，每天的数据都会哗哗的往新的tye中写入，此时你就可以定义一个模板，搞一个脚本，每天都预先生成一个新type的模板，里面讲你的各个Field都匹配到一个你预定义的模板中去，就好了\r\n',8,0,0,1514620874,0,0,0),(169,1,'基于地理位置的搜索，和聚合分析','','','这一讲开始，后面会跟着几讲内容，将地理位置相关的知识给大家讲解一下\r\n\r\n主要是es支持基于地理位置的搜索，和聚合分析的\r\n\r\n举个例子，比如说，我们后面就会给大家演示一下，你现在如果说做了一个酒店o2o app，让你的用户在任何地方，都可以根据当前所在的位置，找到自己身边的符合条件的一些酒店，那么此时就完全可以使用es来实现，非常合适\r\n\r\n我现在在上海某个大厦附近，我要搜索到距离我2公里以内的5星级的带游泳池的一个酒店s，用es就完全可以实现类似这样的基于地理位置的搜索引擎\r\n\r\n1、建立geo_point类型的mapping\r\n\r\n第一个地理位置的数据类型，就是geo_point，geo_point，说白了，就是一个地理位置坐标点，包含了一个经度，一个维度，经纬度，就可以唯一定位一个地球上的坐标\r\n\r\nPUT /my_index \r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;location&quot;: {\r\n          &quot;type&quot;: &quot;geo_point&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2、写入geo_point的3种方法\r\n\r\nPUT my_index/my_type/1\r\n{\r\n  &quot;text&quot;: &quot;Geo-point as an object&quot;,\r\n  &quot;location&quot;: { \r\n    &quot;lat&quot;: 41.12,\r\n    &quot;lon&quot;: -71.34\r\n  }\r\n}\r\n\r\nlatitude：维度\r\nlongitude：经度\r\n\r\n我们这里就不用去关心，这些坐标到底代表什么地方，其实都是我自己随便写的，只要能够作为课程，给大家演示清楚就可以了，自己去找一些提供地理位置的一些公司，供应商，api，百度地图，也是提供各个地方的经纬度的\r\n\r\n不建议用下面两种语法\r\n\r\nPUT my_index/my_type/2\r\n{\r\n  &quot;text&quot;: &quot;Geo-point as a string&quot;,\r\n  &quot;location&quot;: &quot;41.12,-71.34&quot; \r\n}\r\n\r\nPUT my_index/my_type/4\r\n{\r\n  &quot;text&quot;: &quot;Geo-point as an array&quot;,\r\n  &quot;location&quot;: [ -71.34, 41.12 ] \r\n}\r\n\r\n3、根据地理位置进行查询\r\n\r\n最最简单的，根据地理位置查询一些点，比如说，下面geo_bounding_box查询，查询某个矩形的地理位置范围内的坐标点\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;geo_bounding_box&quot;: {\r\n      &quot;location&quot;: {\r\n        &quot;top_left&quot;: {\r\n          &quot;lat&quot;: 42,\r\n          &quot;lon&quot;: -72\r\n        },\r\n        &quot;bottom_right&quot;: {\r\n          &quot;lat&quot;: 40,\r\n          &quot;lon&quot;: -74\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 81,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;my_index&quot;,\r\n        &quot;_type&quot;: &quot;my_type&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;location&quot;: {\r\n            &quot;lat&quot;: 41.12,\r\n            &quot;lon&quot;: -71.34\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n比如41.12,-71.34就是一个酒店，然后我们现在搜索的是从42,-72（代表了大厦A）和40,-74（代表了马路B）作为矩形的范围，在这个范围内的酒店，是什么\r\n',8,0,0,1514620910,0,0,0),(170,1,'搜索指定区域范围内的酒店','','','稍微真实点的案例，酒店o2o app作为一个背景，用各种各样的方式，去搜索你当前所在的地理位置附近的酒店\r\n\r\n搜索指定区域范围内的酒店，比如说，我们可以在搜索的时候，指定两个地点，就要在东方明珠大厦和上海路组成的矩阵的范围内，搜索我想要的酒店\r\n\r\nPUT /hotel_app\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;hotels&quot;: {\r\n            &quot;properties&quot;: {\r\n                &quot;pin&quot;: {\r\n                    &quot;properties&quot;: {\r\n                        &quot;location&quot;: {\r\n                            &quot;type&quot;: &quot;geo_point&quot;\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nPUT /hotel_app/hotels/1\r\n{\r\n    &quot;name&quot;: &quot;喜来登大酒店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}\r\n\r\nGET /hotel_app/hotels/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match_all&quot;: {}\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;geo_bounding_box&quot;: {\r\n          &quot;pin.location&quot;: {\r\n            &quot;top_left&quot; : {\r\n                &quot;lat&quot; : 40.73,\r\n                &quot;lon&quot; : -74.1\r\n            },\r\n            &quot;bottom_right&quot; : {\r\n                &quot;lat&quot; : 40.01,\r\n                &quot;lon&quot; : -71.12\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /hotel_app/hotels/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match_all&quot;: {}\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;geo_polygon&quot;: {\r\n          &quot;pin.location&quot;: {\r\n            &quot;points&quot;: [\r\n              {&quot;lat&quot; : 40.73, &quot;lon&quot; : -74.1},\r\n              {&quot;lat&quot; : 40.01, &quot;lon&quot; : -71.12},\r\n              {&quot;lat&quot; : 50.56, &quot;lon&quot; : -90.58}\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n我们现在要指定东方明珠大厦，上海路，上海博物馆，这三个地区组成的多边形的范围内，我要搜索这里面的酒店\r\n',8,0,0,1514620963,0,0,0),(171,1,'酒店o2o app','','','酒店o2o app，作为案例背景\r\n\r\n比如说，现在用户，所在的位置，是个地理位置的坐标，我是知道我的坐标的，app是知道的，android，地理位置api，都可以拿到当前手机app的经纬度\r\n\r\n我说，我现在就要搜索出，举例我200m，或者1公里内的酒店\r\n\r\n重要！！！！\r\n\r\n我们之前出去玩儿，都会用一些酒店o2o app，典型的代表，很多旅游app，一般来说，我们怎么搜索，到了一个地方，就会搜索说，我这里举例几百米，2公里内的酒店，搜索一下\r\n\r\n上节课讲解的，其实也很重要，一般来说，发生在我们在去旅游之前，会现在旅游app上搜索一个区域内的酒店，比如说，指定了西安火车站、西安博物馆，拿指定的几个地方的地理位置，组成一个多边形区域范围，去搜索这个区域内的酒店\r\n\r\n承认，一些案例，当然不可能说达到讲解真实的复杂的大型的项目的效果来的那么好，光是学知识，学技术而言，有一些案例就非常不错了\r\n\r\n后面，会讲解真正的企业级的大型的搜索引擎，真实复杂业务的数据分析系统的项目\r\n\r\nGET /hotel_app/hotels/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match_all&quot;: {}\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;geo_distance&quot;: {\r\n          &quot;distance&quot;: &quot;200km&quot;,\r\n          &quot;pin.location&quot;: {\r\n            &quot;lat&quot;: 40,\r\n            &quot;lon&quot;: -70\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514621614,0,0,0),(172,1,'基于地理位置进行聚合','','','最后一个知识点，基于地理位置进行聚合分析\r\n\r\n我的需求就是，统计一下，举例我当前坐标的几个范围内的酒店的数量，比如说举例我0~100m有几个酒店，100m~300m有几个酒店，300m以上有几个酒店\r\n\r\n一般来说，做酒店app，一般来说，我们是不是会有一个地图，用户可以在地图上直接查看和搜索酒店，此时就可以显示出来举例你当前的位置，几个举例范围内，有多少家酒店，让用户知道，心里清楚，用户体验就比较好\r\n\r\nGET /hotel_app/hotels/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;agg_by_distance_range&quot;: {\r\n      &quot;geo_distance&quot;: {\r\n        &quot;field&quot;: &quot;pin.location&quot;,\r\n        &quot;origin&quot;: {\r\n          &quot;lat&quot;: 40,\r\n          &quot;lon&quot;: -70\r\n        },\r\n        &quot;unit&quot;: &quot;mi&quot;, \r\n        &quot;ranges&quot;: [\r\n          {\r\n            &quot;to&quot;: 100\r\n          },\r\n          {\r\n            &quot;from&quot;: 100,\r\n            &quot;to&quot;: 300\r\n          },\r\n          {\r\n            &quot;from&quot;: 300\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;agg_by_distance_range&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;*-100.0&quot;,\r\n          &quot;from&quot;: 0,\r\n          &quot;to&quot;: 100,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;100.0-300.0&quot;,\r\n          &quot;from&quot;: 100,\r\n          &quot;to&quot;: 300,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;300.0-*&quot;,\r\n          &quot;from&quot;: 300,\r\n          &quot;doc_count&quot;: 0\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nm (metres) but it can also accept: m (miles), km (kilometers)\r\n\r\nsloppy_arc (the default), arc (most accurate) and plane (fastest)\r\n',8,0,0,1514621664,0,0,0),(173,1,'基本的java api','','','快速入门篇，讲解过了一些基本的java api，包括了document增删改查，基本的搜索，基本的聚合\r\n\r\n高手进阶篇，必须将java api这块深入讲解一下，介绍一些最常用的，最核心的一些api的使用，用一个模拟现实的案例背景，让大家在学习的时候更加贴近业务\r\n\r\n话说在前面，我们是不可能将所有的java api用视频全部录制一遍的，因为api太多了。。。。\r\n\r\n我们之前讲解各种功能，各种知识点，花了那么多的时间，哪儿些才是最最关键的，知识，原理，功能，es restful api，最次最次，哪怕是搞php，搞python的人也可以来学习\r\n\r\n如果说，现在要将所有所有的api全部用java api实现一遍和讲解，太耗费时间了，几乎不可能接受\r\n\r\n采取的粗略，将核心的java api语法，还有最最常用的那些api都给大家上课演示了\r\n\r\n然后最后一讲，会告诉大家，在掌握了之前那些课程讲解的各种知识点之后，如果要用java api去实现和开发，应该怎么自己去探索和掌握\r\n\r\njava api，api的学习，实际上是最最简单的，纯用，没什么难度，技术难度，你掌握了课上讲解的这些api之后，自己应该就可以举一反三，后面自己去探索和尝试出自己要用的各种功能对应的java api是什么。。。\r\n\r\n1、client集群自动探查\r\n\r\n默认情况下，是根据我们手动指定的所有节点，依次轮询这些节点，来发送各种请求的，如下面的代码，我们可以手动为client指定多个节点\r\n\r\nTransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost1&quot;), 9300))\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost2&quot;), 9300))\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost3&quot;), 9300));\r\n\r\n但是问题是，如果我们有成百上千个节点呢？难道也要这样手动添加吗？\r\n\r\nes client提供了一种集群节点自动探查的功能，打开这个自动探查机制以后，es client会根据我们手动指定的几个节点连接过去，然后通过集群状态自动获取当前集群中的所有data node，然后用这份完整的列表更新自己内部要发送请求的node list。默认每隔5秒钟，就会更新一次node list。\r\n\r\n但是注意，es cilent是不会将Master node纳入node list的，因为要避免给master node发送搜索等请求。\r\n\r\n这样的话，我们其实直接就指定几个master node，或者1个node就好了，client会自动去探查集群的所有节点，而且每隔5秒还会自动刷新。非常棒。\r\n\r\nSettings settings = Settings.builder()\r\n        .put(&quot;client.transport.sniff&quot;, true).build();\r\nTransportClient client = new PreBuiltTransportClient(settings);\r\n\r\n使用上述的settings配置，将client.transport.sniff设置为true即可打开集群节点自动探查功能\r\n\r\n在实际的生产环境中，都是这么玩儿的。。。\r\n\r\n2、汽车零售案例背景\r\n\r\n简单来说，会涉及到三个数据，汽车信息，汽车销售记录，汽车4S店信息\r\n',8,0,0,1514621706,0,0,0),(174,1,'汽车零售数据的mapping','','','做一个汽车零售数据的mapping，我们要做的第一份数据，其实汽车信息\r\n\r\nPUT /car_shop\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;cars&quot;: {\r\n            &quot;properties&quot;: {\r\n                &quot;brand&quot;: {\r\n                    &quot;type&quot;: &quot;text&quot;,\r\n                    &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n                    &quot;fields&quot;: {\r\n                        &quot;raw&quot;: {\r\n                            &quot;type&quot;: &quot;keyword&quot;\r\n                        }\r\n                    }\r\n                },\r\n                &quot;name&quot;: {\r\n                    &quot;type&quot;: &quot;text&quot;,\r\n                    &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n                    &quot;fields&quot;: {\r\n                        &quot;raw&quot;: {\r\n                            &quot;type&quot;: &quot;keyword&quot;\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n返回值 (注意 No living connections 时需要重启kibana)\r\n{\r\n  &quot;acknowledged&quot;: true,\r\n  &quot;shards_acknowledged&quot;: true\r\n}\r\n\r\n首先的话呢，第一次调整宝马320这个汽车的售价，我们希望将售价设置为32万，用一个upsert语法，如果这个汽车的信息之前不存在，那么就insert，如果存在，那么就update\r\n\r\nIndexRequest indexRequest = new IndexRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n        .source(jsonBuilder()\r\n            .startObject()\r\n                .field(&quot;brand&quot;, &quot;宝马&quot;)\r\n                .field(&quot;name&quot;, &quot;宝马320&quot;)\r\n                .field(&quot;price&quot;, 320000)\r\n                .field(&quot;produce_date&quot;, &quot;2017-01-01&quot;)\r\n            .endObject());\r\n\r\nUpdateRequest updateRequest = new UpdateRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n        .doc(jsonBuilder()\r\n            .startObject()\r\n                .field(&quot;price&quot;, 320000)\r\n            .endObject())\r\n        .upsert(indexRequest);       \r\n               \r\nclient.update(updateRequest).get();\r\n\r\nIndexRequest indexRequest = new IndexRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n        .source(jsonBuilder()\r\n            .startObject()\r\n                .field(&quot;brand&quot;, &quot;宝马&quot;)\r\n                .field(&quot;name&quot;, &quot;宝马320&quot;)\r\n                .field(&quot;price&quot;, 310000)\r\n                .field(&quot;produce_date&quot;, &quot;2017-01-01&quot;)\r\n            .endObject());\r\nUpdateRequest updateRequest = new UpdateRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n        .doc(jsonBuilder()\r\n            .startObject()\r\n                .field(&quot;price&quot;, 310000)\r\n            .endObject())\r\n        .upsert(indexRequest);              \r\nclient.update(updateRequest).get();\r\n\r\nNew --- Maven Project --- Next --- Group Id:comm.peng.es Artifact Id:es-senior Package:com.peng.es.senior\r\n删除无关文件 \r\n修改pom.xml\r\n\r\n编辑代码 UpsertCarInfoApp.java\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.index.IndexRequest;\r\nimport org.elasticsearch.action.update.UpdateRequest;\r\nimport org.elasticsearch.action.update.UpdateResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.common.xcontent.XContentFactory;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class UpsertCarInfoApp {\r\n	\r\n	@SuppressWarnings({ &quot;unchecked&quot;, &quot;resource&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.put(&quot;client.transport.sniff&quot;, true)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300));\r\n	\r\n		IndexRequest indexRequest = new IndexRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n				.source(XContentFactory.jsonBuilder()\r\n							.startObject()\r\n								.field(&quot;brand&quot;, &quot;宝马&quot;)\r\n								.field(&quot;name&quot;, &quot;宝马320&quot;)\r\n								.field(&quot;price&quot;, 310000)\r\n								.field(&quot;produce_date&quot;, &quot;2017-01-01&quot;)\r\n							.endObject());\r\n		\r\n		UpdateRequest updateRequest = new UpdateRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n				.doc(XContentFactory.jsonBuilder()\r\n						.startObject()\r\n							.field(&quot;price&quot;, 300000)\r\n						.endObject())\r\n				.upsert(indexRequest); \r\n		\r\n		UpdateResponse updateResponse = client.update(updateRequest).get();\r\n	\r\n		System.out.println(updateResponse.getVersion()); \r\n		//System.out.println(updateResponse.getResult().getOp());\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n运行代码 Run As\r\n\r\n查询 GET /car_shop/cars/_search\r\n{\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;cars&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 310000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n运行java代码 返回版本号:2\r\n查询 GET /car_shop/cars/_search\r\n{\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;cars&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 300000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514621746,0,0,0),(175,1,'混合销售','','','场景，一般来说，我们都可以在一些汽车网站上，或者在混合销售多个品牌的汽车4S店的内部，都可以在系统里调出来多个汽车的信息，放在网页上，进行对比\r\n\r\nmget，一次性将多个document的数据查询出来，放在一起显示，多个汽车的型号，一次性拿出了多辆汽车的信息\r\n\r\n手动添加\r\nPUT /car_shop/cars/2\r\n{\r\n	&quot;brand&quot;: &quot;奔驰&quot;,\r\n	&quot;name&quot;: &quot;奔驰C200&quot;,\r\n	&quot;price&quot;: 350000,\r\n	&quot;produce_date&quot;: &quot;2017-01-05&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;cars&quot;,\r\n  &quot;_id&quot;: &quot;2&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nMultiGetResponse multiGetItemResponses = client.prepareMultiGet()\r\n    .add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)           \r\n    .add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;2&quot;)        \r\n    .get();\r\n\r\nfor (MultiGetItemResponse itemResponse : multiGetItemResponses) { \r\n    GetResponse response = itemResponse.getResponse();\r\n    if (response.isExists()) {                      \r\n        String json = response.getSourceAsString(); \r\n    }\r\n}\r\n\r\n编写代码\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.get.GetResponse;\r\nimport org.elasticsearch.action.get.MultiGetItemResponse;\r\nimport org.elasticsearch.action.get.MultiGetResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class MGetMultiCarInfoApp {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300)); \r\n	\r\n		MultiGetResponse multiGetResponse = client.prepareMultiGet()\r\n				.add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n				.add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;2&quot;) \r\n				.get();\r\n		\r\n		for(MultiGetItemResponse multiGetItemResponse : multiGetResponse) {\r\n			GetResponse getResponse = multiGetItemResponse.getResponse();\r\n			if(getResponse.isExists()) {\r\n				System.out.println(getResponse.getSourceAsString());  \r\n			}\r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n运行\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}\r\n{\r\n	&quot;brand&quot;: &quot;奔驰&quot;,\r\n	&quot;name&quot;: &quot;奔驰C200&quot;,\r\n	&quot;price&quot;: 350000,\r\n	&quot;produce_date&quot;: &quot;2017-01-05&quot;\r\n}',8,0,0,1514621794,0,0,0),(176,1,'批量上传到es中去','','','业务场景：有一个汽车销售公司，拥有很多家4S店，这些4S店的数据，都会在一段时间内陆续传递过来，汽车的销售数据，现在希望能够在内存中缓存比如1000条销售数据，然后一次性批量上传到es中去\r\n\r\nPUT /car_shop/sales/1\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 300000,\r\n    &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;sales&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nPUT /car_shop/sales/2\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 300000,\r\n    &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;sales&quot;,\r\n  &quot;_id&quot;: &quot;2&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nBulkRequestBuilder bulkRequest = client.prepareBulk();\r\n\r\nbulkRequest.add(client.prepareIndex(&quot;car_shop&quot;, &quot;sales&quot;, &quot;3&quot;)\r\n        .setSource(jsonBuilder()\r\n                    .startObject()\r\n                        .field(&quot;brand&quot;, &quot;奔驰&quot;)\r\n                        .field(&quot;name&quot;, &quot;奔驰C200&quot;)\r\n                        .field(&quot;price&quot;, 350000)\r\n                        .field(&quot;produce_date&quot;, &quot;2017-01-05&quot;)\r\n                        .field(&quot;sale_price&quot;, 340000)\r\n                        .field(&quot;sale_date&quot;, &quot;2017-02-03&quot;)\r\n                    .endObject()\r\n                  )\r\n        );\r\n\r\nbulkRequest.add(client.prepareUpdate(&quot;car_shop&quot;, &quot;sales&quot;, &quot;1&quot;)\r\n        .setDoc(jsonBuilder()               \r\n		            .startObject()\r\n		                .field(&quot;sale_price&quot;, &quot;290000&quot;)\r\n		            .endObject()\r\n		        )\r\n        );\r\n\r\nbulkRequest.add(client.prepareDelete(&quot;car_shop&quot;, &quot;sales&quot;, &quot;2&quot;));\r\n\r\nBulkResponse bulkResponse = bulkRequest.get();\r\n\r\nif (bulkResponse.hasFailures()) {}\r\n\r\nGET /car_shop/sales/_search\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 300000,\r\n          &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 300000,\r\n          &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n编写代码\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.bulk.BulkItemResponse;\r\nimport org.elasticsearch.action.bulk.BulkRequestBuilder;\r\nimport org.elasticsearch.action.bulk.BulkResponse;\r\nimport org.elasticsearch.action.delete.DeleteRequestBuilder;\r\nimport org.elasticsearch.action.index.IndexRequestBuilder;\r\nimport org.elasticsearch.action.update.UpdateRequestBuilder;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.common.xcontent.XContentFactory;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class BulkUploadSalesDataApp {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300)); \r\n	\r\n		BulkRequestBuilder bulkRequestBuilder = client.prepareBulk();\r\n		\r\n		IndexRequestBuilder indexRequestBuilder = client.prepareIndex(&quot;car_shop&quot;, &quot;sales&quot;, &quot;3&quot;) \r\n				.setSource(XContentFactory.jsonBuilder()\r\n							.startObject()\r\n								.field(&quot;brand&quot;, &quot;奔驰&quot;)\r\n								.field(&quot;name&quot;, &quot;奔驰C200&quot;)\r\n								.field(&quot;price&quot;, 350000)\r\n								.field(&quot;produce_date&quot;, &quot;2017-01-20&quot;)\r\n								.field(&quot;sale_price&quot;, 320000)\r\n								.field(&quot;sale_date&quot;, &quot;2017-01-25&quot;)\r\n							.endObject());\r\n		bulkRequestBuilder.add(indexRequestBuilder);\r\n		\r\n		UpdateRequestBuilder updateRequestBuilder = client.prepareUpdate(&quot;car_shop&quot;, &quot;sales&quot;, &quot;1&quot;)\r\n				.setDoc(XContentFactory.jsonBuilder()\r\n						.startObject()\r\n							.field(&quot;sale_price&quot;, 290000)\r\n						.endObject());\r\n		bulkRequestBuilder.add(updateRequestBuilder);\r\n		\r\n		DeleteRequestBuilder deleteReqeustBuilder = client.prepareDelete(&quot;car_shop&quot;, &quot;sales&quot;, &quot;2&quot;); \r\n		bulkRequestBuilder.add(deleteReqeustBuilder);\r\n		\r\n		BulkResponse bulkResponse = bulkRequestBuilder.get();\r\n		\r\n		for(BulkItemResponse bulkItemResponse : bulkResponse.getItems()) {\r\n			System.out.println(&quot;version: &quot; + bulkItemResponse.getVersion()); \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n运行\r\nversion: 1\r\nversion: 2\r\nversion: 2\r\n\r\n运行后查询 GET /car_shop/sales/_search\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 290000,\r\n          &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;奔驰&quot;,\r\n          &quot;name&quot;: &quot;奔驰C200&quot;,\r\n          &quot;price&quot;: 350000,\r\n          &quot;produce_date&quot;: &quot;2017-01-20&quot;,\r\n          &quot;sale_price&quot;: 320000,\r\n          &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514621918,0,0,0),(177,1,'销售记录','','','比如说，现在要下载大批量的数据，从es，放到excel中，我们说，月度，或者年度，销售记录，很多，比如几千条，几万条，几十万条\r\n\r\n其实就要用到我们之前讲解的es scroll api，对大量数据批量的获取和处理\r\n\r\nPUT /car_shop/sales/4\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 280000,\r\n    &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;sales&quot;,\r\n  &quot;_id&quot;: &quot;4&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nGET /car_shop/sales/_search\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 280000,\r\n          &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 290000,\r\n          &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;奔驰&quot;,\r\n          &quot;name&quot;: &quot;奔驰C200&quot;,\r\n          &quot;price&quot;: 350000,\r\n          &quot;produce_date&quot;: &quot;2017-01-20&quot;,\r\n          &quot;sale_price&quot;: 320000,\r\n          &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n就是要看宝马的销售记录\r\n\r\n2条数据，做一个演示，每个批次下载一条宝马的销售记录，分2个批次给它下载完\r\n\r\nSearchResponse scrollResp = client.prepareSearch(&quot;car_shop&quot;)\r\n		.addTypes(&quot;sales&quot;)\r\n        .setScroll(new TimeValue(60000))\r\n        .setQuery(termQuery(&quot;brand.raw&quot;, &quot;宝马&quot;))\r\n        .setSize(1)\r\n        .get(); \r\n\r\ndo {\r\n    for (SearchHit hit : scrollResp.getHits().getHits()) {\r\n    	\r\n    }\r\n    \r\n    scrollResp = client.prepareSearchScroll(scrollResp.getScrollId())\r\n            .setScroll(new TimeValue(60000))\r\n            .execute()\r\n            .actionGet();\r\n} while(scrollResp.getHits().getHits().length != 0);\r\n\r\n编写代码\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.common.unit.TimeValue;\r\nimport org.elasticsearch.index.query.QueryBuilders;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class ScollDownloadSalesDataApp {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300)); \r\n	\r\n		SearchResponse searchResponse = client.prepareSearch(&quot;car_shop&quot;) \r\n				.setTypes(&quot;sales&quot;)\r\n				.setQuery(QueryBuilders.termQuery(&quot;brand.keyword&quot;, &quot;宝马&quot;))\r\n				.setScroll(new TimeValue(60000))\r\n				.setSize(1)\r\n				.get();\r\n		\r\n		int batchCount = 0;\r\n		\r\n		do {\r\n			for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n				System.out.println(&quot;batch: &quot; + ++batchCount); \r\n				System.out.println(searchHit.getSourceAsString());  \r\n				\r\n				// 每次查询一批数据，比如1000行，然后写入本地的一个excel文件中\r\n				\r\n				// 如果说你一下子查询几十万条数据，不现实，jvm内存可能都会爆掉\r\n			}\r\n			\r\n			searchResponse = client.prepareSearchScroll(searchResponse.getScrollId())\r\n					.setScroll(new TimeValue(60000))\r\n					.execute()\r\n					.actionGet();\r\n		} while(searchResponse.getHits().getHits().length != 0);\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\nbatch: 1\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 280000,\r\n    &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n}\r\n\r\nbatch: 2\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:320000,&quot;produce_date&quot;:&quot;2017-01-01&quot;,&quot;sale_price&quot;:290000,&quot;sale_date&quot;:&quot;2017-01-21&quot;}\r\n',8,0,0,1514621979,0,0,0),(178,1,'调用一个搜索模板','','','搜索模板的功能，java api怎么去调用一个搜索模板\r\n\r\npage_query_by_brand.mustache /es安装目录/config/script #注意编码(全使用UTF-8)\r\n\r\n{\r\n  &quot;from&quot;: {{from}},\r\n  &quot;size&quot;: {{size}},\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;brand.keyword&quot;: &quot;{{brand}}&quot; \r\n    }\r\n  }\r\n}\r\n\r\nSearchResponse sr = new SearchTemplateRequestBuilder(client)\r\n    .setScript(&quot;page_query_by_brand&quot;)                 \r\n    .setScriptType(ScriptService.ScriptType.FILE) \r\n    .setScriptParams(template_params)             \r\n    .setRequest(new SearchRequest())              \r\n    .get()                                        \r\n    .getResponse(); \r\n\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\nimport java.util.HashMap;\r\nimport java.util.Map;\r\n\r\nimport org.elasticsearch.action.search.SearchRequest;\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.script.ScriptType;\r\nimport org.elasticsearch.script.mustache.SearchTemplateRequestBuilder;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class SearchTemplatePageQuery {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300)); \r\n	\r\n		Map&lt;String, Object&gt; scriptParams = new HashMap&lt;String, Object&gt;();\r\n		scriptParams.put(&quot;from&quot;, 0);\r\n		scriptParams.put(&quot;size&quot;, 1);\r\n		scriptParams.put(&quot;brand&quot;, &quot;宝马&quot;);\r\n		\r\n		SearchResponse searchResponse = new SearchTemplateRequestBuilder(client)\r\n				.setScript(&quot;page_query_by_brand&quot;)\r\n				.setScriptType(ScriptType.FILE)\r\n				.setScriptParams(scriptParams)\r\n				.setRequest(new SearchRequest(&quot;car_shop&quot;).types(&quot;sales&quot;))\r\n				.get()\r\n				.getResponse();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 280000,\r\n    &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n}',8,0,0,1514622025,0,0,0),(179,1,'精简查询','','','PUT /car_shop/cars/5\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;cars&quot;,\r\n  &quot;_id&quot;: &quot;5&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;宝马&quot;))                \r\n        .get();\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(QueryBuilders.multiMatchQuery(&quot;宝马&quot;, &quot;brand&quot;, &quot;name&quot;))                \r\n        .get();\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(QueryBuilders.commonTermsQuery(&quot;name&quot;, &quot;宝马320&quot;))                \r\n        .get();\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(QueryBuilders.prefixQuery(&quot;name&quot;, &quot;宝&quot;))                \r\n        .get();\r\n\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.index.query.QueryBuilders;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class FullTextSearchByBrand {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300));  \r\n	\r\n		SearchResponse searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;宝马&quot;))\r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(QueryBuilders.multiMatchQuery(&quot;宝马&quot;, &quot;brand&quot;, &quot;name&quot;))  \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(QueryBuilders.termQuery(&quot;name.raw&quot;, &quot;宝马318&quot;))    \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(QueryBuilders.prefixQuery(&quot;name&quot;, &quot;宝&quot;))      \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}\r\n====================================================\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}\r\n====================================================\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n====================================================\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}',8,0,0,1514622136,0,0,0),(180,1,'多条件组合搜索','','','\r\nQueryBuilder qb = boolQuery()\r\n    .must(matchQuery(&quot;brand&quot;, &quot;宝马&quot;))    \r\n    .mustNot(termQuery(&quot;name.raw&quot;, &quot;宝马318&quot;)) \r\n    .should(termQuery(&quot;produce_date&quot;, &quot;2017-01-02&quot;))  \r\n    .filter(rangeQuery(&quot;price&quot;).gte(&quot;280000&quot;).lt(&quot;350000&quot;));\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(qb)                \r\n        .get();\r\n\r\n代码\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.index.query.QueryBuilder;\r\nimport org.elasticsearch.index.query.QueryBuilders;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class BoolQuerySearchBrand {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300));  \r\n	\r\n		QueryBuilder queryBuilder = QueryBuilders.boolQuery()\r\n				.must(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;宝马&quot;))\r\n				.mustNot(QueryBuilders.termQuery(&quot;name.raw&quot;, &quot;宝马318&quot;))\r\n				.should(QueryBuilders.rangeQuery(&quot;produce_date&quot;).gte(&quot;2017-01-01&quot;).lte(&quot;2017-01-31&quot;))\r\n				.filter(QueryBuilders.rangeQuery(&quot;price&quot;).gte(280000).lte(350000));    \r\n		\r\n		SearchResponse searchResponse = client.prepareSearch(&quot;car_shop&quot;)  \r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(queryBuilder)\r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}',8,0,0,1514622733,0,0,0),(181,1,'基于地理位置的搜索','','','&lt;dependency&gt;\r\n    &lt;groupId&gt;org.locationtech.spatial4j&lt;/groupId&gt;\r\n    &lt;artifactId&gt;spatial4j&lt;/artifactId&gt;\r\n    &lt;version&gt;0.6&lt;/version&gt;                        \r\n&lt;/dependency&gt;\r\n\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;com.vividsolutions&lt;/groupId&gt;\r\n    &lt;artifactId&gt;jts&lt;/artifactId&gt;\r\n    &lt;version&gt;1.13&lt;/version&gt;                         \r\n    &lt;exclusions&gt;\r\n        &lt;exclusion&gt;\r\n            &lt;groupId&gt;xerces&lt;/groupId&gt;\r\n            &lt;artifactId&gt;xercesImpl&lt;/artifactId&gt;\r\n        &lt;/exclusion&gt;\r\n    &lt;/exclusions&gt;\r\n&lt;/dependency&gt;\r\n\r\n比如我们有很多的4s店，然后呢给了用户一个app，在某个地方的时候，可以根据当前的地理位置搜索一下，自己附近的4s店\r\n\r\nPOST /car_shop/_mapping/shops\r\n{\r\n  &quot;properties&quot;: {\r\n      &quot;pin&quot;: {\r\n          &quot;properties&quot;: {\r\n              &quot;location&quot;: {\r\n                  &quot;type&quot;: &quot;geo_point&quot;\r\n              }\r\n          }\r\n      }\r\n  }\r\n}\r\n{\r\n  &quot;acknowledged&quot;: true\r\n}\r\n\r\nPUT /car_shop/shops/1\r\n{\r\n    &quot;name&quot;: &quot;上海至全宝马4S店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;shops&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n第一个需求：搜索两个坐标点组成的一个区域\r\n\r\nQueryBuilder qb = geoBoundingBoxQuery(&quot;pin.location&quot;).setCorners(40.73, -74.1, 40.01, -71.12); \r\n\r\n第二个需求：指定一个区域，由三个坐标点，组成，比如上海大厦，东方明珠塔，上海火车站\r\n\r\nList&lt;GeoPoint&gt; points = new ArrayList&lt;&gt;();             \r\npoints.add(new GeoPoint(40.73, -74.1));\r\npoints.add(new GeoPoint(40.01, -71.12));\r\npoints.add(new GeoPoint(50.56, -90.58));\r\n\r\nQueryBuilder qb = geoPolygonQuery(&quot;pin.location&quot;, points); \r\n\r\n第三个需求：搜索距离当前位置在200公里内的4s店\r\n\r\nQueryBuilder qb = geoDistanceQuery(&quot;pin.location&quot;).point(40, -70).distance(200, DistanceUnit.KILOMETERS);   \r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;shops&quot;)\r\n        .setQuery(qb)                \r\n        .get();\r\n\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\nimport java.util.ArrayList;\r\nimport java.util.List;\r\n\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.geo.GeoPoint;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.common.unit.DistanceUnit;\r\nimport org.elasticsearch.index.query.QueryBuilders;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class GeoLocationShopSearchApp {\r\n\r\n	@SuppressWarnings({ &quot;unchecked&quot;, &quot;resource&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300));\r\n		\r\n		SearchResponse searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;shops&quot;)\r\n				.setQuery(QueryBuilders.geoBoundingBoxQuery(&quot;pin.location&quot;)\r\n								.setCorners(40.73, -74.1, 40.01, -71.12))\r\n				.get();\r\n	\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		List&lt;GeoPoint&gt; points = new ArrayList&lt;GeoPoint&gt;();             \r\n		points.add(new GeoPoint(40.73, -74.1));\r\n		points.add(new GeoPoint(40.01, -71.12));\r\n		points.add(new GeoPoint(50.56, -90.58));\r\n\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;shops&quot;)\r\n				.setQuery(QueryBuilders.geoPolygonQuery(&quot;pin.location&quot;, points))  \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;shops&quot;)\r\n				.setQuery(QueryBuilders.geoDistanceQuery(&quot;pin.location&quot;)\r\n						.point(40, -70)\r\n						.distance(200, DistanceUnit.KILOMETERS))  \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n		\r\n{\r\n    &quot;name&quot;: &quot;上海至全宝马4S店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}\r\n\r\n====================================================\r\n{\r\n    &quot;name&quot;: &quot;上海至全宝马4S店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}\r\n\r\n====================================================\r\n{\r\n    &quot;name&quot;: &quot;上海至全宝马4S店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}',8,0,0,1514624054,0,0,0),(182,1,'logstash 安装','','','yum 方式安装\r\n------------------------------------------\r\njava -version 检测环境是否安装java\r\nrpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch\r\nvi /etc/yum.repos.d/logstash.repo\r\n[logstash-5.x]\r\nname=Elastic repository for 5.x packages\r\nbaseurl=https://artifacts.elastic.co/packages/5.x/yum\r\ngpgcheck=1\r\ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch\r\nenabled=1\r\nautorefresh=1\r\ntype=rpm-md\r\n\r\nsudo yum install logstash\r\n\r\n手动安装\r\n------------------------------------------\r\nhttps://www.elastic.co/downloads/logstash 下载 tar.gz包\r\n\r\nmkdir /opt/logstash\r\n将tar.gz包上传到/opt/logstash目录下\r\ncd /opt/logstash\r\ntar -zxvf logstash-5.1.1.tar.gz\r\ncd logstash-5.1.1\r\n\r\n第一个事件\r\n------------------------------------------\r\nbin/logstash -e &#039;input { stdin { } } output { stdout {} }&#039;\r\nthe stdin plugin is now waiting for input: #控制台提示输入\r\nwelcome to logstash world\r\n2016-12-05T08:04:32.142Z 0.0.0.0 hello welcome to logstash world\r\n\r\nbin/logstash -e &#039;input{stdin{}}output{stdout{codec=&gt;rubydebug}}&#039;\r\nwelcome to logstash world\r\n{\r\n    &quot;@timestamp&quot; =&gt; 2016-12-05T11:51:12.840Z,\r\n      &quot;@version&quot; =&gt; &quot;1&quot;,\r\n          &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,\r\n       &quot;message&quot; =&gt; &quot;welcome to logstash world&quot;,\r\n          &quot;tags&quot; =&gt; []\r\n}\r\n\r\n@timestamp 标记事件的发生时间\r\nmessage 客户端输入信息\r\nhost 标记事件发生在哪里\r\ntags 标记事件的某方面属性\r\n\r\n使用配置文件\r\n------------------------------------------\r\nvi logstash.conf #注意换行\r\n\r\ninput {stdin {}}\r\noutput {stdout {codec =&gt; rubydebug {}}}\r\nbin/logstash -f logstash.conf\r\nwelcome to logstash world\r\n{\r\n    &quot;@timestamp&quot; =&gt; 2016-12-05T11:51:12.840Z,\r\n      &quot;@version&quot; =&gt; &quot;1&quot;,\r\n          &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,\r\n       &quot;message&quot; =&gt; &quot;welcome to logstash world&quot;,\r\n          &quot;tags&quot; =&gt; []\r\n}',8,0,0,1514631304,0,0,0),(183,1,'stdout 标准输出','','','stdout 标准输出\r\n\r\nvi file.conf\r\ninput { stdin { } }\r\nfilter { }\r\noutput { stdout { } }\r\n\r\nbin/logstash -f file.conf\r\nwelcome # 输入\r\n2016-12-06T05:01:27.056Z 0.0.0.0 welcome # 输出\r\n\r\nvi logstash.conf\r\ninput { stdin { } }\r\nfilter { }\r\noutput { stdout { codec =&gt; rubydebug { } } }\r\n\r\nbin/logstash -f logstash.conf\r\nwelcome # 输入\r\n{\r\n    &quot;@timestamp&quot; =&gt; 2016-12-06T05:07:33.662Z,\r\n      &quot;@version&quot; =&gt; &quot;1&quot;,\r\n          &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,\r\n       &quot;message&quot; =&gt; &quot;welcome&quot;,\r\n          &quot;tags&quot; =&gt; []\r\n}',8,0,0,1514631391,0,0,0),(184,1,'软件包管理','','','    软件包管理简介\r\n    rpm 命令管理\r\n    yum 在线管理\r\n    源码包管理\r\n    脚本安装包\r\n\r\n    学习 linux 中最基本的软件安装方法\r\n\r\n    软件包分类\r\n    源码包 脚本安装包\r\n    二进制包 (RPM 包, 系统默认包) 编译\r\n\r\n    源码包\r\n    源码包的优点是\r\n    开源 如果有足够的能力 可以修改源代码\r\n    可以自由选择所需的功能\r\n    软件是编译安装 所以更加适合自己的系统 更加稳定也效率更高\r\n    卸载方便\r\n\r\n    源码包的缺点\r\n    安装过程步骤较多 尤其安装较大的软件集合时 (如LAMP 环境搭建)　容易出现拼写错误\r\n    编译过程时间较长 安装比二进制较长\r\n    因为编译安装 安装过程中一旦报错新手很难解决\r\n\r\n    如果只有源码包可用怎么办呢\r\n    初学者很苦恼\r\n    源码包不适合初学者\r\n\r\n    脚本安装包\r\n    所谓的脚本安装包 就是把复杂的软件包安装过程写成了程序脚本 初学者可以执行程序脚本实现一键安装　但实际安装的还是源码包和二进制包\r\n    优点 安装简单 快捷\r\n    缺点 完全丧失了自定义性\r\n\r\n    RPM 包\r\n    二进制包的优点\r\n    包管理系统简单 只通过几个命令就可以实现包的安装　升级　查询和卸载\r\n    安装速度比源码包安装快的多\r\n\r\n    二进制包缺点\r\n    经过编译　不再可以看到源代码\r\n    功能选择不如源码包灵活\r\n    依赖性\r\n\r\n\r\nRPM 命令管理\r\n    RPM 包命令规则\r\n    安装命令\r\n    升级与卸载\r\n    RPM 包查询\r\n    RPM 包校验\r\n\r\n    RPM 包的来源\r\n    RPM 包在系统光盘中\r\n\r\n    RPM 包命名原则\r\n    httpd-2.2.15-15.el6.centos.1.i686.rpm\r\n    httpd 软件包名\r\n    2.2.15 软件版本\r\n    15 软件发布的次数\r\n    el6.centos 适合的 linux 平台\r\n    i686 适合的硬件平台\r\n    rpm rpm 包扩展名\r\n\r\n    RPM 包依赖性\r\n    树形依赖 a-&gt;b-&gt;c\r\n    环形依赖 a-&gt;b-&gt;c-&gt;a\r\n    模块依赖 模块依赖 查询网站 www.rpmfind.net\r\n\r\n    包全名与包名\r\n    包全名 操作的包是没有安装的软件包时, 使用包全名, 而且要注意路径\r\n    包名   操作已经安装的软件包时 使用包名 是搜索 /var/lib/rpm/ 中的数据库\r\n\r\n    RPM 安装 \r\n    rpm -ivh 包全名\r\n    -i (install) 安装\r\n    -v (verbose) 显示详细信息\r\n    -h (hash) 显示进度\r\n    --nodeps 不检测依赖性\r\n\r\n    RPM 包升级\r\n    -U (upgrade) 升级\r\n\r\n    查询是否安装\r\n    rpm -q 包名 #查询包是否安装 -q 查询 (query)\r\n    rpm -qa #查询所有已经安装的 RPM 包 -a 所有 (all)\r\n\r\n    查询软件包详细信息\r\n    rpm -qi 包名 \r\n    -i 查询软件信息 (information)\r\n    -p 查询未安装包信息 (package)\r\n\r\n    查询包中文件安装位置\r\n    rpm -ql 包名\r\n    -l 列表 (list)\r\n    -p 查询未安装包信息 (package)\r\n\r\n    查询系统文件属于哪个 RPM 包\r\n    rpm -qf 系统文件名\r\n    -f 查询系统文件属于哪个软件包 (file)\r\n    rpm -qf yum.conf\r\n\r\n    查询软件包的依赖性\r\n    rpm -qR 包名\r\n    -R 查询软件包的依赖性 (requires)\r\n    -p 查询未安装包信息 (package)\r\n\r\n    RPM 包校验\r\n    rpm -V 已安装的包名\r\n    -V 校验指定 RPM 包中的文件 (verify)\r\n\r\n    验证内容中的 8 个信息的具体内容如下\r\n    S 文件大小是否改变\r\n    M 文件的类型或文件的权限 (rwx) 是否被改变\r\n    5 文件MD5 校验和是否改变 (可以看成文件内容是否改变)\r\n    D 设备的主从代码是否改变\r\n    L 文件路径是否改变\r\n    U 文件的属主 (所有者) 是否改变\r\n    G 文件的属组是否改变\r\n    T 文件的修改时间是否改变\r\n\r\n    RPM 包中文件提取\r\n    rpm2cpio 包全名 | cpio -idv .文件绝对路径\r\n    -rpm2cpio #将rpm 包转换为 cpio 格式的命令\r\n    -cpio #是一个标准工具 它用于创建软件档案文件和从档案文件中提取文件\r\n\r\n    rpm -qf /bin/ls #查询 ls 命令属于哪个软件包\r\n    mv /bin/ls /tmp/ #造成 ls 命令误删除假象\r\n    rpm2cpio /mnt/cdrom/Packages/coreutils-8.4-19.el6.i686.rpm | cpio -idv ./bin/ls #提取 RPM 包中 ls 命令到当前目录的 /bin/ls 下\r\n    cp /root/bin/ls /bin/ #把 ls 命令复制到 /bin/ 目录 修复文件丢失\r\n\r\n	    脚本安装包\r\n\r\n    强大的 nginx 服务器\r\n    nginx 是一款轻量级的 web 服务器 / 反向代理服务器及电子邮件 (IMAP/POP3) 代理服务器 2004年发布\r\n\r\n    web服务器    nginx  apache lighttpd\r\n    反回代理     非常好 好     一般\r\n    rewrite 规则 非常好 好     一般\r\n    fastcgi      好     差     非常好\r\n    热部署       支持   不支付 不支持\r\n    系统压力比较 很小   小     很大\r\n    稳定性       非常好 好     一般\r\n    安全性       一般   好     一般\r\n    技术资料     很少   非常多 一般\r\n    静态文件处理 非常好 一般   好\r\n    虚拟主机     支持   支持   支持\r\n    内存消耗     非常小 很大   非常小\r\n\r\n    nginx 在反向代理 rewrite 规则 稳定性 静态文件处理 内存消耗等方面 表现出了很强的优势 选用 nginx 取代传统的 apache 服务器 将会获得多方面的性能提升\r\n\r\n    准备工作\r\n    关闭 RPM 包安装的 httpd 和 mysql\r\n    保证 yum 源正常使用\r\n    关闭 selinux 和防火墙 (vi /etc/selinux/config SELINUX=enforcing =&gt; SELINUX=disabled)\r\n\r\n    centos.sh 脚本分析\r\n    所谓的一键安装包 实际上还是安装的源码包与 RPM 包 只是把安装过程写成了脚本 便于初学者安装\r\n    优点 简单 快速 方便\r\n    缺点\r\n    不能定义安装软件的版本\r\n    不能定义所需要的软件功能\r\n    源码包的优势丧失\r\n\r\n    ps aux 查看进程\r\n    php-fpm 在虚拟机中常常卡出导致出错\r\n    pkill -9 php-fpm #杀死 php-fpm 进程\r\n    /etc/rc.d/init/init.d/php-fpm start #重启\r\n	\r\n	    源码包与 RPM 的区别\r\n\r\n    安装之前的区别 概念上的区别\r\n    安装之后的区别 安装位置不同\r\n\r\n    RPM 包安装位置\r\n    是安装在默认位置中\r\n    RPM 包默认安装路径\r\n    /etc/ 配置文件安装目录\r\n    /usr/bin/ 可执行的命令安装目录\r\n    /usr/lib/ 程序所使用的函数库保存位置\r\n    /usr/share/doc/ 基本的软件使用手册保存位置\r\n    /usr/share/man/ 帮助文件保存位置\r\n\r\n    源码包安装过程\r\n    rpm --help | grep prefix\r\n    rpm 安装可以指定安装位置 (--prefix=&lt;div&gt;)\r\n\r\n    安装位置不同带来的影响\r\n    RPM 包安装的服务可以使用系统服务管理命令 (service) 来管理 例如 RPM 包安装的 apache 的启动方法是\r\n    /etc/rc.d/init.d/httpd start\r\n    service httpd start\r\n\r\n    源码包安装位置\r\n    安装在指定位置当中 一般是 /usr/local/软件名/ #源码包没有卸载命令\r\n\r\n    而源码包安装的服务则不能被服务管理命令管理 因为没有安装到默认路径中 所以只能用绝对路径进行服务的管理\r\n    /usr/local/apache2/bin/apachectl start\r\n    (只要把安装目录删除掉, 就卸载了对应的软件)\r\n\r\n    源码包安装\r\n\r\n    安装准备\r\n    安装 C 语言编译器 (rpm -qa | grep gcc 查看是否安装 gcc)\r\n    下载源码包\r\n    http://mirror.bit.edu.cn/apache/httpd/\r\n    npm 包和源码包 选择哪一个呢\r\n\r\n    安装注意事项\r\n    源代码保存位置 /usr/local/src/\r\n    软件安装位置 /usr/local/\r\n    如何确定安装过程报错 安装过程停止 并出现 error warning 或 no 的提示\r\n\r\n    源码包安装过程\r\n    下载源码包\r\n    解压缩下载的源码包\r\n    进入解压缩目录 \r\n\r\n    ./configure 软件配置与检查\r\n    定义需要的功能选项\r\n    检测系统环境是否符合安装要求\r\n    把定义好的功能选项和检测系统环境的信息都写入 makefile 文件 用于后续的编辑\r\n    make 编译 make clean 清除所有编译的文件\r\n    make install 编译安装\r\n    vi INSTALL #打开编译安装文件 查看安装要求\r\n\r\n    源码包的卸载\r\n    不需要卸载命令 直接删除安装目录即可 不会遗留任何垃圾文件\r\n	\r\n	    yum 在线安装\r\n    好处 将所有软件包放到官方服务器上 当进行 yum 在线安装时 可以自动解决依赖性问题\r\n    redhat 的 yum 在线安装是需要付费的\r\n\r\n    yum 源文件\r\n    cd /etc/yum.repos.d/\r\n    ls\r\n    vi /etc/yum.repos.d/CentOS-Base.repo\r\n    [base] 容器名称 一定要放在 [] 中\r\n    name 容器说明 可以自己随便写\r\n    mirrorlist 镜像站点 这个可以注释掉\r\n    baseurl 我们的 yum 源服务器的地址 默认是CentOs 官方的 yum 源服务器 是可以使用的 如果你觉得慢可以改成你喜欢的 yum 源地址\r\n    enabled 此容器是否生效果 如果不写或写成 enable=1 都是生效 写成 enable=0 就是不生效\r\n    gpgcheck 如果是 1 是指 RPM 的数字证书生效 如果是 0 则不生效\r\n    gpgkey 数字证书的公钥文件保存位置 不用修改\r\n\r\n    如果没有网络 如何使用 yum 源\r\n    挂载光盘\r\n    mkdir /mnt/cdrom #建立挂载点\r\n    mount /dev/cdrom /mnt/cdrom/ #挂载光盘\r\n\r\n    使网络yum 源失效\r\n    cd /etc/yum.repos.d/ #进入 yum 源目录\r\n    mv CentOS-Base.repo CentOS-Base.repo.bak #修改 yum 源文件后缀名 使其失效\r\n    vim CentOS-Media.repo\r\n    name=CentOS-$releasever - Media\r\n    baseurl=file:///mnt/cdrom #地址为你自己的光盘挂载地址\r\n            file:///media/cdrom\r\n            file:///media/cdrecorder/ #注释这两个不存在的地址\r\n    gpgcheck=1\r\n    enabled=1 #把enabled=0 改为enabled=1 让这个 yum 源配置文件生效\r\n    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\r\n\r\n    常用 yum 命令\r\n    查询 \r\n    yum list #查询所有可用软件包列表\r\n    yum search 关键字 #搜索服务器上所有和关键字相关的包\r\n\r\n    安装\r\n    yum -y install 包名\r\n    install 安装\r\n    -y 自动回答 yes\r\n    yum -y install gcc\r\n\r\n    升级\r\n    yum -y update 包名\r\n    update 升级\r\n    -y 自动回答 yes\r\n    yum -y update 升级所有的内容 (不建议使用)\r\n    yum -y update httpd 升级 httpd\r\n\r\n    卸载\r\n    yum -y remove 包名\r\n    remove 卸载\r\n    -y 自动回答 yes\r\n    服务器使用最小化安装 用什么软件安装什么 尽量不卸载\r\n\r\n    YUM 软件组管理命令\r\n    yum grouplist #列出所有可用的软件组列表\r\n    yum groupinstall 软件组名 #安装指定软件组 组名可以由 grouplist 查询出来\r\n    yum groupremove 软件组名 #卸载指定软件组',9,0,0,1514631646,0,0,0),(185,1,'vmware 简介','','','vmware 是一个虚拟 PC 的软件\r\n可以在现有的操作系统上虚拟出一个新的硬件环境\r\n相当于模拟出一台新的 PC\r\n以此来实现在一台机器上真正同时运行两个独立的操作系统\r\n官网 http://www.vmware.com\r\n\r\nvmware 主要特点\r\n不需要分区或重新开机就能在同一台 PC 上使用两种以上的操作系统\r\n本机系统可以与虚拟机系统网络通信\r\n可以设定并且随时修改虚拟机操作系统的硬件环境\r\n\r\n需要配置\r\nCPU 建议主频为 1GHz 以上\r\n内存 建议 1GB 以上\r\n硬盘 建议分区空闲空间 8GB 以上\r\n\r\n安装虚拟机\r\n启动 vm\r\n创建新的虚拟机\r\n自定义\r\n稍后安装操作系统 (安装完虚拟机再安装操作系统)\r\n下一步\r\n选择 (安装 linux 系统 centos)\r\n选择安装路径 (d:/files/vm)\r\n选择处理器 (以下几步可以在虚拟机网络设置中调)\r\n选择内存\r\n选择网络连接 (桥接 方便 但占用真实机 nat 使用 vm8虚拟网卡与真实机通信 host-only 使用vm1 虚拟网卡与真实机通信并且不能与外部电脑通信)\r\nI / O 控制类型 (推荐)\r\n虚拟磁盘类型 (推荐)\r\n创建新的虚拟器\r\n磁盘大小 20 G 选择将虚拟磁盘存储为单个文件\r\n完成\r\n\r\n虚拟机安装 linux \r\nCD/DVD 自动检测\r\n浏览 (选择第一个镜像文件)\r\n\r\n开启此虚拟机 (不再显示此提示 进入全屏模式)\r\ninstall or upgrade on existing system 安装或升级现在的系统\r\ninstall system with basic video driver 安装过程采用基本的显卡驱动\r\nrescue installed system 进入系统修复模式\r\nboot from local drive 退出安装从硬盘启动\r\nmemory test 存储介质检测\r\n选择 skip (是否检测你的盘里内容 tab 键 方向键 -&gt; 都可以) 下一步\r\n中文简体\r\n基本存储设备\r\n主机名\r\n时区\r\n密码 (学习时使用最简单的密码 无论如何都使用)\r\n  密码原则\r\n  复杂性\r\n	  八位字符以上 大小写字母 数字 符号\r\n	  不能是英文单词\r\n	  不能是和用户相关的内容\r\n  易记忆性\r\n  时效性\r\n创建自定义布局\r\n选择创建 ---&gt; 标准分区 ---&gt; /boot(挂载点) 200M\r\n选择创建 ---&gt; 标准分区 ---&gt; swap(文件系统类型) 200M (一般内存的1.5倍至二倍)\r\n选择创建 ---&gt; 标准分区 ---&gt; / 使用全部可用空间\r\n在 /dev/sda 中安装引导装载程序\r\n软件包程序选择 basic server\r\n  Desktop 桌面\r\n  Minimal Desktop 最小化桌面\r\n  Minimal 最小化\r\n  Basic Server 基本服务器\r\n  Database Server 数据库服务器\r\n  Web Server 网页服务器\r\n  Virtual Host 虚拟主机\r\n  software development workstation 软件开发工作站\r\n安装引导程序\r\n  \r\nlogin: root # 用户名\r\npassword: \r\n[root@localhost]# \r\n\r\n[root@localhost]# ls\r\ninstall.log install.log.syslog anaconda-ks.cfg\r\n/root/install.log 存储了安装在系统中的软件包及其版本信息\r\n/root/install.log.syslog 存储了安装过程中留下的事件记录\r\n/root/anaconda-ks.log 以Kickstart 配置文件的格式记录安装过程中设置的选项信息\r\n\r\nls /etc/sysconfig/network-scripts/ifcfg-eth0\r\nvim /etc/sysconfig/network-scripts/ifcfg-eth0\r\n修改 ONBOOT = &#039;yes&#039; (按i将no 改为 yes vim 中强制退出 q! Tab 键补全)\r\n按 esc 键\r\n输入 :wq (esc 键进入编辑模式 : 进入命令模式)\r\n重启 service network restart\r\nifconfig (查看ip 地址)\r\ninet addr:192. \r\n开启 vm (可以通过修改网络适配器 的连接方式 来修改 ip)\r\n启动 putty.exe (第三方登录 linux 系统)\r\n主机名称为 ip 地址\r\n转换 ---&gt; utf8 \r\n会话 ---&gt; 默认设置 ---&gt; 保存\r\n打开\r\n\r\nlinux (三大核心技术)\r\n防火墙配置 内核截切 邮箱服务器\r\n\r\n关闭防火墙\r\nsetup\r\n\r\n 配置 IP\r\n\r\n \r\n虚拟机(vmware)安装\r\n------------------------------------------\r\n双击虚拟机安装文件 --- 下一步 --- 选择典型 --- 选择安装目录 --- \r\n软件更新页(去除勾选项) --- 用户体验改进计划页(去除勾选项) \r\n--- 快捷方式页(选择桌面 开始菜单) --- 输入许可证密钥 --- 安装完成\r\n\r\nvm虚拟机\r\n\r\n禁用443端口\r\n编辑 --- 自选项 --- 共享虚拟机 --- 禁用443端口\r\n\r\n删除无用虚拟机\r\n左边框 --- 要删除的计算名 --- 右击 --- 管理 --- 从磁盘删除\r\n\r\n不用时选择挂起 用时选择在后台运行\r\n\r\n进入centos 目录 --- 启动 vmware 10 安装程序 --- 选择典型 --- 下一步 ...\r\n输入密钥 5F0YA-FFK9P-VZCK1-1T07K-2CYNX\r\n\r\nlinux 系统安装\r\n启动 vmware 10 --- 创建新的虚拟机 --- 典型 --- 稍后安装系统 --- linux --- 选择 CentOS 64 位 --- 输入虚拟机名 apeng1 --- 将虚拟磁盘存储为单个文件 --- 完成\r\n\r\nvmware 10 界面 --- 点击 CD/DVD --- 使用 ISO 镜像文件 --- 选择第一个镜像文件 --- 启动虚拟机 --- ...\r\n',9,0,0,1514642238,0,0,0),(186,1,'linux的命令操作','','','1、日常操作命令  \r\n\r\n**查看当前所在的工作目录\r\npwd\r\n\r\n**查看当前系统的时间 \r\ndate\r\n\r\n**查看有谁在线（哪些人登陆到了服务器）\r\nwho  查看当前在线\r\nlast 查看最近的登陆历史记录\r\n\r\n\r\n2、文件系统操作\r\n**\r\nls /    查看根目录下的子节点（文件夹和文件）信息\r\nls -al  -a是显示隐藏文件   -l是以更详细的列表形式显示\r\n\r\n**切换目录\r\ncd  /home\r\n\r\n**创建文件夹\r\nmkdir aaa     这是相对路径的写法 \r\nmkdir -p aaa/bbb/ccc\r\nmkdir  /data    这是绝对路径的写法 \r\n\r\n**删除文件夹\r\nrmdir   可以删除空目录\r\nrm -r aaa   可以把aaa整个文件夹及其中的所有子节点全部删除\r\nrm -rf aaa   强制删除aaa\r\n\r\n**修改文件夹名称\r\nmv aaa angelababy\r\n\r\n**创建文件\r\ntouch  somefile.1   创建一个空文件\r\necho &quot;i miss you,my baby&quot; &gt; somefile.2  利用重定向“&gt;”的功能，将一条指令的输出结果写入到一个文件中，会覆盖原文件内容\r\necho &quot;huangxiaoming ,gun dan&quot; &gt;&gt; somefile.2     将一条指令的输出结果追加到一个文件中，不会覆盖原文件内容\r\n\r\n用vi文本编辑器来编辑生成文件\r\n******最基本用法\r\nvi  somefile.4\r\n1、首先会进入“一般模式”，此模式只接受各种快捷键，不能编辑文件内容\r\n2、按i键，就会从一般模式进入编辑模式，此模式下，敲入的都是文件内容\r\n3、编辑完成之后，按Esc键退出编辑模式，回到一般模式；\r\n4、再按：，进入“底行命令模式”，输入wq命令，回车即可\r\n\r\n******一些常用快捷键\r\n一些有用的快捷键（在一般模式下使用）：\r\na  在光标后一位开始插入\r\nA   在该行的最后插入\r\nI   在该行的最前面插入\r\ngg   直接跳到文件的首行\r\nG    直接跳到文件的末行\r\ndd   删除行，如果  5dd   ，则一次性删除光标后的5行\r\nyy  复制当前行,  复制多行，则  3yy，则复制当前行附近的3行\r\np   粘贴\r\nv  进入字符选择模式，选择完成后，按y复制，按p粘贴\r\nctrl+v  进入块选择模式，选择完成后，按y复制，按p粘贴\r\nshift+v  进入行选择模式，选择完成后，按y复制，按p粘贴\r\n\r\n查找并替换（在底行命令模式中输入）\r\n%s/sad/88888888888888     效果：查找文件中所有sad，替换为88888888888888\r\n/you       效果：查找文件中出现的you，并定位到第一个找到的地方，按n可以定位到下一个匹配位置（按N定位到上一个）\r\n\r\n\r\n3、文件权限的操作\r\n\r\n****linux文件权限的描述格式解读\r\ndrwxr-xr-x      （也可以用二进制表示  111 101 101  --&gt;  755）\r\n\r\nd：标识节点类型（d：文件夹   -：文件  l:链接）\r\nr：可读   w：可写    x：可执行 \r\n第一组rwx：  表示这个文件的拥有者对它的权限：可读可写可执行\r\n第二组r-x：  表示这个文件的所属组对它的权限：可读，不可写，可执行\r\n第三组r-x：  表示这个文件的其他用户（相对于上面两类用户）对它的权限：可读，不可写，可执行\r\n\r\n\r\n****修改文件权限\r\nchmod g-rw haha.dat    表示将haha.dat对所属组的rw权限取消\r\nchmod o-rw haha.dat 	表示将haha.dat对其他人的rw权限取消\r\nchmod u+x haha.dat      表示将haha.dat对所属用户的权限增加x\r\n\r\n也可以用数字的方式来修改权限\r\nchmod 664 haha.dat   \r\n就会修改成   rw-rw-r--\r\n\r\n如果要将一个文件夹的所有内容权限统一修改，则可以-R参数\r\nchmod -R 770 aaa/\r\nchown angela:angela aaa/    &lt;只有root能执行&gt;\r\n\r\n目录没有执行权限的时候普通用户不能进入\r\n文件只有读写权限的时候普通用户是可以删除的(删除文件不是修改它,是操作父及目录),只要父级目录有执行和修改的权限\r\n\r\n4、基本的用户管理\r\n\r\n*****添加用户\r\nuseradd  angela\r\n要修改密码才能登陆 \r\npasswd angela  按提示输入密码即可\r\n\r\n\r\n**为用户配置sudo权限\r\n用root编辑 vi /etc/sudoers\r\n在文件的如下位置，为hadoop添加一行即可\r\nroot    ALL=(ALL)       ALL     \r\nhadoop  ALL=(ALL)       ALL\r\n\r\n然后，hadoop用户就可以用sudo来执行系统级别的指令\r\n[hadoop@shizhan ~]$ sudo useradd huangxiaoming\r\n\r\n\r\n5、系统管理操作\r\n*****查看主机名\r\nhostname\r\n****修改主机名(重启后无效)\r\nhostname hadoop\r\n\r\n*****修改主机名(重启后永久生效)\r\nvi /ect/sysconfig/network\r\n****修改IP(重启后无效)\r\nifconfig eth0 192.168.12.22\r\n\r\n****修改IP(重启后永久生效)\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\n\r\n\r\nmount ****  挂载外部存储设备到文件系统中\r\nmkdir   /mnt/cdrom      创建一个目录，用来挂载\r\nmount -t iso9660 -o ro /dev/cdrom /mnt/cdrom/     将设备/dev/cdrom挂载到 挂载点 ：  /mnt/cdrom中\r\n\r\n*****umount\r\numount /mnt/cdrom\r\n\r\n\r\n*****统计文件或文件夹的大小\r\ndu -sh  /mnt/cdrom/Packages\r\ndf -h    查看磁盘的空间\r\n****关机\r\nhalt\r\n****重启\r\nreboot\r\n\r\n\r\n******配置主机之间的免密ssh登陆\r\n假如 A  要登陆  B\r\n在A上操作：\r\n%%首先生成密钥对\r\nssh-keygen   (提示时，直接回车即可)\r\n%%再将A自己的公钥拷贝并追加到B的授权列表文件authorized_keys中\r\nssh-copy-id   B',9,0,0,1514642350,0,0,0),(187,1,'初始linux环境(基本配置)','','','1.先将虚拟机的网络模式选为NAT\r\n\r\n2.修改主机名\r\nvi /etc/sysconfig/network\r\nNETWORKING=yes\r\nHOSTNAME=server1\r\n\r\n3.修改ip地址 (两种方式)\r\n第一种：通过Linux图形界面进行修改（强烈推荐）\r\n进入Linux图形界面 -&gt; 右键点击右上方的两个小电脑 -&gt; 点击Edit connections -&gt; 选中当前网络System eth0 -&gt; 点击edit按钮 -&gt; 选择IPv4 -&gt; method选择为manual -&gt; 点击add按钮 -&gt; 添加IP：192.168.1.101 子网掩码：255.255.255.0 网关：192.168.1.1 -&gt; apply\r\n第二种：修改配置文件方式（屌丝程序猿专用）\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\nDEVICE=eth0\r\nTYPE=Ethernet\r\nONBOOT=yes\r\nBOOTPROTO=static\r\nIPADDR=192.168.0.101\r\nNETMASK=255.255.255.0\r\nservice network restart\r\n\r\n4.修改ip地址和主机名的映射关系\r\nvi /etc/hosts\r\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\r\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\r\n192.168.0.101 hadoop\r\n\r\n5.关闭防火墙\r\n#查看防火墙状态\r\nservice iptables status\r\n#关闭防火墙\r\nservice iptables stop\r\n#查看防火墙开机启动状态\r\nchkconfig iptables --list\r\n#关闭防火墙开机启动\r\nchkconfig iptables off\r\n\r\nchkconfig iptables on\r\n\r\n6.修改sudo\r\nsu root\r\nadduser hadoop\r\npasswd hadoop\r\nvim /etc/sudoers\r\nhadooop ALL=(ALL) ALL\r\n给hadoop用户添加执行的权限\r\n\r\n关闭linux服务器的图形界面：\r\nvi /etc/inittab \r\n重启Linux\r\nreboot\r\n\r\n7.\r\nyum -y install gcc gcc-c++ autoconf pcre pcre-devel make automake\r\nyum -y install wget httpd-tools vim\r\n\r\nping #确定是否可以连接外网\r\nyum list | grep gcc #确定yum源\r\n\r\ncd /opt\r\nmkdir app\r\nmkdir download\r\nmkdir logs\r\nmkdir work\r\nmkdir backup\r\n\r\n拍快照:初始化\r\n',9,0,0,1514642478,0,0,0),(188,1,'安装JDK','','','1.切换到root用户：\r\nsu – root\r\n\r\n2.查看以前是不是安装了openjdk：\r\n命令：rpm -qa | grep java\r\n显示如下：（有则卸载，没有就不用）\r\ntzdata-java-2013g-1.el6.noarch\r\njava-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64\r\njava-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64\r\n\r\n3.卸载openjdk：\r\n（其中参数“tzdata-java-2013g-1.el6.noarch”为上面查看中显示的结果，粘进来就行）\r\nrpm -e --nodeps  tzdata-java-2013g-1.el6.noarch\r\nrpm -e --nodeps  java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64\r\nrpm -e --nodeps  java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64  \r\n\r\n4.上传jdk-7u45-linux-x64.tar.gz到Linux上 (安装rz命令)\r\n\r\n5.解压jdk到/usr/local目录\r\ntar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local/\r\n[hadoop@apeng1 download]$ sudo tar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local/ #使用hadoop用户\r\n\r\n6.设置环境变量，在/etc/profile文件最后追加相关内容\r\nvi /etc/profile\r\nexport JAVA_HOME=/usr/local/jdk1.7.0_45\r\nexport CLASSPATH=$JAVA_HOME/lib\r\nexport PATH=$PATH:$JAVA_HOME/bin\r\n\r\n7.刷新环境变量\r\nsource /etc/profile\r\n\r\n8.测试java命令是否可用\r\njava -version\r\n\r\n9.使用脚本安装JDK\r\n#!/bin/bash\r\n\r\nBASE_SERVER=172.16.203.100 #相关安装包(服务器)\r\nyum install -y wget #安装wget命令\r\nwget $BASE_SERVER/soft/jdk-7u45-linux-x64.tar.gz #从服务器拉取jdk安装包\r\ntar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local #解压\r\ncat &gt;&gt; /etc/profile &lt;&lt; EOF\r\nexport JAVA_HOME=/usr/local/jdk1.7.0_45\r\nexport PATH=\\$PATH:\\$JAVA_HOME/bin\r\nEOF\r\n\r\n直接复制安装第二 ... 台\r\nscp -r /usr/local/jdk1.7.0_45/ root@apeng2:/usr/local/jdk1.7.0_45\r\nscp -r /etc/profile root@apeng2:/etc/\r\n\r\n登录第二台\r\nsource /etc/profile\r\n',9,0,0,1514642531,0,0,0),(189,1,'  安装Tomcat','','','    1. 上传apache-tomcat-7.0.68.tar.gz到Linux上\r\n    2. 解压tomcat\r\ntar -zxvf apache-tomcat-7.0.68.tar.gz -C /usr/local/\r\n    3. 启动tomcat\r\n/usr/local/apache-tomcat-7.0.68/bin/startup.sh\r\n    4. 查看tomcat进程是否启动\r\njps\r\n\r\n    5. 查看tomcat进程端口\r\nnetstat -anpt | grep 2465\r\n    6. 通过浏览器访问tomcat\r\nhttp://192.168.0.101:8080/',9,0,0,1514642574,0,0,0),(190,1,'安装MySQL','','','    1. 上传MySQL-server-5.5.48-1.linux2.6.x86_64.rpm、MySQL-client-5.5.48-1.linux2.6.x86_64.rpm到Linux上\r\n    2. 使用rpm命令安装MySQL-server-5.5.48-1.linux2.6.x86_64.rpm，缺少perl依赖\r\nrpm -ivh MySQL-server-5.5.48-1.linux2.6.x86_64.rpm \r\n\r\n    3. 安装perl依赖，上传6个perl相关的rpm包\r\n\r\nrpm -ivh perl-*\r\n    4. 再安装MySQL-server，rpm包冲突\r\nrpm -ivh MySQL-server-5.5.48-1.linux2.6.x86_64.rpm\r\n\r\n    5. 卸载冲突的rpm包\r\nrpm -e mysql-libs-5.1.73-5.el6_6.x86_64 --nodeps\r\n    6. 再安装MySQL-client和MySQL-server\r\nrpm -ivh MySQL-client-5.5.48-1.linux2.6.x86_64.rpm\r\nrpm -ivh MySQL-server-5.5.48-1.linux2.6.x86_64.rpm\r\n    7. 启动MySQL服务，然后初始化MySQL\r\nservice mysql start\r\n/usr/bin/mysql_secure_installation\r\n    8. 测试MySQL\r\nmysql -u root -p',9,0,0,1514642618,0,0,0),(191,1,'YUM相关概念','','','        1.1. 什么是YUM\r\nYUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。\r\n        1.2. YUM的作用\r\n在Linux上使用源码的方式安装软件非常满分，使用yum可以简化安装的过程\r\n    2. YUM的常用命令\r\n安装httpd并确认安装\r\nyum instll -y httpd\r\n\r\n列出所有可用的package和package组\r\nyum list\r\n\r\n清除所有缓冲数据\r\nyum clean all\r\n\r\n列出一个包所有依赖的包\r\nyum deplist httpd\r\n\r\n删除httpd\r\nyum remove httpd',9,0,0,1514642731,0,0,0),(192,1,'制作本地YUM源','','','        3.1. 为什么要制作本地YUM源\r\nYUM源虽然可以简化我们在Linux上安装软件的过程，但是生成环境通常无法上网，不能连接外网的YUM源，说以接就无法使用yum命令安装软件了。为了在内网中也可以使用yum安装相关的软件，就要配置yum源。\r\n        3.2. YUM源的原理\r\nYUM源其实就是一个保存了多个RPM包的服务器，可以通过http的方式来检索、下载并安装相关的RPM包\r\n\r\n        3.3. 制作本地YUM源\r\n    1. 准备一台Linux服务器，用最简单的版本CentOS-6.7-x86_64-minimal.iso\r\n    2. 配置好这台服务器的IP地址\r\n    3. 上传CentOS-6.7-x86_64-bin-DVD1.iso到服务器\r\n    4. 将CentOS-6.7-x86_64-bin-DVD1.iso镜像挂载到某个目录\r\nmkdir /var/iso\r\nmount -o loop CentOS-6.7-x86_64-bin-DVD1.iso /var/iso\r\n    5. 修改本机上的YUM源配置文件，将源指向自己\r\n备份原有的YUM源的配置文件\r\ncd /etc/yum.repos.d/\r\nrename .repo .repo.bak *\r\nvi CentOS-Local.repo\r\n[base]\r\nname=CentOS-Local\r\nbaseurl=file:///var/iso\r\ngpgcheck=1\r\nenabled=1   #很重要，1才启用\r\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\r\n添加上面内容保存退出\r\n    6. 清除YUM缓冲\r\nyum clean all\r\n    7. 列出可用的YUM源\r\nyum repolist\r\n    8. 安装相应的软件\r\nyum install -y httpd\r\n9.开启httpd使用浏览器访问http://192.168.0.100:80（如果访问不通，检查防火墙是否开启了80端口或关闭防火墙）\r\nservice httpd start\r\n10.将YUM源配置到httpd（Apache Server）中，其他的服务器即可通过网络访问这个内网中的YUM源了\r\ncp -r /var/iso/ /var/www/html/CentOS-6.7\r\n11.取消先前挂载的镜像\r\numount /var/iso\r\n12.在浏览器中访问http://192.168.0.100/CentOS-6.7/\r\n\r\n    13. 让其他需要安装RPM包的服务器指向这个YUM源，准备一台新的服务器，备份或删除原有的YUM源配置文件\r\ncd /etc/yum.repos.d/\r\nrename .repo .repo.bak *\r\nvi CentOS-Local.repo\r\n[base]\r\nname=CentOS-Local\r\nbaseurl=http://192.168.0.100/CentOS-6.7\r\ngpgcheck=1\r\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\r\n添加上面内容保存退出\r\n    14. 在这台新的服务器上执行YUM的命令\r\nyum clean all\r\nyum repolist\r\n    15. 安装相应的软件\r\nyum install -y gcc\r\n\r\n    16、 加入依赖包到私有yum的repository\r\n进入到repo目录\r\n执行命令：  createrepo  .\r\n\r\n1、本地yum仓库的安装配置\r\n两种方式：  a、每一台机器都配一个本地文件系统上的yum仓库 file:///packege/path/\r\n			b、在局域网内部配置一台节点(server-base)的本地文件系统yum仓库，然后将其发布到web服务器中，其他节点就可以通过http://server-base/pagekege/path/\r\n			\r\n			制作流程：  先挑选一台机器mini4，挂载一个系统光盘到本地目录/mnt/cdrom，然后启动一个httpd服务器，将/mnt/cdrom 软连接到httpd服务器的/var/www/html目录中 (cd /var/www/html; ln -s /mnt/cdrom ./centos )\r\n			然后通过网页访问测试一下：  http://mini4/centos   会看到光盘的目录内容\r\n			至此：网络版yum私有仓库已经建立完毕  \r\n			剩下就是去各台yum的客户端配置这个http地址到repo配置文件中\r\n			\r\n\r\n			\r\n\r\n			\r\n无论哪种配置，都需要先将光盘挂在到本地文件目录中\r\nmount -t iso9660 /dev/cdrom   /mnt/cdrom\r\n为了避免每次重启后都要手动mount，可以在/etc/fstab中加入一行挂载配置，即可自动挂载\r\nvi  /etc/fstab\r\n/dev/cdrom              /mnt/cdrom              iso9660 defaults        0 0			\r\n			\r\n\r\n\r\n2、minimal安装的系统出现的问题：缺各种命令，安装软件时缺各种依赖\r\n\r\nscp命令都没有：yum install -y openssh-clients\r\n每台机器上都要安装才行',9,0,0,1514642833,0,0,0),(193,1,'VMware虚拟机三种联网方法及原理','','','一、Brigde——桥接：默认使用VMnet0 \r\n \r\n1、原理： \r\n \r\nBridge  桥&quot;就是一个主机，这个机器拥有两块网卡，分别处于两个局域网中，同时在&quot;桥&quot;上，运行着程序，让局域网A中的所有数据包原封不动的流入B，反之亦然。这样，局域网A和B就无缝的在链路层连接起来了，在桥接时，VMWare网卡和物理网卡应该处于同一IP网段  当然要保证两个局域网没有冲突的IP. \r\n \r\nVMWare 的桥也是同样的道理，只不过，本来作为硬件的一块网卡，现在由VMWare软件虚拟了！当采用桥接时，VMWare会虚拟一块网卡和真正的物理网卡就行桥接，这样，发到物理网卡的所有数据包就到了VMWare虚拟机，而由VMWare发出的数据包也会通过桥从物理网卡的那端发出。 \r\n \r\n所以，如果物理网卡可以上网，那么桥接的软网卡也没有问题了，这就是桥接上网的原理了。  　　    \r\n2、联网方式： \r\n \r\n这一种联网方式最简单，在局域网内，你的主机是怎么联网的，你在虚拟机里就怎么连网。把虚拟机看成局域网内的另一台电脑就行了！ \r\n \r\n提示：主机网卡处在一个可以访问Internet的局域网中，虚拟机才能通过Bridge访问Internet。 \r\n \r\n\r\n````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` \r\n \r\n二、NAT——网络地址转换  ：默认使用VMnet8 \r\n \r\n1、原理： \r\n \r\nNAT 是  Network  address  translate的简称。NAT技术应用在internet网关和路由器上，比如192.168.0.123这个地址要访问internet，它的数据包就要通过一个网关或者路由器，而网关或者路由器拥有一个能访问internet的ip地址，这样的网关和路由器就要在收发数据包时，对数据包的IP协议层数据进行更改（即  NAT），以使私有网段的主机能够顺利访问internet。此技术解决了IP地址稀缺的问题。同样的私有IP可以网关NAT  上网。    \r\n \r\nVMWare的NAT上网也是同样的道理，它在主机和虚拟机之间用软件伪造出一块网卡，这块网卡和虚拟机的ip处于一个地址段。同时，在这块网卡和主机的网络接口之间进行NAT。虚拟机发出的每一块数据包都会经过虚拟网卡，然后NAT，然后由主机的接口发出。 \r\n \r\n虚拟网卡和虚拟机处于一个地址段，虚拟机和主机不同一个地址段，主机相当于虚拟机的网关，所以虚拟机能ping到主机的IP，但是主机ping不到虚拟机的IP。 \r\n　　　　 \r\n2、联网方式： \r\n \r\n方法1、动态IP地址。 \r\n \r\n主机是静态IP或动态IP，都无所谓，将虚拟机设置成使用DHCP方式上网,Windows下选择“自动获取IP“，linux下开启DHCP服务即可。（这种方法最简单，不用过多的设置，但要在VMware中进行“编辑→虚拟网络设置”，将NAT和DHCP都开启了。一般NAT默认开启，DHCP默认关闭） \r\n　　 \r\n方法2、静态IP地址。 \r\n \r\n如果不想使用DHCP，也可以手动设置：　 \r\n \r\nIP设置与vmnet1同网段,网关设置成vmnet8的网关（在“虚拟网络设置”里的Net选项卡里能找到Gateway）通常是xxx.xxx.xxx.2。 \r\n子网掩码设置与VMnet8相同（设置好IP地址后，子网掩码自动生成） \r\nDNS设置与主机相同。 \r\n \r\n例如：主机IP是10.70.54.31,设置虚拟机IP为10.70.54.22。Netmask,Gateway,DNS都与主机相同即可实现  虚拟机  ---主机  虚拟机&lt;----&gt;互联网  通信。    \r\n \r\n提示：使用NAT技术，主机能上网，虚拟机就可以访问Internet，但是主机不能访问虚拟机。 \r\n\r\n\r\n\r\n`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\r\n\r\n\r\n \r\n三、Host-Only——私有网络共享主机：默认使用VMnet1 \r\n \r\n1、原理： \r\n \r\n提供的是主机和虚拟机之间的网络互访。只想让虚拟机和主机之间有数据交换，而不想让虚拟机访问Internet，就要采用这个设置了。 \r\n \r\nHost-only的条件下，VMWare在真正的Windows系统中，建立一块软网卡。这块网卡可以在网络连接中看到，一般是VMNET1，这块网卡的作用就是使Windows看到虚拟机的IP。 \r\n \r\n2、联网方法： \r\n \r\n方法1、动态IP地址。 \r\n像上面那样开启DHCP后，虚拟机直接自动获取IP地址和DNS。就可以和主机相连了。当然，还要进行一些局域网共享的操作，这里不再赘述。 \r\n \r\n方法2、静态IP地址。    \r\n \r\n也可以手动设置，将虚拟机IP设置与VMnet1同网段,网关设置成VMnet1的网关相同,其余设置与VMnet1相同,DNS设置与主机相同。 \r\n \r\n例如：VMnet1  IP:172.16.249.1        Gateway  :172.16.249.2 \r\n　　 \r\n那么虚拟机  IP:172.16.249.100        Gateway:  172.16.249.2 \r\n　　 \r\n这样、      虚拟机&lt;---&gt;主机              可以通信        \r\n但是、        虚拟机&lt;---&gt;互联网      无法通信 \r\n \r\n提示：Host-only技术只用于主机和虚拟机互访，于访问internet无关。\r\n',9,0,0,1514642947,0,0,0),(194,1,'linux相关问题一','','','批量自动删除rpm包：\r\nrpm -qa | grep mysql | while read c; do rpm -e $c --nodeps; done\r\n\r\nminimal最小化安装\r\neth0默认没有自启用\r\n修改配置文件\r\nonboot=true\r\n\r\n修改静态地址后发现无法ping外网\r\n需要设置网关\r\nroute add default gw 192.168.33.1\r\n添加nameserver\r\nvi /etc/resolv.conf\r\nnameserver 192.168.33.1\r\n\r\n挂载光盘\r\nmkdir /mnt/cdrom\r\nmount -t iso9660 -o ro /dev/cdrom/ /mnt/cdrom \r\n\r\n\r\n解决克隆后eth0不见的问题\r\n\r\n直接修改  /etc/sysconfig/network-script/ifcfg-eth0\r\n删掉UUID  HWADDR\r\n配置静态地址\r\n然后：\r\nrm -rf 　/etc/udev/rules.d/70-persistent-net.rules\r\n然后 reboot',9,0,0,1514643054,0,0,0),(195,1,'man_page','','','1.内部命令：echo\r\n查看内部命令帮助：help echo 或者 man echo\r\n\r\n2.外部命令：ls\r\n查看外部命令帮助：ls --help 或者 man ls 或者 info ls\r\n\r\n3.man文档的类型(1~9)\r\nman 7 man\r\nman 5 passwd\r\n\r\n4.快捷键：\r\nctrl + c：停止进程\r\n\r\nctrl + l：清屏\r\n\r\nctrl + r：搜索历史命令\r\n\r\nctrl + q：退出\r\n\r\n5.善于用tab键',9,0,0,1514643149,0,0,0),(196,1,'linux常用命令','','','说明：安装linux时，创建一个itcast用户，然后使用root用户登陆系统\r\n\r\n1.进入到用户根目录\r\ncd ~ 或 cd\r\n\r\n2.查看当前所在目录\r\npwd\r\n\r\n3.进入到itcast用户根目录\r\ncd ~itcast\r\n\r\n4.返回到原来目录\r\ncd -\r\n\r\n5.返回到上一级目录\r\ncd ..\r\n\r\n6.查看itcast用户根目录下的所有文件\r\nls -la\r\n\r\n7.在根目录下创建一个itcast的文件夹\r\nmkdir /itcast\r\n\r\n8.在/itcast目录下创建src和WebRoot两个文件夹\r\n分别创建：mkdir /itcast/src\r\n		  mkdir /itcast/WebRoot\r\n同时创建：mkdir /itcast/{src,WebRoot}\r\n\r\n进入到/itcast目录，在该目录下创建.classpath和README文件\r\n分别创建：touch .classpath\r\n		  touch README\r\n同时创建：touch {.classpath,README}\r\n\r\n查看/itcast目录下面的所有文件\r\nls -la\r\n\r\n在/itcast目录下面创建一个test.txt文件,同时写入内容&quot;this is test&quot;\r\necho &quot;this is test&quot; &gt; test.txt\r\n\r\n查看一下test.txt的内容\r\ncat test.txt\r\nmore test.txt\r\nless test.txt\r\n\r\n向README文件追加写入&quot;please read me first&quot;\r\necho &quot;please read me first&quot; &gt;&gt; README\r\n\r\n将test.txt的内容追加到README文件中\r\ncat test.txt &gt;&gt; README\r\n\r\n拷贝/itcast目录下的所有文件到/itcast-bak\r\ncp -r /itcast /itcast-bak\r\n\r\n进入到/itcast-bak目录，将test.txt移动到src目录下，并修改文件名为Student.java\r\nmv test.txt src/Student.java\r\n\r\n在src目录下创建一个struts.xml\r\n&gt; struts.xml\r\n\r\n删除所有的xml类型的文件\r\nrm -rf *.xml\r\n\r\n删除/itcast-bak目录和下面的所有文件\r\nrm -rf /itcast-bak\r\n\r\n返回到/itcast目录，查看一下README文件有多单词，多少个少行\r\nwc -w README\r\nwc -l README\r\n\r\n返回到根目录，将/itcast目录先打包，再用gzip压缩\r\n分步完成：tar -cvf itcast.tar itcast\r\n		  gzip itcast.tar\r\n一步完成：tar -zcvf itcast.tar.gz itcast\r\n		  \r\n将其解压缩，再取消打包\r\n分步完成：gzip -d itcast.tar.gz 或 gunzip itcast.tar.gz\r\n一步完成：tar -zxvf itcast.tar.gz\r\n\r\n将/itcast目录先打包，同时用bzip2压缩，并保存到/tmp目录下\r\ntar -jcvf /tmp/itcast.tar.bz2 itcast\r\n\r\n将/tmp/itcast.tar.bz2解压到/usr目录下面\r\ntar -jxvf itcast.tar.bz2 -C /usr/',9,0,0,1514643190,0,0,0),(197,1,'linux文件相关命令','','','1.进入到用户根目录\r\ncd ~ 或者 cd\r\ncd ~hadoop\r\n回到原来路径\r\ncd -\r\n\r\n2.查看文件详情\r\nstat a.txt\r\n\r\n3.移动\r\nmv a.txt /ect/\r\n改名\r\nmv b.txt a.txt\r\n移动并改名\r\nmv a.txt ../b.txt\r\n\r\n4拷贝并改名\r\ncp a.txt /etc/b.txt\r\n\r\n5.vi撤销修改\r\nctrl + u (undo)\r\n恢复\r\nctrl + r (redo)\r\n\r\n6.名令设置别名(重启后无效)\r\nalias ll=&quot;ls -l&quot;\r\n取消\r\nunalias ll\r\n\r\n7.如果想让别名重启后仍然有效需要修改\r\nvi ~/.bashrc\r\n\r\n8.添加用户\r\nuseradd hadoop\r\npasswd hadoop\r\n\r\n9创建多个文件\r\ntouch a.txt b.txt\r\ntouch /home/{a.txt,b.txt}\r\n\r\n10.将一个文件的内容复制到里另一个文件中\r\ncat a.txt &gt; b.txt\r\n追加内容\r\ncat a.txt &gt;&gt; b.txt \r\n\r\n\r\n11.将a.txt 与b.txt设为其拥有者和其所属同一个组者可写入，但其他以外的人则不可写入:\r\nchmod ug+w,o-w a.txt b.txt\r\n\r\nchmod a=wx c.txt\r\n\r\n12.将当前目录下的所有文件与子目录皆设为任何人可读取:\r\nchmod -R a+r *\r\n\r\n13.将a.txt的用户拥有者设为users,组的拥有者设为jessie:\r\nchown users:jessie a.txt\r\n\r\n14.将当前目录下的所有文件与子目录的用户的使用者为lamport,组拥有者皆设为users，\r\nchown -R lamport:users *\r\n\r\n15.将所有的java语言程式拷贝至finished子目录中:\r\ncp *.java finished\r\n\r\n16.将目前目录及其子目录下所有扩展名是java的文件列出来。\r\nfind -name &quot;*.java&quot;\r\n查找当前目录下扩展名是java 的文件\r\nfind -name *.java\r\n\r\n17.删除当前目录下扩展名是java的文件\r\nrm -f *.java',9,0,0,1514643223,0,0,0),(198,1,'linux系统命令','','','1.查看主机名\r\nhostname\r\n\r\n2.修改主机名(重启后无效)\r\nhostname hadoop\r\n\r\n3.修改主机名(重启后永久生效)\r\nvi /ect/sysconfig/network\r\n\r\n4.修改IP(重启后无效)\r\nifconfig eth0 192.168.12.22\r\n\r\n5.修改IP(重启后永久生效)\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\n\r\n6.查看系统信息\r\nuname -a\r\nuname -r\r\n\r\n7.查看ID命令\r\nid -u\r\nid -g\r\n\r\n8.日期\r\ndate\r\ndate +%Y-%m-%d\r\ndate +%T\r\ndate +%Y-%m-%d&quot; &quot;%T\r\n\r\n9.日历\r\ncal 2012\r\n\r\n10.查看文件信息\r\nfile filename\r\n\r\n11.挂载硬盘\r\nmount\r\numount\r\n加载windows共享\r\nmount -t cifs //192.168.1.100/tools /mnt\r\n\r\n12.查看文件大小\r\ndu -h\r\ndu -ah\r\n\r\n13.查看分区\r\ndf -h\r\n\r\n14.ssh\r\nssh hadoop@192.168.1.1\r\n\r\n15.关机\r\nshutdown -h now /init 0\r\nshutdown -r now /reboot',9,0,0,1514643256,0,0,0),(199,1,'用户与组','','','添加一个tom用户，设置它属于users组，并添加注释信息\r\n分步完成：useradd tom\r\n          usermod -g users tom\r\n	      usermod -c &quot;hr tom&quot; tom\r\n一步完成：useradd -g users -c &quot;hr tom&quot; tom\r\n\r\n设置tom用户的密码\r\npasswd tom\r\n\r\n修改tom用户的登陆名为tomcat\r\nusermod -l tomcat tom\r\n\r\n将tomcat添加到sys和root组中\r\nusermod -G sys,root tomcat\r\n\r\n查看tomcat的组信息\r\ngroups tomcat\r\n\r\n添加一个jerry用户并设置密码\r\nuseradd jerry\r\npasswd jerry\r\n\r\n添加一个交america的组\r\ngroupadd america\r\n\r\n将jerry添加到america组中\r\nusermod -g america jerry\r\n\r\n将tomcat用户从root组和sys组删除\r\ngpasswd -d tomcat root\r\ngpasswd -d tomcat sys\r\n\r\n将america组名修改为am\r\ngroupmod -n am america',9,0,0,1514643300,0,0,0),(200,1,'linux权限','','','创建a.txt和b.txt文件，将他们设为其拥有者和所在组可写入，但其他以外的人则不可写入:\r\nchmod ug+w,o-w a.txt b.txt\r\n\r\n创建c.txt文件所有人都可以写和执行\r\nchmod a=wx c.txt 或chmod 666 c.txt\r\n\r\n将/itcast目录下的所有文件与子目录皆设为任何人可读取\r\nchmod -R a+r /itcast\r\n\r\n将/itcast目录下的所有文件与子目录的拥有者设为root，用户拥有组为users\r\nchown -R root:users /itcast\r\n\r\n将当前目录下的所有文件与子目录的用户皆设为itcast，组设为users\r\nchown -R itcast:users *',9,0,0,1514643352,0,0,0),(201,1,'文件夹属性','','','1.查看文件夹属性\r\nls -ld test\r\n\r\n2.文件夹的rwx\r\n--x:可以cd进去\r\nr-x:可以cd进去并ls\r\n-wx:可以cd进去并touch，rm自己的文件，并且可以vi其他用户的文件\r\n-wt:可以cd进去并touch，rm自己的文件\r\n\r\nls -ld /tmp\r\ndrwxrwxrwt的权限值是1777(sticky)',9,0,0,1514643380,0,0,0),(202,1,'vim','','','i\r\na/A\r\no/O\r\nr + ?替换\r\n\r\n0:文件当前行的开头\r\n$:文件当前行的末尾\r\nG:文件的最后一行开头\r\n1 + G到第一行 \r\n9 + G到第九行 = :9\r\n\r\ndd:删除一行\r\n3dd：删除3行\r\nyy:复制一行\r\n3yy:复制3行\r\np:粘贴\r\nu:undo\r\nctrl + r:redo\r\n\r\n&quot;a剪切板a\r\n&quot;b剪切板b\r\n\r\n&quot;ap粘贴剪切板a的内容\r\n\r\n每次进入vi就有行号\r\nvi ~/.vimrc\r\nset nu\r\n\r\n:w a.txt另存为\r\n:w &gt;&gt; a.txt内容追加到a.txt\r\n\r\n:e!恢复到最初状态\r\n\r\n:1,$s/hadoop/root/g 将第一行到追后一行的hadoop替换为root\r\n:1,$s/hadoop/root/c 将第一行到追后一行的hadoop替换为root(有提示)\r\n\r\n用vi文本编辑器来编辑生成文件\r\n******最基本用法\r\nvi  somefile.4\r\n1、首先会进入“一般模式”，此模式只接受各种快捷键，不能编辑文件内容\r\n2、按i键，就会从一般模式进入编辑模式，此模式下，敲入的都是文件内容\r\n3、编辑完成之后，按Esc键退出编辑模式，回到一般模式；\r\n4、再按：，进入“底行命令模式”，输入wq命令，回车即可\r\n\r\n******一些常用快捷键\r\n一些有用的快捷键（在一般模式下使用）：\r\na  在光标后一位开始插入\r\nA   在该行的最后插入\r\nI   在该行的最前面插入\r\ngg   直接跳到文件的首行\r\nG    直接跳到文件的末行\r\ndd   删除行，如果  5dd   ，则一次性删除光标后的5行\r\nyy  复制当前行,  复制多行，则  3yy，则复制当前行附近的3行\r\np   粘贴\r\nv  进入字符选择模式，选择完成后，按y复制，按p粘贴\r\nctrl+v  进入块选择模式，选择完成后，按y复制，按p粘贴\r\nshift+v  进入行选择模式，选择完成后，按y复制，按p粘贴\r\n\r\n查找并替换（在底行命令模式中输入）\r\n%s/sad/88888888888888     效果：查找文件中所有sad，替换为88888888888888\r\n/you       效果：查找文件中出现的you，并定位到第一个找到的地方，按n可以定位到下一个匹配位置（按N定位到上一个）\r\n\r\n\r\n\r\n*****拷贝文件\r\ncp  somefile.1   /home/hadoop/\r\n\r\n\r\n*****查看文件内容\r\ncat    somefile    一次性将文件内容全部输出（控制台）\r\nmore   somefile     可以翻页查看, 下翻一页(空格)    上翻一页（b）   退出（q）\r\nless   somefile      可以翻页查看,下翻一页(空格)    上翻一页（b），上翻一行(↑)  下翻一行（↓）  可以搜索关键字（/keyword）\r\n\r\ntail -10  install.log   查看文件尾部的10行\r\ntail -f install.log    小f跟踪文件的唯一inode号，就算文件改名后，还是跟踪原来这个inode表示的文件\r\ntail -F install.log    大F按照文件名来跟踪\r\n\r\nhead -10  install.log   查看文件头部的10行\r\n\r\n\r\n\r\n\r\n',9,0,0,1514643408,0,0,0),(203,1,'查找','','','1.查找可执行的命令：\r\nwhich ls\r\n\r\n2.查找可执行的命令和帮助的位置：\r\nwhereis ls\r\n\r\n3.查找文件(需要更新库:updatedb)\r\nlocate hadoop.txt\r\n\r\n4.从某个文件夹开始查找\r\nfind / -name &quot;hadooop*&quot;\r\nfind / -name &quot;hadooop*&quot; -ls\r\n\r\n5.查找并删除\r\nfind / -name &quot;hadooop*&quot; -ok rm {} \\;\r\nfind / -name &quot;hadooop*&quot; -exec rm {} \\;\r\n\r\n6.查找用户为hadoop的文件\r\nfind /usr -user hadoop -ls\r\n\r\n7.查找用户为hadoop并且(-a)拥有组为root的文件\r\nfind /usr -user hadoop -a -group root -ls\r\n\r\n8.查找用户为hadoop或者(-o)拥有组为root并且是文件夹类型的文件\r\nfind /usr -user hadoop -o -group root -a -type d\r\n\r\n9.查找权限为777的文件\r\nfind / -perm -777 -type d -ls\r\n\r\n10.显示命令历史\r\nhistory\r\n\r\n11.grep\r\ngrep hadoop /etc/password',9,0,0,1514643560,0,0,0),(204,1,'打包与压缩','','','常用压缩格式 .zip .gz .bz2\r\n常用压缩格式 .tar.gz .tar.bz2\r\n\r\n1.gzip压缩 .zip 格式压缩\r\nzip 压缩文件名 源文件 #压缩文件 gzip a.txt\r\nzip -r 压缩文件名 源目录 #压缩目录\r\n\r\n.gz 格式压缩\r\ngzip 源文件 #压缩为 .gz 格式的压缩文件 源文件会消失\r\ngzip -c 源文件 &gt; 压缩文件 #压缩为 .gz 格式 源文件保留\r\ngzip -c cangls &gt; cangls.gz\r\ngzip -r 目录 #压缩目录下所有的子文件 但是不能压缩目录\r\n\r\n\r\n2.解压\r\ngunzip a.txt.gz\r\ngzip -d a.txt.gz\r\n\r\n3.bzip2压缩\r\nbzip2 a\r\n\r\n4.解压\r\nbunzip2 a.bz2\r\nbzip2 -d a.bz2\r\n\r\nbzip2 源文件 #压缩为 .bz2 格式 不保留源文件\r\nbzip2 -k 源文件 #压缩之后保留源文件\r\n注意 bzip2 命令不能压缩目录\r\nbzip2 -d 压缩文件 #解压缩 -k 保留压缩文件\r\nbunzip2 压缩文件 #解压缩 -k 保留压缩文件\r\n\r\n5.将当前目录的文件打包\r\ntar -cvf bak.tar .\r\n将/etc/password追加文件到bak.tar中(r)\r\ntar -rvf bak.tar /etc/password\r\n\r\n6.解压\r\n打包命令 tar\r\ntar -cvf 打包文件名 源文件\r\n-c 打包\r\n-v 显示过程\r\n-f 指定打包后的文件名\r\ntar -cvf longzls.tar longzls\r\n\r\ntar -xvf 打包文件名\r\n-x 解打包\r\ntar -xvf longzls.tar\r\n\r\ntar -xvf bak.tar\r\n\r\n7.打包并压缩gzip\r\ntar -zcvf a.tar.gz\r\ntar -cvf spark-2.1.0-bin-2.6.0-cdh5.7.0.tar.gz spark-2.1.0-bin-2.6.0-cdh5.7.0/\r\n\r\n\r\n8.解压缩\r\ntar -zxvf a.tar.gz\r\n解压到/usr/下\r\ntar -zxvf a.tar.gz -C /usr\r\n\r\n9.查看压缩包内容\r\ntar -ztvf a.tar.gz\r\n\r\nzip/unzip\r\n\r\n10.打包并压缩成bz2\r\ntar -jcvf a.tar.bz2\r\n\r\n11.解压bz2\r\ntar -jxvf a.tar.bz2\r\n\r\n\r\n',9,0,0,1514643586,0,0,0),(205,1,'正则表达式','','','1.cut截取以:分割保留第七段\r\ngrep hadoop /etc/passwd | cut -d: -f7\r\n\r\n2.排序\r\ndu | sort -n \r\n\r\n3.查询不包含hadoop的\r\ngrep -v hadoop /etc/passwd\r\n\r\n4.正则表达包含hadoop\r\ngrep &#039;hadoop&#039; /etc/passwd\r\n\r\n5.正则表达(点代表任意一个字符)\r\ngrep &#039;h.*p&#039; /etc/passwd\r\n\r\n6.正则表达以hadoop开头\r\ngrep &#039;^hadoop&#039; /etc/passwd\r\n\r\n7.正则表达以hadoop结尾\r\ngrep &#039;hadoop$&#039; /etc/passwd\r\n\r\n规则：\r\n.  : 任意一个字符\r\na* : 任意多个a(零个或多个a)\r\na? : 零个或一个a\r\na+ : 一个或多个a\r\n.* : 任意多个任意字符\r\n\\. : 转义.\r\n\\&lt;h.*p\\&gt; ：以h开头，p结尾的一个单词\r\no\\{2\\} : o重复两次\r\n\r\ngrep &#039;^i.\\{18\\}n$&#039; /usr/share/dict/words\r\n\r\n查找不是以#开头的行\r\ngrep -v &#039;^#&#039; a.txt | grep -v &#039;^$&#039; \r\n\r\n以h或r开头的\r\ngrep &#039;^[hr]&#039; /etc/passwd\r\n\r\n不是以h和r开头的\r\ngrep &#039;^[^hr]&#039; /etc/passwd\r\n\r\n不是以h到r开头的\r\ngrep &#039;^[^h-r]&#039; /etc/passwd',9,0,0,1514643616,0,0,0),(206,1,'输入输出重定向及管道','','','1.新建一个文件\r\ntouch a.txt\r\n&gt; b.txt\r\n\r\n2.错误重定向:2&gt;\r\nfind /etc -name zhaoxing.txt 2&gt; error.txt\r\n\r\n3.将正确或错误的信息都输入到log.txt中\r\nfind /etc -name passwd &gt; /tmp/log.txt 2&gt;&amp;1 \r\nfind /etc -name passwd &amp;&gt; /tmp/log.txt\r\n\r\n4.追加&gt;&gt;\r\n\r\n5.将小写转为大写（输入重定向）\r\ntr &quot;a-z&quot; &quot;A-Z&quot; &lt; /etc/passwd\r\n\r\n6.自动创建文件\r\ncat &gt; log.txt &lt;&lt; EXIT\r\n&gt; ccc\r\n&gt; ddd\r\n&gt; EXI\r\n\r\n7.查看/etc下的文件有多少个？\r\nls -l /etc/ | grep &#039;^d&#039; | wc -l\r\n\r\n8.查看/etc下的文件有多少个，并将文件详情输入到result.txt中\r\nls -l /etc/ | grep &#039;^d&#039; | tee result.txt | wc -l',9,0,0,1514643646,0,0,0),(207,1,'进程控制','','','1.查看用户最近登录情况\r\nlast\r\nlastlog\r\n\r\n2.查看硬盘使用情况\r\ndf\r\n\r\n3.查看文件大小\r\ndu\r\n\r\n4.查看内存使用情况\r\nfree\r\n\r\n5.查看文件系统\r\n/proc\r\n\r\n6.查看日志\r\nls /var/log/\r\n\r\n7.查看系统报错日志\r\ntail /var/log/messages\r\n\r\n8.查看进程\r\ntop\r\n\r\n9.结束进程\r\nkill 1234\r\nkill -9 4333',9,0,0,1514643667,0,0,0),(208,1,'awk简介','','','awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。\r\n\r\nawk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。\r\n\r\nawk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。\r\n\r\n \r\n\r\n使用方法\r\nawk &#039;{pattern + action}&#039; {filenames}\r\n尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。\r\n\r\nawk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。\r\n\r\n通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。\r\n\r\n \r\n\r\n调用awk\r\n有三种方式调用awk\r\n\r\n \r\n1.命令行方式\r\nawk [-F  field-separator]  &#039;commands&#039;  input-file(s)\r\n其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。\r\n在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。\r\n\r\n2.shell脚本方式\r\n将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。\r\n相当于shell脚本首行的：#!/bin/sh\r\n可以换成：#!/bin/awk\r\n\r\n3.将所有的awk命令插入一个单独文件，然后调用：\r\nawk -f awk-script-file input-file(s)\r\n其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。\r\n \r\n 本章重点介绍命令行方式。\r\n\r\n \r\n\r\n入门实例\r\n假设last -n 5的输出如下\r\n\r\n[root@www ~]# last -n 5 &lt;==仅取出前五行\r\nroot     pts/1   192.168.1.100  Tue Feb 10 11:21   still logged in\r\nroot     pts/1   192.168.1.100  Tue Feb 10 00:46 - 02:28  (01:41)\r\nroot     pts/1   192.168.1.100  Mon Feb  9 11:41 - 18:30  (06:48)\r\ndmtsai   pts/1   192.168.1.100  Mon Feb  9 11:41 - 11:41  (00:00)\r\nroot     tty1                   Fri Sep  5 14:09 - 14:10  (00:01)\r\n如果只是显示最近登录的5个帐号\r\n\r\n#last -n 5 | awk  &#039;{print $1}&#039;\r\nroot\r\nroot\r\nroot\r\ndmtsai\r\nroot\r\nawk工作流程是这样的：读入有&#039;\\n&#039;换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是&quot;空白键&quot; 或 &quot;[tab]键&quot;,所以$1表示登录用户，$3表示登录用户ip,以此类推。\r\n\r\n \r\n\r\n如果只是显示/etc/passwd的账户\r\n\r\n#cat /etc/passwd |awk  -F &#039;:&#039;  &#039;{print $1}&#039;  \r\nroot\r\ndaemon\r\nbin\r\nsys\r\n这种是awk+action的示例，每行都会执行action{print $1}。\r\n\r\n-F指定域分隔符为&#039;:&#039;。\r\n\r\n \r\n\r\n如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以tab键分割\r\n\r\n#cat /etc/passwd |awk  -F &#039;:&#039;  &#039;{print $1&quot;\\t&quot;$7}&#039;\r\nroot    /bin/bash\r\ndaemon  /bin/sh\r\nbin     /bin/sh\r\nsys     /bin/sh\r\n \r\n\r\n如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加&quot;blue,/bin/nosh&quot;。\r\n\r\n \r\ncat /etc/passwd |awk  -F &#039;:&#039;  &#039;BEGIN {print &quot;name,shell&quot;}  {print $1&quot;,&quot;$7} END {print &quot;blue,/bin/nosh&quot;}&#039;\r\nname,shell\r\nroot,/bin/bash\r\ndaemon,/bin/sh\r\nbin,/bin/sh\r\nsys,/bin/sh\r\n....\r\nblue,/bin/nosh\r\n \r\nawk工作流程是这样的：先执行BEGING，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域,随后开始执行模式所对应的动作action。接着开始读入第二条记录······直到所有的记录都读完，最后执行END操作。\r\n\r\n \r\n\r\n搜索/etc/passwd有root关键字的所有行\r\n\r\n#awk -F: &#039;/root/&#039; /etc/passwd\r\nroot:x:0:0:root:/root:/bin/bash\r\n这种是pattern的使用示例，匹配了pattern(这里是root)的行才会执行action(没有指定action，默认输出每行的内容)。\r\n\r\n搜索支持正则，例如找root开头的: awk -F: &#039;/^root/&#039; /etc/passwd\r\n\r\n \r\n\r\n搜索/etc/passwd有root关键字的所有行，并显示对应的shell\r\n\r\n# awk -F: &#039;/root/{print $7}&#039; /etc/passwd             \r\n/bin/bash\r\n 这里指定了action{print $7}\r\n\r\n \r\n\r\nawk内置变量\r\nawk有许多内置变量用来设置环境信息，这些变量可以被改变，下面给出了最常用的一些变量。\r\n\r\n \r\nARGC               命令行参数个数\r\nARGV               命令行参数排列\r\nENVIRON            支持队列中系统环境变量的使用\r\nFILENAME           awk浏览的文件名\r\nFNR                浏览文件的记录数\r\nFS                 设置输入域分隔符，等价于命令行 -F选项\r\nNF                 浏览记录的域的个数\r\nNR                 已读的记录数\r\nOFS                输出域分隔符\r\nORS                输出记录分隔符\r\nRS                 控制记录分隔符\r\n \r\n 此外,$0变量是指整条记录。$1表示当前行的第一个域,$2表示当前行的第二个域,......以此类推。\r\n\r\n \r\n\r\n统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容:\r\n\r\n#awk  -F &#039;:&#039;  &#039;{print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0}&#039; /etc/passwd\r\nfilename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/bash\r\nfilename:/etc/passwd,linenumber:2,columns:7,linecontent:daemon:x:1:1:daemon:/usr/sbin:/bin/sh\r\nfilename:/etc/passwd,linenumber:3,columns:7,linecontent:bin:x:2:2:bin:/bin:/bin/sh\r\nfilename:/etc/passwd,linenumber:4,columns:7,linecontent:sys:x:3:3:sys:/dev:/bin/sh\r\n \r\n\r\n使用printf替代print,可以让代码更加简洁，易读\r\n\r\n awk  -F &#039;:&#039;  &#039;{printf(&quot;filename:%s,linenumber:%s,columns:%s,linecontent:%s\\n&quot;,FILENAME,NR,NF,$0)}&#039; /etc/passwd\r\n \r\n\r\nprint和printf\r\nawk中同时提供了print和printf两种打印输出的函数。\r\n\r\n其中print函数的参数可以是变量、数值或者字符串。字符串必须用双引号引用，参数用逗号分隔。如果没有逗号，参数就串联在一起而无法区分。这里，逗号的作用与输出文件的分隔符的作用是一样的，只是后者是空格而已。\r\n\r\nprintf函数，其用法和c语言中printf基本相似,可以格式化字符串,输出复杂时，printf更加好用，代码更易懂。\r\n\r\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \r\n\r\n awk编程\r\n 变量和赋值\r\n\r\n除了awk的内置变量，awk还可以自定义变量。\r\n\r\n下面统计/etc/passwd的账户人数\r\n\r\nawk &#039;{count++;print $0;} END{print &quot;user count is &quot;, count}&#039; /etc/passwd\r\nroot:x:0:0:root:/root:/bin/bash\r\n......\r\nuser count is  40\r\ncount是自定义变量。之前的action{}里都是只有一个print,其实print只是一个语句，而action{}可以有多个语句，以;号隔开。\r\n\r\n \r\n\r\n这里没有初始化count，虽然默认是0，但是妥当的做法还是初始化为0:\r\n\r\nawk &#039;BEGIN {count=0;print &quot;[start]user count is &quot;, count} {count=count+1;print $0;} END{print &quot;[end]user count is &quot;, count}&#039; /etc/passwd\r\n[start]user count is  0\r\nroot:x:0:0:root:/root:/bin/bash\r\n...\r\n[end]user count is  40\r\n \r\n\r\n统计某个文件夹下的文件占用的字节数\r\n\r\nls -l |awk &#039;BEGIN {size=0;} {size=size+$5;} END{print &quot;[end]size is &quot;, size}&#039;\r\n[end]size is  8657198\r\n \r\n\r\n如果以M为单位显示:\r\n\r\nls -l |awk &#039;BEGIN {size=0;} {size=size+$5;} END{print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;}&#039; \r\n[end]size is  8.25889 M\r\n注意，统计不包括文件夹的子目录。\r\n\r\n \r\n\r\n条件语句\r\n\r\n awk中的条件语句是从C语言中借鉴来的，见如下声明方式：\r\n\r\n \r\nif (expression) {\r\n    statement;\r\n    statement;\r\n    ... ...\r\n}\r\n\r\nif (expression) {\r\n    statement;\r\n} else {\r\n    statement2;\r\n}\r\n\r\nif (expression) {\r\n    statement1;\r\n} else if (expression1) {\r\n    statement2;\r\n} else {\r\n    statement3;\r\n}\r\n \r\n \r\n\r\n统计某个文件夹下的文件占用的字节数,过滤4096大小的文件(一般都是文件夹):\r\n\r\nls -l |awk &#039;BEGIN {size=0;print &quot;[start]size is &quot;, size} {if($5!=4096){size=size+$5;}} END{print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;}&#039; \r\n[end]size is  8.22339 M\r\n \r\n\r\n循环语句\r\n\r\nawk中的循环语句同样借鉴于C语言，支持while、do/while、for、break、continue，这些关键字的语义和C语言中的语义完全相同。\r\n\r\n \r\n\r\n数组\r\n\r\n  因为awk中数组的下标可以是数字和字母，数组的下标通常被称为关键字(key)。值和关键字都存储在内部的一张针对key/value应用hash的表格里。由于hash不是顺序存储，因此在显示数组内容时会发现，它们并不是按照你预料的顺序显示出来的。数组和变量一样，都是在使用时自动创建的，awk也同样会自动判断其存储的是数字还是字符串。一般而言，awk中的数组用来从记录中收集信息，可以用于计算总和、统计单词以及跟踪模板被匹配的次数等等。\r\n\r\n \r\n\r\n显示/etc/passwd的账户\r\n\r\n \r\nawk -F &#039;:&#039; &#039;BEGIN {count=0;} {name[count] = $1;count++;}; END{for (i = 0; i &lt; NR; i++) print i, name[i]}&#039; /etc/passwd\r\n0 root\r\n1 daemon\r\n2 bin\r\n3 sys\r\n4 sync\r\n5 games\r\n......\r\n \r\n这里使用for循环遍历数组\r\n\r\n \r\n\r\nawk编程的内容极多，这里只罗列简单常用的用法，更多请参考 http://www.gnu.org/software/gawk/manual/gawk.html\r\n',9,0,0,1514648047,0,0,0),(209,1,'crontab的使用','','','基本格式 : \r\n*　　*　　*　　*　　*　　command \r\n\r\n\r\n分　 时　 日　 月　 周　 命令 \r\n第1列表示分钟1～59 每分钟用*或者 */1表示 \r\n第2列表示小时0～23（0表示0点） \r\n第3列表示日期1～31 \r\n第4列表示月份1～12 \r\n第5列标识号星期0～6（0表示星期天） \r\n第6列要运行的命令 \r\ncrontab文件的一些例子： \r\n30 21 * * * /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示每晚的21:30重启apache。 \r\n45 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示每月1、10、22日的4 : 45重启apache。 \r\n10 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示每周六、周日的1 : 10重启apache。 \r\n0,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache。 \r\n0 23 * * 6 /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示每星期六的11 : 00 pm重启apache。 \r\n* */1 * * * /usr/local/etc/rc.d/lighttpd restart \r\n每一小时重启apache \r\n* 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart \r\n晚上11点到早上7点之间，每隔一小时重启apache \r\n0 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart \r\n每月的4号与每周一到周三的11点重启apache \r\n0 4 1 jan * /usr/local/etc/rc.d/lighttpd restart \r\n一月一号的4点重启apache \r\n名称 : crontab \r\n使用权限 : 所有使用者 \r\n使用方式 : \r\ncrontab file [-u user]-用指定的文件替代目前的crontab。 \r\ncrontab-[-u user]-用标准输入替代目前的crontab. \r\ncrontab-1[user]-列出用户目前的crontab. \r\ncrontab-e[user]-编辑用户目前的crontab. \r\ncrontab-d[user]-删除用户目前的crontab. \r\ncrontab-c dir- 指定crontab的目录。 \r\ncrontab文件的格式：M H D m d cmd. \r\nM: 分钟（0-59）。 \r\nH：小时（0-23）。 \r\nD：天（1-31）。 \r\nm: 月（1-12）。 \r\nd: 一星期内的天（0~6，0为星期天）。 \r\ncmd要运行的程序，程序被送入sh执行，这个shell只有USER,HOME,SHELL这三个环境变量 \r\n说明 : \r\ncrontab 是用来让使用者在固定时间或固定间隔执行程序之用，换句话说，也就是类似使用者的时程表。-u user 是指设定指定 \r\nuser 的时程表，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设 \r\n定自己的时程表。 \r\n参数 : \r\ncrontab -e : 执行文字编辑器来设定时程表，内定的文字编辑器是 VI，如果你想用别的文字编辑器，则请先设定 VISUAL 环境变数 \r\n来指定使用那个文字编辑器(比如说 setenv VISUAL joe) \r\ncrontab -r : 删除目前的时程表 \r\ncrontab -l : 列出目前的时程表 \r\ncrontab file [-u user]-用指定的文件替代目前的crontab。 \r\n时程表的格式如下 : \r\nf1 f2 f3 f4 f5 program \r\n其中 f1 是表示分钟，f2 表示小时，f3 表示一个月份中的第几日，f4 表示月份，f5 表示一个星期中的第几天。program 表示要执 \r\n行的程序。 \r\n当 f1 为 * 时表示每分钟都要执行 program，f2 为 * 时表示每小时都要执行程序，其馀类推 \r\n当 f1 为 a-b 时表示从第 a 分钟到第 b 分钟这段时间内要执行，f2 为 a-b 时表示从第 a 到第 b 小时都要执行，其馀类推 \r\n当 f1 为 */n 时表示每 n 分钟个时间间隔执行一次，f2 为 */n 表示每 n 小时个时间间隔执行一次，其馀类推 \r\n当 f1 为 a, b, c,... 时表示第 a, b, c,... 分钟要执行，f2 为 a, b, c,... 时表示第 a, b, c...个小时要执行，其馀类推 \r\n使用者也可以将所有的设定先存放在档案 file 中，用 crontab file 的方式来设定时程表。 \r\n例子 : \r\n#每天早上7点执行一次 /bin/ls : \r\n0 7 * * * /bin/ls \r\n在 12 月内, 每天的早上 6 点到 12 点中，每隔3个小时执行一次 /usr/bin/backup : \r\n0 6-12/3 * 12 * /usr/bin/backup \r\n周一到周五每天下午 5:00 寄一封信给 alex@domain.name : \r\n0 17 * * 1-5 mail -s &quot;hi&quot; alex@domain.name &lt; /tmp/maildata \r\n每月每天的午夜 0 点 20 分, 2 点 20 分, 4 点 20 分....执行 echo &quot;haha&quot; \r\n20 0-23/2 * * * echo &quot;haha&quot; \r\n注意 : \r\n当程序在你所指定的时间执行后，系统会寄一封信给你，显示该程序执行的内容，若是你不希望收到这样的信，请在每一行空一格之 \r\n后加上 &gt; /dev/null 2&gt;&amp;1 即可 \r\n例子2 : \r\n#每天早上6点10分 \r\n10 6 * * * date \r\n#每两个小时 \r\n0 */2 * * * date \r\n#晚上11点到早上8点之间每两个小时，早上8点 \r\n0 23-7/2，8 * * * date \r\n#每个月的4号和每个礼拜的礼拜一到礼拜三的早上11点 \r\n0 11 4 * mon-wed date \r\n#1月份日早上4点 \r\n0 4 1 jan * date \r\n范例 \r\n$crontab -l 列出用户目前的crontab.',9,0,0,1514648164,0,0,0),(210,1,'iptables的使用','','','#查看帮助\r\niptables -h\r\nman iptables\r\n\r\n列出iptables规则\r\niptables -L -n\r\n列出iptables规则并显示规则编号\r\niptables -L -n --line-numbers\r\n\r\n列出iptables nat表规则（默认是filter表）\r\niptables -L -n -t nat\r\n\r\n清除默认规则（注意默认是filter表，如果对nat表操作要加-t nat）\r\n#清楚所有规则\r\niptables -F \r\n\r\n#重启iptables发现规则依然存在，因为没有保存\r\nservice iptables restart\r\n\r\n#保存配置\r\nservice iptables save\r\n\r\n#禁止ssh登陆（若果服务器在机房，一定要小心）\r\niptables -A INPUT -p tcp --dport 22 -j DROP\r\n#删除规则\r\niptables -D INPUT -p tcp --dport 22 -j DROP\r\n\r\n-A, --append chain	追加到规则的最后一条\r\n-D, --delete chain [rulenum]	Delete rule rulenum (1 = first) from chain\r\n-I, --insert chain [rulenum]	Insert in chain as rulenum (default 1=first) 添加到规则的第一条\r\n-p, --proto  proto	protocol: by number or name, eg. &#039;tcp&#039;,常用协议有tcp、udp、icmp、all\r\n-j, --jump target 常见的行为有ACCEPT、DROP和REJECT三种，但一般不用REJECT，会带来安全隐患\r\n\r\n注意：INPUT和DROP这样的关键字需要大写\r\n\r\n#禁止192.168.33.0网段从eth0网卡接入\r\niptables -A INPUT -p tcp -i eth0 -s 192.168.33.0 -j DROP\r\niptables -A INPUT -p tcp --dport 22 -i eth0 -s 192.168.33.61  -j ACCEPT\r\n\r\n#禁止ip地址非192.168.10.10的所有类型数据接入\r\niptables -A INPUT ! -s 192.168.10.10 -j DROP\r\n\r\n#禁止ip地址非192.168.10.10的ping请求\r\niptables -I INPUT -p icmp --icmp-type 8 -s 192.168.50.100 -j DROP\r\n\r\n#扩展匹配：1.隐式扩展 2.显示扩展\r\n	#隐式扩展\r\n	-p tcp\r\n		--sport PORT 源端口\r\n		--dport PORT 目标端口\r\n\r\n	#显示扩展：使用额外的匹配规则\r\n	-m EXTENSTION --SUB-OPT\r\n	-p tcp --dport 22 与 -p tcp -m tcp --dport 22功能相同\r\n\r\n	state：状态扩展，接口ip_contrack追踪会话状态\r\n		NEW：新的连接请求\r\n		ESTABLISHED：已建立的连接请求\r\n		INVALID：非法连接\r\n		RELATED：相关联的连接\r\n	\r\n\r\n#匹配端口范围\r\niptables -I INPUT -p tcp --dport 22:80 -j DROP\r\n\r\n#匹配多个端口\r\niptables -I INPUT -p tcp -m multiport --dport 22,80,3306 -j ACCEPT\r\n\r\n#不允许源端口为80的数据流出\r\niptables -I OUTPUT -p tcp --sport 80 -j DROP\r\n',9,0,0,1514648205,0,0,0),(211,1,'sed简介','','','1. Sed简介\r\nsed 是一种在线编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。以下介绍的是Gnu版本的Sed 3.02。\r\n2. 定址\r\n可以通过定址来定位你所希望编辑的行，该地址用数字构成，用逗号分隔的两个行数表示以这两行为起止的行的范围（包括行数表示的那两行）。如1，3表示1，2，3行，美元符号($)表示最后一行。范围可以通过数据，正则表达式或者二者结合的方式确定 。\r\n\r\n3. Sed命令\r\n调用sed命令有两种形式：\r\n*\r\nsed [options] &#039;command&#039; file(s)\r\n*\r\nsed [options] -f scriptfile file(s)\r\na\\\r\n在当前行后面加入一行文本。\r\nb lable\r\n分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。\r\nc\\\r\n用新的文本改变本行的文本。\r\nd\r\n从模板块（Pattern space）位置删除行。\r\nD\r\n删除模板块的第一行。\r\ni\\\r\n在当前行上面插入文本。\r\nh\r\n拷贝模板块的内容到内存中的缓冲区。\r\nH\r\n追加模板块的内容到内存中的缓冲区\r\ng\r\n获得内存缓冲区的内容，并替代当前模板块中的文本。\r\nG\r\n获得内存缓冲区的内容，并追加到当前模板块文本的后面。\r\nl\r\n列表不能打印字符的清单。\r\nn\r\n读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。\r\nN\r\n追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。\r\np\r\n打印模板块的行。\r\nP（大写）\r\n打印模板块的第一行。\r\nq\r\n退出Sed。\r\nr file\r\n从file中读行。\r\nt label\r\nif分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。\r\nT label\r\n错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。\r\nw file\r\n写并追加模板块到file末尾。\r\nW file\r\n写并追加模板块的第一行到file末尾。\r\n!\r\n表示后面的命令对所有没有被选定的行发生作用。\r\ns/re/string\r\n用string替换正则表达式re。\r\n=\r\n打印当前行号码。\r\n#\r\n把注释扩展到下一个换行符以前。\r\n以下的是替换标记\r\n*\r\ng表示行内全面替换。\r\n*\r\np表示打印行。\r\n*\r\nw表示把行写入一个文件。\r\n*\r\nx表示互换模板块中的文本和缓冲区中的文本。\r\n*\r\ny表示把一个字符翻译为另外的字符（但是不用于正则表达式）\r\n\r\n4. 选项\r\n-e command, --expression=command\r\n允许多台编辑。\r\n-h, --help\r\n打印帮助，并显示bug列表的地址。\r\n-n, --quiet, --silent\r\n取消默认输出。\r\n-f, --filer=script-file\r\n引导sed脚本文件名。\r\n-V, --version\r\n打印版本和版权信息。\r\n\r\n5. 元字符集^\r\n锚定行的开始 如：/^sed/匹配所有以sed开头的行。 \r\n$\r\n锚定行的结束 如：/sed$/匹配所有以sed结尾的行。 \r\n.\r\n匹配一个非换行符的字符 如：/s.d/匹配s后接一个任意字符，然后是d。 \r\n*\r\n匹配零或多个字符 如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。 \r\n[]\r\n匹配一个指定范围内的字符，如/[Ss]ed/匹配sed和Sed。 \r\n[^]\r\n匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。 \r\n\\(..\\)\r\n保存匹配的字符，如s/\\(love\\)able/\\1rs，loveable被替换成lovers。 \r\n&amp;\r\n保存搜索字符用来替换其他字符，如s/love/**&amp;**/，love这成**love**。 \r\n\\&lt;\r\n锚定单词的开始，如:/\\&lt;love/匹配包含以love开头的单词的行。 \r\n\\&gt;\r\n锚定单词的结束，如/love\\&gt;/匹配包含以love结尾的单词的行。 \r\nx\\{m\\}\r\n重复字符x，m次，如：/o\\{5\\}/匹配包含5个o的行。 \r\nx\\{m,\\}\r\n重复字符x,至少m次，如：/o\\{5,\\}/匹配至少有5个o的行。 \r\nx\\{m,n\\}\r\n重复字符x，至少m次，不多于n次，如：/o\\{5,10\\}/匹配5--10个o的行。\r\n6. 实例\r\n删除：d命令\r\n*\r\n$ sed &#039;2d&#039; example-----删除example文件的第二行。\r\n*\r\n$ sed &#039;2,$d&#039; example-----删除example文件的第二行到末尾所有行。\r\n*\r\n$ sed &#039;$d&#039; example-----删除example文件的最后一行。\r\n*\r\n$ sed &#039;/test/&#039;d example-----删除example文件所有包含test的行。\r\n替换：s命令\r\n*\r\n$ sed &#039;s/test/mytest/g&#039; example-----在整行范围内把test替换为mytest。如果没有g标记，则只有每行第一个匹配的test被替换成mytest。\r\n*\r\n$ sed -n &#039;s/^test/mytest/p&#039; example-----(-n)选项和p标志一起使用表示只打印那些发生替换的行。也就是说，如果某一行开头的test被替换成mytest，就打印它。\r\n*\r\n$ sed &#039;s/^192.168.0.1/&amp;localhost/&#039; example-----&amp;符号表示替换换字符串中被找到的部份。所有以192.168.0.1开头的行都会被替换成它自已加 localhost，变成192.168.0.1localhost。\r\n*\r\n$ sed -n &#039;s/\\(love\\)able/\\1rs/p&#039; example-----love被标记为1，所有loveable会被替换成lovers，而且替换的行会被打印出来。\r\n*\r\n$ sed &#039;s#10#100#g&#039; example-----不论什么字符，紧跟着s命令的都被认为是新的分隔符，所以，“#”在这里是分隔符，代替了默认的“/”分隔符。表示把所有10替换成100。\r\n选定行的范围：逗号\r\n*\r\n$ sed -n &#039;/test/,/check/p&#039; example-----所有在模板test和check所确定的范围内的行都被打印。\r\n*\r\n$ sed -n &#039;5,/^test/p&#039; example-----打印从第五行开始到第一个包含以test开始的行之间的所有行。\r\n*\r\n$ sed &#039;/test/,/check/s/$/sed test/&#039; example-----对于模板test和west之间的行，每行的末尾用字符串sed test替换。\r\n多点编辑：e命令\r\n*\r\n$ sed -e &#039;1,5d&#039; -e &#039;s/test/check/&#039; example-----(-e)选项允许在同一行里执行多条命令。如例子所示，第一条命令删除1至5行，第二条命令用check替换test。命令的执 行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。\r\n*\r\n$ sed --expression=&#039;s/test/check/&#039; --expression=&#039;/love/d&#039; example-----一个比-e更好的命令是--expression。它能给sed表达式赋值。\r\n从文件读入：r命令\r\n*\r\n$ sed &#039;/test/r file&#039; example-----file里的内容被读进来，显示在与test匹配的行后面，如果匹配多行，则file的内容将显示在所有匹配行的下面。\r\n写入文件：w命令\r\n*\r\n$ sed -n &#039;/test/w file&#039; example-----在example中所有包含test的行都被写入file里。\r\n追加命令：a命令\r\n*\r\n$ sed &#039;/^test/a\\\\---&gt;this is a example&#039; example    &#039;-----&gt;this is a example&#039;被追加到以test开头的行后面，sed要求命令a后面有一个反斜杠。\r\n插入：i命令\r\n$ sed &#039;/test/i\\\\\r\nnew line\r\n-------------------------&#039; example\r\n如果test被匹配，则把反斜杠后面的文本插入到匹配行的前面。\r\n下一个：n命令\r\n*\r\n$ sed &#039;/test/{ n; s/aa/bb/; }&#039; example-----如果test被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb，并打印该行，然后继续。\r\n变形：y命令\r\n*\r\n$ sed &#039;1,10y/abcde/ABCDE/&#039; example-----把1--10行内所有abcde转变为大写，注意，正则表达式元字符不能使用这个命令。\r\n退出：q命令\r\n*\r\n$ sed &#039;10q&#039; example-----打印完第10行后，退出sed。\r\n保持和获取：h命令和G命令\r\n*\r\n$ sed -e &#039;/test/h&#039; -e &#039;$G example-----在sed处理文件的时候，每一行都被保存在一个叫模式空间的临时缓冲区中，除非行被删除或者输出被取消，否则所有被处理的行都将 打印在屏幕上。接着模式空间被清空，并存入新的一行等待处理。在这个例子里，匹配test的行被找到后，将存入模式空间，h命令将其复制并存入一个称为保 持缓存区的特殊缓冲区内。第二条语句的意思是，当到达最后一行后，G命令取出保持缓冲区的行，然后把它放回模式空间中，且追加到现在已经存在于模式空间中 的行的末尾。在这个例子中就是追加到最后一行。简单来说，任何包含test的行都被复制并追加到该文件的末尾。\r\n保持和互换：h命令和x命令\r\n*\r\n$ sed -e &#039;/test/h&#039; -e &#039;/check/x&#039; example -----互换模式空间和保持缓冲区的内容。也就是把包含test与check的行互换。\r\n7. 脚本\r\nSed脚本是一个sed的命令清单，启动Sed时以-f选项引导脚本文件名。Sed对于脚本中输入的命令非常挑剔，在命令的末尾不能有任何空白或文本，如果在一行中有多个命令，要用分号分隔。以#开头的行为注释行，且不能跨行。\r\n',9,0,0,1514648285,0,0,0),(212,1,'sort的使用','','','sort\r\nsort 命令对 File 参数指定的文件中的行排序，并将结果写到标准输出。如果 File 参数指定多个文件，那么 sort 命令将这些文件连接起来，并当作一个文件进行排序。\r\n\r\nsort语法\r\n\r\n \r\n[root@www ~]# sort [-fbMnrtuk] [file or stdin]\r\n选项与参数：\r\n-f  ：忽略大小写的差异，例如 A 与 a 视为编码相同；\r\n-b  ：忽略最前面的空格符部分；\r\n-M  ：以月份的名字来排序，例如 JAN, DEC 等等的排序方法；\r\n-n  ：使用『纯数字』进行排序(默认是以文字型态来排序的)；\r\n-r  ：反向排序；\r\n-u  ：就是 uniq ，相同的数据中，仅出现一行代表；\r\n-t  ：分隔符，默认是用 [tab] 键来分隔；\r\n-k  ：以那个区间 (field) 来进行排序的意思\r\n \r\n\r\n对/etc/passwd 的账号进行排序\r\n[root@www ~]# cat /etc/passwd | sort\r\nadm:x:3:4:adm:/var/adm:/sbin/nologin\r\napache:x:48:48:Apache:/var/www:/sbin/nologin\r\nbin:x:1:1:bin:/bin:/sbin/nologin\r\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\r\nsort 是默认以第一个数据来排序，而且默认是以字符串形式来排序,所以由字母 a 开始升序排序。\r\n\r\n \r\n\r\n/etc/passwd 内容是以 : 来分隔的，我想以第三栏来排序，该如何\r\n\r\n[root@www ~]# cat /etc/passwd | sort -t &#039;:&#039; -k 3\r\nroot:x:0:0:root:/root:/bin/bash\r\nuucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\r\noperator:x:11:0:operator:/root:/sbin/nologin\r\nbin:x:1:1:bin:/bin:/sbin/nologin\r\ngames:x:12:100:games:/usr/games:/sbin/nologin\r\n默认是以字符串来排序的，如果想要使用数字排序：\r\n\r\ncat /etc/passwd | sort -t &#039;:&#039; -k 3n\r\nroot:x:0:0:root:/root:/bin/bash\r\ndaemon:x:1:1:daemon:/usr/sbin:/bin/sh\r\nbin:x:2:2:bin:/bin:/bin/sh\r\n默认是升序排序，如果要倒序排序，如下\r\n\r\ncat /etc/passwd | sort -t &#039;:&#039; -k 3nr\r\nnobody:x:65534:65534:nobody:/nonexistent:/bin/sh\r\nntp:x:106:113::/home/ntp:/bin/false\r\nmessagebus:x:105:109::/var/run/dbus:/bin/false\r\nsshd:x:104:65534::/var/run/sshd:/usr/sbin/nologin\r\n \r\n\r\n如果要对/etc/passwd,先以第六个域的第2个字符到第4个字符进行正向排序，再基于第一个域进行反向排序。\r\n\r\ncat /etc/passwd |  sort -t&#039;:&#039; -k 6.2,6.4 -k 1r      \r\nsync:x:4:65534:sync:/bin:/bin/sync\r\nproxy:x:13:13:proxy:/bin:/bin/sh\r\nbin:x:2:2:bin:/bin:/bin/sh\r\nsys:x:3:3:sys:/dev:/bin/sh\r\n \r\n\r\n查看/etc/passwd有多少个shell:对/etc/passwd的第七个域进行排序，然后去重:\r\n\r\ncat /etc/passwd |  sort -t&#039;:&#039; -k 7 -u\r\nroot:x:0:0:root:/root:/bin/bash\r\nsyslog:x:101:102::/home/syslog:/bin/false\r\ndaemon:x:1:1:daemon:/usr/sbin:/bin/sh\r\nsync:x:4:65534:sync:/bin:/bin/sync\r\nsshd:x:104:65534::/var/run/sshd:/usr/sbin/nologin\r\n \r\n\r\nuniq\r\n uniq命令可以去除排序过的文件中的重复行，因此uniq经常和sort合用。也就是说，为了使uniq起作用，所有的重复行必须是相邻的。\r\n\r\nuniq语法\r\n\r\n[root@www ~]# uniq [-icu]\r\n选项与参数：\r\n-i   ：忽略大小写字符的不同；\r\n-c  ：进行计数\r\n-u  ：只显示唯一的行\r\n \r\n\r\ntestfile的内容如下\r\n\r\n \r\ncat testfile\r\nhello\r\nworld\r\nfriend\r\nhello\r\nworld\r\nhello\r\n \r\n \r\n\r\n直接删除未经排序的文件，将会发现没有任何行被删除\r\n\r\n \r\n#uniq testfile  \r\nhello\r\nworld\r\nfriend\r\nhello\r\nworld\r\nhello\r\n \r\n \r\n\r\n排序文件，默认是去重\r\n\r\n#cat testfile | sort |uniq\r\nfriend\r\nhello\r\nworld\r\n \r\n\r\n排序之后删除了重复行，同时在行首位置输出该行重复的次数\r\n\r\n#sort testfile | uniq -c\r\n1 friend\r\n3 hello\r\n2 world\r\n \r\n\r\n仅显示存在重复的行，并在行首显示该行重复的次数\r\n\r\n#sort testfile | uniq -dc\r\n3 hello\r\n2 world\r\n \r\n\r\n仅显示不重复的行\r\n\r\nsort testfile | uniq -u\r\nfriend  \r\n \r\n\r\ncut\r\ncut命令可以从一个文本文件或者文本流中提取文本列。\r\n\r\ncut语法\r\n\r\n[root@www ~]# cut -d&#039;分隔字符&#039; -f fields &lt;==用于有特定分隔字符\r\n[root@www ~]# cut -c 字符区间            &lt;==用于排列整齐的信息\r\n选项与参数：\r\n-d  ：后面接分隔字符。与 -f 一起使用；\r\n-f  ：依据 -d 的分隔字符将一段信息分割成为数段，用 -f 取出第几段的意思；\r\n-c  ：以字符 (characters) 的单位取出固定字符区间；\r\n \r\n\r\nPATH 变量如下\r\n\r\n[root@www ~]# echo $PATH\r\n/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/X11R6/bin:/usr/games\r\n# 1 | 2       | 3   | 4       | 5            | 6            | 7\r\n \r\n\r\n将 PATH 变量取出，我要找出第五个路径。\r\n\r\n#echo $PATH | cut -d &#039;:&#039; -f 5\r\n/usr/local/bin\r\n \r\n\r\n将 PATH 变量取出，我要找出第三和第五个路径。\r\n\r\n#echo $PATH | cut -d &#039;:&#039; -f 3,5\r\n/sbin:/usr/local/bin\r\n \r\n\r\n将 PATH 变量取出，我要找出第三到最后一个路径。\r\n\r\necho $PATH | cut -d &#039;:&#039; -f 3-\r\n/sbin:/usr/sbin:/usr/local/bin:/usr/X11R6/bin:/usr/games\r\n \r\n\r\n将 PATH 变量取出，我要找出第一到第三个路径。\r\n\r\n#echo $PATH | cut -d &#039;:&#039; -f 1-3\r\n/bin:/usr/bin:/sbin:\r\n \r\n \r\n\r\n将 PATH 变量取出，我要找出第一到第三，还有第五个路径。\r\n\r\necho $PATH | cut -d &#039;:&#039; -f 1-3,5\r\n/bin:/usr/bin:/sbin:/usr/local/bin\r\n \r\n\r\n实用例子:只显示/etc/passwd的用户和shell\r\n\r\n#cat /etc/passwd | cut -d &#039;:&#039; -f 1,7 \r\nroot:/bin/bash\r\ndaemon:/bin/sh\r\nbin:/bin/sh\r\n \r\n\r\n wc\r\n统计文件里面有多少单词，多少行，多少字符。\r\n\r\nwc语法\r\n\r\n[root@www ~]# wc [-lwm]\r\n选项与参数：\r\n-l  ：仅列出行；\r\n-w  ：仅列出多少字(英文单字)；\r\n-m  ：多少字符；\r\n \r\n\r\n默认使用wc统计/etc/passwd\r\n\r\n#wc /etc/passwd\r\n40   45 1719 /etc/passwd\r\n40是行数，45是单词数，1719是字节数\r\n\r\n \r\n\r\nwc的命令比较简单使用，每个参数使用如下：\r\n\r\n \r\n#wc -l /etc/passwd   #统计行数，在对记录数时，很常用\r\n40 /etc/passwd       #表示系统有40个账户\r\n\r\n#wc -w /etc/passwd  #统计单词出现次数\r\n45 /etc/passwd\r\n\r\n#wc -m /etc/passwd  #统计文件的字符数\r\n1719',9,0,0,1514648412,0,0,0),(213,1,'特殊文件: /dev/null和/dev/tty','','','    Linux系统提供了两个对Shell编程非常有用的特殊文件，/dev/null和/dev/tty。其中/dev/null将会丢掉所有写入它的数据，换句换说，当程序将数据写入到此文件时，会认为它已经成功完成写入数据的操作，但实际上什么事都没有做。如果你需要的是命令的退出状态，而非它的输出，此功能会非常有用，见如下Shell代码：\r\n    /&gt; vi test_dev_null.sh\r\n    \r\n    #!/bin/bash\r\n    if grep hello TestFile &gt; /dev/null\r\n    then\r\n        echo &quot;Found&quot;\r\n    else\r\n        echo &quot;NOT Found&quot;\r\n    fi\r\n    在vi中保存并退出后执行以下命令：\r\n    /&gt; chmod +x test_dev_null.sh  #使该文件成为可执行文件\r\n    /&gt; cat &gt; TestFile\r\n    hello my friend\r\n    CTRL + D                             #退出命令行文件编辑状态\r\n    /&gt; ./test_dev_null.sh\r\n    Found                                 #这里并没有输出grep命令的执行结果。\r\n    将以上Shell脚本做如下修改：\r\n    /&gt; vi test_dev_null.sh\r\n    \r\n    #!/bin/bash\r\n    if grep hello TestFile\r\n    then\r\n        echo &quot;Found&quot;\r\n    else\r\n        echo &quot;NOT Found&quot;\r\n    fi\r\n    在vi中保存退出后，再次执行该脚本：\r\n    /&gt; ./test_dev_null.sh\r\n    hello my friend                      #grep命令的执行结果被输出了。\r\n    Found\r\n    \r\n    下面我们再来看/dev/tty的用途。当程序打开此文件是，Linux会自动将它重定向到一个终端窗口，因此该文件对于读取人工输入时特别有用。见如下Shell代码：\r\n    /&gt; vi test_dev_tty.sh\r\n    \r\n    #!/bin/bash\r\n    printf &quot;Enter new password: &quot;    #提示输入\r\n    stty -echo                               #关闭自动打印输入字符的功能\r\n    read password &lt; /dev/tty         #读取密码\r\n    printf &quot;\\nEnter again: &quot;             #换行后提示再输入一次\r\n    read password2 &lt; /dev/tty       #再读取一次以确认\r\n    printf &quot;\\n&quot;                               #换行\r\n    stty echo                                #记着打开自动打印输入字符的功能\r\n    echo &quot;Password = &quot; $password #输出读入变量\r\n    echo &quot;Password2 = &quot; $password2\r\n    echo &quot;All Done&quot;\r\n\r\n    在vi中保存并退出后执行以下命令：\r\n    /&gt; chmod +x test_dev_tty.sh #使该文件成为可执行文件\r\n    /&gt; ./test_dev_tty\r\n    Enter new password:             #这里密码的输入被读入到脚本中的password变量\r\n    Enter again:                          #这里密码的输入被读入到脚本中的password2变量\r\n    Password = hello\r\n    Password2 = hello\r\n    All Done',9,0,0,1514648447,0,0,0),(214,1,'简单的命令跟踪','','','  Linux Shell提供了两种方式来跟踪Shell脚本中的命令，以帮助我们准确的定位程序中存在的问题。下面的代码为第一种方式，该方式会将Shell脚本中所有被执行的命令打印到终端，并在命令前加&quot;+&quot;：加号的后面还跟着一个空格。\r\n    /&gt; cat &gt; trace_all_command.sh\r\n    who | wc -l                          #这两条Shell命令将输出当前Linux服务器登录的用户数量\r\n    CTRL + D                            #退出命令行文件编辑状态\r\n    /&gt; chmod +x trace_all_command.sh\r\n    /&gt; sh -x ./trace_all_command.sh #Shell执行器的-x选项将打开脚本的执行跟踪功能。\r\n    + wc -l                               #被跟踪的两条Shell命令\r\n    + who\r\n    2                                       #实际输出结果。\r\n    Linux Shell提供的另一种方式可以只打印部分被执行的Shell命令，该方法在调试较为复杂的脚本时，显得尤为有用。\r\n    /&gt; cat &gt; trace_patial_command.sh\r\n    #! /bin/bash\r\n    set -x                                #从该命令之后打开跟踪功能\r\n    echo 1st echo                     #将被打印输出的Shell命令\r\n    set +x                               #该Shell命令也将被打印输出，然而在该命令被执行之后，所有的命令将不再打印输出\r\n    echo 2nd echo                    #该Shell命令将不再被打印输出。\r\n    CTRL + D                           #退出命令行文件编辑状态\r\n    /&gt; chmod +x trace_patial_command.sh\r\n    /&gt; ./trace_patial_command.sh\r\n    + echo 1st echo\r\n    1st echo\r\n    + set +x\r\n    2nd echo',9,0,0,1514648475,0,0,0),(215,1,'正则表达式基本语法描述','','','Linux Shell环境下提供了两种正则表达式规则，一个是基本正则表达式(BRE)，另一个是扩展正则表达式(ERE)。\r\n    下面是这两种表达式的语法列表，需要注意的是，如果没有明确指出的Meta字符，其将可同时用于BRE和ERE，否则将尽适用于指定的模式。\r\n\r\n正则元字符	模式含义	用例\r\n\\	通常用于关闭其后续字符的特殊意义，恢复其原意。	\\(...\\)，这里的括号仅仅表示括号。\r\n.	匹配任何单个字符。	a.b，将匹配abb、acb等\r\n*	匹配它之前的0-n个的单个字符。	a*b，将匹配ab、aab、aaab等。\r\n^	匹配紧接着的正则表达式，在行的起始处。	^ab，将匹配abc、abd等，但是不匹配cab。\r\n$	匹配紧接着的正则表达式，在行的结尾处。	ab$，将匹配ab、cab等，但是不匹配abc。\r\n[...]	方括号表达式，匹配其内部任何字符。其中-表示连续字符的范围，^符号置于方括号里第一个字符则有反向的含义，即匹配不在列表内(方括号)的任何字符。如果想让]和-表示其原意，需要将其放置在方括号的首字符位置，如[]ab]或[-ab]，如这两个字符同时存在，则将]放置在首字符位置，-放置在最尾部，如[]ab-]。	[a-bA-Z0-9!]表示所有的大小写字母，数字和感叹号。[^abc]表示a、b、c之外的所有字符。[Tt]om，可以匹配Tom和tom。\r\n\\{n,m\\}	区间表达式，匹配在它前面的单个字符重复出现的次数区间，\\{n\\}表示重复n次；\\{n,\\}表示至少重复n次；\\{n,m\\}表示重复n到m次。	ab\\{2\\}表示abb；ab\\{2,\\}表示abb、abbb等。ab\\{2,4\\}表示abb、abbb和abbbb。\r\n\\(...\\)	将圆括号之间的模式存储在特殊“保留空间”。最多可以将9个独立的子模式存储在单个模式中。匹配于子模式的文本，可以通过转义序列\\1到\\9，被重复使用在相同模式里。	\\(ab\\).*\\1表示ab组合出现两次，两次之间可存在任何数目的任何字符，如abcdab、abab等。\r\n{n,m}(ERE)	其功能等同于上面的\\{n,m\\}，只是不再写\\转义符了。	ab+匹配ab、abbb等，但是不匹配a。\r\n+(ERE)	和前面的星号相比，+匹配的是前面正则表达式的1-n个实例。	 \r\n?(ERE)	匹配前面正则表达式的0个或1个。	ab?仅匹配a或ab。\r\n|(ERE)	匹配于|符号前后的正则表达式。	(ab|cd)匹配ab或cd。\r\n[:alpha:]	匹配字母字符。	[[:alpha:]!]ab$匹配cab、dab和!ab。\r\n[:alnum:]	匹配字母和数字字符。	[[:alnum:]]ab$匹配1ab、aab。\r\n[:blank:]	匹配空格(space)和Tab字符。	[[:alnum:]]ab$匹配1ab、aab。\r\n[:cntrl:]	匹配控制字符。	 \r\n[:digit:]	匹配数字字符。	 \r\n[:graph:]	匹配非空格字符。	 \r\n[:lower:]	匹配小写字母字符。	 \r\n[:upper:]	匹配大写字母字符。	 \r\n[:punct:]	匹配标点字符。	 \r\n[:space:]	匹配空白(whitespace)字符。	 \r\n[:xdigit:]	匹配十六进制数字。	 \r\n\\w	匹配任何字母和数字组成的字符，等同于[[:alnum:]_]	 \r\n\\W	匹配任何非字母和数字组成的字符，等同于[^[:alnum:]_]	 \r\n\\&lt;\\&gt;	匹配单词的起始和结尾。	\\&lt;read匹配readme，me\\&gt;匹配readme。',9,0,0,1514648506,0,0,0),(216,1,'使用cut命令选定字段','','','    cut命令是用来剪下文本文件里的数据，文本文件可以是字段类型或是字符类型。下面给出应用实例：\r\n    /&gt; cat /etc/passwd\r\n    root:x:0:0:root:/root:/bin/bash\r\n    bin:x:1:1:bin:/bin:/sbin/nologin\r\n    daemon:x:2:2:daemon:/sbin:/sbin/nologin\r\n    adm:x:3:4:adm:/var/adm:/sbin/nologin\r\n    ... ...\r\n    /&gt; cut -d : -f 1,5 /etc/passwd     #-d后面的冒号表示字段之间的分隔符，-f表示取分割后的哪些字段\r\n    root:root                                 #这里取出的是第一个和第五个字段。\r\n    bin:bin\r\n    daemon:daemon\r\n    adm:adm\r\n    ... ...\r\n    /&gt; cut -d: -f 3- /etc/passwd       #从第三个字段开始显示，直到最后一个字段。\r\n    0:0:root:/root:/bin/bash\r\n    1:1:bin:/bin:/sbin/nologin\r\n    2:2:daemon:/sbin:/sbin/nologin\r\n    3:4:adm:/var/adm:/sbin/nologin\r\n    4:7:lp:/var/spool/lpd:/sbin/nologin\r\n    ... ...    \r\n    这里需要进一步说明的是，使用cut命令还可以剪切以字符数量为标量的部分字符，该功能通过-c选项实现，其不能与-d选项共存。\r\n    /&gt; cut -c 1-4 /etc/passwd          #取每行的前1-4个字符。\r\n    /&gt; cut -c-4 /etc/passwd            #取每行的前4个字符。 \r\n    root\r\n    bin:\r\n    daem\r\n    adm:\r\n    ... ...\r\n    /&gt; cut -c4- /etc/passwd            #取每行的第4个到最后字符。\r\n    t:x:0:0:root:/root:/bin/bash\r\n    :x:1:1:bin:/bin:/sbin/nologin\r\n    mon:x:2:2:daemon:/sbin:/sbin/nologin\r\n    :x:3:4:adm:/var/adm:/sbin/nologin\r\n    ... ...\r\n    /&gt; cut -c1,4 /etc/passwd           #取每行的第一个和第四个字符。\r\n    rt\r\n    b:\r\n    dm\r\n    a:\r\n    ... ...\r\n    /&gt; cut -c1-4,5 /etc/passwd        #取每行的1-4和第5个字符。\r\n    root:\r\n    bin:x\r\n    daemo\r\n    adm:x',9,0,0,1514648533,0,0,0),(217,1,'计算行数、字数以及字符数','','','    Linux提供了一个简单的工具wc用于完成该功能，见如下用例：\r\n    /&gt; echo This is a test of the emergency broadcast system | wc\r\n    1    9    49                              #1行，9个单词，49个字符\r\n    /&gt; echo Testing one two three | wc -c\r\n    22                                         #22个字符\r\n    /&gt; echo Testing one two three | wc -l\r\n    1                                           #1行\r\n    /&gt; echo Testing one two three | wc -w\r\n    4                                           #4个单词\r\n    /&gt; wc /etc/passwd /etc/group    #计算两个文件里的数据。\r\n    39   71  1933  /etc/passwd\r\n    62   62  906    /etc/group\r\n    101 133 2839  总用量',9,0,0,1514648551,0,0,0),(218,1,'提取开头或结尾数行','','','    有时，你会需要从文本文件里把几行字，多半是靠近开头或结尾的几行提取出来。如查看工作日志等操作。Linux Shell提供head和tail两个命令来完成此项工作。见如下用例：\r\n    /&gt; head -n 5 /etc/passwd           #显示输入文件的前五行。\r\n    root:x:0:0:root:/root:/bin/bash\r\n    bin:x:1:1:bin:/bin:/sbin/nologin\r\n    daemon:x:2:2:daemon:/sbin:/sbin/nologin\r\n    adm:x:3:4:adm:/var/adm:/sbin/nologin\r\n    lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\r\n\r\n    /&gt; tail -n 5 /etc/passwd             #显示输入文件的最后五行。\r\n    sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\r\n    mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/bash\r\n    pulse:x:496:494:PulseAudio System Daemon:/var/run/pulse:/sbin/nologin\r\n    gdm:x:42:42::/var/lib/gdm:/sbin/nologin\r\n    stephen:x:500:500:stephen:/home/stephen:/bin/bash\r\n    如果使用者想查看不间断增长的日志(如服务程序输出的)，可以使用tail的-f选项，这样可以让tail命令不会自动退出，必须通过CTRL+C命令强制退出，因此该选项不适合用于Shell脚本中，见如下用例：\r\n    /&gt; tail -f -n 5 my_server_log',9,0,0,1514648569,0,0,0),(219,1,'日常操作命令 ','','','**查看当前所在的工作目录\r\npwd\r\n\r\n**查看当前系统的时间 \r\ndate\r\n\r\n**查看有谁在线（哪些人登陆到了服务器）\r\nwho  查看当前在线\r\nlast 查看最近的登陆历史记录',9,0,0,1514648755,0,0,0),(220,1,'文件系统操作','','','ls /    查看根目录下的子节点（文件夹和文件）信息\r\nls -al  -a是显示隐藏文件   -l是以更详细的列表形式显示\r\n\r\n**切换目录\r\ncd  /home\r\n\r\n**创建文件夹\r\nmkdir aaa     这是相对路径的写法 \r\nmkdir -p aaa/bbb/ccc\r\nmkdir  /data    这是绝对路径的写法 \r\n\r\n**删除文件夹\r\nrmdir   可以删除空目录\r\nrm -r aaa   可以把aaa整个文件夹及其中的所有子节点全部删除\r\nrm -rf aaa   强制删除aaa\r\n\r\n**修改文件夹名称\r\nmv aaa angelababy\r\n\r\n**创建文件\r\ntouch  somefile.1   创建一个空文件\r\necho &quot;i miss you,my baby&quot; &gt; somefile.2  利用重定向“&gt;”的功能，将一条指令的输出结果写入到一个文件中，会覆盖原文件内容\r\necho &quot;huangxiaoming ,gun dan&quot; &gt;&gt; somefile.2     将一条指令的输出结果追加到一个文件中，不会覆盖原文件内容\r\n',9,0,0,1514648785,0,0,0),(221,1,'文件权限的操作','','','****linux文件权限的描述格式解读\r\ndrwxr-xr-x      （也可以用二进制表示  111 101 101  --&gt;  755）\r\n\r\nd：标识节点类型（d：文件夹   -：文件  l:链接）\r\nr：可读   w：可写    x：可执行 \r\n第一组rwx：  表示这个文件的拥有者对它的权限：可读可写可执行\r\n第二组r-x：  表示这个文件的所属组对它的权限：可读，不可写，可执行\r\n第三组r-x：  表示这个文件的其他用户（相对于上面两类用户）对它的权限：可读，不可写，可执行\r\n\r\n\r\n****修改文件权限\r\nchmod g-rw haha.dat    表示将haha.dat对所属组的rw权限取消\r\nchmod o-rw haha.dat 	表示将haha.dat对其他人的rw权限取消\r\nchmod u+x haha.dat      表示将haha.dat对所属用户的权限增加x\r\n\r\n也可以用数字的方式来修改权限\r\nchmod 664 haha.dat   \r\n就会修改成   rw-rw-r--\r\n\r\n如果要将一个文件夹的所有内容权限统一修改，则可以-R参数\r\nchmod -R 770 aaa/\r\nchown angela:angela aaa/    &lt;只有root能执行&gt;',9,0,0,1514649921,0,0,0),(222,1,'基本的用户管理','','','*****添加用户\r\nuseradd  angela\r\n要修改密码才能登陆 \r\npasswd angela  按提示输入密码即可\r\n\r\n\r\n**为用户配置sudo权限\r\n用root编辑 vi /etc/sudoers\r\n在文件的如下位置，为hadoop添加一行即可\r\nroot    ALL=(ALL)       ALL     \r\nhadoop  ALL=(ALL)       ALL\r\n\r\n然后，hadoop用户就可以用sudo来执行系统级别的指令\r\n[hadoop@shizhan ~]$ sudo useradd huangxiaoming\r\n\r\nhadoop 用户\r\nsudo chown -R hadoop:hadoop /opt #hadoop对opt目录及子目录都拥有权限',9,0,0,1514650033,0,0,0),(223,1,'系统管理操作','','','*****查看主机名\r\nhostname\r\n****修改主机名(重启后无效)\r\nhostname hadoop\r\n\r\n*****修改主机名(重启后永久生效)\r\nvi /ect/sysconfig/network\r\n****修改IP(重启后无效)\r\nifconfig eth0 192.168.12.22\r\n\r\n****修改IP(重启后永久生效)\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\n\r\n\r\nmount ****  挂载外部存储设备到文件系统中\r\nmkdir   /mnt/cdrom      创建一个目录，用来挂载\r\nmount -t iso9660 -o ro /dev/cdrom /mnt/cdrom/     将设备/dev/cdrom挂载到 挂载点 ：  /mnt/cdrom中\r\n\r\n*****umount\r\numount /mnt/cdrom\r\n\r\n\r\n*****统计文件或文件夹的大小\r\ndu -sh  /mnt/cdrom/Packages\r\ndf -h    查看磁盘的空间\r\n****关机\r\nhalt\r\n****重启\r\nreboot\r\n\r\n\r\n******配置主机之间的免密ssh登陆\r\n假如 A  要登陆  B\r\n在A上操作：\r\n%%首先生成密钥对\r\nssh-keygen   (提示时，直接回车即可)\r\n%%再将A自己的公钥拷贝并追加到B的授权列表文件authorized_keys中\r\nssh-copy-id   B\r\n\r\n\r\n\r\n******后台服务管理\r\nservice network status   查看指定服务的状态\r\nservice network stop     停止指定服务\r\nservice network start    启动指定服务\r\nservice network restart  重启指定服务\r\nservice --status-all  查看系统中所有的后台服务\r\n\r\n设置后台服务的自启配置\r\nchkconfig   查看所有服务器自启配置\r\nchkconfig iptables off   关掉指定服务的自动启动\r\nchkconfig iptables on   开启指定服务的自动启动\r\n\r\n\r\n*****系统启动级别管理\r\nvi  /etc/inittab\r\n\r\n# Default runlevel. The runlevels used are:\r\n#   0 - halt (Do NOT set initdefault to this)\r\n#   1 - Single user mode\r\n#   2 - Multiuser, without NFS (The same as 3, if you do not have networking)\r\n#   3 - Full multiuser mode\r\n#   4 - unused\r\n#   5 - X11\r\n#   6 - reboot (Do NOT set initdefault to this)\r\n#\r\nid:3:initdefault:',9,0,0,1514650071,0,0,0),(224,1,'iptables防火墙','','','iptables也叫netfilter是Linux下自带的一款免费且优秀的基于包过滤的防火墙工具，它的功能十分强大，使用非常灵活，可以对流入、流出、流经服务器的数据包进行精细的控制。iptables是Linux2.4及2.6内核中集成的模块。\r\n\r\niptables服务相关命令\r\n1. 查看iptables状态\r\nservice iptables status\r\n    \r\n2. 开启/关闭iptables\r\nservice iptables start\r\nservice iptables stop\r\n[root@peng1 bin]# service iptables stop\r\niptables：清除防火墙规则：                                 [确定]\r\niptables：将链设置为政策 ACCEPT：filter                    [确定]\r\niptables：正在卸载模块： \r\n    \r\n3. 查看iptables是否开机启动\r\nchkconfig iptables --list\r\n    \r\n4. 设置iptables开机启动/不启动\r\nchkconfig iptables on\r\nchkconfig iptables off\r\n\r\nvi /etc/selinux/config \r\nSELINUX=disabled\r\n\r\n在iptables中有四张表，分别是filter、nat、mangle和raw每一个表中都包含了各自不同的链，最常用的是filter表\r\n\r\n    • filter表：\r\nfilter是iptables默认使用的表，负责对流入、流出本机的数据包进行过滤，该表中定义了3个链：\r\n	INPOUT 	负责过滤所有目标地址是本机地址的数据包，就是过滤进入主机的数据包。\r\n	FORWARD	负责转发流经本机但不进入本机的数据包，起到转发的作用。\r\n	OUTPUT	负责处理所有源地址是本机地址的数据包，就是处理从主机发出去的数据包。',9,0,0,1514650533,0,0,0),(225,1,'redis','','','    1. 什么是Redis\r\nRedis是目前一个非常优秀的key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set有序集合)和hash（哈希类型）。\r\n    2. 为什么要安装Redis3集群\r\nRedis3.x支持集群模式，更加可靠！\r\n    3. 安装Redis3集群（6台Linux）\r\n参考文章：http://blog.csdn.net/myrainblues/article/details/25881535\r\n\r\n    1. 下载redis3的稳定版本，下载地址http://download.redis.io/releases/redis-3.0.7.tar.gz\r\n    2. 上传redis-3.0.7.tar.gz到服务器\r\n    3. 解压redis源码包\r\ntar -zxvf redis-3.0.7.tar.gz -C /usr/local/src/\r\n    4. 进入到源码包中，编译并安装redis\r\ncd /usr/local/src/redis-3.0.7/\r\nmake &amp;&amp; make install\r\n    5. 报错，缺少依赖的包\r\n\r\n    6. 配置本地YUM源并安装redis依赖的rpm包\r\nyum -y install gcc\r\n    7. 编译并安装\r\nmake &amp;&amp; make install\r\n    8. 报错，原因是没有安装jemalloc内存分配器，可以安装jemalloc或直接输入\r\nmake MALLOC=libc &amp;&amp; make install\r\n\r\n    9. 重新编译安装\r\nmake MALLOC=libc &amp;&amp; make install\r\n    10. 用同样的方式在其他的机器上编译安装redis\r\n    11. 在所有机器的/usr/local/下创建一个redis目录，然后拷贝redis自带的配置文件redis.conf到/usr/local/redis\r\nmkdir /usr/local/redis\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis\r\n    12. 修改所有机器的配置文件redis.conf\r\ndaemonize yes  #redis后台运行\r\ncluster-enabled yes  #开启集群把注释去掉\r\nappendonly yes  #开启aof日志，它会每次写操作都记录一条日志\r\nsed -i &#039;s/daemonize no/daemonize yes/&#039; /usr/local/redis/redis.conf\r\nsed -i &#039;s/# cluster-enabled yes/cluster-enabled yes/&#039; /usr/local/redis/redis.conf\r\nsed -i &#039;s/appendonly no/appendonly yes/&#039; /usr/local/redis/redis.conf\r\nsed -i &#039;s/# cluster-node-timeout 15000/cluster-node-timeout 5000/&#039; /usr/local/redis/redis.conf\r\n    13. 启动所有的redis节点\r\ncd /usr/local/redis\r\nredis-server redis.conf\r\n    14. 查看redis进程状态\r\nps -ef | grep redis\r\n\r\n    15. 配置集群：安装ruby和ruby gem工具（redis3集群配置需要ruby的gem工具，类似yum）\r\nyum -y install ruby rubygems\r\n(centos6.5的光盘可能缺失rubygems包，需要这样处理：\r\n先安装yum -y install ruby，\r\n再安装rubygems的依赖：\r\nyum install -y ruby-irb\r\nyum install -y ruby-rdoc\r\n再用rpm命令安装rubygems包\r\nrpm -ivh /root/rubygems-1.3.7-5.el6.noarch.rpm\r\n)\r\n    16. 使用gem下载redis集群的配置脚本\r\ngem install redis\r\n\r\n    17. gem需要上网才能下载，由于安装redis的服务器可能无法访问外网，可以找一台可以上网的服务器执行下面的命令\r\nyum -y install ruby rubygems\r\ngem install redis\r\n将下载好的redis gem（/usr/lib/ruby/gems/1.8/cache/redis-3.2.2.gem）拷贝到其他服务器\r\ncd /usr/lib/ruby/gems/1.8/cache\r\nfor n in {2..6}; do scp redis-3.2.2.gem 192.168.0.3$n:$PWD; done\r\n    18. 使用gem本地模式安装redis-3.2.2.gem\r\ngem install --local /usr/lib/ruby/gems/1.8/cache/redis-3.2.2.gem\r\n    19. 使用脚本配置redis集群（在一台机器上执行即可，想要把哪些节点配置成Master节点就放在后面）\r\ncd /usr/local/src/redis-3.0.7/src/\r\nservice iptables stop\r\n./redis-trib.rb create --replicas 1 192.168.0.34:6379 192.168.0.35:6379 192.168.0.36:6379 192.168.0.31:6379 192.168.0.32:6379 192.168.0.33:6379\r\n    20. 测试\r\nredis-cli -c -p 6379\r\n\r\n\r\n\r\n    4. Redis3伪分布式安装（1台Linux）\r\n    1. 下载redis3的稳定版本，下载地址http://download.redis.io/releases/redis-3.0.7.tar.gz\r\n    2. 上传redis-3.0.7.tar.gz到服务器\r\n3.解压redis源码包\r\ntar -zxvf redis-3.0.7.tar.gz -C /usr/local/src/\r\n4.进入到源码包中，编译并安装redis\r\ncd /usr/local/src/redis-3.0.7/\r\nmake &amp;&amp; make install\r\n5.在/usr/local/下创建一个redis目录，然后分别在/usr/local/redis目录创建6个文件夹7000,7001,7002,7003,7004,7005然后拷贝redis自带的配置文件redis.conf到这六个目录中\r\nmkdir /usr/local/redis\r\nmkdir /usr/local/redis/{7000,7001,7002,7003,7004,7005}\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7000\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7001\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7002\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7003\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7004\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7005\r\n6.分别修改这六个目录中的配置文件\r\nport 7000 #端口要与其所在的文件名一致\r\npidfile /var/run/redis-7000.pid  #pid要与其所在的文件名一致\r\ndaemonize yes\r\ncluster-enabled yes\r\nappendonly yes\r\n    7. 分别进入到这六个目录启动redis进程\r\ncd /usr/local/redis/7000\r\nredis-server redis.conf\r\ncd /usr/local/redis/7001\r\nredis-server redis.conf\r\ncd /usr/local/redis/7002\r\nredis-server redis.conf\r\ncd /usr/local/redis/7003\r\nredis-server redis.conf\r\ncd /usr/local/redis/7004\r\nredis-server redis.conf\r\ncd /usr/local/redis/7005\r\nredis-server redis.conf',9,0,0,1514650699,0,0,0),(226,1,'nginx反向代理','','','反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。',10,0,0,1514651962,0,0,0),(227,1,'nginx负载均衡','','','负载均衡，英文名称为Load Balance，是指建立在现有网络结构之上，并提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。其原理就是数据流量分摊到多个服务器上执行，减轻每台服务器的压力，多台服务器共同完成工作任务，从而提高了数据的吞吐量。\r\n\r\n\r\n在http这个节下面配置一个叫upstream的，后面的名字可以随意取，但是要和location下的proxy_pass http://后的保持一致。\r\n\r\n    是在http里面的, 已有http, 不是在server里,在server外面\r\n    upstream tomcats { \r\n        server shizhan02:8080 weight=1;#weight表示多少个\r\n        server shizhan03:8080 weight=1;\r\n        server shizhan04:8080 weight=1;\r\n}\r\n#卸载server里\r\nlocation ~ .*\\.(jsp|do|action) {\r\n    proxy_pass http://tomcats;        #tomcats是后面的tomcat服务器组的逻辑组号\r\n}\r\n}',10,0,0,1514652038,0,0,0),(228,1,'nginx的安装','','','        3.1. 下载nginx\r\n官网：http://nginx.org/\r\n        3.2. 上传并解压nginx\r\ntar -zxvf nginx-1.8.1.tar.gz -C /usr/local/src\r\n        3.3. 编译nginx\r\n#进入到nginx源码目录\r\ncd /usr/local/src/nginx-1.8.1\r\n\r\n#检查安装环境,并指定将来要安装的路径\r\n./configure --prefix=/usr/local/nginx\r\n\r\n#缺包报错 ./configure: error: C compiler cc is not found\r\n\r\n#使用YUM安装缺少的包\r\nyum -y install gcc pcre-devel openssl openssl-devel\r\n\r\n#编译安装\r\nmake &amp;&amp; make install\r\n\r\n安装完后测试是否正常：\r\n/usr/loca/nginx/sbin/nginx\r\n查看端口是否有ngnix进程监听\r\nnetstat -ntlp | grep 80',10,0,0,1514652211,0,0,0),(229,1,'配置nginx','','','    1. 修改nginx配置文件\r\nserver {\r\n    listen       80;\r\n    server_name  nginx-01.itcast.cn;    #nginx所在服务器的主机名\r\n#反向代理的配置\r\nlocation / {             #拦截所有请求\r\n    root html;\r\n        proxy_pass http://192.168.0.21:8080;   #这里是代理走向的目标服务器：tomcat\r\n    }\r\n}\r\n    2. 启动tomcat-01上的tomcat\r\n\r\n3.启动nginx-01上的nginx\r\n./nginx\r\n\r\n重启:\r\nkill -HUP `cat /usr/local/nginx/logs/nginx.pid `\r\n参考网址:http://www.cnblogs.com/jianxie/p/3990377.html',10,0,0,1514652267,0,0,0),(230,1,'        4.2. 动静分离','','','动态资源 index.jsp\r\nlocation ~ .*\\.(jsp|do|action)$ {\r\n    proxy_pass http://tomcat-01.itcast.cn:8080;\r\n}\r\n\r\n#静态资源\r\nlocation ~ .*\\.(html|js|css|gif|jpg|jpeg|png)$ {\r\n    expires 3d;\r\n}',10,0,0,1514652307,0,0,0),(231,1,'利用keepalived实现高可靠（HA）','','','        5.1. 高可靠概念\r\nHA(High Available), 高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。\r\n\r\n        5.2. 高可靠软件keepalived\r\nkeepalive是一款可以实现高可靠的软件，通常部署在2台服务器上，分为一主一备。Keepalived可以对本机上的进程进行检测，一旦Master检测出某个进程出现问题，将自己切换成Backup状态，然后通知另外一个节点切换成Master状态。\r\n        5.3. keepalived安装\r\n下载keepalived官网:http://keepalived.org\r\n\r\n将keepalived解压到/usr/local/src目录下\r\ntar -zxvf  keepalived-1.2.19.tar.gz -C /usr/local/src\r\n\r\n进入到/usr/local/src/keepalived-1.2.19目录\r\ncd /usr/local/src/keepalived-1.2.19\r\n\r\n开始configure\r\n./configure --prefix=/usr/local/keepalived\r\n\r\n#编译并安装\r\nmake &amp;&amp; make install',10,0,0,1514652434,0,0,0),(232,1,'将keepalived添加到系统服务中','','','拷贝执行文件\r\ncp /usr/local/keepalived/sbin/keepalived /usr/sbin/\r\n将init.d文件拷贝到etc下,加入开机启动项\r\ncp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived\r\n将keepalived文件拷贝到etc下\r\ncp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ \r\n创建keepalived文件夹\r\nmkdir -p /etc/keepalived\r\n将keepalived配置文件拷贝到etc下\r\ncp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf\r\n添加可执行权限\r\n\r\nchmod +x /etc/init.d/keepalived\r\n\r\n##以上所有命令一次性执行：\r\ncp /usr/local/keepalived/sbin/keepalived /usr/sbin/\r\ncp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived\r\ncp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ \r\nmkdir -p /etc/keepalived\r\ncp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf\r\nchmod +x /etc/init.d/keepalived\r\nchkconfig --add keepalived	\r\nchkconfig keepalived on\r\n\r\n添加keepalived到开机启动\r\nchkconfig --add keepalived	\r\nchkconfig keepalived on',10,0,0,1514652565,0,0,0),(233,1,'配置keepalived虚拟IP','','','修改配置文件： /etc/keepalived/keepalived.conf\r\n#MASTER节点\r\nglobal_defs {\r\n}\r\nvrrp_instance VI_1 {\r\n    state MASTER   #指定A节点为主节点 备用节点上设置为BACKUP即可\r\n    interface eth0    #绑定虚拟IP的网络接口\r\n    virtual_router_id 51   #VRRP组名，两个节点的设置必须一样，以指明各个节点属于同一VRRP组\r\n    priority 100   #主节点的优先级（1-254之间），备用节点必须比主节点优先级低\r\n    advert_int 1  #组播信息发送间隔，两个节点设置必须一样\r\n    authentication {    #设置验证信息，两个节点必须一致\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {    #指定虚拟IP, 两个节点设置必须一样\r\n        192.168.33.60/24    #如果两个nginx的ip分别是192.168.33.61,,...62，则此处的虚拟ip跟它俩同一个网段即可\r\n    }\r\n}\r\n\r\n#BACKUP节点\r\nglobal_defs {\r\n}\r\nvrrp_instance VI_1 {\r\n    state BACKUP\r\n    interface eth0\r\n    virtual_router_id 51\r\n    priority 99\r\n    advert_int 1\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {\r\n        192.168.33.60/24\r\n    }\r\n}\r\n\r\n#分别启动两台机器上的keepalived\r\nservice keepalived start\r\n测试：\r\n杀掉master上的keepalived进程，你会发现，在slave机器上的eth0网卡多了一个ip地址\r\n查看ip地址的命令：  ip addr ',10,0,0,1514652607,0,0,0),(234,1,'配置keepalived心跳检查','','','原理：\r\nKeepalived并不跟nginx耦合，它俩完全不是一家人\r\n但是keepalived提供一个机制：让用户自定义一个shell脚本去检测用户自己的程序，返回状态给keepalived就可以了\r\n\r\n\r\n#MASTER节点\r\nglobal_defs {\r\n}\r\n\r\nvrrp_script chk_health {\r\n    script &quot;[[ `ps -ef | grep nginx | grep -v grep | wc -l` -ge 2 ]] &amp;&amp; exit 0 || exit 1&quot;\r\n    interval 1    #每隔1秒执行上述的脚本，去检查用户的程序ngnix\r\n    weight -2\r\n}\r\n\r\nvrrp_instance VI_1 {\r\n    state MASTER\r\n    interface eth0\r\n    virtual_router_id 1\r\n    priority 100\r\n    advert_int 2\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n\r\n    track_script {\r\n        chk_health\r\n    }\r\n\r\n    virtual_ipaddress {\r\n        10.0.0.10/24\r\n    }\r\n\r\n    notify_master &quot;/usr/local/keepalived/sbin/notify.sh master&quot;\r\n    notify_backup &quot;/usr/local/keepalived/sbin/notify.sh backup&quot;\r\n    notify_fault &quot;/usr/local/keepalived/sbin/notify.sh fault&quot;\r\n}\r\n\r\n#添加切换通知脚本\r\nvi /usr/local/keepalived/sbin/notify.sh\r\n#!/bin/bash\r\n\r\ncase &quot;$1&quot; in\r\n    master)\r\n        /usr/local/nginx/sbin/nginx\r\n        exit 0\r\n    ;;\r\nbackup)\r\n        /usr/local/nginx/sbin/nginx -s stop\r\n        /usr/local/nginx/sbin/nginx\r\n        exit 0\r\n    ;;\r\n    fault)\r\n        /usr/local/nginx/sbin/nginx -s stop\r\n        exit 0\r\n    ;;\r\n    *)\r\n        echo &#039;Usage: notify.sh {master|backup|fault}&#039;\r\n        exit 1\r\n    ;;\r\nesac\r\n\r\n#添加执行权限\r\nchmod +x /usr/local/keepalived/sbin/notify.sh\r\nglobal_defs {\r\n}\r\n\r\nvrrp_script chk_health {\r\n    script &quot;[[ `ps -ef | grep nginx | grep -v grep | wc -l` -ge 2 ]] &amp;&amp; exit 0 || exit 1&quot;\r\n    interval 1\r\n    weight -2\r\n}\r\n\r\nvrrp_instance VI_1 {\r\n    state BACKUP\r\n    interface eth0\r\n    virtual_router_id 1\r\n    priority 99\r\n    advert_int 1\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n\r\n    track_script {\r\n        chk_health\r\n    }\r\n\r\n    virtual_ipaddress {\r\n        10.0.0.10/24\r\n    }\r\n	\r\n    notify_master &quot;/usr/local/keepalived/sbin/notify.sh master&quot;\r\n    notify_backup &quot;/usr/local/keepalived/sbin/notify.sh backup&quot;\r\n    notify_fault &quot;/usr/local/keepalived/sbin/notify.sh fault&quot;\r\n}\r\n\r\n#在第二台机器上添加notify.sh脚本\r\n#分别在两台机器上启动keepalived\r\nservice keepalived start \r\nchkconfig keepalived on',10,0,0,1514652641,0,0,0),(235,1,'crond简介','','','前一天学习了 at 命令是针对仅运行一次的任务，循环运行的例行性计划任务，linux系统则是由 cron (crond) 这个系统服务来控制的。Linux 系统上面原本就有非常多的计划性工作，因此这个系统服务是默认启动的。另外, 由于使用者自己也可以设置计划任务，所以， Linux 系统也提供了使用者控制计划任务的命令 :crontab 命令。\r\n一、crond简介\r\ncrond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。\r\nLinux下的任务调度分为两类，系统任务调度和用户任务调度。\r\n系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。\r\n/etc/crontab文件包括下面几行：\r\n[root@localhost ~]# cat /etc/crontab \r\nSHELL=/bin/bash\r\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\r\nMAILTO=&quot;&quot;HOME=/\r\n# run-parts\r\n51 * * * * root run-parts /etc/cron.hourly\r\n24 7 * * * root run-parts /etc/cron.daily\r\n22 4 * * 0 root run-parts /etc/cron.weekly\r\n42 4 1 * * root run-parts /etc/cron.monthly\r\n[root@localhost ~]#\r\n前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。第六至九行表示的含义将在下个小节详细讲述。这里不在多说。\r\n用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab 文件都被保存在 /var/spool/cron目录中。其文件名与用户名一致。\r\n使用者权限文件：\r\n文件：\r\n/etc/cron.deny\r\n说明：\r\n该文件中所列用户不允许使用crontab命令\r\n文件：\r\n/etc/cron.allow\r\n说明：\r\n该文件中所列用户允许使用crontab命令\r\n文件：\r\n/var/spool/cron/\r\n说明：\r\n所有用户crontab文件存放的目录,以用户名命名\r\ncrontab文件的含义：\r\n用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下：\r\nminute   hour   day   month   week   command\r\n其中：\r\nminute： 表示分钟，可以是从0到59之间的任何整数。\r\nhour：表示小时，可以是从0到23之间的任何整数。\r\nday：表示日期，可以是从1到31之间的任何整数。\r\nmonth：表示月份，可以是从1到12之间的任何整数。\r\nweek：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。\r\ncommand：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。\r\n \r\n在以上各个字段中，还可以使用以下特殊字符：\r\n星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。\r\n逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\r\n中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\r\n正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。',9,0,0,1514653171,0,0,0),(236,1,'crond服务','','','安装crontab：\r\nyum install crontabs\r\n服务操作说明：\r\n/sbin/service crond start //启动服务\r\n/sbin/service crond stop //关闭服务\r\n/sbin/service crond restart //重启服务\r\n/sbin/service crond reload //重新载入配置\r\n查看crontab服务状态：\r\nservice crond status\r\n手动启动crontab服务：\r\nservice crond start\r\n查看crontab服务是否已设置为开机启动，执行命令：\r\nntsysv\r\n加入开机自动启动：\r\nchkconfig –level 35 crond on',9,0,0,1514653198,0,0,0),(237,1,'crontab命令详解','','','1．命令格式：\r\ncrontab [-u user] file\r\ncrontab [-u user] [ -e | -l | -r ]\r\n2．命令功能：\r\n通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常设合周期性的日志分析或数据备份等工作。\r\n3．命令参数：\r\n-u user：用来设定某个用户的crontab服务，例如，“-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。\r\nfile：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。\r\n-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。\r\n-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。\r\n-r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。\r\n-i：在删除用户的crontab文件时给确认提示。\r\n\r\n */1 * * * * sh /export/servers/shell/uploadFile2Hdfs.v2.sh\r\n */1 * * * * sh  source /etc/profile;sh /export/servers/shell/uploadFile2Hdfs.v1.sh\r\n \r\n# 编辑命令是crontab -e\r\n# 查看命令是crontab -l',9,0,0,1514653221,0,0,0),(238,1,'常用方法','','','1). 创建一个新的crontab文件\r\n在考虑向cron进程提交一个crontab文件之前，首先要做的一件事情就是设置环境变量EDITOR。cron进程根据它来确定使用哪个编辑器编辑crontab文件。9 9 %的UNIX和LINUX用户都使用vi，如果你也是这样，那么你就编辑$ HOME目录下的. profile文件，在其中加入这样一行：\r\nEDITOR=vi; export EDITOR\r\n然后保存并退出。不妨创建一个名为&lt;user&gt; cron的文件，其中&lt;user&gt;是用户名，例如， davecron。在该文件中加入如下的内容。\r\n      # (put your own initials here)echo the date to the console every\r\n      # 15minutes between 6pm and 6am\r\n      0,15,30,45 18-06 * * * /bin/echo &#039;date&#039; &gt; /dev/console\r\n    保存并退出。确信前面5个域用空格分隔。\r\n在上面的例子中，系统将每隔1 5分钟向控制台输出一次当前时间。如果系统崩溃或挂起，从最后所显示的时间就可以一眼看出系统是什么时间停止工作的。在有些系统中，用tty1来表示控制台，可以根据实际情况对上面的例子进行相应的修改。为了提交你刚刚创建的crontab文件，可以把这个新创建的文件作为cron命令的参数：\r\n     $ crontab davecron\r\n现在该文件已经提交给cron进程，它将每隔1 5分钟运行一次。\r\n同时，新创建文件的一个副本已经被放在/var/spool/cron目录中，文件名就是用户名(即dave)。\r\n2). 列出crontab文件\r\n   为了列出crontab文件，可以用：\r\n     $ crontab -l\r\n     0,15,30,45,18-06 * * * /bin/echo `date` &gt; dev/tty1\r\n你将会看到和上面类似的内容。可以使用这种方法在$ H O M E目录中对crontab文件做一备份：\r\n     $ crontab -l &gt; $HOME/mycron\r\n    这样，一旦不小心误删了crontab文件，可以用上一节所讲述的方法迅速恢复。\r\n3). 编辑crontab文件\r\n   如果希望添加、删除或编辑crontab文件中的条目，而E D I TO R环境变量又设置为v i，那么就可以用v i来编辑crontab文件，相应的命令为：\r\n     $ crontab -e\r\n可以像使用v i编辑其他任何文件那样修改crontab文件并退出。如果修改了某些条目或添加了新的条目，那么在保存该文件时， c r o n会对其进行必要的完整性检查。如果其中的某个域出现了超出允许范围的值，它会提示你。\r\n我们在编辑crontab文件时，没准会加入新的条目。例如，加入下面的一条：\r\n    # DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month\r\n     30 3 1,7,14,21,26 * * /bin/find -name &quot;core&#039; -exec rm {} \\;\r\n现在保存并退出。最好在crontab文件的每一个条目之上加入一条注释，这样就可以知道它的功能、运行时间，更为重要的是，知道这是哪位用户的作业。\r\n现在让我们使用前面讲过的crontab -l命令列出它的全部信息：\r\n    $ crontab -l \r\n    # (crondave installed on Tue May 4 13:07:43 1999)\r\n    # DT:ech the date to the console every 30 minites\r\n   0,15,30,45 18-06 * * * /bin/echo `date` &gt; /dev/tty1\r\n    # DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month\r\n    30 3 1,7,14,21,26 * * /bin/find -name &quot;core&#039; -exec rm {} \\;\r\n4). 删除crontab文件\r\n要删除crontab文件，可以用：\r\n    $ crontab -r\r\n5). 恢复丢失的crontab文件\r\n如果不小心误删了crontab文件，假设你在自己的$ H O M E目录下还有一个备份，那么可以将其拷贝到/var/spool/cron/&lt;username&gt;，其中&lt;username&gt;是用户名。如果由于权限问题无法完成拷贝，可以用：\r\n     $ crontab &lt;filename&gt;\r\n    其中，&lt;filename&gt;是你在$ H O M E目录中副本的文件名。\r\n我建议你在自己的$ H O M E目录中保存一个该文件的副本。我就有过类似的经历，有数次误删了crontab文件（因为r键紧挨在e键的右边）。这就是为什么有些系统文档建议不要直接编辑crontab文件，而是编辑该文件的一个副本，然后重新提交新的文件。\r\n有些crontab的变体有些怪异，所以在使用crontab命令时要格外小心。如果遗漏了任何选项，crontab可能会打开一个空文件，或者看起来像是个空文件。这时敲delete键退出，不要按&lt;Ctrl-D&gt;，否则你将丢失crontab文件。',9,0,0,1514653262,0,0,0),(239,1,'使用实例','','','实例1：每1分钟执行一次command\r\n命令：\r\n* * * * * command\r\n \r\n实例2：每小时的第3和第15分钟执行\r\n命令：\r\n3,15 * * * * command\r\n \r\n实例3：在上午8点到11点的第3和第15分钟执行\r\n命令：\r\n3,15 8-11 * * * command\r\n \r\n实例4：每隔两天的上午8点到11点的第3和第15分钟执行\r\n命令：\r\n3,15 8-11 */2 * * command\r\n \r\n实例5：每个星期一的上午8点到11点的第3和第15分钟执行\r\n命令：\r\n3,15 8-11 * * 1 command\r\n \r\n实例6：每晚的21:30重启smb \r\n命令：\r\n30 21 * * * /etc/init.d/smb restart\r\n \r\n实例7：每月1、10、22日的4 : 45重启smb \r\n命令：\r\n45 4 1,10,22 * * /etc/init.d/smb restart\r\n \r\n实例8：每周六、周日的1 : 10重启smb\r\n命令：\r\n10 1 * * 6,0 /etc/init.d/smb restart\r\n \r\n实例9：每天18 : 00至23 : 00之间每隔30分钟重启smb \r\n命令：\r\n0,30 18-23 * * * /etc/init.d/smb restart\r\n \r\n实例10：每星期六的晚上11 : 00 pm重启smb \r\n命令：\r\n0 23 * * 6 /etc/init.d/smb restart\r\n \r\n实例11：每一小时重启smb \r\n命令：\r\n* */1 * * * /etc/init.d/smb restart\r\n \r\n实例12：晚上11点到早上7点之间，每隔一小时重启smb \r\n命令：\r\n* 23-7/1 * * * /etc/init.d/smb restart\r\n \r\n实例13：每月的4号与每周一到周三的11点重启smb \r\n命令：\r\n0 11 4 * mon-wed /etc/init.d/smb restart\r\n \r\n实例14：一月一号的4点重启smb \r\n命令：\r\n0 4 1 jan * /etc/init.d/smb restart\r\n实例15：每小时执行/etc/cron.hourly目录内的脚本\r\n命令：\r\n01   *   *   *   *     root run-parts /etc/cron.hourly\r\n说明：\r\nrun-parts这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是目录名了',9,0,0,1514653289,0,0,0),(240,1,'使用注意事项','','','1. 注意环境变量问题\r\n有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。\r\n在crontab文件中定义多个调度任务时，需要特别注意的一个问题就是环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。\r\n不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shelll脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点：\r\n1）脚本中涉及文件路径时写全局路径；\r\n2）脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如：\r\ncat start_cbp.sh\r\n#!/bin/sh\r\nsource /etc/profile\r\nexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf\r\n/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp;\r\n3）当手动执行脚本OK，但是crontab死活不执行时。这时必须大胆怀疑是环境变量惹的祸，并可以尝试在crontab中直接引入环境变量解决问题。如：\r\n0 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh\r\n2. 注意清理系统用户的邮件日志\r\n每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。\r\n例如，可以在crontab文件中设置如下形式，忽略日志输出：\r\n0 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1\r\n“/dev/null 2&gt;&amp;1”表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。\r\n3. 系统级任务调度与用户级任务调度\r\n系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行，root用户的任务调度操作可以通过“crontab –uroot –e”来设置，也可以将调度任务直接写入/etc/crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。\r\n4. 其他注意事项\r\n新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。\r\n当crontab突然失效时，可以尝试/etc/init.d/crond restart解决问题。或者查看日志看某个job有没有执行/报错tail -f /var/log/cron。\r\n千万别乱运行crontab -r。它从Crontab目录（/var/spool/cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了。\r\n在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义\\%，如经常用的date ‘+%Y%m%d’在crontab里是不会执行的，应该换成date ‘+\\%Y\\%m\\%d\r\n',9,0,0,1514653313,0,0,0),(241,1,'自动化安装软件','','','#!/bin/bash\r\n\r\nSERVERS=&quot;node-3.itcast.cn node-4.itcast.cn&quot;\r\nPASSWORD=123456\r\nBASE_SERVER=172.16.203.100\r\n\r\nauto_ssh_copy_id() {\r\n    expect -c &quot;set timeout -1;\r\n        spawn ssh-copy-id $1;\r\n        expect {\r\n            *(yes/no)* {send -- yes\\r;exp_continue;}\r\n            *assword:* {send -- $2\\r;exp_continue;}\r\n            eof        {exit 0;}\r\n        }&quot;;\r\n}\r\n\r\nssh_copy_id_to_all() {\r\n    for SERVER in $SERVERS\r\n    do\r\n        auto_ssh_copy_id $SERVER $PASSWORD\r\n    done\r\n}\r\n\r\nssh_copy_id_to_all\r\n\r\n\r\nfor SERVER in $SERVERS\r\ndo\r\n    scp install.sh root@$SERVER:/root\r\n    ssh root@$SERVER /root/install.sh\r\ndone\r\n\r\n\r\n#!/bin/bash\r\n\r\nBASE_SERVER=mini4\r\nyum install -y wget\r\nwget $BASE_SERVER/soft/jdk-7u45-linux-x64.tar.gz\r\ntar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local\r\ncat &gt;&gt; /etc/profile &lt;&lt; EOF\r\nexport JAVA_HOME=/usr/local/jdk1.7.0_45\r\nexport PATH=\\$PATH:\\$JAVA_HOME/bin\r\nEOF\r\n',9,0,0,1514726150,0,0,0),(242,1,'rz','','','上传文件到linux上, 是上传到当前所在的目录下\r\n\r\n[hadoop@apeng1 root]$ yum list | grep rz\r\nlrzsz.x86_64                               0.12.20-27.1.el6              base   \r\n[hadoop@apeng1 root]$ sudo yum -y install lrzsz.x86_64\r\nLoaded plugins: fastestmirror, security\r\nLoading mirror speeds from cached hostfile\r\n * base: mirrors.aliyun.com\r\n * extras: mirrors.aliyun.com\r\n * updates: mirrors.163.com\r\nSetting up Install Process\r\nResolving Dependencies\r\n--&gt; Running transaction check\r\n---&gt; Package lrzsz.x86_64 0:0.12.20-27.1.el6 will be installed\r\n--&gt; Finished Dependency Resolution\r\n\r\nDependencies Resolved\r\n\r\n=========================================================================================================================================\r\n Package                      Arch                          Version                                    Repository                   Size\r\n=========================================================================================================================================\r\nInstalling:\r\n lrzsz                        x86_64                        0.12.20-27.1.el6                           base                         71 k\r\n\r\nTransaction Summary\r\n=========================================================================================================================================\r\nInstall       1 Package(s)\r\n\r\nTotal download size: 71 k\r\nInstalled size: 159 k\r\nDownloading Packages:\r\nlrzsz-0.12.20-27.1.el6.x86_64.rpm                                                                                 |  71 kB     00:00     \r\nRunning rpm_check_debug\r\nRunning Transaction Test\r\nTransaction Test Succeeded\r\nRunning Transaction\r\n  Installing : lrzsz-0.12.20-27.1.el6.x86_64                                                                                         1/1 \r\n  Verifying  : lrzsz-0.12.20-27.1.el6.x86_64                                                                                         1/1 \r\n\r\nInstalled:\r\n  lrzsz.x86_64 0:0.12.20-27.1.el6                                                                                                        \r\n\r\nComplete!\r\n\r\n命令:(参数 -y 如果linux上有相同的文件, 会覆盖\r\nrz\r\nrz -y',9,0,0,1514736003,0,0,0),(243,1,'zk启动脚本','','','启动（三台集群）\r\n1.手动启动\r\napeng1, apeng2 apeng3\r\nzkServer.sh start\r\n\r\nzkServer.sh status #查看状态 leader 是主节点 follower 是从节点\r\n[root@apeng1 bin]# ./zkServer.sh status\r\nJMX enabled by default\r\nUsing config: /opt/app/zookeeper/bin/../conf/zoo.cfg\r\nMode: follower\r\n\r\n[root@apeng2 bin]# ./zkServer.sh status\r\nJMX enabled by default\r\nUsing config: /opt/app/zookeeper/bin/../conf/zoo.cfg\r\nMode: leader\r\n\r\n[root@apeng3 bin]# ./zkServer.sh status\r\nJMX enabled by default\r\nUsing config: /opt/app/zookeeper/bin/../conf/zoo.cfg\r\nMode: follower\r\n\r\n关闭 zkServer.sh stop\r\n\r\nexport A=1 定义的变量 会对自己所在的shell进程及其子进程生效\r\nB=1 定义的变量 只对自己所在的shell进程生效\r\n在script.sh中定义的变量 在当前登录的shell进程中source script.sh时 脚本中定义的变量也会进入当前登录的进程\r\n\r\n2.使用脚本启动\r\napeng1\r\n[root@apeng1 app]# vi zk_start.sh\r\n\r\n#!/bin/bash\r\necho &quot;start zkServer ...&quot;\r\n\r\nfor i in 1 2 3\r\ndo\r\nssh apeng$i &quot;source /etc/profile;/opt/app/zookeeper/bin/zkServer.sh start&quot;\r\ndone\r\n\r\n[root@apeng1 app]# chmod +x zk_start.sh\r\n[root@apeng1 app]# ./zk_start.sh \r\n启动 zookeeper 服务 ...\r\nThe authenticity of host &#039;apeng1 (192.168.179.135)&#039; can&#039;t be established.\r\nRSA key fingerprint is a4:32:48:27:16:61:1d:3d:09:59:d5:c4:b7:16:19:41.\r\nAre you sure you want to continue connecting (yes/no)? yes\r\nWarning: Permanently added &#039;apeng1,192.168.179.135&#039; (RSA) to the list of known hosts.\r\nroot@apeng1&#039;s password: \r\nJMX enabled by default\r\nUsing config: /opt/app/zookeeper/bin/../conf/zoo.cfg\r\nStarting zookeeper ... STARTED\r\nJMX enabled by default\r\nUsing config: /opt/app/zookeeper/bin/../conf/zoo.cfg\r\nStarting zookeeper ... already running as process 30954.\r\n\r\nssh apeng2 &quot;source /etc/profile;/opt/app/zookeeper/bin/zkServer.sh start&quot;\r\nssh apeng3 &quot;source /etc/profile;/opt/app/zookeeper/bin/zkServer.sh start&quot;\r\n',11,0,0,1514737849,0,0,0),(244,1,'安装keepalived','','','1. 安装依赖\r\nsu - root\r\nyum -y install kernel-devel*\r\nyum -y install openssl-*\r\nyum -y install popt-devel\r\nyum -y install lrzsz\r\nyum -y install openssh-clients\r\n\r\n2.1. 上传\r\n1、cd /usr/local\r\n2、rz –y\r\n3、选择keepalived安装文件\r\n2.2. 解压\r\ntar –zxvf keepalived-1.2.2.tar.gz\r\n2.3. 重命名\r\nmv keepalived-1.2.2 keepalived\r\n2.4. 安装keepalived\r\n1、cd keepalived\r\n2、执行命令\r\n./configure --prefix=/usr/local/keepalived -enable-lvs-syncd --enable-lvs --with-kernel-dir=/lib/modules/2.6.32-431.el6.x86_64/build\r\n3、编译\r\nmake\r\n4、安装\r\nmake install',9,0,0,1514751414,0,0,0),(245,1,'keepalived配置','','','        2.5. 配置服务和加入开机启动\r\ncp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/ \r\ncp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/\r\nmkdir -p /etc/keepalived\r\ncp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/\r\nln -s /usr/local/keepalived/sbin/keepalived /sbin/\r\nchkconfig keepalived on\r\n        2.6. 修改配置文件\r\n    1、 vi /etc/keepalived/keepalived.conf\r\n    2、 详解：\r\nglobal_defs {\r\n   notification_email {#指定keepalived在发生切换时需要发送email到的对象，一行一个\r\n     #acassen@firewall.loc\r\n     #failover@firewall.loc\r\n     #sysadmin@firewall.loc\r\n   }\r\n   notification_email_from Alexandre.Cassen@firewall.loc#指定发件人\r\n   #smtp_server 192.168.200.1#指定smtp服务器地址\r\n   #smtp_connect_timeout 30 #指定smtp连接超时时间\r\n   router_id LVS_DEVEL#运行keepalived机器的一个标识\r\n}\r\n\r\nvrrp_instance VI_1 {\r\n    state BACKUP#指定那个为master，那个为backup\r\n    interface eth1#设置实例绑定的网卡\r\n    virtual_router_id 51#同一实例下virtual_router_id必须相同\r\n    priority 100#定义优先级，数字越大，优先级越高,备机要小于主\r\n	advert_int 1#MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒\r\n	nopreempt#设置为不抢占,从启动后主不会自动切换回来, 注：这个配置只能设置在backup主机上，而且这个主机优先级要比另外一台高\r\n    \r\n    authentication {#设置认证\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {#设置vip\r\n        192.168.56.70#虚拟IP\r\n    }\r\n}\r\n\r\nvirtual_server 192.168.56.70 8080 {\r\n    delay_loop 6#健康检查时间间隔\r\n    lb_algo rr #调度算法rr|wrr|lc|wlc|lblc|sh|dh\r\n    lb_kind DR #负载均衡转发规则NAT|DR|RUN\r\n    #nat_mask 255.255.255.0 #需要验证\r\n    persistence_timeout 1#会话保持时间\r\n    protocol TCP#使用的协议\r\n\r\n    real_server 192.168.56.201 8080 {\r\n        weight 10 #默认为1,0为失效\r\n        SSL_GET {\r\n            url { #检查url，可以指定多个\r\n              path /\r\n              digest ff20ad2481f97b1754ef3e12ecd3a9cc #检查后的摘要信息\r\n            }\r\n            url {\r\n              path /mrtg/\r\n              digest 9b3a0c85a887a256d6939da88aabd8cd\r\n            }\r\n            connect_timeout 3#连接超时时间\r\n            nb_get_retry 3#重连次数\r\n            delay_before_retry 3#重连间隔时间\r\n        }\r\n    }\r\n\r\n}\r\n\r\n\r\n    3. 按照上面步骤安装备机器\r\n注意：备的配置文件不相同。\r\n\r\n    4. 两台机器启动keepalived：\r\nservice keepalived start\r\n\r\n    5. 验证\r\nip a\r\n    6. 监控\r\n因为keepalive只能监控机器的死活，所以当软件死掉后，keepalived仍然不会切换；\r\n所以需要写一个脚本，监控软件的死活。\r\n运行wangsf.sh，监控软件',9,0,0,1514751485,0,0,0),(246,1,'keepalived脚本','','','#!/bin/bash\r\nwhile true;\r\ndo\r\n    A=`ps -ef|grep tomcat |wc -l`\r\n	B=`ps -ef|grep keepalived |wc -l`\r\necho $A\r\nif [ $A -eq 1 ];then\r\n                echo &#039;restart tomcat!!!!&#039;\r\n                /usr/local/server/apache-tomcat-6.0.37/bin/startupsss.sh\r\n				 if [ $A -eq 1 ];then\r\n					if [ $B -gt 1 ];then\r\n						   echo &#039;tomcat dead  !!!! kill keepalived&#039;\r\n						   killall keepalived\r\n					fi\r\n				fi				\r\nfi\r\nif [ $A -eq 2 ];then\r\n					if [ $B -eq 1 ];then\r\n						   echo &#039;tomcat live  !!!! start keepalived&#039;\r\n						   service keepalived start\r\n					fi\r\n				fi\r\nsleep 3\r\ndone',9,0,0,1514751597,0,0,0),(247,1,'keepalived配置主从','','','主配置 keepalived.conf\r\n! Configuration File for keepalived\r\n\r\nglobal_defs {\r\n   notification_email {\r\n     #acassen@firewall.loc\r\n     #failover@firewall.loc\r\n     #sysadmin@firewall.loc\r\n   }\r\n   notification_email_from Alexandre.Cassen@firewall.loc\r\n   #smtp_server 192.168.200.1\r\n   #smtp_connect_timeout 30\r\n   router_id LVS_DEVEL\r\n}\r\n\r\nvrrp_instance VI_1 {\r\n    state MASTER\r\n    interface eth1\r\n    virtual_router_id 51\r\n    priority 200\r\n\r\n    advert_int 1\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {\r\n        192.168.56.70\r\n        #192.168.200.17\r\n        #192.168.200.18\r\n    }\r\n}\r\n\r\nvirtual_server 192.168.56.70 8080 {\r\n    delay_loop 6\r\n    lb_algo rr\r\n    lb_kind DR\r\n    #nat_mask 255.255.255.0\r\n    persistence_timeout 1\r\n    protocol TCP\r\n\r\nreal_server 192.168.56.200 8080 {\r\n        weight 20\r\n        SSL_GET {\r\n            url {\r\n              path /\r\n              digest ff20ad2481f97b1754ef3e12ecd3a9cc\r\n            }\r\n            url {\r\n              path /mrtg/\r\n              digest 9b3a0c85a887a256d6939da88aabd8cd\r\n            }\r\n            connect_timeout 3\r\n            nb_get_retry 3\r\n            delay_before_retry 3\r\n        }\r\n    }\r\n}\r\n\r\n从配置 keepalived.conf\r\n! Configuration File for keepalived\r\n\r\nglobal_defs {\r\n   notification_email {\r\n     #acassen@firewall.loc\r\n     #failover@firewall.loc\r\n     #sysadmin@firewall.loc\r\n   }\r\n   notification_email_from Alexandre.Cassen@firewall.loc\r\n   #smtp_server 192.168.200.1\r\n   #smtp_connect_timeout 30\r\n   router_id LVS_DEVEL\r\n}\r\n\r\nvrrp_instance VI_1 {\r\n    state BACKUP\r\n    interface eth1\r\n    virtual_router_id 51\r\n    priority 100\r\n	nopreempt\r\n    advert_int 1\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {\r\n        192.168.56.70\r\n    }\r\n}\r\n\r\nvirtual_server 192.168.56.70 8080 {\r\n    delay_loop 6\r\n    lb_algo rr\r\n    lb_kind DR\r\n    #nat_mask 255.255.255.0\r\n    persistence_timeout 1\r\n    protocol TCP\r\n\r\nreal_server 192.168.56.201 8080 {\r\n        weight 20\r\n        SSL_GET {\r\n            url {\r\n              path /\r\n              digest ff20ad2481f97b1754ef3e12ecd3a9cc\r\n            }\r\n            url {\r\n              path /mrtg/\r\n              digest 9b3a0c85a887a256d6939da88aabd8cd\r\n            }\r\n            connect_timeout 3\r\n            nb_get_retry 3\r\n            delay_before_retry 3\r\n        }\r\n    }\r\n}',9,0,0,1514751789,0,0,0),(248,1,'nginx.conf介绍','','','user  nobody nobody;	#定义Nginx运行的用户和用户组\r\nworker_processes  4;	#nginx进程数，建议设置为等于CPU总核心数。\r\nerror_log  logs/error.log	info;	#全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]\r\nworker_rlimit_nofile 1024;	#一个nginx进程打开的最多文件描述符数目，所以建议与ulimit -n的值保持一致。\r\npid	logs/nginx.pid;	#进程文件\r\n\r\n#工作模式及连接数上限\r\nevents {\r\n		use epoll;#参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型\r\n	    worker_connections  1024;#单个进程最大连接数（最大连接数=连接数*进程数）\r\n}\r\n\r\n#设定http服务器，利用它的反向代理功能提供负载均衡支持\r\nhttp {\r\n    include       mime.types;#文件扩展名与文件类型映射表\r\n    default_type  application/octet-stream;#默认文件类型\r\n#设定负载均衡的服务器列表\r\nupstream  tomcatxxxcom  {  \r\n     server   192.168.56.200:8080;  \r\n     server   192.168.56.201:8080; 	 \r\n}\r\n#设定日志格式\r\n    log_format  www_xy_com  &#039;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#039;\r\n                      &#039;$status $body_bytes_sent &quot;$http_referer&quot; &#039;\r\n                      &#039;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#039;;\r\n					  \r\n    sendfile        on;#开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。\r\n    keepalive_timeout  65; #长连接超时时间，单位是秒\r\n\r\n    #gzip  on;\r\n#设定虚拟主机，默认为监听80端口\r\n    server {\r\n        listen       80;\r\n        server_name  tomcat.xxx.com;#域名可以有多个，用空格隔开\r\n\r\n        #charset koi8-r;\r\n#设定本虚拟主机的访问日志\r\n        access_log  /data/logs/access.log  www_xy_com;\r\n#对 &quot;/&quot; 启用反向代理 \r\n	   location / {\r\n			   proxy_pass        http://tomcatxxxcom;  \r\n               proxy_set_header   Host             $host;  \r\n               proxy_set_header   X-Real-IP        $remote_addr;  \r\n               proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;\r\n        }\r\n        \r\n        #error_page   500 502 503 504  /50x.html;\r\n        location = /50x.html {\r\n            root   html;\r\n        }\r\n    }\r\n}',9,0,0,1514752084,0,0,0),(249,1,'lvs-rs.sh：与之前的不同在于修改了vip','','','#!/bin/sh\r\n#description start realserver\r\n#chkconfig 235 26 26\r\nVIP1=192.168.56.90\r\n/etc/rc.d/init.d/functions\r\ncase &quot;$1&quot; in\r\nstart)\r\n \r\necho &quot;start LVS of realserver&quot;\r\n/sbin/ifconfig lo:0 $VIP1 broadcast $VIP1 netmask 255.255.255.255 up\r\n \r\necho &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore\r\necho &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce\r\necho &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore\r\necho &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce\r\n;;\r\nstop)\r\n/sbin/ifconfig lo:0 down\r\necho &quot;close lvs dirctorserver&quot;\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce\r\n;;\r\n*)\r\necho &quot;usage:$0{start|stop}&quot;\r\nexit 1\r\nesac',9,0,0,1514752242,0,0,0),(250,1,'lvs-dr.sh:和之前对比，变化之处就是vip和转发的端','','','#!/bin/bash\r\n#description:start lvs server\r\necho &quot;1&quot; &gt;/proc/sys/net/ipv4/ip_forward\r\n \r\nWEB1=192.168.56.200\r\nWEB2=192.168.56.201\r\n \r\nVIP1=192.168.56.90\r\n \r\n/etc/rc.d/init.d/functions\r\n \r\ncase &quot;$1&quot; in\r\nstart)\r\necho &quot;start LVS of directorServer&quot;\r\n#set the Virtual address and sysctl parameter\r\n/sbin/ifconfig eth1:0 $VIP1 broadcast $VIP1 netmask 255.255.255.255 up\r\n#clear ipvs table\r\n/sbin/ipvsadm -C\r\n \r\n#set LVS\r\n#web apache or tomcat\r\n/sbin/ipvsadm -A -t $VIP1:80 -s rr\r\n/sbin/ipvsadm -a -t $VIP1:80 -r $WEB1:80  -g\r\n/sbin/ipvsadm -a -t $VIP1:80 -r $WEB2:80  -g\r\n \r\n#run LVS\r\n/sbin/ipvsadm\r\n\r\n;;\r\n\r\nstop)\r\necho &quot;close LVS directorserver&quot;\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/ip_forward\r\n\r\n/sbin/ipvsadm -C\r\n\r\n/sbin/ipvsadm -Z\r\n\r\n;;\r\n*)\r\necho &quot;usage:$0 {start|stop}&quot;\r\nexit 1\r\nesac',9,0,0,1514752266,0,0,0),(251,1,'lvs安装','','','    1. 安装lvs应用模块\r\n1、安装依赖包：\r\nyum -y install ipvs*\r\n2、验证本机ip_vs模块是否加载\r\n[root@client lvs]# grep -i &#039;ip_vs&#039; /boot/config-2.6.32-431.el6.x86_64 \r\nCONFIG_IP_VS=m \r\nCONFIG_IP_VS_IPV6=y \r\n# CONFIG_IP_VS_DEBUG is not set \r\nCONFIG_IP_VS_TAB_BITS=12 \r\nCONFIG_IP_VS_PROTO_TCP=y \r\nCONFIG_IP_VS_PROTO_UDP=y \r\nCONFIG_IP_VS_PROTO_AH_ESP=y \r\nCONFIG_IP_VS_PROTO_ESP=y \r\nCONFIG_IP_VS_PROTO_AH=y \r\nCONFIG_IP_VS_PROTO_SCTP=y \r\nCONFIG_IP_VS_RR=m \r\nCONFIG_IP_VS_WRR=m \r\nCONFIG_IP_VS_LC=m \r\nCONFIG_IP_VS_WLC=m \r\nCONFIG_IP_VS_LBLC=m \r\nCONFIG_IP_VS_LBLCR=m \r\nCONFIG_IP_VS_DH=m \r\nCONFIG_IP_VS_SH=m \r\nCONFIG_IP_VS_SED=m \r\nCONFIG_IP_VS_NQ=m \r\nCONFIG_IP_VS_FTP=m \r\nCONFIG_IP_VS_PE_SIP=m\r\n    2. 安装lvs\r\n        2.1. 编写lvs drsrever脚本：\r\n            2.1.1. 修改functions权限：\r\n（functions这个脚本是给/etc/init.d里边的文件使用的（可理解为全局文件）。）\r\nchmod 755 /etc/rc.d/init.d/functions\r\n            2.1.2. 创建lvs文件夹\r\ncd /usr/local \r\nmkdir –m 755 lvs\r\ncd /lvs\r\n            2.1.3. 编写脚本\r\nvi  lvs_dr.sh\r\n#!/bin/bash\r\n#description:start lvs server\r\necho &quot;1&quot; &gt;/proc/sys/net/ipv4/ip_forward  		#开启ip转发\r\nWEB1=192.168.56.200						#真实的webip\r\nWEB2=192.168.56.201						#真实的webip\r\nVIP1=192.168.56.80						#虚拟lvs的ip\r\n/etc/rc.d/init.d/functions 					#初始化function\r\ncase &quot;$1&quot; in								#第一个参数\r\nstart)									#第一个参数是start\r\necho &quot;start LVS of directorServer&quot;				#打印\r\n/sbin/ifconfig eth0:0 $VIP1 broadcast $VIP1 netmask 255.255.255.255 up		#设置虚拟网络\r\n/sbin/ipvsadm –C					#清除内核虚拟服务器表中的所有记录，清除lvs设置\r\n/sbin/ipvsadm -A -t $VIP1:8080 -s rr	#设置rr模式，轮询模式\r\n/sbin/ipvsadm -a -t $VIP1:8080 -r $WEB1:8080 –g		#轮询的机器，-g采用DR模式\r\n/sbin/ipvsadm -a -t $VIP1:8080 -r $WEB2:8080 –g\r\n/sbin/ipvsadm								#启动lvs\r\n;;\r\nstop)							#如果第一个参数是stop\r\necho &quot;close LVS directorserver&quot;		#打印\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/ip_forward	#关闭ip转发\r\n/sbin/ipvsadm –C					#清除内核虚拟服务器表中的所有记录\r\n/sbin/ipvsadm –Z					#虚拟服务表计数器清零（清空当前的连接数量等）\r\n;;\r\n*)								#如果第一个参数是其他任何值\r\necho &quot;usage:$0 {start|stop}&quot;			#打印：提示输入start或者stop\r\nexit 1							#退出\r\nesac								#循环结束\r\n            2.1.4. 执行脚本\r\nchmod 755 lvs_dr.sh\r\n./lvs-dr.sh  start\r\n\r\n            2.1.5. 查看：\r\nipvsadm –ln\r\n \r\n看到上面信息说明ipvsadm启动成功。\r\n        2.2.  编写lvs realserver脚本\r\n            2.2.1. 在web1 和web2机器上修改functions权限：\r\n（functions这个脚本是给/etc/init.d里边的文件使用的（可理解为全局文件）。）\r\nchmod 755 /etc/rc.d/init.d/functions\r\n\r\n            2.2.2. 在分别在web1 和web2服务器上创建lvs文件夹： \r\ncd /usr/local\r\nmkdir –m 755 lvs\r\ncd lvs\r\nrz –y\r\n            2.2.3. 编写监本\r\n\r\nvi  lvs-rs.sh\r\n#!/bin/sh\r\nVIP1=192.168.56.80					#虚拟ip\r\n/etc/rc.d/init.d/functions				#初始化function\r\ncase &quot;$1&quot; in							#第一个参数\r\nstart)								#如果第一个参数是start\r\necho &quot;start LVS of realserver&quot;				#打印\r\n/sbin/ifconfig lo:0 $VIP1 broadcast $VIP1 netmask 255.255.255.255 up	#设置虚拟网络\r\necho &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore		#定义接收到ARP请求时的响应级别\r\necho &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce	#定义将自己的地址向外通告时的级别\r\necho &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore\r\necho &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce\r\n;;\r\nstop)								#如果第一个参数是stop\r\n/sbin/ifconfig lo:0 down					#停止网卡\r\necho &quot;close lvs dirctorserver&quot;				#打印\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore		#定义接收到ARP请求时的响应级别\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce	#定义将自己的地址向外通告时的级别\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore\r\necho &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce\r\n;;\r\n*)\r\necho &quot;usage:$0{start|stop}&quot;\r\nexit 1\r\nesac\r\n            2.2.4. 启动在web1 和web2机器上lvs：\r\nchmod 755 lvs-rs.sh\r\n./lvs-rs.sh start \r\n \r\n    3. 设置dr机器上设置连接超时值(秒) \r\nipvsadm --set 1 1 1\r\n    4. 关闭\r\n./lvs-rs.sh stop\r\n./lvs-dr.sh stop',9,0,0,1514752351,0,0,0),(252,1,'nginx简介','','','1.1.  nginx简介\r\nNginx是一个自由、开源、高性能及轻量级的HTTP服务器及反转代理服务器。Nginx以其高性能、稳定、功能丰富、配置简单及占用系统资源少而著称。\r\nNginx 超越 Apache 的高性能和稳定性，使得国内使用 Nginx 作为 Web 服务器的网站也越来越多.\r\n \r\n\r\n1.2. 基础功能 \r\n反向代理加速，简单的负载均衡和容错； \r\n1.3. 优势\r\n1、Nginx专为性能优化而开发，性能是其最重要的考量, 实现上非常注重效率 。有报告表明能支持高达 50,000 个并发连接数。 \r\n2、Nginx具有很高的稳定性。其它HTTP服务器，当遇到访问的峰值，或者有人恶意发起慢速连接时，也很可能会导致服务器物理内存耗尽频繁交换，失去响应，只能重启服务器。\r\n例如当前apache一旦上到200个以上进程，web响应速度就明显非常缓慢了。而Nginx采取了分阶段资源分配技术，使得它的CPU与内存占用率非常低。\r\n3、nginx官方表示保持10,000个没有活动的连接，它只占2.5M内存，就稳定性而言, nginx比其他代理服务器更胜一筹。 \r\n4、Nginx支持热部署。它的启动特别容易, 并且几乎可以做到7*24不间断运行，即使运行数个月也不需要重新启动。你还能够在不间断服务的情况下，对软件版本进行进行升级。 \r\n5、Nginx采用C进行编写, 不论是系统资源开销还是CPU使用效率都高很多。',9,0,0,1514752661,0,0,0),(253,1,'keepalived介绍','','','2.1. 简介\r\nKeepalived的作用是检测web服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的web服务器从系统中剔除，当web服务器工作正常后Keepalived自动将web服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的web服务器。\r\n\r\n2.2. 作用\r\n主要用作RealServer的健康状态检查以及LoadBalance主机和BackUP主机之间failover的实现。',9,0,0,1514752723,0,0,0),(254,1,'lvs介绍','','','3.1. LVS是什么\r\n1、LVS的英文全称是Linux Virtual Server，即Linux虚拟服务器。\r\n2、它是我们国家的章文嵩博士的一个开源项目。\r\n3.2. LVS能干什么\r\n    1、 LVS主要用于多服务器的负载均衡。\r\n    2、 它工作在网络层，可以实现高性能，高可用的服务器集群技术。\r\n    3、 它可把许多低性能的服务器组合在一起形成一个超级服务器。\r\n    4、 它配置非常简单，且有多种负载均衡的方法。\r\n    5、 它稳定可靠，即使在集群的服务器中某台服务器无法正常工作，也不影响整体效果。\r\n    6、 可扩展性也非常好。\r\n\r\n3.3. nginx和lvs作对比的结果：\r\n1、nginx工作在网络的应用层，主要做反向代理；lvs工作在网络层，主要做负载均衡。nginx也同样能承受很高负载且稳定，但负载度和稳定度不及lvs。  \r\n2、nginx对网络的依赖较小，lvs就比较依赖于网络环境。\r\n3、在使用上，一般最前端所采取的策略应是lvs。 nginx可作为lvs节点机器使用。\r\n\r\n3.4. 负载均衡机制\r\n前面我们说了LVS是工作在网络层。相对于其它负载均衡的解决办法，它的效率是非常高的。LVS的通过控制IP来实现负载均衡。IPVS是其具体的实现模块。IPVS的主要作用：安装在Director Server上面，在Director Server虚拟一个对外访问的IP（VIP）。用户访问VIP，到达Director Server，Director Server根据一定的规则选择一个Real Server，处理完成后然后返回给客户端数据。这些步骤产生了一些具体的问题，比如如何选择具体的Real Server，Real Server如果返回给客户端数据等等。IPVS为此有三种机制：\r\n\r\n1.	VS/NAT(Virtual Server via Network Address Translation)，即网络地址翻转技术实现虚拟服务器。\r\n当请求来到时，Diretor server上处理的程序将数据报文中的目标地址（即虚拟IP地址）改成具体的某台Real Server,端口也改成Real Server的端口，然后把报文发给Real Server。Real Server处理完数据后，需要返回给Diretor Server，然后Diretor server将数据包中的源地址和源端口改成VIP的地址和端口，最后把数据发送出去。由此可以看出，用户的请求和返回都要经过Diretor Server，如果数据过多，Diretor Server肯定会不堪重负。\r\n \r\n\r\n2.	VS/TUN（Virtual Server via IP Tunneling）,即IP隧道技术实现虚拟服务器。\r\nIP隧道（IP tunneling）是将一个IP报文封装在另一个IP报文的技术，这可以使得目标为一个IP地址的数据报文能被封装和转发到另一个IP地址。IP隧道技术亦称为IP封装技术（IP encapsulation）。它跟VS/NAT基本一样，但是Real server是直接返回数据给客户端，不需要经过Diretor server,这大大降低了Diretor server的压力。\r\n\r\n3.	VS/DR（Virtual Server via Direct Routing），即用直接路由技术实现虚拟服务器。\r\n跟前面两种方式，它的报文转发方法有所不同，VS/DR通过改写请求报文的MAC地址，将请求发送到Real Server，而Real Server将响应直接返回给客户，免去了VS/TUN中的IP隧道开销。这种方式是三种负载调度机制中性能最高最好的，但是必须要求Director Server与Real Server都有一块网卡连在同一物理网段上。',9,0,0,1514752779,0,0,0),(255,1,'Zookeeper概念简介','','','Zookeeper是一个分布式协调服务；就是为用户的分布式应用程序提供协调服务\r\n    A、 zookeeper是为别的分布式程序服务的\r\n    B、 Zookeeper本身就是一个分布式程序（只要有半数以上节点存活，zk就能正常服务）\r\n    C、 Zookeeper所提供的服务涵盖：主从协调、服务器节点动态上下线、统一配置管理、分布式共享锁、统一名称服务……\r\n    D、 虽然说可以提供各种服务，但是zookeeper在底层其实只提供了两个功能：\r\n管理(存储，读取)用户程序提交的数据；\r\n并为用户程序提供数据节点监听服务；\r\n\r\nZookeeper常用应用场景：\r\n《见图》\r\n\r\nZookeeper集群的角色：  Leader 和  follower  （Observer）\r\n只要集群中有半数以上节点存活，集群就能提供服务',11,0,0,1514752892,0,0,0),(256,1,'zookeeper集群机制','','','查看集群状态\r\n    1、 jps（查看进程）\r\n    2、 zkServer.sh status（查看集群状态，主从信息）\r\n\r\n\r\n半数机制：集群中半数以上机器存活，集群可用。\r\nzookeeper适合装在奇数台机器上！！！\r\n\r\n',11,0,0,1514752991,0,0,0),(257,1,'zk的配置(三台主机)','','','zk安装后配置\r\n\r\nsu - hadoop\r\n修改配置文件\r\n\r\n1、用hadoop用户操作\r\ncd /opt/app/zookeeper/conf\r\nsudo cp zoo_sample.cfg zoo.cfg\r\n\r\n2、sudo vi zoo.cfg\r\n\r\n3、添加内容：\r\ndataDir=/opt/app/zookeeper/data\r\ndataLogDir=/opt/app/zookeeper/log\r\nserver.1=apeng1:2888:3888 (主机名, 心跳端口、数据端口)\r\nserver.2=apeng2:2888:3888\r\nserver.3=apeng3:2888:3888\r\n\r\n4、创建文件夹：\r\ncd /opt/app/zookeeper/\r\nsudo mkdir -m 755 data\r\nsudo mkdir -m 755 log\r\n\r\n5、在data文件夹下新建myid文件，myid的文件内容为：\r\ncd data\r\nsudo vi myid\r\n添加内容：\r\n1\r\n\r\n6、将集群下发到其他机器上\r\nsu root\r\n[root@apeng1 app]# scp -r /opt/app/zookeeper/ root@apeng2:/opt/app/\r\n[root@apeng1 app]# scp -r /opt/app/zookeeper/ root@apeng3:/opt/app/\r\n\r\n[root@apeng1 zookeeper]# scp -r /etc/hosts root@apeng2:/etc/\r\nhosts                                                                                                  100%  228     0.2KB/s   00:00    \r\n[root@apeng1 zookeeper]# scp -r /etc/hosts root@apeng3:/etc/\r\nhosts          \r\n\r\n7、修改其他机器的配置文件\r\nssh apeng2 \r\nvi /opt/app/zookeeper/data/myid\r\n\r\nssh apeng2 \r\nvi /opt/app/zookeeper/data/myid\r\n到apeng2上：修改myid为：2\r\n到apeng3上：修改myid为：3',11,0,0,1514753050,0,0,0),(258,1,'zookeeper特性','','','        4.1. zookeeper特性\r\n1、Zookeeper：一个leader，多个follower组成的集群\r\n2、全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的\r\n3、分布式读写，更新请求转发，由leader实施\r\n4、更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行\r\n5、数据更新原子性，一次数据更新要么成功，要么失败\r\n6、实时性，在一定时间范围内，client能读到最新数据\r\n\r\n        4.2. zookeeper数据结构\r\n1、层次化的目录结构，命名符合常规文件系统规范(见下图)\r\n2、每个节点在zookeeper中叫做znode,并且其有一个唯一的路径标识\r\n3、节点Znode可以包含数据和子节点（但是EPHEMERAL类型的节点不能有子节点，下一页详细讲解）\r\n4、客户端应用可以在节点上设置监视器（后续详细讲解）	',11,0,0,1514753177,0,0,0),(259,1,'zookeeper节点类型','','','1、Znode有两种类型：\r\n短暂（ephemeral）（断开连接自己删除）\r\n持久（persistent）（断开连接不删除）\r\n2、Znode有四种形式的目录节点（默认是persistent ）\r\nPERSISTENT\r\nPERSISTENT_SEQUENTIAL（持久序列/test0000000019 ）\r\nEPHEMERAL\r\nEPHEMERAL_SEQUENTIAL\r\n3、创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护\r\n4、在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序',11,0,0,1514753224,0,0,0),(260,1,'zk客户端(命令行操作)','','','运行 zkCli.sh –server &lt;ip&gt;进入命令行工具\r\n[root@apeng1 zookeeper]# bin/zkCli.sh -server 192.168.179.135\r\n\r\n1、使用 ls 命令来查看当前 ZooKeeper 中所包含的内容：\r\n[zk: 192.168.179.135(CONNECTED) 0] ls /\r\n[zookeeper]\r\n\r\n2、创建一个新的 znode ，使用 create /zk myData 。这个命令创建了一个新的 znode 节点“ zk ”以及与它关联的字符串：\r\n[zk: 192.168.179.135(CONNECTED) 1] create /zk &quot;myData&quot;\r\nCreated /zk\r\n\r\n3、我们运行 get 命令来确认 znode 是否包含我们所创建的字符串：\r\n[zk: 192.168.179.135(CONNECTED) 2] get /zk\r\n&quot;myData&quot;\r\ncZxid = 0x200000002\r\nctime = Thu Jan 11 10:25:29 CST 2018\r\nmZxid = 0x200000002\r\nmtime = Thu Jan 11 10:25:29 CST 2018\r\npZxid = 0x200000002\r\ncversion = 0\r\ndataVersion = 0\r\naclVersion = 0\r\nephemeralOwner = 0x0\r\ndataLength = 8\r\nnumChildren = 0\r\n\r\n#监听这个节点的变化,当另外一个客户端改变/zk时,它会打出下面的\r\n#WATCHER::\r\n#WatchedEvent state:SyncConnected type:NodeDataChanged path:/zk\r\n[zk: localhost:2181(CONNECTED) 4] get /zk watch\r\n\r\n4、下面我们通过 set 命令来对 zk 所关联的字符串进行设置：\r\n[zk: 192.168.179.135(CONNECTED) 3] get /zk watch\r\n&quot;myData&quot;\r\n\r\n5、下面我们将刚才创建的 znode 删除：\r\n[zk: 192.168.179.135(CONNECTED) 5] delete /zk\r\n[zk: 192.168.179.135(CONNECTED) 6] ls /zk \r\nNode does not exist: /zk\r\n\r\n6、删除节点：rmr\r\n[zk: 192.168.179.135(CONNECTED) 7] rmr /zk\r\nNode does not exist: /zk\r\n\r\n',11,0,0,1514753267,0,0,0),(261,1,'zk-api应用','','','基本使用\r\n org.apache.zookeeper.Zookeeper是客户端入口主类，负责建立与server的会话\r\n它提供了表 1 所示几类主要方法  ：\r\n\r\ncreate        在本地目录树中创建一个节点\r\ndelete        删除一个节点\r\nexists        测试本地是否存在目标节点\r\nget/set data  从目标节点上读取 / 写数据\r\nget/set ACL   获取 / 设置目标节点访问控制列表信息\r\nget children  检索一个子节点上的列表\r\nsync          等待要被传送的数据\r\n\r\n创建Java Project --- 创建lib目录 --- 导入相关jar包\r\n\r\nzookeeper-3.4.5.jar zookeeper 核心包\r\n\r\nlib目录下五个jar包\r\njline-...jar\r\nlog4j-...jar\r\n\r\n选中 --- 右键 --- build path --- add to build path\r\n\r\n编码\r\npackage com.peng.zk;\r\n\r\nimport java.io.IOException;\r\n\r\nimport org.apache.zookeeper.CreateMode;\r\nimport org.apache.zookeeper.KeeperException;\r\nimport org.apache.zookeeper.WatchedEvent;\r\nimport org.apache.zookeeper.Watcher;\r\nimport org.apache.zookeeper.ZooDefs.Ids;\r\nimport org.apache.zookeeper.ZooKeeper;\r\n\r\npublic class SimpleZkClient {\r\n	\r\n	private static final String connectString=&quot;apeng1:2181,apeng2:2181,apeng3:2181&quot;;\r\n	private static final int sessionTimeout = 2000;\r\n	\r\n	public static void main(String[] args) throws IOException, KeeperException, InterruptedException {\r\n		\r\n		ZooKeeper zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() {\r\n			\r\n			public void process(WatchedEvent event) {\r\n				// 收到事件通知后的回调函数(应该是我们自己的事件处理逻辑)\r\n				System.out.println(event.getType() + &quot;---&quot; + event.getPath());\r\n			}\r\n		});\r\n		\r\n		/**\r\n		 * 数据的增删改查\r\n		 */\r\n		//参数1 要创建的节点的路径 参数2 节点大数据 参数3 节点的权限 参数4 节点的类型\r\n		String nodeCreated = zkClient.create(&quot;/eclipse&quot;, &quot;hellozk&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\r\n	}\r\n	\r\n}\r\n\r\nRun 运行程序\r\n',11,0,0,1514753516,0,0,0),(262,1,'zookeeper增删改查','','','demo增删改查\r\n \r\npublic class SimpleDemo {\r\n	// 会话超时时间，设置为与系统默认时间一致\r\n	private static final int SESSION_TIMEOUT = 30000;\r\n	// 创建 ZooKeeper 实例\r\n	ZooKeeper zk;\r\n	// 创建 Watcher 实例\r\n	Watcher wh = new Watcher() {\r\n		public void process(org.apache.zookeeper.WatchedEvent event)\r\n		{\r\n			System.out.println(event.toString());\r\n		}\r\n	};\r\n	// 初始化 ZooKeeper 实例\r\n	private void createZKInstance() throws IOException\r\n	{\r\n		zk = new ZooKeeper(&quot;weekend01:2181&quot;, SimpleDemo.SESSION_TIMEOUT, this.wh);\r\n	}\r\n	private void ZKOperations() throws IOException, InterruptedException, KeeperException\r\n	{\r\n		System.out.println(&quot;/n1. 创建 ZooKeeper 节点 (znode ： zoo2, 数据： myData2 ，权限： OPEN_ACL_UNSAFE ，节点类型： Persistent&quot;);\r\n		zk.create(&quot;/zoo2&quot;, &quot;myData2&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\r\n		System.out.println(&quot;/n2. 查看是否创建成功： &quot;);\r\n		System.out.println(new String(zk.getData(&quot;/zoo2&quot;, false, null)));\r\n		System.out.println(&quot;/n3. 修改节点数据 &quot;);\r\n		zk.setData(&quot;/zoo2&quot;, &quot;shenlan211314&quot;.getBytes(), -1);\r\n		System.out.println(&quot;/n4. 查看是否修改成功： &quot;);\r\n		System.out.println(new String(zk.getData(&quot;/zoo2&quot;, false, null)));\r\n		System.out.println(&quot;/n5. 删除节点 &quot;);\r\n		zk.delete(&quot;/zoo2&quot;, -1);\r\n		System.out.println(&quot;/n6. 查看节点是否被删除： &quot;);\r\n		System.out.println(&quot; 节点状态： [&quot; + zk.exists(&quot;/zoo2&quot;, false) + &quot;]&quot;);\r\n	}\r\n	private void ZKClose() throws InterruptedException\r\n	{\r\n		zk.close();\r\n	}\r\n	public static void main(String[] args) throws IOException, InterruptedException, KeeperException {\r\n		SimpleDemo dm = new SimpleDemo();\r\n		dm.createZKInstance();\r\n		dm.ZKOperations();\r\n		dm.ZKClose();\r\n	}\r\n}\r\n\r\n \r\nZookeeper的监听器工作机制\r\n\r\n\r\n监听器是一个接口，我们的代码中可以实现Wather这个接口，实现其中的process方法，方法中即我们自己的业务逻辑\r\n\r\n监听器的注册是在获取数据的操作中实现： \r\ngetData(path,watch?)监听的事件是：节点数据变化事件\r\ngetChildren(path,watch?)监听的事件是：节点下的子节点增减变化事件\r\n',11,0,0,1514753594,0,0,0),(263,1,'zookeeper应用案例','','','        4.7. zookeeper应用案例（分布式应用HA||分布式锁）\r\n3.7.1 实现分布式应用的(主节点HA)及客户端动态更新主节点状态\r\n某分布式系统中，主节点可以有多台，可以动态上下线\r\n任意一台客户端都能实时感知到主节点服务器的上下线\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nA、客户端实现\r\npublic class AppClient {\r\n	private String groupNode = &quot;sgroup&quot;;\r\n	private ZooKeeper zk;\r\n	private Stat stat = new Stat();\r\n	private volatile List&lt;String&gt; serverList;\r\n\r\n	/**\r\n	 * 连接zookeeper\r\n	 */\r\n	public void connectZookeeper() throws Exception {\r\n		zk \r\n= new ZooKeeper(&quot;localhost:4180,localhost:4181,localhost:4182&quot;, 5000, new Watcher() {\r\n			public void process(WatchedEvent event) {\r\n				// 如果发生了&quot;/sgroup&quot;节点下的子节点变化事件, 更新server列表, 并重新注册监听\r\n				if (event.getType() == EventType.NodeChildrenChanged \r\n					&amp;&amp; (&quot;/&quot; + groupNode).equals(event.getPath())) {\r\n					try {\r\n						updateServerList();\r\n					} catch (Exception e) {\r\n						e.printStackTrace();\r\n					}\r\n				}\r\n			}\r\n		});\r\n\r\n		updateServerList();\r\n	}\r\n\r\n	/**\r\n	 * 更新server列表\r\n	 */\r\n	private void updateServerList() throws Exception {\r\n		List&lt;String&gt; newServerList = new ArrayList&lt;String&gt;();\r\n\r\n		// 获取并监听groupNode的子节点变化\r\n		// watch参数为true, 表示监听子节点变化事件. \r\n		// 每次都需要重新注册监听, 因为一次注册, 只能监听一次事件, 如果还想继续保持监听, 必须重新注册\r\n		List&lt;String&gt; subList = zk.getChildren(&quot;/&quot; + groupNode, true);\r\n		for (String subNode : subList) {\r\n			// 获取每个子节点下关联的server地址\r\n			byte[] data = zk.getData(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, false, stat);\r\n			newServerList.add(new String(data, &quot;utf-8&quot;));\r\n		}\r\n\r\n		// 替换server列表\r\n		serverList = newServerList;\r\n\r\n		System.out.println(&quot;server list updated: &quot; + serverList);\r\n	}\r\n\r\n	/**\r\n	 * client的工作逻辑写在这个方法中\r\n	 * 此处不做任何处理, 只让client sleep\r\n	 */\r\n	public void handle() throws InterruptedException {\r\n		Thread.sleep(Long.MAX_VALUE);\r\n	}\r\n\r\n	public static void main(String[] args) throws Exception {\r\n		AppClient ac = new AppClient();\r\n		ac.connectZookeeper();\r\n\r\n		ac.handle();\r\n	}\r\n}\r\n\r\n\r\n\r\nB、服务器端实现\r\npublic class AppServer {\r\n	private String groupNode = &quot;sgroup&quot;;\r\n	private String subNode = &quot;sub&quot;;\r\n\r\n	/**\r\n	 * 连接zookeeper\r\n	 * @param address server的地址\r\n	 */\r\n	public void connectZookeeper(String address) throws Exception {\r\n		ZooKeeper zk = new ZooKeeper(\r\n&quot;localhost:4180,localhost:4181,localhost:4182&quot;, \r\n5000, new Watcher() {\r\n			public void process(WatchedEvent event) {\r\n				// 不做处理\r\n			}\r\n		});\r\n		// 在&quot;/sgroup&quot;下创建子节点\r\n		// 子节点的类型设置为EPHEMERAL_SEQUENTIAL, 表明这是一个临时节点, 且在子节点的名称后面加上一串数字后缀\r\n		// 将server的地址数据关联到新创建的子节点上\r\n		String createdPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, address.getBytes(&quot;utf-8&quot;), \r\n			Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);\r\n		System.out.println(&quot;create: &quot; + createdPath);\r\n	}\r\n	\r\n	/**\r\n	 * server的工作逻辑写在这个方法中\r\n	 * 此处不做任何处理, 只让server sleep\r\n	 */\r\n	public void handle() throws InterruptedException {\r\n		Thread.sleep(Long.MAX_VALUE);\r\n	}\r\n	\r\n	public static void main(String[] args) throws Exception {\r\n		// 在参数中指定server的地址\r\n		if (args.length == 0) {\r\n			System.err.println(&quot;The first argument must be server address&quot;);\r\n			System.exit(1);\r\n		}\r\n		\r\n		AppServer as = new AppServer();\r\n		as.connectZookeeper(args[0]);\r\n		as.handle();\r\n	}\r\n}\r\n\r\n\r\n\r\n3.7.2分布式共享锁的简单实现\r\n    • 客户端A\r\npublic class DistributedClient {\r\n    // 超时时间\r\n    private static final int SESSION_TIMEOUT = 5000;\r\n    // zookeeper server列表\r\n    private String hosts = &quot;localhost:4180,localhost:4181,localhost:4182&quot;;\r\n    private String groupNode = &quot;locks&quot;;\r\n    private String subNode = &quot;sub&quot;;\r\n\r\n    private ZooKeeper zk;\r\n    // 当前client创建的子节点\r\n    private String thisPath;\r\n    // 当前client等待的子节点\r\n    private String waitPath;\r\n\r\n    private CountDownLatch latch = new CountDownLatch(1);\r\n\r\n    /**\r\n     * 连接zookeeper\r\n     */\r\n    public void connectZookeeper() throws Exception {\r\n        zk = new ZooKeeper(hosts, SESSION_TIMEOUT, new Watcher() {\r\n            public void process(WatchedEvent event) {\r\n                try {\r\n                    // 连接建立时, 打开latch, 唤醒wait在该latch上的线程\r\n                    if (event.getState() == KeeperState.SyncConnected) {\r\n                        latch.countDown();\r\n                    }\r\n\r\n                    // 发生了waitPath的删除事件\r\n                    if (event.getType() == EventType.NodeDeleted &amp;&amp; event.getPath().equals(waitPath)) {\r\n                        doSomething();\r\n                    }\r\n                } catch (Exception e) {\r\n                    e.printStackTrace();\r\n                }\r\n            }\r\n        });\r\n\r\n        // 等待连接建立\r\n        latch.await();\r\n\r\n        // 创建子节点\r\n        thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE,\r\n                CreateMode.EPHEMERAL_SEQUENTIAL);\r\n\r\n        // wait一小会, 让结果更清晰一些\r\n        Thread.sleep(10);\r\n\r\n        // 注意, 没有必要监听&quot;/locks&quot;的子节点的变化情况\r\n        List&lt;String&gt; childrenNodes = zk.getChildren(&quot;/&quot; + groupNode, false);\r\n\r\n        // 列表中只有一个子节点, 那肯定就是thisPath, 说明client获得锁\r\n        if (childrenNodes.size() == 1) {\r\n            doSomething();\r\n        } else {\r\n            String thisNode = thisPath.substring((&quot;/&quot; + groupNode + &quot;/&quot;).length());\r\n            // 排序\r\n            Collections.sort(childrenNodes);\r\n            int index = childrenNodes.indexOf(thisNode);\r\n            if (index == -1) {\r\n                // never happened\r\n            } else if (index == 0) {\r\n                // inddx == 0, 说明thisNode在列表中最小, 当前client获得锁\r\n                doSomething();\r\n            } else {\r\n                // 获得排名比thisPath前1位的节点\r\n                this.waitPath = &quot;/&quot; + groupNode + &quot;/&quot; + childrenNodes.get(index - 1);\r\n                // 在waitPath上注册监听器, 当waitPath被删除时, zookeeper会回调监听器的process方法\r\n                zk.getData(waitPath, true, new Stat());\r\n            }\r\n        }\r\n    }\r\n\r\n    private void doSomething() throws Exception {\r\n        try {\r\n            System.out.println(&quot;gain lock: &quot; + thisPath);\r\n            Thread.sleep(2000);\r\n            // do something\r\n        } finally {\r\n            System.out.println(&quot;finished: &quot; + thisPath);\r\n            // 将thisPath删除, 监听thisPath的client将获得通知\r\n            // 相当于释放锁\r\n            zk.delete(this.thisPath, -1);\r\n        }\r\n    }\r\n\r\n    public static void main(String[] args) throws Exception {\r\n        for (int i = 0; i &lt; 10; i++) {\r\n            new Thread() {\r\n                public void run() {\r\n                    try {\r\n                        DistributedClient dl = new DistributedClient();\r\n                        dl.connectZookeeper();\r\n                    } catch (Exception e) {\r\n                        e.printStackTrace();\r\n                    }\r\n                }\r\n            }.start();\r\n        }\r\n\r\n        Thread.sleep(Long.MAX_VALUE);\r\n    }\r\n}\r\n\r\n    • 分布式多进程模式实现：\r\npublic class DistributedClientMy {\r\n	\r\n\r\n	// 超时时间\r\n	private static final int SESSION_TIMEOUT = 5000;\r\n	// zookeeper server列表\r\n	private String hosts = &quot;spark01:2181,spark02:2181,spark03:2181&quot;;\r\n	private String groupNode = &quot;locks&quot;;\r\n	private String subNode = &quot;sub&quot;;\r\n	private boolean haveLock = false;\r\n\r\n	private ZooKeeper zk;\r\n	// 当前client创建的子节点\r\n	private volatile String thisPath;\r\n\r\n	/**\r\n	 * 连接zookeeper\r\n	 */\r\n	public void connectZookeeper() throws Exception {\r\n		zk = new ZooKeeper(&quot;spark01:2181&quot;, SESSION_TIMEOUT, new Watcher() {\r\n			public void process(WatchedEvent event) {\r\n				try {\r\n\r\n					// 子节点发生变化\r\n					if (event.getType() == EventType.NodeChildrenChanged &amp;&amp; event.getPath().equals(&quot;/&quot; + groupNode)) {\r\n						// thisPath是否是列表中的最小节点\r\n						List&lt;String&gt; childrenNodes = zk.getChildren(&quot;/&quot; + groupNode, true);\r\n						String thisNode = thisPath.substring((&quot;/&quot; + groupNode + &quot;/&quot;).length());\r\n						// 排序\r\n						Collections.sort(childrenNodes);\r\n						if (childrenNodes.indexOf(thisNode) == 0) {\r\n							doSomething();\r\n							thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE,\r\n									CreateMode.EPHEMERAL_SEQUENTIAL);\r\n						}\r\n					}\r\n				} catch (Exception e) {\r\n					e.printStackTrace();\r\n				}\r\n			}\r\n		});\r\n\r\n		// 创建子节点\r\n		thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE,\r\n				CreateMode.EPHEMERAL_SEQUENTIAL);\r\n\r\n		// wait一小会, 让结果更清晰一些\r\n		Thread.sleep(new Random().nextInt(1000));\r\n\r\n		// 监听子节点的变化\r\n		List&lt;String&gt; childrenNodes = zk.getChildren(&quot;/&quot; + groupNode, true);\r\n\r\n		// 列表中只有一个子节点, 那肯定就是thisPath, 说明client获得锁\r\n		if (childrenNodes.size() == 1) {\r\n			doSomething();\r\n			thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE,\r\n					CreateMode.EPHEMERAL_SEQUENTIAL);\r\n		}\r\n	}\r\n\r\n	/**\r\n	 * 共享资源的访问逻辑写在这个方法中\r\n	 */\r\n	private void doSomething() throws Exception {\r\n		try {\r\n			System.out.println(&quot;gain lock: &quot; + thisPath);\r\n			Thread.sleep(2000);\r\n			// do something\r\n		} finally {\r\n			System.out.println(&quot;finished: &quot; + thisPath);\r\n			// 将thisPath删除, 监听thisPath的client将获得通知\r\n			// 相当于释放锁\r\n			zk.delete(this.thisPath, -1);\r\n		}\r\n	}\r\n\r\n	public static void main(String[] args) throws Exception {\r\n		DistributedClientMy dl = new DistributedClientMy();\r\n		dl.connectZookeeper();\r\n		Thread.sleep(Long.MAX_VALUE);\r\n	}\r\n\r\n	\r\n}\r\n\r\n',11,0,0,1514753665,0,0,0),(264,1,'zookeeper的选举机制','','','    5. zookeeper原理\r\nZookeeper虽然在配置文件中并没有指定master和slave\r\n但是，zookeeper工作时，是有一个节点为leader，其他则为follower\r\nLeader是通过内部的选举机制临时产生的\r\n\r\n\r\n\r\n        5.1. zookeeper的选举机制（全新集群paxos）\r\n以一个简单的例子来说明整个选举的过程.\r\n假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.\r\n1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态\r\n2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态.\r\n3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.\r\n4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.\r\n5) 服务器5启动,同4一样,当小弟.',11,0,0,1514753704,0,0,0),(265,1,'非全新集群的选举机制(数据恢复)','','','        5.2. 非全新集群的选举机制(数据恢复)\r\n那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。\r\n需要加入数据id、leader id和逻辑时钟。\r\n数据id：数据新的id就大，数据每次更新都会更新id。\r\nLeader id：就是我们配置的myid中的值，每个机器一个。\r\n逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说:  如果在同一次选举中,那么这个值应该是一致的 ;  逻辑时钟值越大,说明这一次选举leader的进程更新.\r\n选举的标准就变成：\r\n		1、逻辑时钟小的选举结果被忽略，重新投票\r\n		2、统一逻辑时钟后，数据id大的胜出\r\n		3、数据id相同的情况下，leader id大的胜出\r\n根据这个规则选出leader。',11,0,0,1514753725,0,0,0),(266,1,'ActiveMQ','','','1.下载ActiveMQ\r\n去官方网站下载：http://activemq.apache.org/\r\n\r\n2.运行ActiveMQ\r\n解压缩apache-activemq-5.5.1-bin.zip，\r\n修改配置文件activeMQ.xml，将0.0.0.0修改为localhost\r\n&lt;transportConnectors&gt;\r\n       &lt;transportConnector name=&quot;openwire&quot; uri=&quot;tcp://localhost:61616&quot;/&gt;\r\n       &lt;transportConnector name=&quot;ssl&quot;     uri=&quot;ssl://localhost:61617&quot;/&gt;\r\n       &lt;transportConnector name=&quot;stomp&quot;   uri=&quot;stomp://localhost:61613&quot;/&gt;\r\n      &lt;transportConnector uri=&quot;http://localhost:8081&quot;/&gt;\r\n       &lt;transportConnector uri=&quot;udp://localhost:61618&quot;/&gt;\r\n然后双击apache-activemq-5.5.1\\bin\\activemq.bat运行ActiveMQ程序。\r\n启动ActiveMQ以后，登陆：http://localhost:8161/admin/，创建一个Queue，命名为FirstQueue。',11,0,0,1514753973,0,0,0),(267,1,'高性能硬件上的程序部署策略','','','例如，一个15万PV/天左右的在线文档类型网站最近更换了硬件系统，新的硬件为4CPU、16G内存，操作系统为64位Centos5.4，整个服务器暂时没有其他的应用，所有硬件资源都可以提供这个访问量不大的网站使用。管理员为了尽量利用硬件资源选用了64位的JDK1.5，并设置java堆内存为12G，使用了一段时间，网站经常出现长时间无响应的情况。\r\n	监控服务器运行状况后发现是由于GC停顿导致的，虚拟机是server模式，默认使用吞吐量有限收集器，回收12G的堆，一次full GC的停顿时间高达14秒。\r\n	城西部署上的主要问题是过大的堆内存进行回收带来的长时间停顿。硬件升级前使用的是32位的系统1.5的堆内存，用户感觉使用网站比较缓慢，但不会出现明显的卡顿现象，因此升级硬件提高性能。\r\n	此案例中管理员采用64位JDK来使用更大的内存，对于用户交互性强、对停顿时间敏感的系统，可以给java虚拟机分配超大堆的前提是有把握把应用程序的Full GC频率控制的足够低，至少',11,0,0,1514754148,0,0,0),(268,1,' java多线程基本知识','','','        1. java多线程基本知识\r\n            1.1. 进程介绍\r\n	不管是我们开发的应用程序，还是我们运行的其他的应用程序，都需要先把程序安装在本地的硬盘上。然后找到这个程序的启动文件，启动程序的时候，其实是电脑把当前的这个程序加载到内存中，在内存中需要给当前的程序分配一段独立的运行空间。这片空间就专门负责当前这个程序的运行。\r\n	不同的应用程序运行的过程中都需要在内存中分配自己独立的运行空间，彼此之间不会相互的影响。我们把每个独立应用程序在内存的独立空间称为当前应用程序运行的一个进程。\r\n进程：它是内存中的一段独立的空间，可以负责当前应用程序的运行。当前这个进程负责调度当前程序中的所有运行细节。\r\n            1.2. 线程介绍\r\n	启动的QQ聊天软件，需要和多个人进行聊天。这时多个人之间是不能相互影响，但是它们都位于当前QQ这个软件运行时所分配的内容的独立空间中。\r\n	在一个进程中，每个独立的功能都需要独立的去运行，这时又需要把当前这个进程划分成多个运行区域，每个独立的小区域（小单元）称为一个线程。\r\n线程：它是位于进程中，负责当前进程中的某个具备独立运行资格的空间。\r\n进程是负责整个程序的运行，而线程是程序中具体的某个独立功能的运行。一个进程中至少应该有一个线程。\r\n            1.3. 多线程介绍\r\n	现在的操作系统基本都是多用户，多任务的操作系统。每个任务就是一个进程。而在这个进程中就会有线程。\r\n	真正可以完成程序运行和功能的实现靠的是进程中的线程。\r\n多线程：在一个进程中，我们同时开启多个线程，让多个线程同时去完成某些任务（功能）。\r\n多线程的目的：提高程序的运行效率。\r\n            1.4. 多线程运行的原理\r\n	cpu在线程中做时间片的切换。\r\n\r\n	其实真正电脑中的程序的运行不是同时在运行的。CPU负责程序的运行，而CPU在运行程序的过程中某个时刻点上，它其实只能运行一个程序。而不是多个程序。而CPU它可以在多个程序之间进行高速的切换。而切换频率和速度太快，导致人的肉看看不到。\r\n每个程序就是进程， 而每个进程中会有多个线程，而CPU是在这些线程之间进行切换。\r\n了解了CPU对一个任务的执行过程，我们就必须知道，多线程可以提高程序的运行效率，但不能无限制的开线程。\r\n            1.5. 实现线程的两种方式\r\n1、继承Thread的原理\r\n	见代码MyThreadWithExtends\r\n2、声明实现 Runnable 接口的类\r\n	见代码MyThreadWithImpliment',12,0,0,1514754256,0,0,0),(269,1,'synchronized','','','加同步格式：\r\n		synchronized( 需要一个任意的对象（锁） ){\r\n			代码块中放操作共享数据的代码。\r\n		}\r\n	见代码MySynchronized\r\n    • synchronized的缺陷\r\nsynchronized是java中的一个关键字，也就是说是Java语言内置的特性。\r\n如果一个代码块被synchronized修饰了，当一个线程获取了对应的锁，并执行该代码块时，其他线程便只能一直等待，等待获取锁的线程释放锁，而这里获取锁的线程释放锁只会有两种情况：\r\n　　1）获取锁的线程执行完了该代码块，然后线程释放对锁的占有；\r\n2）线程执行发生异常，此时JVM会让线程自动释放锁。\r\n\r\n例子1：\r\n　　如果这个获取锁的线程由于要等待IO或者其他原因（比如调用sleep方法）被阻塞了，但是又没有释放锁，其他线程便只能干巴巴地等待，试想一下，这多么影响程序执行效率。\r\n　　因此就需要有一种机制可以不让等待的线程一直无期限地等待下去（比如只等待一定的时间或者能够响应中断），通过Lock就可以办到。\r\n例子2：\r\n当有多个线程读写文件时，读操作和写操作会发生冲突现象，写操作和写操作会发生冲突现象，但是读操作和读操作不会发生冲突现象。\r\n　　但是采用synchronized关键字来实现同步的话，就会导致一个问题：\r\n如果多个线程都只是进行读操作，当一个线程在进行读操作时，其他线程只能等待无法进行读操作。\r\n\r\n　　因此就需要一种机制来使得多个线程都只是进行读操作时，线程之间不会发生冲突，通过Lock就可以办到。\r\n　　另外，通过Lock可以知道线程有没有成功获取到锁。这个是synchronized无法办到的。\r\n　　总的来说，也就是说Lock提供了比synchronized更多的功能。',12,0,0,1514754369,0,0,0),(270,1,'java lock','','','    • lock和synchronized的区别\r\n　　1）Lock不是Java语言内置的，synchronized是Java语言的关键字，因此是内置特性。Lock是一个类，通过这个类可以实现同步访问；\r\n　　2）Lock和synchronized有一点非常大的不同，采用synchronized不需要用户去手动释放锁，当synchronized方法或者synchronized代码块执行完之后，系统会自动让线程释放对锁的占用；而Lock则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现象。\r\n\r\n    • java.util.concurrent.locks包下常用的类\r\n\r\n                    ◦ Lock\r\n　　首先要说明的就是Lock，通过查看Lock的源码可知，Lock是一个接口：\r\npublic interface Lock {\r\n    void lock();\r\n    void lockInterruptibly() throws InterruptedException;\r\n    boolean tryLock();\r\n    boolean tryLock(long time, TimeUnit unit) throws InterruptedException;\r\n    void unlock();\r\n    }\r\n\r\nLock接口中每个方法的使用：\r\nlock()、tryLock()、tryLock(long time, TimeUnit unit)、lockInterruptibly()是用来获取锁的。	unLock()方法是用来释放锁的。\r\n\r\n四个获取锁方法的区别：\r\n\r\n　　lock()方法是平常使用得最多的一个方法，就是用来获取锁。如果锁已被其他线程获取，则进行等待。\r\n由于在前面讲到如果采用Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用Lock必须在try{}catch{}块中进行，并且将释放锁的操作放在finally块中进行，以保证锁一定被被释放，防止死锁的发生。\r\n\r\ntryLock()方法是有返回值的，它表示用来尝试获取锁，如果获取成功，则返回true，如果获取失败（即锁已被其他线程获取），则返回false，也就说这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。\r\n\r\n　　tryLock(long time, TimeUnit unit)方法和tryLock()方法是类似的，只不过区别在于这个方法在拿不到锁时会等待一定的时间，在时间期限之内如果还拿不到锁，就返回false。如果如果一开始拿到锁或者在等待期间内拿到了锁，则返回true。\r\n\r\n　　lockInterruptibly()方法比较特殊，当通过这个方法去获取锁时，如果线程正在等待获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通过lock.lockInterruptibly()想获取某个锁时，假若此时线程A获取到了锁，而线程B只有在等待\r\n\r\n那么对线程B调用threadB.interrupt()方法能够中断线程B的等待过程。\r\n　　注意，当一个线程获取了锁之后，是不会被interrupt()方法中断的。\r\n　　因此当通过lockInterruptibly()方法获取某个锁时，如果不能获取到，只有进行等待的情况下，是可以响应中断的。\r\n　　而用synchronized修饰的话，当一个线程处于等待某个锁的状态，是无法被中断的，只有一直等待下去。\r\n\r\n        ◦ ReentrantLock\r\n直接使用lock接口的话，我们需要实现很多方法，不太方便，ReentrantLock是唯一实现了Lock接口的类，并且ReentrantLock提供了更多的方法，ReentrantLock，意思是“可重入锁”。\r\n\r\n以下是ReentrantLock的使用案例：\r\n\r\n　　例子1，lock()的正确使用方法\r\n	见代码MyLockTest\r\n\r\n例子2，tryLock()的使用方法\r\n见代码MyTryLock\r\n\r\n例子3，lockInterruptibly()响应中断的使用方法： \r\n见代码MyInterruptibly\r\n\r\n    • ReadWriteLock\r\n　　ReadWriteLock也是一个接口，在它里面只定义了两个方法：\r\npublic interface ReadWriteLock {\r\n    /**\r\n     * Returns the lock used for reading.\r\n     *\r\n     * @return the lock used for reading.\r\n     */\r\n    Lock readLock();\r\n \r\n    /**\r\n     * Returns the lock used for writing.\r\n     *\r\n     * @return the lock used for writing.\r\n     */\r\n    Lock writeLock();\r\n}\r\n　　一个用来获取读锁，一个用来获取写锁。也就是说将文件的读写操作分开，分成2个锁来分配给线程，从而使得多个线程可以同时进行读操作。下面的ReentrantReadWriteLock实现了ReadWriteLock接口。\r\n\r\n    • ReentrantReadWriteLock\r\n　　ReentrantReadWriteLock里面提供了很多丰富的方法，不过最主要的有两个方法：readLock()和writeLock()用来获取读锁和写锁。\r\n\r\n下面通过几个例子来看一下ReentrantReadWriteLock具体用法。\r\n例子1：　　假如有多个线程要同时进行读操作的话，先看一下synchronized达到的效果\r\n见代码MySynchronizedReadWrite \r\n\r\n例子2：改成用读写锁的话：\r\n见代码MyReentrantReadWriteLock\r\n \r\n注意：\r\n　　不过要注意的是，如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待释放读锁。\r\n如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程会一直等待释放写锁。\r\n　　\r\n    • Lock和synchronized的选择\r\n　　\r\n　　1）Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现；\r\n　　2）synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁；\r\n　　3）Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断；\r\n　　4）通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。\r\n　　5）Lock可以提高多个线程进行读操作的效率。\r\n　　在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。\r\n',12,0,0,1514754447,0,0,0),(271,1,'java并发包','','','        1. java并发包介绍\r\n	JDK5.0 以后的版本都引入了高级并发特性，大多数的特性在java.util.concurrent 包中，是专门用于多线程发编程的，充分利用了现代多处理器和多核心系统的功能以编写大规模并发应用程序。主要包含原子量、并发集合、同步器、可重入锁，并对线程池的构造提供\r\n了强力的支持。\r\n	线程池\r\n    • 线程池的5中创建方式：\r\n\r\n    1、 Single Thread Executor : 只有一个线程的线程池，因此所有提交的任务是顺序执行，\r\n代码： Executors.newSingleThreadExecutor()\r\n\r\n    2、 Cached Thread Pool : 线程池里有很多线程需要同时执行，老的可用线程将被新的任务触发重新执行，如果线程超过60秒内没执行，那么将被终止并从池中删除，\r\n代码：Executors.newCachedThreadPool()\r\n\r\n    3、 Fixed Thread Pool : 拥有固定线程数的线程池，如果没有任务执行，那么线程会一直等待，\r\n代码： Executors.newFixedThreadPool(4)\r\n在构造函数中的参数4是线程池的大小，你可以随意设置，也可以和cpu的数量保持一致，获取cpu的数量int cpuNums = Runtime.getRuntime().availableProcessors();\r\n\r\n    4、 Scheduled Thread Pool : 用来调度即将执行的任务的线程池，\r\n代码：Executors.newScheduledThreadPool()\r\n\r\n    5、 Single Thread Scheduled Pool : 只有一个线程，用来调度执行将来的任务，代码：Executors.newSingleThreadScheduledExecutor()\r\n\r\n    • 线程池的使用\r\n\r\n提交 Runnable ，任务完成后 Future 对象返回 null\r\n见代码：ThreadPoolWithRunable\r\n\r\n提交 Callable，该方法返回一个 Future 实例表示任务的状态\r\n见代码：ThreadPoolWithcallable\r\n        2. java并发包消息队列及在开源软件中的应用\r\nBlockingQueue也是java.util.concurrent下的主要用来控制线程同步的工具。\r\n主要的方法是：put、take一对阻塞存取；add、poll一对非阻塞存取。\r\n	插入:\r\n		1)add(anObject):把anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则抛出\r\n        2)offer(anObject):表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false.\r\n        3)put(anObject):把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续.\r\n	读取：\r\n        4)poll(time):取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null\r\n        5)take():取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到Blocking有新的对象被加入为止\r\n	其他\r\nint remainingCapacity();返回队列剩余的容量，在队列插入和获取的时候，不要瞎搞，数	据可能不准\r\nboolean remove(Object o); 从队列移除元素，如果存在，即移除一个或者更多，队列改	变了返回true\r\npublic boolean contains(Object o); 查看队列是否存在这个元素，存在返回true\r\nint drainTo(Collection&lt;? super E&gt; c); 传入的集合中的元素，如果在队列中存在，那么将	队列中的元素移动到集合中\r\nint drainTo(Collection&lt;? super E&gt; c, int maxElements); 和上面方法的区别在于，制定了移	动的数量\r\n\r\nBlockingQueue有四个具体的实现类,常用的两种实现类为：\r\n\r\n1、ArrayBlockingQueue：一个由数组支持的有界阻塞队列，规定大小的BlockingQueue,其构造函数必须带一个int参数来指明其大小.其所含的对象是以FIFO(先入先出)顺序排序的。\r\n\r\n2、LinkedBlockingQueue：大小不定的BlockingQueue,若其构造函数带一个规定大小的参数,生成的BlockingQueue有大小限制,若不带大小参数,所生成的BlockingQueue的大小由Integer.MAX_VALUE来决定.其所含的对象是以FIFO(先入先出)顺序排序的。 \r\n	LinkedBlockingQueue 可以指定容量，也可以不指定，不指定的话，默认最大是Integer.MAX_VALUE,其中主要用到put和take方法，put方法在队列满的时候会阻塞直到有队列成员被消费，take方法在队列空的时候会阻塞，直到有队列成员被放进来。\r\n\r\nLinkedBlockingQueue和ArrayBlockingQueue区别：\r\n\r\nLinkedBlockingQueue和ArrayBlockingQueue比较起来,它们背后所用的数据结构不一样,导致LinkedBlockingQueue的数据吞吐量要大于ArrayBlockingQueue,但在线程数量很大时其性能的可预见性低于ArrayBlockingQueue.\r\n\r\n生产者消费者的示例代码：\r\n见代码',12,0,0,1514754490,0,0,0),(272,1,' java并发编程的一些总结','','','        1. 不应用线程池的缺点\r\n有些开发者图省事，遇到需要多线程处理的地方，直接new Thread(...).start()，对于一般场景是没问题的，但如果是在并发请求很高的情况下，就会有些隐患：\r\n    • 新建线程的开销。线程虽然比进程要轻量许多，但对于JVM来说，新建一个线程的代价还是挺大的，决不同于新建一个对象\r\n    • 资源消耗量。没有一个池来限制线程的数量，会导致线程的数量直接取决于应用的并发量，这样有潜在的线程数据巨大的可能，那么资源消耗量将是巨大的\r\n    • 稳定性。当线程数量超过系统资源所能承受的程度，稳定性就会成问题\r\n        2. 制定执行策略\r\n在每个需要多线程处理的地方，不管并发量有多大，需要考虑线程的执行策略\r\n    • 任务以什么顺序执行\r\n    • 可以有多少个任何并发执行\r\n    • 可以有多少个任务进入等待执行队列\r\n    • 系统过载的时候，应该放弃哪些任务？如何通知到应用程序？\r\n    • 一个任务的执行前后应该做什么处理\r\n        3. 线程池的类型\r\n不管是通过Executors创建线程池，还是通过Spring来管理，都得清楚知道有哪几种线程池：\r\n    • FixedThreadPool：定长线程池，提交任务时创建线程，直到池的最大容量，如果有线程非预期结束，会补充新线程\r\n    • CachedThreadPool：可变线程池，它犹如一个弹簧，如果没有任务需求时，它回收空闲线程，如果需求增加，则按需增加线程，不对池的大小做限制\r\n    • SingleThreadExecutor：单线程。处理不过来的任务会进入FIFO队列等待执行\r\n    • SecheduledThreadPool：周期性线程池。支持执行周期性线程任务\r\n其实，这些不同类型的线程池都是通过构建一个ThreadPoolExecutor来完成的，所不同的是corePoolSize,maximumPoolSize,keepAliveTime,unit,workQueue,threadFactory这么几个参数。具体可以参见JDK DOC。\r\n        4. 线程池饱和策略\r\n由以上线程池类型可知，除了CachedThreadPool其他线程池都有饱和的可能，当饱和以后就需要相应的策略处理请求线程的任务，ThreadPoolExecutor采取的方式通过队列来存储这些任务，当然会根据池类型不同选择不同的队列，比如FixedThreadPool和SingleThreadExecutor默认采用的是无限长度的LinkedBlockingQueue。但从系统可控性讲，最好的做法是使用定长的ArrayBlockingQueue或有限的LinkedBlockingQueue，并且当达到上限时通过ThreadPoolExecutor.setRejectedExecutionHandler方法设置一个拒绝任务的策略，JDK提供了AbortPolicy、CallerRunsPolicy、DiscardPolicy、DiscardOldestPolicy几种策略，具体差异可见JDK DOC\r\n        5. 线程无依赖性\r\n多线程任务设计上尽量使得各任务是独立无依赖的，所谓依赖性可两个方面：\r\n    • 线程之间的依赖性。如果线程有依赖可能会造成死锁或饥饿\r\n    • 调用者与线程的依赖性。调用者得监视线程的完成情况，影响可并发量\r\n当然，在有些业务里确实需要一定的依赖性，比如调用者需要得到线程完成后结果，传统的Thread是不便完成的，因为run方法无返回值，只能通过一些共享的变量来传递结果，但在Executor框架里可以通过Future和Callable实现需要有返回值的任务，当然线程的异步性导致需要有相应机制来保证调用者能等待任务完成，关于Future和Callable的用法前文已讲解；',12,0,0,1514754546,0,0,0),(273,1,'java JMS技术','','','        1. 什么是JMS\r\n	JMS即Java消息服务（Java Message Service）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持。\r\n	JMS是一种与厂商无关的 API，用来访问消息收发系统消息。它类似于JDBC(Java Database Connectivity)：这里，JDBC 是可以用来访问许多不同关系数据库的 API，而 JMS 则提供同样与厂商无关的访问方法，以访问消息收发服务。许多厂商都支持 JMS，包括 IBM 的 MQSeries、BEA的 Weblogic JMS service和 Progress 的 SonicMQ，这只是几个例子。 JMS 使您能够通过消息收发服务（有时称为消息中介程序或路由器）从一个 JMS 客户机向另一个 JMS客户机发送消息。消息是 JMS 中的一种类型对象，由两部分组成：报头和消息主体。报头由路由信息以及有关该消息的元数据组成。消息主体则携带着应用程序的数据或有效负载。根据有效负载的类型来划分，可以将消息分为几种类型，它们分别携带：简单文本(TextMessage)、可序列化的对象 (ObjectMessage)、属性集合 (MapMessage)、字节流 (BytesMessage)、原始值流 (StreamMessage)，还有无有效负载的消息 (Message)。\r\n\r\n        2. JMS规范\r\n            2.1. 专业技术规范\r\nJMS（Java Messaging Service）是Java平台上有关面向消息中间件(MOM)的技术规范，它便于消息系统中的Java应用程序进行消息交换,并且通过提供标准的产生、发送、接收消息的接口简化企业应用的开发，翻译为Java消息服务。\r\n            2.2. 体系架构\r\nJMS由以下元素组成。\r\nJMS提供者：连接面向消息中间件的，JMS接口的一个实现。提供者可以是Java平台的JMS实现，也可以是非Java平台的面向消息中间件的适配器。\r\nJMS客户：生产或消费基于消息的Java的应用程序或对象。\r\nJMS生产者：创建并发送消息的JMS客户。\r\nJMS消费者：接收消息的JMS客户。\r\nJMS消息：包括可以在JMS客户之间传递的数据的对象\r\nJMS队列：一个容纳那些被发送的等待阅读的消息的区域。与队列名字所暗示的意思不同，消息的接受顺序并不一定要与消息的发送顺序相同。一旦一个消息被阅读，该消息将被从队列中移走。\r\nJMS主题：一种支持发送消息给多个订阅者的机制。\r\n\r\n            2.3. Java消息服务应用程序结构支持两种模型\r\n    1、 点对点或队列模型\r\n在点对点或队列模型下，一个生产者向一个特定的队列发布消息，一个消费者从该队列中读取消息。这里，生产者知道消费者的队列，并直接将消息发送到消费者的队列。\r\n\r\n这种模式被概括为：\r\n只有一个消费者将获得消息\r\n生产者不需要在接收者消费该消息期间处于运行状态，接收者也同样不需要在消息发送时处于运行状态。\r\n每一个成功处理的消息都由接收者签收\r\n2、发布者/订阅者模型\r\n发布者/订阅者模型支持向一个特定的消息主题发布消息。0或多个订阅者可能对接收来自特定消息主题的消息感兴趣。在这种模型下，发布者和订阅者彼此不知道对方。这种模式好比是匿名公告板。\r\n这种模式被概括为：\r\n多个消费者可以获得消息\r\n在发布者和订阅者之间存在时间依赖性。发布者需要建立一个订阅（subscription），以便客户能够订阅。订阅者必须保持持续的活动状态以接收消息，除非订阅者建立了持久的订阅。在那种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。\r\n使用Java语言，JMS提供了将应用与提供数据的传输层相分离的方式。同一组Java类可以通过JNDI中关于提供者的信息，连接不同的JMS提供者。这一组类首先使用一个连接工厂以连接到队列或主题，然后发送或发布消息。在接收端，客户接收或订阅这些消息。\r\n',12,0,0,1514754670,0,0,0),(274,1,'常用的JMS实现','','','要使用Java消息服务，你必须要有一个JMS提供者，管理会话和队列。既有开源的提供者也有专有的提供者。\r\n开源的提供者包括：\r\nApache ActiveMQ\r\nJBoss 社区所研发的 HornetQ\r\nJoram\r\nCoridan的MantaRay\r\nThe OpenJMS Group的OpenJMS\r\n专有的提供者包括：\r\nBEA的BEA WebLogic Server JMS\r\nTIBCO Software的EMS\r\nGigaSpaces Technologies的GigaSpaces\r\nSoftwired 2006的iBus\r\nIONA Technologies的IONA JMS\r\nSeeBeyond的IQManager（2005年8月被Sun Microsystems并购）\r\nwebMethods的JMS+ -\r\nmy-channels的Nirvana\r\nSonic Software的SonicMQ\r\nSwiftMQ的SwiftMQ\r\nIBM的WebSphere MQ\r\n',12,0,0,1514754714,0,0,0),(275,1,'java监控工具使用','','','            1.1. jconsole \r\n	jconsole是一种集成了上面所有命令功能的可视化工具，可以分析jvm的内存使用情况和线程等信息。\r\n启动jconsole\r\n通过JDK/bin目录下的“jconsole.exe”启动Jconsole后，将自动搜索出本机运行的所有虚拟机进程，不需要用户使用jps来查询了，双击其中一个进程即可开始监控。也可以“远程连接服务器，进行远程虚拟机的监控。”\r\n\r\n            1.2. jvisualvm\r\n提供了和jconsole的功能类似，提供了一大堆的插件。\r\n插件中，Visual GC（可视化GC）还是比较好用的，可视化GC可以看到内存的具体使用情况。',12,0,0,1514754782,0,0,0),(276,1,'java内存模型','','','            2.1. 内存模型图解\r\n	Java虚拟机在执行Java程序的过程中，会把它所管理的内存划分为若干个不同的数据区。这些区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有的区域则依赖用户线程的启动和结束而建立和销毁，我们可以将这些区域统称为Java运行时数据区域。\r\n	如下图是一个内存模型的关系图（详情见图：内存划分.png）：\r\n\r\n如上图所示，Java虚拟机运行时数据区域被分为五个区域：堆(Heap)、栈(Stack)、本地方法栈(Native Stack)、方法区(Method Area)、程序计数器(Program Count Register)。\r\n\r\n            2.2.  堆（Heap）\r\n	对于大多数应用来说，Java Heap是Java虚拟机管理的内存的最大一块，这块区域随着虚拟机的启动而创建。在实际的运用中，我们创建的对象和数组就是存放在堆里面。如果你听说线程安全的问题，就会很明确的知道Java Heap是一块共享的区域，操作共享区域的成员就有了锁和同步。\r\n	与Java Heap相关的还有Java的垃圾回收机制（GC）,Java Heap是垃圾回收器管理的主要区域。程序猿所熟悉的新生代、老生代、永久代的概念就是在堆里面，现在大多数的GC基本都采用了分代收集算法。如果再细致一点，Java Heap还有Eden空间，From Survivor空间,To Survivor空间等。\r\n	Java Heap可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。\r\n\r\n             栈（Stack）\r\n	相对于Java Heap来讲，Java Stack是线程私有的，她的生命周期与线程相同。Java Stack描述的是Java方法执行时的内存模型，每个方法执行时都会创建一个栈帧（Stack Frame）用语存储局部变量表、操作数栈、动态链接、方法出口等信息。从下图从可以看到，每个线程在执行一个方法时，都意味着有一个栈帧在当前线程对应的栈帧中入栈和出栈。\r\n\r\n            2.4. 本地方法栈（Native Stack）\r\n	本地方法栈（Native Stack）与Java虚拟机站（Java Stack）所发挥的作用非常相似，他们之间的区别在于虚拟机栈为虚拟机栈执行java方法（也就是字节码）服务，而本地方法栈则为使用到Native方法服务。\r\n\r\n            2.5. 方法区（Method Area）\r\n	方法区（Method Area）与堆（Java Heap）一样，是各个线程共享的内存区域，它用于存储虚拟机加载的类信息，常量，静态变量，即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是她却有一个别名叫做非堆（Non-Heap）。分析下Java虚拟机规范，之所以把方法区描述为堆的一个逻辑部分，应该觉得她们都是存储数据的角度出发的。一个存储对象数据（堆），一个存储静态信息(方法区)。\r\n	在上文中，我们看到堆中有新生代、老生代、永久代的描述。为什么我们将新生代、老生代、永久代三个概念一起说，那是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。这样HotSpot的垃圾收集器就能想管理Java堆一样管理这部分内存。简单点说就是HotSpot虚拟机中内存模型的分代，其中新生代和老生代在堆中，永久代使用方法区实现。根据官方发布的路线图信息，现在也有放弃永久代并逐步采用Native Memory来实现方法区的规划，在JDK1.7的HotSpot中，已经把原本放在永久代的字符串常量池移出。\r\n\r\n            2.6. 总结\r\n    1、 线程私有的数据区域有：\r\n Java虚拟机栈（Java Stack）\r\n本地方法栈（Native Stack）\r\n    2、 线程共有的数据区域有：\r\n堆（Java Heap）\r\n方法区',12,0,0,1514754880,0,0,0),(277,1,'JVM参数列表','','','java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m  -XX:MaxTenuringThreshold=0\r\n-Xmx3550m：最大堆内存为3550M。\r\n-Xms3550m：初始堆内存为3550m。\r\n此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。\r\n-Xmn2g：设置年轻代大小为2G。\r\n整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。\r\n-Xss128k：设置每个线程的堆栈大小。\r\nJDK5.0以后每个线程堆栈大小为1M，在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000左右。 \r\n-XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5\r\n-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。\r\n设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6\r\n-XX:MaxPermSize=16m:设置持久代大小为16m。\r\n-XX:MaxTenuringThreshold=0：设置垃圾最大年龄。\r\n如果设置为0的话，则年轻代对象不经过Survivor区，直 接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象 再年轻代的存活时间，增加在年轻代即被回收的概论。\r\n\r\n收集器设置\r\n-XX:+UseSerialGC:设置串行收集器\r\n-XX:+UseParallelGC:设置并行收集器\r\n-XX:+UseParalledlOldGC:设置并行年老代收集器\r\n-XX:+UseConcMarkSweepGC:设置并发收集器\r\n垃圾回收统计信息\r\n-XX:+PrintGC\r\n-XX:+PrintGCDetails\r\n-XX:+PrintGCTimeStamps\r\n-Xloggc:filename\r\n并行收集器设置\r\n-XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。\r\n-XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间\r\n-XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n)\r\n并发收集器设置\r\n-XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。\r\n-XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。',12,0,0,1514754931,0,0,0),(278,1,' jvm案例演示','','','内存：\r\n	内存标签相当于可视化的jstat命令，用于监视收集器管理的虚拟机内存（java堆和永久代）的变化趋势。\r\n\r\n我们通过下面的一段代码体验一下它的监视功能。运行时设置的虚拟机参数为：-Xms100m -Xmx100m -XX:+UseSerialGC，这段代码的作用是以64kb/50毫秒的速度往java堆内存中填充数据。\r\npublic class TestMemory {\r\n	static class OOMObject {\r\n		public byte[] placeholder = new byte[64 * 1024];\r\n	}\r\n\r\n	public static void fillHeap(int num) throws Exception {\r\n		ArrayList&lt;OOMObject&gt; list = new ArrayList&lt;OOMObject&gt;();\r\n		for (int i = 0; i &lt; num; i++) {\r\n			Thread.sleep(50);\r\n			list.add(new OOMObject());\r\n		}\r\n		System.gc();\r\n	}\r\n\r\n	public static void main(String[] args) throws Exception {\r\n		fillHeap(1000);\r\n		Thread.sleep(500000);\r\n	}\r\n}\r\n\r\n从图中可以看出，运行轨迹成曲线增长，循环1000次后，虽然整个新生代Eden和Survivor区都基本上被清空了，但是老年代仍然保持峰值状态，这说明，填充的数据在GC后仍然存活，因为list的作用域没有结束。如果把System.gc();移到fillHeap(1000);后，就可以全部回收掉。\r\n',12,0,0,1514755017,0,0,0),(279,1,'线程','','','线程相当于可视化了jstack命令，遇到线程停顿时，可以使用这个也签进行监控分析。线程长时间停顿的主要原因有：等待外部资源（数据库连接等），死循环、锁等待。下面的代码将演示这几种情况：\r\npackage cn.java.jvm;\r\n\r\nimport java.io.BufferedReader;\r\nimport java.io.IOException;\r\nimport java.io.InputStreamReader;\r\n\r\npublic class TestThread {\r\n	/**\r\n	 * 死循环演示\r\n	 * \r\n	 * @param args\r\n	 */\r\n	public static void createBusyThread() {\r\n		Thread thread = new Thread(new Runnable() {\r\n			@Override\r\n			public void run() {\r\n				System.out.println(&quot;createBusyThread&quot;);\r\n				while (true)\r\n					;\r\n			}\r\n		}, &quot;testBusyThread&quot;);\r\n		thread.start();\r\n	}\r\n\r\n	/**\r\n	 * 线程锁等待\r\n	 * \r\n	 * @param args\r\n	 */\r\n	public static void createLockThread(final Object lock) {\r\n		Thread thread = new Thread(new Runnable() {\r\n			@Override\r\n			public void run() {\r\n				System.out.println(&quot;createLockThread&quot;);\r\n				synchronized (lock) {\r\n					try {\r\n						lock.wait();\r\n					} catch (InterruptedException e) {\r\n						e.printStackTrace();\r\n					}\r\n				}\r\n\r\n			}\r\n		}, &quot;testLockThread&quot;);\r\n		thread.start();\r\n	}\r\n\r\n	public static void main(String[] args) throws Exception {\r\n		BufferedReader br = new BufferedReader(new InputStreamReader(System.in));\r\n		br.readLine();\r\n		createBusyThread();\r\n		br.readLine();\r\n		Object object = new Object();\r\n		createLockThread(object);\r\n	}\r\n}\r\nmain线程：追踪到需要键盘录入\r\ntestBusyThread线程：线程阻塞在18行的while（true），直到线程切换，很耗性能\r\ntestLockThread线程：出于waitting状态，等待notify\r\n\r\n死锁：\r\npackage cn.java.jvm;\r\n\r\npublic class TestDeadThread implements Runnable {\r\n	int a, b;\r\n\r\n	public TestDeadThread(int a, int b) {\r\n		this.a = a;\r\n		this.b = b;\r\n	}\r\n\r\n	@Override\r\n	public void run() {\r\n		System.out.println(&quot;createDeadThread&quot;);\r\n		synchronized (Integer.valueOf(a)) {\r\n			synchronized (Integer.valueOf(b)) {\r\n				System.out.println(a + b);\r\n			}\r\n		}\r\n	}\r\n\r\n	public static void main(String[] args) {\r\n		for (int i = 0; i &lt; 100; i++) {\r\n			new Thread(new TestDeadThread(1, 2)).start();\r\n			new Thread(new TestDeadThread(2, 1)).start();\r\n		}\r\n	}\r\n}\r\n点击检查死锁，会出现死锁的详情。',12,0,0,1514755056,0,0,0),(280,1,'java动态代理、反射','','','            5.1. 反射\r\n通过反射的方式可以获取class对象中的属性、方法、构造函数等，一下是实例：\r\n	\r\npackage cn.java.reflect;\r\n\r\nimport java.lang.reflect.Constructor;\r\nimport java.lang.reflect.Field;\r\nimport java.lang.reflect.Method;\r\nimport java.util.ArrayList;\r\nimport java.util.List;\r\n\r\nimport org.junit.Before;\r\nimport org.junit.Test;\r\n\r\npublic class MyReflect {\r\n	public String className = null;\r\n	@SuppressWarnings(&quot;rawtypes&quot;)\r\n	public Class personClass = null;\r\n	/**\r\n	 * 反射Person类\r\n	 * @throws Exception \r\n	 */\r\n	@Before\r\n	public void init() throws Exception {\r\n		className = &quot;cn.java.reflect.Person&quot;;\r\n		personClass = Class.forName(className);\r\n	}\r\n	/**\r\n	 *获取某个class文件对象\r\n	 */\r\n	@Test\r\n	public void getClassName() throws Exception {\r\n		System.out.println(personClass);\r\n	}\r\n	/**\r\n	 *获取某个class文件对象的另一种方式\r\n	 */\r\n	@Test\r\n	public void getClassName2() throws Exception {\r\n		System.out.println(Person.class);\r\n	}\r\n	/**\r\n	 *创建一个class文件表示的真实对象，底层会调用空参数的构造方法\r\n	 */\r\n	@Test\r\n	public void getNewInstance() throws Exception {\r\n		System.out.println(personClass.newInstance());\r\n	}\r\n	/**\r\n	 *获取非私有的构造函数\r\n	 */\r\n	@SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; })\r\n	@Test\r\n	public void getPublicConstructor() throws Exception {\r\n		Constructor  constructor  = personClass.getConstructor(Long.class,String.class);\r\n		Person person = (Person)constructor.newInstance(100L,&quot;zhangsan&quot;);\r\n		System.out.println(person.getId());\r\n		System.out.println(person.getName());\r\n	}\r\n	/**\r\n	 *获得私有的构造函数\r\n	 */\r\n	@SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; })\r\n	@Test\r\n	public void getPrivateConstructor() throws Exception {\r\n		Constructor con = personClass.getDeclaredConstructor(String.class);\r\n		con.setAccessible(true);//强制取消Java的权限检测\r\n		Person person2 = (Person)con.newInstance(&quot;zhangsan&quot;);\r\n		System.out.println(person2.getName());\r\n	}\r\n	/**\r\n	 *获取非私有的成员变量\r\n	 */\r\n	@SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; })\r\n	@Test\r\n	public void getNotPrivateField() throws Exception {\r\n		Constructor  constructor  = personClass.getConstructor(Long.class,String.class);\r\n		Object obj = constructor.newInstance(100L,&quot;zhangsan&quot;);\r\n		\r\n		Field field = personClass.getField(&quot;name&quot;);\r\n		field.set(obj, &quot;lisi&quot;);\r\n		System.out.println(field.get(obj));\r\n	}\r\n	/**\r\n	 *获取私有的成员变量\r\n	 */\r\n	@SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; })\r\n	@Test\r\n	public void getPrivateField() throws Exception {\r\n		Constructor  constructor  = personClass.getConstructor(Long.class);\r\n		Object obj = constructor.newInstance(100L);\r\n		\r\n		Field field2 = personClass.getDeclaredField(&quot;id&quot;);\r\n		field2.setAccessible(true);//强制取消Java的权限检测\r\n		field2.set(obj,10000L);\r\n		System.out.println(field2.get(obj));\r\n	}\r\n	/**\r\n	 *获取非私有的成员函数\r\n	 */\r\n	@SuppressWarnings({ &quot;unchecked&quot; })\r\n	@Test\r\n	public void getNotPrivateMethod() throws Exception {\r\n		System.out.println(personClass.getMethod(&quot;toString&quot;));\r\n		\r\n		Object obj = personClass.newInstance();//获取空参的构造函数\r\n		Object object = personClass.getMethod(&quot;toString&quot;).invoke(obj);\r\n		System.out.println(object);\r\n	}\r\n	/**\r\n	 *获取私有的成员函数\r\n	 */\r\n	@SuppressWarnings(&quot;unchecked&quot;)\r\n	@Test\r\n	public void getPrivateMethod() throws Exception {\r\n		Object obj = personClass.newInstance();//获取空参的构造函数\r\n		Method method = personClass.getDeclaredMethod(&quot;getSomeThing&quot;);\r\n		method.setAccessible(true);\r\n		Object value = method.invoke(obj);\r\n		System.out.println(value);\r\n\r\n	}\r\n	/**\r\n	 *\r\n	 */\r\n	@Test\r\n	public void otherMethod() throws Exception {\r\n		//当前加载这个class文件的那个类加载器对象\r\n		System.out.println(personClass.getClassLoader());\r\n		//获取某个类实现的所有接口\r\n		Class[] interfaces = personClass.getInterfaces();\r\n		for (Class class1 : interfaces) {\r\n			System.out.println(class1);\r\n		}\r\n		//反射当前这个类的直接父类\r\n		System.out.println(personClass.getGenericSuperclass());\r\n		/**\r\n		 * getResourceAsStream这个方法可以获取到一个输入流，这个输入流会关联到name所表示的那个文件上。\r\n		 */\r\n		//path 不以’/&#039;开头时默认是从此类所在的包下取资源，以’/&#039;开头则是从ClassPath根下获取。其只是通过path构造一个绝对路径，最终还是由ClassLoader获取资源。\r\n		System.out.println(personClass.getResourceAsStream(&quot;/log4j.properties&quot;));\r\n		//默认则是从ClassPath根下获取，path不能以’/&#039;开头，最终是由ClassLoader获取资源。\r\n		System.out.println(personClass.getResourceAsStream(&quot;/log4j.properties&quot;));\r\n		\r\n		//判断当前的Class对象表示是否是数组\r\n		System.out.println(personClass.isArray());\r\n		System.out.println(new String[3].getClass().isArray());\r\n		\r\n		//判断当前的Class对象表示是否是枚举类\r\n		System.out.println(personClass.isEnum());\r\n		System.out.println(Class.forName(&quot;cn.java.reflect.City&quot;).isEnum());\r\n		\r\n		//判断当前的Class对象表示是否是接口\r\n		System.out.println(personClass.isInterface());\r\n		System.out.println(Class.forName(&quot;cn.java.reflect.TestInterface&quot;).isInterface());\r\n		\r\n		\r\n	}\r\n}\r\n',12,0,0,1514755090,0,0,0),(281,1,'动态代理','','','在之前的代码调用阶段，我们用action调用service的方法实现业务即可。\r\n	由于之前在service中实现的业务可能不能够满足当先客户的要求，需要我们重新修改service中的方法，但是service的方法不只在我们这个模块使用，在其他模块也在调用，其他模块调用的时候，现有的service方法已经能够满足业务需求，所以我们不能只为了我们的业务而修改service，导致其他模块授影响。\r\n	那怎么办呢？\r\n	可以通过动态代理的方式，扩展我们的service中的方法实现，使得在原油的方法中增加更多的业务，而不是实际修改service中的方法，这种实现技术就叫做动态代理。\r\n\r\n动态代理：在不修改原业务的基础上，基于原业务方法，进行重新的扩展，实现新的业务。\r\n	例如下面的例子：\r\n    1、 旧业务\r\n买家调用action，购买衣服，衣服在数据库的标价为50元，购买流程就是简单的调用。\r\n    2、 新业务\r\n在原先的价格上可以使用优惠券，但是这个功能在以前没有实现过，我们通过代理类，代理了原先的接口方法，在这个方法的基础上，修改了返回值。\r\n\r\n	代理实现流程：\r\n    1、 书写代理类和代理方法，在代理方法中实现代理Proxy.newProxyInstance\r\n    2、 代理中需要的参数分别为：被代理的类的类加载器soneObjectclass.getClassLoader()，被代理类的所有实现接口new Class[] { Interface.class }，句柄方法new InvocationHandler()\r\n    3、 在句柄方法中复写invoke方法，invoke方法的输入有3个参数Object proxy（代理类对象）, Method method（被代理类的方法）,Object[] args（被代理类方法的传入参数），在这个方法中，我们可以定制化的开发新的业务。\r\n    4、 获取代理类，强转成被代理的接口\r\n    5、 最后，我们可以像没被代理一样，调用接口的认可方法，方法被调用后，方法名和参数列表将被传入代理类的invoke方法中，进行新业务的逻辑流程。\r\n		原业务接口IBoss\r\n\r\npublic interface IBoss {//接口\r\n	int yifu(String size);\r\n}\r\n原业务实现类\r\npublic class Boss implements IBoss{\r\n	public int yifu(String size){\r\n		System.err.println(&quot;天猫小强旗舰店，老板给客户发快递----衣服型号：&quot;+size);\r\n		//这件衣服的价钱，从数据库读取\r\n		return 50;\r\n	}\r\n	public void kuzi(){\r\n		System.err.println(&quot;天猫小强旗舰店，老板给客户发快递----裤子&quot;);\r\n	}\r\n}\r\n原业务调用\r\npublic class SaleAction {\r\n		@Test\r\n	public void saleByBossSelf() throws Exception {\r\n		IBoss boss = new Boss();\r\n		System.out.println(&quot;老板自营！&quot;);\r\n		int money = boss.yifu(&quot;xxl&quot;);\r\n		System.out.println(&quot;衣服成交价：&quot; + money);\r\n	}\r\n}\r\n代理类\r\npublic static IBoss getProxyBoss(final int discountCoupon) throws Exception {\r\n	Object proxedObj = Proxy.newProxyInstance(Boss.class.getClassLoader(),\r\n			new Class[] { IBoss.class }, new InvocationHandler() {\r\n				public Object invoke(Object proxy, Method method,\r\n						Object[] args) throws Throwable {\r\n						Integer returnValue = (Integer) method.invoke(new Boss(),\r\n								args);// 调用原始对象以后返回的值\r\n						return returnValue - discountCoupon;\r\n				}\r\n			});\r\n	return (IBoss)proxedObj;\r\n}\r\n}\r\n新业务调用\r\npublic class ProxySaleAction {\r\n		@Test\r\n	public void saleByProxy() throws Exception {\r\n		IBoss boss = ProxyBoss.getProxyBoss(20);// 将代理的方法实例化成接口\r\n		System.out.println(&quot;代理经营！&quot;);\r\n		int money = boss.yifu(&quot;xxl&quot;);// 调用接口的方法，实际上调用方式没有变\r\n		System.out.println(&quot;衣服成交价：&quot; + money);\r\n	}\r\n}\r\n',12,0,0,1514755173,0,0,0),(282,1,'安装hadoop2.4','','','安装hadoop2.4.1\r\n\r\n	先上传hadoop的安装包到服务器上去/home/hadoop/\r\n	注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop\r\n	伪分布式需要修改5个配置文件\r\n	3.1配置hadoop\r\n	第一个：hadoop-env.sh\r\n		vim hadoop-env.sh\r\n		#第27行\r\n		export JAVA_HOME=/usr/java/jdk1.7.0_65\r\n		\r\n	第二个：core-site.xml\r\n\r\n		&lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt;\r\n		&lt;property&gt;\r\n			&lt;name&gt;fs.defaultFS&lt;/name&gt;\r\n			&lt;value&gt;hdfs://weekend-1206-01:9000&lt;/value&gt;\r\n		&lt;/property&gt;\r\n		&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;\r\n		&lt;property&gt;\r\n			&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\r\n			&lt;value&gt;/home/hadoop/hadoop-2.4.1/tmp&lt;/value&gt;\r\n    &lt;/property&gt;\r\n		\r\n	第三个：hdfs-site.xml   \r\n		&lt;!-- 指定HDFS副本的数量 --&gt;\r\n		&lt;property&gt;\r\n			&lt;name&gt;dfs.replication&lt;/name&gt;\r\n			&lt;value&gt;1&lt;/value&gt;\r\n		&lt;/property&gt;\r\n		\r\n		&lt;property&gt;\r\n			&lt;name&gt;dfs.secondary.http.address&lt;/name&gt;\r\n			&lt;value&gt;192.168.1.152:50090&lt;/value&gt;\r\n		&lt;/property&gt;\r\n\r\n    \r\n    \r\n		\r\n	第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml)\r\n		mv mapred-site.xml.template mapred-site.xml\r\n		vim mapred-site.xml\r\n		&lt;!-- 指定mr运行在yarn上 --&gt;\r\n		&lt;property&gt;\r\n			&lt;name&gt;mapreduce.framework.name&lt;/name&gt;\r\n			&lt;value&gt;yarn&lt;/value&gt;\r\n		&lt;/property&gt;\r\n		\r\n	第五个：yarn-site.xml\r\n		&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt;\r\n		&lt;property&gt;\r\n			&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\r\n			&lt;value&gt;weekend-1206-01&lt;/value&gt;\r\n		&lt;/property&gt;\r\n		&lt;!-- reducer获取数据的方式 --&gt;\r\n		&lt;property&gt;\r\n			&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\r\n			&lt;value&gt;mapreduce_shuffle&lt;/value&gt;\r\n		&lt;/property&gt;\r\n     	\r\n	3.2将hadoop添加到环境变量\r\n	\r\n	vim /etc/proflie\r\n		export JAVA_HOME=/usr/java/jdk1.7.0_65\r\n		export HADOOP_HOME=/itcast/hadoop-2.4.1\r\n		export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\r\n\r\n	source /etc/profile\r\n	\r\n	3.3格式化namenode（是对namenode进行初始化）\r\n		hdfs namenode -format (hadoop namenode -format)\r\n		\r\n	3.4启动hadoop\r\n		先启动HDFS\r\n		sbin/start-dfs.sh\r\n		\r\n		再启动YARN\r\n		sbin/start-yarn.sh\r\n		\r\n	3.5验证是否启动成功\r\n		使用jps命令验证\r\n		27408 NameNode\r\n		28218 Jps\r\n		27643 SecondaryNameNode\r\n		28066 NodeManager\r\n		27803 ResourceManager\r\n		27512 DataNode\r\n	\r\n		http://192.168.1.101:50070 （HDFS管理界面）\r\n		http://192.168.1.101:8088 （MR管理界面）\r\n		',12,0,0,1514759504,0,0,0),(283,1,'配置ssh免密码登陆','','','ssh免登陆：\r\n1.生成key:ssh-keygen\r\n2.复制从A复制到B上: scp id_dsa.pub root@apeng3:~\r\n3.验证：ssh localhost/exit，ps -e|grep ssh\r\nssh A  #在B中执行\r\n\r\napeng1 192.168.179.135\r\napeng2 192.168.179.136\r\napeng3 192.168.179.137\r\napeng4 192.168.179.138\r\n\r\n配置三台主机无密码登录\r\nssh-keygen -t dsa -P &#039;&#039; -f ~/.ssh/id_dsa (apeng1,apeng2,apeng3三台主机都执行)\r\nssh-keygen -t rsa （四个回车）\r\n执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）,文件在 ~/.ssh目录下\r\n将公钥拷贝到要免密登陆的目标机器上\r\n\r\napeng1 vi /etc/hosts #配置主机\r\n192.168.179.135 apeng1\r\n192.168.179.136 apeng2\r\n192.168.179.137 apeng3\r\n192.168.179.138 apeng4\r\n\r\ncd /root/.ssh/\r\nscp id_dsa.pub root@apeng2:~\r\nscp id_dsa.pub root@apeng3:~\r\nscp id_dsa.pub root@apeng4:~\r\n\r\napeng2\r\ncat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys\r\n\r\napeng3\r\ncat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys\r\n\r\napeng4\r\ncat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys\r\n\r\nscp -r /etc/hosts root@apeng2:/etc/\r\nscp -r /etc/hosts root@apeng3:/etc/\r\nscp -r /etc/hosts root@apeng4:/etc/\r\n\r\napeng1\r\nssh apeng2\r\nssh apeng3\r\nssh apeng4\r\n\r\nhadoop 用户免密登录\r\napeng1\r\ncd /opt/app/hadoop-2.6.4/etc/hadoop\r\nssh-keygen(四个回车)\r\nssh-copy-id apeng1\r\nssh-copy-id apeng2\r\nssh-copy-id apeng3\r\nssh-copy-id apeng4',9,0,0,1514759609,0,0,0),(284,1,'hadoop安装环境','','','环境：虚拟机 VirtualBox，操作系统 64 位 CentOS 6.4\r\n下载重新编译需要的软件包\r\napache-ant-1.9.4-bin.tar.gz\r\nfindbugs-3.0.0.tar.gz\r\nprotobuf-2.5.0.tar.gz\r\napache-maven-3.0.5-bin.tar.gz\r\n下载 hadoop2.4.0 的源码包\r\nhadoop-2.4.0-src.tar.gz\r\n压解源码包\r\n[grid@hadoopMaster01 ~]$ tar -zxvf hadoop-2.4.0-src.tar.gz',13,0,0,1514759770,0,0,0),(285,1,'安装maven','','','1.下载maven（apache-maven-3.3.9-bin.tar.gz）\r\nhttp://archive.apache.org/dist/maven/maven-3/3.3.3/binaries/apache-maven-3.3.9-bin.tar.gz\r\n上传apache-maven-3.3.9-bin.tar.gz 到/opt/download\r\n\r\n2.安装maven\r\ntar -zxvf apache-maven-3.3.3-bin.tar.gz -C /usr/local\r\n\r\n3.添加环境变量\r\nvim /etc/profile\r\nexport MAVEN_HOME=/usr/local/apache-maven-3.3.9\r\nexport PATH=$PATH:$MAVEN_HOME/bin\r\n\r\n4.加载环境变量\r\nsource /etc/proflie\r\nmvn -version #使用 mvn -v 命令进行验证，如图所示表示安装配置成功',12,0,0,1515147309,0,0,0),(286,1,'编译hadoop','','','安装 ANT\r\n压解 apache-ant-1.9.4-bin.tar.gz 到/opt/目录\r\n[root@hadoopMaster01 grid]# tar -zxvf apache-ant-1.9.4-bin.tar.gz -C /opt/\r\n修改/etc/profile 配置，增加 ANT 环境配置 ANT_HOME   PATH\r\n保存后使用 source /etc/profile 使修改配置即时生效\r\n[root@hadoopMaster01 apache-ant-1.9.4]# source /etc/profile\r\n使用 ant-version 命令进行验证，如图所示表示安装配置成功\r\n安装 FINDBUGS\r\n压解 findbugs-3.0.0.tar.gz 到/opt/目录\r\n[root@hadoopMaster01 grid]# tar -zxvf findbugs-3.0.0.tar.gz -C /opt/\r\n修改/etc/profile 配置，增加 FINDBUGS 环境配置\r\n保存后使用 source /etc/profile 使修改配置即时生效\r\n[root@hadoopMaster01 apache-ant-1.9.4]# source /etc/profile\r\n使用 findbugs-version 命令进行验证，如图所示表示安装配置成功\r\n安装 PROTOBUF\r\n编译 Hadoop 2.4.0，需要 protobuf 的编译器protoc，一定要是 protobuf 2.5.0 以上\r\n直接压解 protobuf-2.5.0.tar.gz\r\n[root@hadoopMaster01 grid]# tar -zxvf protobuf-2.5.0.tar.gz\r\n安装 protobuf，依次执行如下命令\r\n[root@hadoopMaster01 grid]# cd protobuf-2.5.0\r\n[root@hadoopMaster01 protobuf-2.5.0]# ls\r\naclocal.m4 config.guess configure COPYING.txt examples\r\ninstall-sh ltmain.sh Makefile.in protobuf.pc.in src\r\nautogen.sh config.h.in configure.ac depcomp generate_descriptor_proto.sh\r\nINSTALL.txt m4 missing python vsprojects\r\nCHANGES.txt config.sub CONTRIBUTORS.txt editors gtest\r\njava Makefile.am protobuf-lite.pc.in README.txt\r\n[root@hadoopMaster01 protobuf-2.5.0]# ./configure\r\n[root@hadoopMaster01 protobuf-2.5.0]# make\r\n[root@hadoopMaster01 protobuf-2.5.0]# make check\r\n[root@hadoopMaster01 protobuf-2.5.0]# make install\r\n使用 protoc --version 命令进行验证，如图所示表示安装配置成功\r\n安装 依赖包\r\n安装 cmake,openssl-devel,ncurses-devel  依赖包(root  用户且能够连上互联网)\r\n[root@hadoopMaster01 ~]# yum install cmake\r\n如下图表示安装成功\r\n[root@hadoopMaster01 ~]# yum install openssl-devel\r\n如下图表示安装成功\r\n[root@hadoopMaster01 ~]# yum install ncurses-devel\r\n如下图表示依赖包系统中已经安装并且为最新版本\r\n编译 64  位本地库\r\n进入已压解的 hadoop 源码目录\r\n[grid@hadoopMaster01 ~]$ cd hadoop-2.4.0-src\r\n[grid@hadoopMaster01 hadoop-2.4.0-src]$ pwd\r\n/home/grid/hadoop-2.4.0-src\r\n执行 mvn clean install -DskipTests 命令，等待完成(会自动联网下载很多东西)\r\n[grid@hadoopMaster01 hadoop-2.4.0-src]$ mvn clean install -DskipTests\r\n执行 mvn package -Pdist,native -DskipTests -Dtar 命令，开始编译，等待完成\r\ngrid@hadoopMaster01 hadoop-2.4.0-src]$ mvn package -Pdist,native -DskipTests -Dtar\r\n\r\n表示编译成功\r\n进入/home/grid/hadoop-2.4.0-src/hadoop-dist/target/hadoop-2.4.0/lib/native 检查，使用 file *命\r\n令，如下图已经成功将编译 64 本地库\r\n将 64 位的 native 文件夹替换原 32 位的文件夹即可',13,0,0,1515147426,0,0,0),(287,1,'hadoop介绍','','','    1. HADOOP是apache旗下的一套开源软件平台\r\n    2. HADOOP提供的功能：利用服务器集群，根据用户的自定义业务逻辑，对海量数据进行分布式处理\r\n    3. HADOOP的核心组件有\r\n            A. HDFS（分布式文件系统）\r\n            B. YARN（运算资源调度系统）\r\n            C. MAPREDUCE（分布式运算编程框架）\r\n    4. 广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈',13,0,0,1515153334,0,0,0),(288,1,'HADOOP产生背景','','','    1. HADOOP最早起源于Nutch。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，遇到了严重的可扩展性问题——如何解决数十亿网页的存储和索引问题。\r\n    2. 2003年、2004年谷歌发表的两篇论文为该问题提供了可行的解决方案。\r\n——分布式文件系统（GFS），可用于处理海量网页的存储\r\n——分布式计算框架MAPREDUCE，可用于处理海量网页的索引计算问题。\r\n    3. Nutch的开发人员完成了相应的开源实现HDFS和MAPREDUCE，并从Nutch中剥离成为独立项目HADOOP，到2008年1月，HADOOP成为Apache顶级项目，迎来了它的快速发展期。',13,0,0,1515153363,0,0,0),(289,1,'HADOOP在大数据、云计算中的位置和关系','','','    1. 云计算是分布式计算、并行计算、网格计算、多核计算、网络存储、虚拟化、负载均衡等传统计算机技术和互联网技术融合发展的产物。借助IaaS(基础设施即服务)、PaaS(平台即服务)、SaaS（软件即服务）等业务模式，把强大的计算能力提供给终端用户。\r\n\r\n    2. 现阶段，云计算的两大底层支撑技术为“虚拟化”和“大数据技术”\r\n\r\n    3. 而HADOOP则是云计算的PaaS层的解决方案之一，并不等同于PaaS，更不等同于云计算本身。\r\n',13,0,0,1515153391,0,0,0),(290,1,'国内HADOOP的就业情况分析','','','    1、 HADOOP就业整体情况\r\n    A. 大数据产业已纳入国家十三五规划\r\n    B. 各大城市都在进行智慧城市项目建设，而智慧城市的根基就是大数据综合平台\r\n    C. 互联网时代数据的种类，增长都呈现爆发式增长，各行业对数据的价值日益重视\r\n    D. 相对于传统JAVAEE技术领域来说，大数据领域的人才相对稀缺\r\n    E. 随着现代社会的发展，数据处理和数据挖掘的重要性只会增不会减，因此，大数据技术是一个尚在蓬勃发展且具有长远前景的领域\r\n\r\n\r\n    2、 HADOOP就业职位要求\r\n大数据是个复合专业，包括应用开发、软件平台、算法、数据挖掘等，因此，大数据技术领域的就业选择是多样的，但就HADOOP而言，通常都需要具备以下技能或知识：\r\n    A. HADOOP分布式集群的平台搭建\r\n    B. HADOOP分布式文件系统HDFS的原理理解及使用\r\n    C. HADOOP分布式运算框架MAPREDUCE的原理理解及编程\r\n    D. Hive数据仓库工具的熟练应用\r\n    E. Flume、sqoop、oozie等辅助工具的熟练使用\r\n    F. Shell/python等脚本语言的开发能力\r\n\r\n    3、 HADOOP相关职位的薪资水平\r\n大数据技术或具体到HADOOP的就业需求目前主要集中在北上广深一线城市，薪资待遇普遍高于传统JAVAEE开发人员，以北京为例：',13,0,0,1515153487,0,0,0),(291,1,'HADOOP生态圈以及各组成部分的简介','','','HDFS：分布式文件系统\r\nMAPREDUCE：分布式运算程序开发框架\r\nHIVE：基于大数据技术（文件系统+运算框架）的SQL数据仓库工具\r\nHBASE：基于HADOOP的分布式海量数据库\r\nZOOKEEPER：分布式协调服务基础组件\r\nMahout：基于mapreduce/spark/flink等分布式运算框架的机器学习算法库\r\nOozie：工作流调度框架\r\nSqoop：数据导入导出工具\r\nFlume：日志数据采集框架',13,0,0,1515153521,0,0,0),(292,1,'分布式系统概述','','','注：由于大数据技术领域的各类技术框架基本上都是分布式系统，因此，理解hadoop、storm、spark等技术框架，都需要具备基本的分布式系统概念\r\n\r\n2.1 分布式软件系统(Distributed Software Systems)\r\n    • 该软件系统会划分成多个子系统或模块，各自运行在不同的机器上，子系统或模块之间通过网络通信进行协作，实现最终的整体功能\r\n    • 比如分布式操作系统、分布式程序设计语言及其编译(解释)系统、分布式文件系统和分布式数据库系统等。\r\n\r\n2.2 分布式软件系统举例：solrcloud \r\n    A. 一个solrcloud集群通常有多台solr服务器\r\n    B. 每一个solr服务器节点负责存储整个索引库的若干个shard（数据分片）\r\n    C. 每一个shard又有多台服务器存放若干个副本互为主备用\r\n    D. 索引的建立和查询会在整个集群的各个节点上并发执行\r\n    E. solrcloud集群作为整体对外服务，而其内部细节可对客户端透明\r\n总结：利用多个节点共同协作完成一项或多项具体业务功能的系统就是分布式系统。\r\n\r\n2.3 分布式应用系统模拟开发\r\n需求：可以实现由主节点将运算任务发往从节点，并将各从节点上的任务启动；\r\n程序清单：\r\nAppMaster\r\nAppSlave/APPSlaveThread\r\nTask\r\n程序运行逻辑流程：',13,0,0,1515153580,0,0,0),(293,1,'离线数据分析流程介绍','','','3.1.2 案例需求描述\r\n“Web点击流日志”包含着网站运营很重要的信息，通过日志分析，我们可以知道网站的访问量，哪个网页访问人数最多，哪个网页最有价值，广告转化率、访客的来源信息，访客的终端信息等。\r\n\r\n3.1.3 数据来源\r\n本案例的数据主要由用户的点击行为记录\r\n获取方式：在页面预埋一段js程序，为页面上想要监听的标签绑定事件，只要用户点击或移动到标签，即可触发ajax请求到后台servlet程序，用log4j记录下事件信息，从而在web服务器（nginx、tomcat等）上形成不断增长的日志文件。\r\n形如：\r\n58.215.204.118 - - [18/Sep/2013:06:51:35 +0000] &quot;GET /wp-includes/js/jquery/jquery.js?ver=1.10.2 HTTP/1.1&quot; 304 0 &quot;http://blog.fens.me/nodejs-socketio-chat/&quot; &quot;Mozilla/5.0 (Windows NT 5.1; rv:23.0) Gecko/20100101 Firefox/23.0&quot;\r\n\r\n\r\n\r\n3.2 数据处理流程\r\n3.2.1 流程图解析\r\n本案例跟典型的BI系统极其类似，整体流程如下：\r\n\r\n但是，由于本案例的前提是处理海量数据，因而，流程中各环节所使用的技术则跟传统BI完全不同，后续课程都会一一讲解：\r\n    1) 数据采集：定制开发采集程序，或使用开源框架FLUME\r\n    2) 数据预处理：定制开发mapreduce程序运行于hadoop集群\r\n    3) 数据仓库技术：基于hadoop之上的Hive\r\n    4) 数据导出：基于hadoop的sqoop数据导入导出工具\r\n    5) 数据可视化：定制开发web程序或使用kettle等产品\r\n    6) 整个过程的流程调度：hadoop生态圈中的oozie工具或其他类似开源产品\r\n\r\n3.2.2 项目技术架构图\r\n\r\n3.2.3 项目相关截图（感性认识，欣赏即可）\r\n    a) Mapreudce程序运行\r\n\r\n\r\n    b) 在Hive中查询数据\r\n    c) 将统计结果导入mysql\r\n./sqoop export --connect jdbc:mysql://localhost:3306/weblogdb --username root --password root  --table t_display_xx  --export-dir /user/hive/warehouse/uv/dt=2014-08-03\r\n\r\n\r\n\r\n',13,0,0,1515153653,0,0,0),(294,1,'集群搭建','','','4.1.1集群简介\r\nHADOOP集群具体来说包含两个集群：HDFS集群和YARN集群，两者逻辑上分离，但物理上常在一起\r\nHDFS集群：\r\n负责海量数据的存储，集群中的角色主要有 NameNode / DataNode\r\nYARN集群：\r\n负责海量数据运算时的资源调度，集群中的角色主要有 ResourceManager /NodeManager\r\n(那mapreduce是什么呢？它其实是一个应用程序开发包)\r\n\r\n本集群搭建案例，以5节点为例进行搭建，角色分配如下：\r\nhdp-node-01    NameNode  SecondaryNameNode\r\nhdp-node-02    ResourceManager \r\nhdp-node-03		DataNode    NodeManager\r\nhdp-node-04		DataNode    NodeManager\r\nhdp-node-05		DataNode    NodeManager\r\n部署图如下：\r\n\r\n4.1.2服务器准备\r\n本案例使用虚拟机服务器来搭建HADOOP集群，所用软件及版本：\r\n    • Vmware 11.0\r\n    • Centos  6.5  64bit\r\n\r\n4.1.3网络环境准备\r\n    • 采用NAT方式联网\r\n    • 网关地址：192.168.33.1\r\n    • 3个服务器节点IP地址：192.168.33.101、192.168.33.102、192.168.33.103\r\n    • 子网掩码：255.255.255.0\r\n4.1.4服务器系统设置\r\n    • 添加HADOOP用户\r\n    • 为HADOOP用户分配sudoer权限\r\n    • 同步时间\r\n    • 设置主机名\r\n        ◦ hdp-node-01\r\n        ◦ hdp-node-02\r\n        ◦ hdp-node-03\r\n    • 配置内网域名映射：\r\n        ◦ 192.168.33.101          hdp-node-01\r\n        ◦ 192.168.33.102          hdp-node-02\r\n        ◦ 192.168.33.103          hdp-node-03\r\n    • 配置ssh免密登陆\r\n    • 配置防火墙\r\n\r\n4.1.5 Jdk环境安装\r\n    • 上传jdk安装包\r\n    • 规划安装目录  /home/hadoop/apps/jdk_1.7.65\r\n    • 解压安装包\r\n    • 配置环境变量 /etc/profile\r\n\r\n\r\n4.1.6 HADOOP安装部署\r\n    • 上传HADOOP安装包\r\n    • 规划安装目录  /home/hadoop/apps/hadoop-2.6.1\r\n    • 解压安装包\r\n    • 修改配置文件  $HADOOP_HOME/etc/hadoop/\r\n最简化配置如下：\r\nvi  hadoop-env.sh\r\n# The java implementation to use.\r\nexport JAVA_HOME=/home/hadoop/apps/jdk1.7.0_51\r\n\r\nvi  core-site.xml\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;fs.defaultFS&lt;/name&gt;\r\n&lt;value&gt;hdfs://hdp-node-01:9000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\r\n&lt;value&gt;/home/HADOOP/apps/hadoop-2.6.1/tmp&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\nvi  hdfs-site.xml\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\r\n&lt;value&gt;/home/hadoop/data/name&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\r\n&lt;value&gt;/home/hadoop/data/data&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.replication&lt;/name&gt;\r\n&lt;value&gt;3&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.secondary.http.address&lt;/name&gt;\r\n&lt;value&gt;hdp-node-01:50090&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n\r\n\r\n\r\nvi  mapred-site.xml\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapreduce.framework.name&lt;/name&gt;\r\n&lt;value&gt;yarn&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n\r\nvi  yarn-site.xml\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\r\n&lt;value&gt;hadoop01&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\r\n&lt;value&gt;mapreduce_shuffle&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n\r\nvi  salves\r\nhdp-node-01\r\nhdp-node-02\r\nhdp-node-03\r\n\r\n\r\n4.1.7 启动集群\r\n初始化HDFS\r\nbin/hadoop  namenode  -format\r\n\r\n启动HDFS\r\nsbin/start-dfs.sh\r\n\r\n启动YARN\r\nsbin/start-yarn.sh\r\n',13,0,0,1515153686,0,0,0),(295,1,'hadoop 使用','','','1、上传文件到HDFS\r\n从本地上传一个文本文件到hdfs的/wordcount/input目录下\r\n[HADOOP@hdp-node-01 ~]$ HADOOP fs -mkdir -p /wordcount/input\r\n[HADOOP@hdp-node-01 ~]$ HADOOP fs -put /home/HADOOP/somewords.txt  /wordcount/input\r\n\r\n2、运行一个mapreduce程序\r\n在HADOOP安装目录下，运行一个示例mr程序\r\ncd $HADOOP_HOME/share/hadoop/mapreduce/\r\nhadoop jar mapredcue-example-2.6.1.jar wordcount /wordcount/input  /wordcount/output \r\n\r\n\r\n\r\n\r\n\r\n5 集群使用初步\r\n5.1 HDFS使用\r\n1、查看集群状态\r\n命令：   hdfs  dfsadmin  –report \r\n\r\n可以看出，集群共有3个datanode可用\r\n也可打开web控制台查看HDFS集群信息，在浏览器打开http://hdp-node-01:50070/\r\n\r\n2、上传文件到HDFS\r\n    • 查看HDFS中的目录信息\r\n命令：   hadoop  fs  –ls  /\r\n\r\n\r\n    • 上传文件\r\n命令：   hadoop  fs  -put  ./ scala-2.10.6.tgz  to  /\r\n\r\n\r\n    • 从HDFS下载文件\r\n命令：  hadoop  fs  -get  /yarn-site.xml\r\n\r\n\r\n\r\n\r\n\r\n',13,0,0,1515153723,0,0,0),(296,1,'MAPREDUCE使用','','','mapreduce是hadoop中的分布式运算编程框架，只要按照其编程规范，只需要编写少量的业务逻辑代码即可实现一个强大的海量数据并发处理程序\r\n5.2.1 Demo开发——wordcount\r\n1、需求\r\n从大量（比如T级别）文本文件中，统计出每一个单词出现的总次数\r\n\r\n2、mapreduce实现思路\r\nMap阶段：\r\n    a) 从HDFS的源数据文件中逐行读取数据\r\n    b) 将每一行数据切分出单词\r\n    c) 为每一个单词构造一个键值对(单词，1)\r\n    d) 将键值对发送给reduce\r\n\r\nReduce阶段：\r\n    a) 接收map阶段输出的单词键值对\r\n    b) 将相同单词的键值对汇聚成一组\r\n    c) 对每一组，遍历组中的所有“值”，累加求和，即得到每一个单词的总次数\r\n    d) 将(单词，总次数)输出到HDFS的文件中\r\n\r\n\r\n    4、 具体编码实现\r\n(1)定义一个mapper类\r\n//首先要定义四个泛型的类型\r\n//keyin:  LongWritable    valuein: Text\r\n//keyout: Text            valueout:IntWritable\r\n\r\npublic class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{\r\n	//map方法的生命周期：  框架每传一行数据就被调用一次\r\n	//key :  这一行的起始点在文件中的偏移量\r\n	//value: 这一行的内容\r\n	@Override\r\n	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n		//拿到一行数据转换为string\r\n		String line = value.toString();\r\n		//将这一行切分出各个单词\r\n		String[] words = line.split(&quot; &quot;);\r\n		//遍历数组，输出&lt;单词，1&gt;\r\n		for(String word:words){\r\n			context.write(new Text(word), new IntWritable(1));\r\n		}\r\n	}\r\n}\r\n\r\n(2)定义一个reducer类\r\n	//生命周期：框架每传递进来一个kv 组，reduce方法被调用一次\r\n	@Override\r\n	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\r\n		//定义一个计数器\r\n		int count = 0;\r\n		//遍历这一组kv的所有v，累加到count中\r\n		for(IntWritable value:values){\r\n			count += value.get();\r\n		}\r\n		context.write(key, new IntWritable(count));\r\n	}\r\n}\r\n\r\n(3)定义一个主类，用来描述job并提交job\r\npublic class WordCountRunner {\r\n	//把业务逻辑相关的信息（哪个是mapper，哪个是reducer，要处理的数据在哪里，输出的结果放哪里。。。。。。）描述成一个job对象\r\n	//把这个描述好的job提交给集群去运行\r\n	public static void main(String[] args) throws Exception {\r\n		Configuration conf = new Configuration();\r\n		Job wcjob = Job.getInstance(conf);\r\n		//指定我这个job所在的jar包\r\n//		wcjob.setJar(&quot;/home/hadoop/wordcount.jar&quot;);\r\n		wcjob.setJarByClass(WordCountRunner.class);\r\n		\r\n		wcjob.setMapperClass(WordCountMapper.class);\r\n		wcjob.setReducerClass(WordCountReducer.class);\r\n		//设置我们的业务逻辑Mapper类的输出key和value的数据类型\r\n		wcjob.setMapOutputKeyClass(Text.class);\r\n		wcjob.setMapOutputValueClass(IntWritable.class);\r\n		//设置我们的业务逻辑Reducer类的输出key和value的数据类型\r\n		wcjob.setOutputKeyClass(Text.class);\r\n		wcjob.setOutputValueClass(IntWritable.class);\r\n		\r\n		//指定要处理的数据所在的位置\r\n		FileInputFormat.setInputPaths(wcjob, &quot;hdfs://hdp-server01:9000/wordcount/data/big.txt&quot;);\r\n		//指定处理完成之后的结果所保存的位置\r\n		FileOutputFormat.setOutputPath(wcjob, new Path(&quot;hdfs://hdp-server01:9000/wordcount/output/&quot;));\r\n		\r\n		//向yarn集群提交这个job\r\n		boolean res = wcjob.waitForCompletion(true);\r\n		System.exit(res?0:1);\r\n	}\r\n\r\n\r\n\r\n\r\n5.2.2 程序打包运行\r\n    1. 将程序打包\r\n    2. 准备输入数据\r\nvi  /home/hadoop/test.txt\r\nHello tom\r\nHello jim\r\nHello ketty\r\nHello world\r\nKetty tom\r\n在hdfs上创建输入数据文件夹：\r\nhadoop   fs  mkdir  -p  /wordcount/input\r\n将words.txt上传到hdfs上\r\n	hadoop  fs  –put  /home/hadoop/words.txt  /wordcount/input\r\n\r\n\r\n    3. 将程序jar包上传到集群的任意一台服务器上\r\n\r\n    4. 使用命令启动执行wordcount程序jar包\r\n$ hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriver /wordcount/input /wordcount/out\r\n\r\n    5. 查看执行结果\r\n$ hadoop fs –cat /wordcount/out/part-r-00000\r\n',13,0,0,1515153773,0,0,0),(297,1,'hdfs介绍','','','Hadoop HDFS\r\n分布式文件系统DFS简介\r\n\r\nHDFS的系统组成介绍\r\n\r\nHDFS的组成部分详解\r\n\r\n副本存放策略及路由规则\r\n\r\n命令行接口\r\n\r\nJava接口\r\n\r\n客户端与HDFS的数据流讲解\r\n\r\nHDFS前言\r\n    • 设计思想\r\n分而治之：将大文件、大批量文件，分布式存放在大量服务器上，以便于采取分而治之的方式对海量数据进行运算分析；\r\n\r\n    • 在大数据系统中作用：\r\n为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务\r\n\r\n    • 重点概念：文件切块，副本存放，元数据',13,0,0,1515153830,0,0,0),(298,1,'HDFS的概念和特性','','','首先，它是一个文件系统，用于存储文件，通过统一的命名空间——目录树来定位文件\r\n\r\n其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色；\r\n\r\n重要特性如下：\r\n    （1） HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M\r\n\r\n    （2） HDFS文件系统会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data\r\n\r\n    （3） 目录结构及文件分块信息(元数据)的管理由namenode节点承担\r\n——namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器）\r\n\r\n    （4） 文件的各个block的存储管理由datanode节点承担\r\n---- datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication）\r\n\r\n    （5） HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改\r\n\r\n(注：适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高)',13,0,0,1515153908,0,0,0),(299,1,'hdfs的shell操作','','','HDFS命令行客户端使用\r\nHDFS提供shell命令行客户端，使用方法如下：\r\n\r\n\r\n\r\n3.2 命令行客户端支持的命令参数\r\n  [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]\r\n        [-cat [-ignoreCrc] &lt;src&gt; ...]\r\n        [-checksum &lt;src&gt; ...]\r\n        [-chgrp [-R] GROUP PATH...]\r\n        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]\r\n        [-chown [-R] [OWNER][:[GROUP]] PATH...]\r\n        [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]\r\n        [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\r\n        [-count [-q] &lt;path&gt; ...]\r\n        [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]\r\n        [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]\r\n        [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]\r\n        [-df [-h] [&lt;path&gt; ...]]\r\n        [-du [-s] [-h] &lt;path&gt; ...]\r\n        [-expunge]\r\n        [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\r\n        [-getfacl [-R] &lt;path&gt;]\r\n        [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]\r\n        [-help [cmd ...]]\r\n        [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]\r\n        [-mkdir [-p] &lt;path&gt; ...]\r\n        [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]\r\n        [-moveToLocal &lt;src&gt; &lt;localdst&gt;]\r\n        [-mv &lt;src&gt; ... &lt;dst&gt;]\r\n        [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]\r\n        [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]\r\n        [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]\r\n        [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]\r\n        [-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]\r\n        [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]\r\n        [-stat [format] &lt;path&gt; ...]\r\n        [-tail [-f] &lt;file&gt;]\r\n        [-test -[defsz] &lt;path&gt;]\r\n        [-text [-ignoreCrc] &lt;src&gt; ...]\r\n        [-touchz &lt;path&gt; ...]\r\n        [-usage [cmd ...]]\r\n\r\n常用命令参数介绍\r\n-help             \r\n功能：输出这个命令参数手册\r\n-ls                  \r\n功能：显示目录信息\r\n示例： hadoop fs -ls hdfs://hadoop-server01:9000/\r\n备注：这些参数中，所有的hdfs路径都可以简写\r\n--&gt;hadoop fs -ls /   等同于上一条命令的效果\r\n-mkdir              \r\n功能：在hdfs上创建目录\r\n示例：hadoop fs  -mkdir  -p  /aaa/bbb/cc/dd\r\n-moveFromLocal            \r\n功能：从本地剪切粘贴到hdfs\r\n示例：hadoop  fs  - moveFromLocal  /home/hadoop/a.txt  /aaa/bbb/cc/dd\r\n-moveToLocal              \r\n功能：从hdfs剪切粘贴到本地\r\n示例：hadoop  fs  - moveToLocal   /aaa/bbb/cc/dd  /home/hadoop/a.txt \r\n--appendToFile  \r\n功能：追加一个文件到已经存在的文件末尾\r\n示例：hadoop  fs  -appendToFile  ./hello.txt  hdfs://hadoop-server01:9000/hello.txt\r\n可以简写为：\r\nHadoop  fs  -appendToFile  ./hello.txt  /hello.txt\r\n\r\n-cat  \r\n功能：显示文件内容  \r\n示例：hadoop fs -cat  /hello.txt\r\n\r\n-tail                 \r\n功能：显示一个文件的末尾\r\n示例：hadoop  fs  -tail  /weblog/access_log.1\r\n-text                  \r\n功能：以字符形式打印一个文件的内容\r\n示例：hadoop  fs  -text  /weblog/access_log.1\r\n-chgrp \r\n-chmod\r\n-chown\r\n功能：linux文件系统中的用法一样，对文件所属权限\r\n示例：\r\nhadoop  fs  -chmod  666  /hello.txt\r\nhadoop  fs  -chown  someuser:somegrp   /hello.txt\r\n-copyFromLocal    \r\n功能：从本地文件系统中拷贝文件到hdfs路径去\r\n示例：hadoop  fs  -copyFromLocal  ./jdk.tar.gz  /aaa/\r\n-copyToLocal      \r\n功能：从hdfs拷贝到本地\r\n示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz\r\n-cp              \r\n功能：从hdfs的一个路径拷贝hdfs的另一个路径\r\n示例： hadoop  fs  -cp  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2\r\n\r\n-mv                     \r\n功能：在hdfs目录中移动文件\r\n示例： hadoop  fs  -mv  /aaa/jdk.tar.gz  /\r\n-get              \r\n功能：等同于copyToLocal，就是从hdfs下载文件到本地\r\n示例：hadoop fs -get  /aaa/jdk.tar.gz\r\n-getmerge             \r\n功能：合并下载多个文件\r\n示例：比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,...\r\nhadoop fs -getmerge /aaa/log.* ./log.sum\r\n-put                \r\n功能：等同于copyFromLocal\r\n示例：hadoop  fs  -put  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2\r\n\r\n-rm                \r\n功能：删除文件或文件夹\r\n示例：hadoop fs -rm -r /aaa/bbb/\r\n\r\n-rmdir                 \r\n功能：删除空目录\r\n示例：hadoop  fs  -rmdir   /aaa/bbb/ccc\r\n-df               \r\n功能：统计文件系统的可用空间信息\r\n示例：hadoop  fs  -df  -h  /\r\n\r\n-du \r\n功能：统计文件夹的大小信息\r\n示例：\r\nhadoop  fs  -du  -s  -h /aaa/*\r\n\r\n-count         \r\n功能：统计一个指定目录下的文件节点数量\r\n示例：hadoop fs -count /aaa/\r\n\r\n-setrep                \r\n功能：设置hdfs中文件的副本数量\r\n示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz\r\n&lt;这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量&gt;\r\n\r\n\r\n\r\n',13,0,0,1515153983,0,0,0),(300,1,'hdfs的工作机制','','','（工作机制的学习主要是为加深对分布式系统的理解，以及增强遇到各种问题时的分析解决能力，形成一定的集群运维能力）\r\n\r\n注：很多不是真正理解hadoop技术体系的人会常常觉得HDFS可用于网盘类应用，但实际并非如此。要想将技术准确用在恰当的地方，必须对技术有深刻的理解\r\n4.1 概述\r\n    1. HDFS集群分为两大角色：NameNode、DataNode\r\n    2. NameNode负责管理整个文件系统的元数据\r\n    3. DataNode 负责管理用户的文件数据块\r\n    4. 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上\r\n    5. 每一个文件块可以有多个副本，并存放在不同的datanode上\r\n    6. Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量\r\n    7. HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行\r\n\r\n\r\n4.2 HDFS写数据流程\r\n4.2.1 概述\r\n客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他datanode复制block的副本\r\n4.2.2 详细步骤图\r\n\r\n4.2.3 详细步骤解析\r\n1、根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在\r\n2、namenode返回是否可以上传\r\n3、client请求第一个 block该传输到哪些datanode服务器上\r\n4、namenode返回3个datanode服务器ABC\r\n5、client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端\r\n6、client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答\r\n7、当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。\r\n4.3. HDFS读数据流程\r\n4.3.1 概述\r\n客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件\r\n\r\n4.3.2 详细步骤图\r\n\r\n\r\n4.3.3 详细步骤解析\r\n1、跟namenode通信查询元数据，找到文件块所在的datanode服务器\r\n2、挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流\r\n3、datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）\r\n4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件\r\n\r\n',13,0,0,1515154037,0,0,0),(301,1,'NAMENODE工作机制','','','学习目标：理解namenode的工作机制尤其是元数据管理机制，以增强对HDFS工作原理的理解，及培养hadoop集群运营中“性能调优”、“namenode”故障问题的分析解决能力\r\n\r\n问题场景：\r\n1、集群启动后，可以查看文件，但是上传文件时报错，打开web页面可看到namenode正处于safemode状态，怎么处理？\r\n2、Namenode服务器的磁盘故障导致namenode宕机，如何挽救集群及数据？\r\n3、Namenode是否可以有多个？namenode内存要配置多大？namenode跟集群数据存储能力有关系吗？\r\n4、文件的blocksize究竟调大好还是调小好？\r\n……\r\n\r\n诸如此类问题的回答，都需要基于对namenode自身的工作原理的深刻理解\r\n\r\n5.1 NAMENODE职责\r\nNAMENODE职责：\r\n负责客户端请求的响应\r\n元数据的管理（查询，修改）',13,0,0,1515154085,0,0,0),(302,1,'元数据管理','','','namenode对数据的管理采用了三种存储形式：\r\n内存元数据(NameSystem)\r\n磁盘元数据镜像文件\r\n数据操作日志文件（可通过日志运算出元数据）\r\n5.2.1 元数据存储机制\r\nA、内存中有一份完整的元数据(内存meta data)\r\nB、磁盘有一个“准完整”的元数据镜像（fsimage）文件(在namenode的工作目录中)\r\n\r\nC、用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中\r\n5.2.2 元数据手动查看\r\n可以通过hdfs的一个工具来查看edits中的信息\r\nbin/hdfs oev -i edits -o edits.xml\r\nbin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml\r\n\r\n5.2.3 元数据的checkpoint\r\n每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）\r\n\r\ncheckpoint的详细过程\r\ncheckpoint操作的触发条件配置参数\r\ndfs.namenode.checkpoint.check.period=60  #检查触发条件是否满足的频率，60秒\r\ndfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary\r\n#以上两个参数做checkpoint操作时，secondary namenode的本地工作目录\r\ndfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}\r\n\r\ndfs.namenode.checkpoint.max-retries=3  #最大重试次数\r\ndfs.namenode.checkpoint.period=3600  #两次checkpoint之间的时间间隔3600秒\r\ndfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录\r\ncheckpoint的附带作用\r\nnamenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据',13,0,0,1515154154,0,0,0),(303,1,'DATANODE的工作机制','','','问题场景：\r\n1、集群容量不够，怎么扩容？\r\n2、如果有一些datanode宕机，该怎么办？\r\n3、datanode明明已启动，但是集群中的可用datanode列表中就是没有，怎么办？\r\n\r\n以上这类问题的解答，有赖于对datanode工作机制的深刻理解\r\n6.1 概述\r\n1、Datanode工作职责：\r\n存储管理用户的文件块数据\r\n定期向namenode汇报自身所持有的block信息（通过心跳信息上报）\r\n（这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题）\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt;\r\n	&lt;value&gt;3600000&lt;/value&gt;\r\n	&lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;\r\n&lt;/property&gt;\r\n\r\n2、Datanode掉线判断时限参数\r\ndatanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：\r\n	timeout  = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。\r\n	而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。\r\n	需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。\r\n\r\n\r\n&lt;property&gt;\r\n        &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt;\r\n        &lt;value&gt;2000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n        &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;\r\n        &lt;value&gt;1&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n观察验证DATANODE功能\r\n上传一个文件，观察文件的block具体的物理存放情况：\r\n\r\n在每一台datanode机器上的这个目录中能找到文件的切块：\r\n/home/hadoop/app/hadoop-2.4.1/tmp/dfs/data/current/BP-193442119-192.168.2.120-1432457733977/current/finalized',13,0,0,1515154234,0,0,0),(304,1,'HDFS的java操作','','','hdfs在生产应用中主要是客户端的开发，其核心步骤是从hdfs提供的api中构造一个HDFS的访问客户端对象，然后通过该客户端对象操作（增删改查）HDFS上的文件\r\n7.1 搭建开发环境\r\n1、引入依赖\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\r\n    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;\r\n    &lt;version&gt;2.6.1&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n\r\n注：如需手动引入jar包，hdfs的jar包----hadoop的安装目录的share下\r\n2、window下开发的说明\r\n建议在linux下进行hadoop应用的开发，不会存在兼容性问题。如在window上做客户端应用开发，需要设置以下环境：\r\n    A、 在windows的某个目录下解压一个hadoop的安装包\r\n    B、 将安装包下的lib和bin目录用对应windows版本平台编译的本地库替换\r\n    C、 在window系统中配置HADOOP_HOME指向你解压的安装包\r\n    D、 在windows系统的path变量中加入hadoop的bin目录\r\n获取api中的客户端对象\r\n在java中操作hdfs，首先要获得一个客户端实例\r\nConfiguration conf = new Configuration()\r\nFileSystem fs = FileSystem.get(conf)\r\n\r\n而我们的操作目标是HDFS，所以获取到的fs对象应该是DistributedFileSystem的实例；\r\nget方法是从何处判断具体实例化那种客户端类呢？\r\n从conf中的一个参数 fs.defaultFS的配置值判断；\r\n\r\n如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:///，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象',13,0,0,1515154309,0,0,0),(305,1,'文件的增删改查','','','HDFS客户端操作数据代码示例：\r\n7.4.1 文件的增删改查\r\npublic class HdfsClient {\r\n\r\n	FileSystem fs = null;\r\n\r\n	@Before\r\n	public void init() throws Exception {\r\n\r\n		// 构造一个配置参数对象，设置一个参数：我们要访问的hdfs的URI\r\n		// 从而FileSystem.get()方法就知道应该是去构造一个访问hdfs文件系统的客户端，以及hdfs的访问地址\r\n		// new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml\r\n		// 然后再加载classpath下的hdfs-site.xml\r\n		Configuration conf = new Configuration();\r\n		conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hdp-node01:9000&quot;);\r\n		/**\r\n		 * 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置\r\n		 */\r\n		conf.set(&quot;dfs.replication&quot;, &quot;3&quot;);\r\n\r\n		// 获取一个hdfs的访问客户端，根据参数，这个实例应该是DistributedFileSystem的实例\r\n		// fs = FileSystem.get(conf);\r\n\r\n		// 如果这样去获取，那conf里面就可以不要配&quot;fs.defaultFS&quot;参数，而且，这个客户端的身份标识已经是hadoop用户\r\n		fs = FileSystem.get(new URI(&quot;hdfs://hdp-node01:9000&quot;), conf, &quot;hadoop&quot;);\r\n\r\n	}\r\n\r\n	/**\r\n	 * 往hdfs上传文件\r\n	 * \r\n	 * @throws Exception\r\n	 */\r\n	@Test\r\n	public void testAddFileToHdfs() throws Exception {\r\n\r\n		// 要上传的文件所在的本地路径\r\n		Path src = new Path(&quot;g:/redis-recommend.zip&quot;);\r\n		// 要上传到hdfs的目标路径\r\n		Path dst = new Path(&quot;/aaa&quot;);\r\n		fs.copyFromLocalFile(src, dst);\r\n		fs.close();\r\n	}\r\n\r\n	/**\r\n	 * 从hdfs中复制文件到本地文件系统\r\n	 * \r\n	 * @throws IOException\r\n	 * @throws IllegalArgumentException\r\n	 */\r\n	@Test\r\n	public void testDownloadFileToLocal() throws IllegalArgumentException, IOException {\r\n		fs.copyToLocalFile(new Path(&quot;/jdk-7u65-linux-i586.tar.gz&quot;), new Path(&quot;d:/&quot;));\r\n		fs.close();\r\n	}\r\n\r\n	@Test\r\n	public void testMkdirAndDeleteAndRename() throws IllegalArgumentException, IOException {\r\n\r\n		// 创建目录\r\n		fs.mkdirs(new Path(&quot;/a1/b1/c1&quot;));\r\n\r\n		// 删除文件夹 ，如果是非空文件夹，参数2必须给值true\r\n		fs.delete(new Path(&quot;/aaa&quot;), true);\r\n\r\n		// 重命名文件或文件夹\r\n		fs.rename(new Path(&quot;/a1&quot;), new Path(&quot;/a2&quot;));\r\n\r\n	}\r\n\r\n	/**\r\n	 * 查看目录信息，只显示文件\r\n	 * \r\n	 * @throws IOException\r\n	 * @throws IllegalArgumentException\r\n	 * @throws FileNotFoundException\r\n	 */\r\n	@Test\r\n	public void testListFiles() throws FileNotFoundException, IllegalArgumentException, IOException {\r\n\r\n		// 思考：为什么返回迭代器，而不是List之类的容器\r\n		RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true);\r\n\r\n		while (listFiles.hasNext()) {\r\n			LocatedFileStatus fileStatus = listFiles.next();\r\n			System.out.println(fileStatus.getPath().getName());\r\n			System.out.println(fileStatus.getBlockSize());\r\n			System.out.println(fileStatus.getPermission());\r\n			System.out.println(fileStatus.getLen());\r\n			BlockLocation[] blockLocations = fileStatus.getBlockLocations();\r\n			for (BlockLocation bl : blockLocations) {\r\n				System.out.println(&quot;block-length:&quot; + bl.getLength() + &quot;--&quot; + &quot;block-offset:&quot; + bl.getOffset());\r\n				String[] hosts = bl.getHosts();\r\n				for (String host : hosts) {\r\n					System.out.println(host);\r\n				}\r\n			}\r\n			System.out.println(&quot;--------------为angelababy打印的分割线--------------&quot;);\r\n		}\r\n	}\r\n\r\n	/**\r\n	 * 查看文件及文件夹信息\r\n	 * \r\n	 * @throws IOException\r\n	 * @throws IllegalArgumentException\r\n	 * @throws FileNotFoundException\r\n	 */\r\n	@Test\r\n	public void testListAll() throws FileNotFoundException, IllegalArgumentException, IOException {\r\n\r\n		FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;));\r\n\r\n		String flag = &quot;d--             &quot;;\r\n		for (FileStatus fstatus : listStatus) {\r\n			if (fstatus.isFile())  flag = &quot;f--         &quot;;\r\n			System.out.println(flag + fstatus.getPath().getName());\r\n		}\r\n	}\r\n}\r\n',13,0,0,1515154342,0,0,0),(306,1,'通过流的方式访问hdfs','','','/**\r\n * 相对那些封装好的方法而言的更底层一些的操作方式\r\n * 上层那些mapreduce   spark等运算框架，去hdfs中获取数据的时候，就是调的这种底层的api\r\n * @author\r\n *\r\n */\r\npublic class StreamAccess {\r\n	\r\n	FileSystem fs = null;\r\n\r\n	@Before\r\n	public void init() throws Exception {\r\n\r\n		Configuration conf = new Configuration();\r\n		fs = FileSystem.get(new URI(&quot;hdfs://hdp-node01:9000&quot;), conf, &quot;hadoop&quot;);\r\n\r\n	}\r\n	\r\n	\r\n	\r\n	@Test\r\n	public void testDownLoadFileToLocal() throws IllegalArgumentException, IOException{\r\n		\r\n		//先获取一个文件的输入流----针对hdfs上的\r\n		FSDataInputStream in = fs.open(new Path(&quot;/jdk-7u65-linux-i586.tar.gz&quot;));\r\n		\r\n		//再构造一个文件的输出流----针对本地的\r\n		FileOutputStream out = new FileOutputStream(new File(&quot;c:/jdk.tar.gz&quot;));\r\n		\r\n		//再将输入流中数据传输到输出流\r\n		IOUtils.copyBytes(in, out, 4096);\r\n		\r\n		\r\n	}\r\n	\r\n	\r\n	/**\r\n	 * hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度\r\n	 * 用于上层分布式运算框架并发处理数据\r\n	 * @throws IllegalArgumentException\r\n	 * @throws IOException\r\n	 */\r\n	@Test\r\n	public void testRandomAccess() throws IllegalArgumentException, IOException{\r\n		//先获取一个文件的输入流----针对hdfs上的\r\n		FSDataInputStream in = fs.open(new Path(&quot;/iloveyou.txt&quot;));\r\n		\r\n		\r\n		//可以将流的起始偏移量进行自定义\r\n		in.seek(22);\r\n		\r\n		//再构造一个文件的输出流----针对本地的\r\n		FileOutputStream out = new FileOutputStream(new File(&quot;c:/iloveyou.line.2.txt&quot;));\r\n		\r\n		IOUtils.copyBytes(in,out,19L,true);\r\n		\r\n	}\r\n	\r\n	\r\n	\r\n	/**\r\n	 * 显示hdfs上文件的内容\r\n	 * @throws IOException \r\n	 * @throws IllegalArgumentException \r\n	 */\r\n	@Test\r\n	public void testCat() throws IllegalArgumentException, IOException{\r\n		\r\n		FSDataInputStream in = fs.open(new Path(&quot;/iloveyou.txt&quot;));\r\n		\r\n		IOUtils.copyBytes(in, System.out, 1024);\r\n	}\r\n}',13,0,0,1515154381,0,0,0),(307,1,'场景编程','','','在mapreduce 、spark等运算框架中，有一个核心思想就是将运算移往数据，或者说，就是要在并发计算中尽可能让运算本地化，这就需要获取数据所在位置的信息并进行相应范围读取\r\n以下模拟实现：获取一个文件的所有block位置信息，然后读取指定block中的内容\r\n	@Test\r\n	public void testCat() throws IllegalArgumentException, IOException{\r\n		\r\n		FSDataInputStream in = fs.open(new Path(&quot;/weblog/input/access.log.10&quot;));\r\n		//拿到文件信息\r\n		FileStatus[] listStatus = fs.listStatus(new Path(&quot;/weblog/input/access.log.10&quot;));\r\n		//获取这个文件的所有block的信息\r\n		BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(listStatus[0], 0L, listStatus[0].getLen());\r\n		//第一个block的长度\r\n		long length = fileBlockLocations[0].getLength();\r\n		//第一个block的起始偏移量\r\n		long offset = fileBlockLocations[0].getOffset();\r\n		\r\n		System.out.println(length);\r\n		System.out.println(offset);\r\n		\r\n		//获取第一个block写入输出流\r\n//		IOUtils.copyBytes(in, System.out, (int)length);\r\n		byte[] b = new byte[4096];\r\n		\r\n		FileOutputStream os = new FileOutputStream(new File(&quot;d:/block0&quot;));\r\n		while(in.read(offset, b, 0, 4096)!=-1){\r\n			os.write(b);\r\n			offset += 4096;\r\n			if(offset&gt;=length) return;\r\n		};\r\n		os.flush();\r\n		os.close();\r\n		in.close();\r\n	}\r\n\r\n\r\n',13,0,0,1515154405,0,0,0),(308,1,'开发shell采集脚本','','','需求说明\r\n点击流日志每天都10T，在业务应用服务器上，需要准实时上传至数据仓库（Hadoop HDFS）上\r\n8.2需求分析\r\n一般上传文件都是在凌晨24点操作，由于很多种类的业务数据都要在晚上进行传输，为了减轻服务器的压力，避开高峰期。\r\n如果需要伪实时的上传，则采用定时上传的方式\r\n	技术分析\r\n	 HDFS SHELL:  hadoop fs  –put   xxxx.tar  /data    还可以使用 Java Api\r\n	 		满足上传一个文件，不能满足定时、周期性传入。\r\n	 定时调度器：\r\n		Linux crontab\r\n		crontab -e\r\n*/5 * * * * $home/bin/command.sh   //五分钟执行一次\r\n系统会自动执行脚本，每5分钟一次，执行时判断文件是否符合上传规则，符合则上传\r\n实现流程\r\n8.4.1日志产生程序\r\n日志产生程序将日志生成后，产生一个一个的文件，使用滚动模式创建文件名。\r\n\r\n	日志生成的逻辑由业务系统决定，比如在log4j配置文件中配置生成规则，如：当xxxx.log 等于10G时，滚动生成新日志\r\n	log4j.logger.msg=info,msg\r\nlog4j.appender.msg=cn.maoxiangyi.MyRollingFileAppender\r\nlog4j.appender.msg.layout=org.apache.log4j.PatternLayout\r\nlog4j.appender.msg.layout.ConversionPattern=%m%n\r\nlog4j.appender.msg.datePattern=&#039;.&#039;yyyy-MM-dd\r\nlog4j.appender.msg.Threshold=info\r\nlog4j.appender.msg.append=true\r\nlog4j.appender.msg.encoding=UTF-8\r\nlog4j.appender.msg.MaxBackupIndex=100\r\nlog4j.appender.msg.MaxFileSize=10GB\r\nlog4j.appender.msg.File=/home/hadoop/logs/log/access.log\r\n细节：\r\n    1、 如果日志文件后缀是1\\2\\3等数字，该文件满足需求可以上传的话。把该文件移动到准备上传的工作区间。\r\n    2、 工作区间有文件之后，可以使用hadoop put命令将文件上传。\r\n阶段问题：\r\n    1、 待上传文件的工作区间的文件，在上传完成之后，是否需要删除掉。\r\n伪代码\r\n	使用ls命令读取指定路径下的所有文件信息，\r\n	ls  | while read  line\r\n	 //判断line这个文件名称是否符合规则\r\nif	 line=access.log.* (\r\n		将文件移动到待上传的工作区间\r\n	)\r\n\r\n//批量上传工作区间的文件\r\nhadoop fs  –put   xxx\r\n\r\n	\r\n脚本写完之后，配置linux定时任务，每5分钟运行一次。\r\n	\r\n8.5代码实现\r\n代码第一版本，实现基本的上传功能和定时调度功能\r\n\r\n\r\n代码第二版本：增强版V2(基本能用，还是不够健全)\r\n\r\n代码第二版本：增强版V2(基本能用，还是不够健全)\r\n\r\n\r\n8.6效果展示及操作步骤\r\n1、日志收集文件收集数据，并将数据保存起来，效果如下：\r\n	\r\n2、上传程序通过crontab定时调度\r\n\r\n3、程序运行时产生的临时文件\r\n\r\n4、Hadoo hdfs上的效果\r\n\r\n\r\n',13,0,0,1515154512,0,0,0),(309,1,'开发JAVA采集程序','','','需求\r\n从外部购买数据，数据提供方会实时将数据推送到6台FTP服务器上，我方部署6台接口采集机来对接采集数据，并上传到HDFS中\r\n\r\n提供商在FTP上生成数据的规则是以小时为单位建立文件夹(2016-03-11-10)，每分钟生成一个文件（00.dat,01.data,02.dat,........）\r\n\r\n提供方不提供数据备份，推送到FTP服务器的数据如果丢失，不再重新提供，且FTP服务器磁盘空间有限，最多存储最近10小时内的数据\r\n\r\n由于每一个文件比较小，只有150M左右，因此，我方在上传到HDFS过程中，需要将15分钟时段的数据合并成一个文件上传到HDFS\r\n\r\n为了区分数据丢失的责任，我方在下载数据时最好进行校验\r\n9.2 设计分析\r\n\r\n\r\n\r\n',13,0,0,1515154544,0,0,0),(310,1,'云计算的三种服务模式：IaaS，PaaS和SaaS','','','　　”云服务”现在已经快成了一个家喻户晓的词了。如果你不知道PaaS, IaaS 和SaaS的区别，那么也没啥，因为很多人确实不知道。\r\n　　“云”其实是互联网的一个隐喻，“云计算”其实就是使用互联网来接入存储或者运行在远程服务器端的应用，数据，或者服务。\r\n　　任何一个使用基于互联网的方法来计算，存储和开发的公司，都可以从技术上叫做从事云的公司。然而，不是所有的云公司都一样。不是所有人都是CTO，所以有时候看到云技术背后的一些词可能会比较头疼。\r\n云也是分层的\r\n　　任何一个在互联网上提供其服务的公司都可以叫做云计算公司。其实云计算分几层的，分别是Infrastructure（基础设施）-as-a-Service，Platform（平台）-as-a-Service，Software（软件）-as-a-Service。基础设施在最下端，平台在中间，软件在顶端。别的一些“软”的层可以在这些层上面添加。\r\n\r\n IaaS: Infrastructure-as-a-Service（基础设施即服务）\r\n　　第一层叫做IaaS，有时候也叫做Hardware-as-a-Service，几年前如果你想在办公室或者公司的网站上运行一些企业应用，你需要去买服务器，或者别的高昂的硬件来控制本地应用，让你的业务运行起来。云计算的三种服务模式：IaaS，PaaS和SaaS\r\n　　但是现在有IaaS，你可以将硬件外包到别的地方去。IaaS公司会提供场外服务器，存储和网络硬件，你可以租用。节省了维护成本和办公场地，公司可以在任何时候利用这些硬件来运行其应用。\r\n　　一些大的IaaS公司包括Amazon, Microsoft, VMWare, Rackspace和Red Hat.不过这些公司又都有自己的专长，比如Amazon和微软给你提供的不只是IaaS，他们还会将其计算能力出租给你来host你的网站。\r\nPaaS: Platform-as-a-Service（平台即服务）\r\n　　第二层就是所谓的PaaS，某些时候也叫做中间件。你公司所有的开发都可以在这一层进行，节省了时间和资源。\r\n　　PaaS公司在网上提供各种开发和分发应用的解决方案，比如虚拟服务器和操作系统。这节省了你在硬件上的费用，也让分散的工作室之间的合作变得更加容易。网页应用管理，应用设计，应用虚拟主机，存储，安全以及应用开发协作工具等。\r\n　　一些大的PaaS提供者有Google App Engine,Microsoft Azure，Force.com,Heroku，Engine Yard。最近兴起的公司有AppFog, Mendix 和 Standing Cloud\r\nSaaS: Software-as-a-Service（软件即服务）\r\n　　第三层也就是所谓SaaS。这一层是和你的生活每天接触的一层，大多是通过网页浏览器来接入。任何一个远程服务器上的应用都可以通过网络来运行，就是SaaS了。\r\n　　你消费的服务完全是从网页如Netflix, MOG, Google Apps, Box.net, Dropbox或者苹果的iCloud那里进入这些分类。尽管这些网页服务是用作商务和娱乐或者两者都有，但这也算是云技术的一部分。\r\n　　一些用作商务的SaaS应用包括Citrix的GoToMeeting，Cisco的WebEx，Salesforce的CRM，ADP，Workday和SuccessFactors。\r\nIaas和Paas之间的比较\r\n    PaaS的主要作用是将一个开发和运行平台作为服务提供给用户，而IaaS的主要作用是提供虚拟机或者其他资源作为服务提供给用户。接下来，将在七个方面对PaaS和IaaS进行比较：\r\n    1) 开发环境：PaaS基本都会给开发者提供一整套包括IDE在内的开发和测试环境，而IaaS方面用户主要还是沿用之前比较熟悉那套开发环境，但是因为之前那套开发环境在和云的整合方面比较欠缺，所以使用起来不是很方便。\r\n    2) 支持的应用：因为IaaS主要是提供虚拟机，而且普通的虚拟机能支持多种操作系统，所以IaaS支持的应用的范围是非常广泛的。但如果要让一个应用能跑在某个PaaS平台不是一件轻松的事，因为不仅需要确保这个应用是基于这个平台所支持的语言，而且也要确保这个应用只能调用这个平台所支持的API，如果这个应用调用了平台所不支持的API，那么就需要对这个应用进行修改。\r\n　3) 开放标准：虽然很多IaaS平台都存在一定的私有功能，但是由于OVF等协议的存在，使得IaaS在跨平台和避免被供应商锁定这两面是稳步前进的。而PaaS平台的情况则不容乐观，因为不论是Google的App Engine，还是Salesforce的Force.com都存在一定的私有API。\r\n    4) 可伸缩性：PaaS平台会自动调整资源来帮助运行于其上的应用更好地应对突发流量。而IaaS平台则需要开发人员手动对资源进行调整才能应对。\r\n    5) 整合率和经济性： PaaS平台整合率是非常高，比如PaaS的代表Google App Engine能在一台服务器上承载成千上万的应用，而普通的IaaS平台的整合率最多也不会超过100，而且普遍在10左右，使得IaaS的经济性不如PaaS。\r\n    6) 计费和监管：因为PaaS平台在计费和监管这两方面不仅达到了IaaS平台所能企及的操作系统层面，比如，CPU和内存的使用量等，而且还能做到应用层面，比如，应用的反应时间（Response Time）或者应用所消耗的事务多少等，这将提高计费和管理的精确性。\r\n    7) 学习难度：因为在IaaS上面开发和管理应用和现有的方式比较接近，而PaaS上面开发则有可能需要学一门新的语言或者新的框架，所以IaaS学习难度更低。\r\n \r\nPaaS\r\nIaaS\r\n开发环境\r\n完善\r\n普通\r\n支持的应用\r\n有限\r\n广\r\n通用性\r\n欠缺\r\n稍好\r\n可伸缩性\r\n自动伸缩\r\n手动伸缩\r\n整合率和经济性\r\n高整合率，更经济\r\n低整合率\r\n计费和监管\r\n精细\r\n简单\r\n学习难度\r\n略难\r\n低\r\n表1. PaaS和IaaS之间的比较\r\n未来的PK\r\n    在当今云计算环境当中，IaaS是非常主流的，无论是Amazon EC2还是Linode或者Joyent等，都占有一席之地，但是随着Google的App Engine，Salesforce的Force.com还是微软的Windows Azure等PaaS平台的推出，使得PaaS也开始崭露头角。谈到这两者的未来，特别是这两者之间的竞争关系，我个人认为，短期而言，因为IaaS模式在支持的应用和学习难度这两方面的优势，使得IaaS将会在短期之内会成为开发者的首选，但是从长期而言，因为PaaS模式的高整合率所带来经济型使得如果PaaS能解决诸如通用性和支持的应用等方面的挑战，它将会替代IaaS成为开发者的“新宠”。',13,0,0,1515154616,0,0,0),(311,1,'hdfs相关问题','','','1/运行mr程序出错\r\nconnecting to resoucemanager \r\nretrying ....  \r\nretrying .....\r\n\r\n原因是没有启动yarn或者启动失败\r\n\r\n2/初始化工作目录结构\r\nhdfs namenode -format 只是初始化了namenode的工作目录\r\n而datanode的工作目录是在datanode启动后自己初始化的\r\n\r\n3/datanode不被namenode识别的问题\r\nnamenode在format初始化的时候会形成两个标识：\r\nblockPoolId：\r\nclusterId：\r\n\r\n新的datanode加入时，会获取这两个标识作为自己工作目录中的标识\r\n\r\n一旦namenode重新format后，namenode的身份标识已变，而datanode如果依然\r\n持有原来的id，就不会被namenode识别\r\n\r\n\r\n4/datanode下线后多久看到效果\r\ndatanode不是一下线就会被namenode认定为下线的，有一个超时时间\r\n\r\n\r\n5/关于副本数量的问题\r\n副本数由客户端的参数dfs.replication决定（优先级： conf.set &gt;  自定义配置文件 &gt; jar包中的hdfs-default.xml）\r\n',13,0,0,1515154799,0,0,0),(312,1,'hadoop datanode节点超时时间设置','','','hadoop datanode节点超时时间设置\r\n\r\ndatanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：\r\n	timeout  = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。\r\n	而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。\r\n	需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。\r\n	hdfs-site.xml中的参数设置格式：\r\n\r\n&lt;property&gt;\r\n        &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt;\r\n        &lt;value&gt;2000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n        &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;\r\n        &lt;value&gt;1&lt;/value&gt;\r\n&lt;/property&gt;',13,0,0,1515154911,0,0,0),(313,1,'hadoop的日志目录','','','hadoop的日志目录（/home/hadoop/app/hadoop-2.6.4/logs）\r\n\r\n1、hadoop启动不正常\r\n用浏览器访问namenode的50070端口，不正常，需要诊断问题出在哪里：\r\na、在服务器的终端命令行使用jps查看相关进程\r\n（namenode1个节点   datanode3个节点   secondary namenode1个节点）\r\nb、如果已经知道了启动失败的服务进程，进入到相关进程的日志目录下，查看日志，分析异常的原因\r\n1）配置文件出错，saxparser  exception； ——找到错误提示中所指出的配置文件检查修改即可\r\n2）unknown host——主机名不认识，配置/etc/hosts文件即可，或者是配置文件中所用主机名跟实际不一致\r\n   （注：在配置文件中，统一使用主机名，而不要用ip地址）\r\n3）directory 访问异常—— 检查namenode的工作目录，看权限是否正常\r\n\r\n\r\nstart-dfs.sh启动后，发现有datanode启动不正常\r\na）查看datanode的日志，看是否有异常，如果没有异常，手动将datanode启动起来\r\nsbin/hadoop-daemon.sh start datanode\r\nb）很有可能是slaves文件中就没有列出需要启动的datanode\r\nc）排除上述两种情况后，基本上，能在日志中看到异常信息：\r\n   1、配置文件\r\n   2、ssh免密登陆没有配置好\r\n   3、datanode的身份标识跟namenode的集群身份标识不一致（删掉datanode的工作目录）',13,0,0,1515154935,0,0,0),(314,1,'HDFS冗余数据块的自动删除','','','HDFS冗余数据块的自动删除\r\n\r\n在日常维护hadoop集群的过程中发现这样一种情况：\r\n	某个节点由于网络故障或者DataNode进程死亡，被NameNode判定为死亡，HDFS马上自动开始数据块的容错拷贝；当该节点重新添加到集群中时，由于该节点上的数据其实并没有损坏，所以造成了HDFS上某些block的备份数超过了设定的备份数。通过观察发现，这些多余的数据块经过很长的一段时间才会被完全删除掉，那么这个时间取决于什么呢？\r\n	该时间的长短跟数据块报告的间隔时间有关。Datanode会定期将当前该结点上所有的BLOCK信息报告给Namenode，参数dfs.blockreport.intervalMsec就是控制这个报告间隔的参数。\r\n	hdfs-site.xml文件中有一个参数：\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt;\r\n	&lt;value&gt;3600000&lt;/value&gt;\r\n	&lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;\r\n&lt;/property&gt;\r\n	其中3600000为默认设置，3600000毫秒，即1个小时，也就是说，块报告的时间间隔为1个小时，所以经过了很长时间这些多余的块才被删除掉。通过实际测试发现，当把该参数调整的稍小一点的时候（60秒），多余的数据块确实很快就被删除了。',13,0,0,1515154971,0,0,0),(315,1,'namenode就进入安全模式','','','当namenode发现集群中的block丢失数量达到一个阀值时，namenode就进入安全模式状态，不再接受客户端的数据更新请求\r\n\r\n在正常情况下，namenode也有可能进入安全模式：\r\n	集群启动时（namenode启动时）必定会进入安全模式，然后过一段时间会自动退出安全模式（原因是datanode汇报的过程有一段持续时间）\r\n	\r\n也确实有异常情况下导致的安全模式\r\n	原因：block确实有缺失\r\n	措施：可以手动让namenode退出安全模式，bin/hdfs dfsadmin -safemode leave\r\n		  或者：调整safemode门限值：  dfs.safemode.threshold.pct=0.999f\r\n	',13,0,0,1515154997,0,0,0),(316,1,'ngp时间服务同步','','','第一种方式：同步到网络时间服务器\r\n\r\n # ntpdate time.windows.com\r\n将硬件时间设置为当前系统时间。 \r\n#hwclock –w \r\n加入crontab：    \r\n30 8 * * * root /usr/sbin/ntpdate 192.168.0.1; /sbin/hwclock -w 每天的8:30将进行一次时间同步。\r\n重启crond服务：\r\nservice crond restart\r\n\r\n\r\n第二种方式：同步到局域网内部的一台时间同步服务器\r\n一、搭建时间同步服务器\r\n1、编译安装ntp server\r\nrpm -qa | grep ntp\r\n若没有找到，则说明没有安装ntp包，从光盘上找到ntp包，使用\r\nrpm -Uvh ntp***.rpm\r\n进行安装\r\n2、修改ntp.conf配置文件\r\nvi /etc/ntp.conf\r\n①、第一种配置：允许任何IP的客户机都可以进行时间同步\r\n将“restrict default nomodify notrap noquery”这行修改成：\r\nrestrict default nomodify notrap\r\n配置文件示例：/etc/ntp.conf\r\n②、第二种配置：只允许192.168.211.***网段的客户机进行时间同步\r\n在restrict default nomodify notrap noquery（表示默认拒绝所有IP的时间同步）之后增加一行：\r\nrestrict 192.168.211.0 mask 255.255.255.0 nomodify notrap\r\n3、启动ntp服务\r\nservice ntpd start\r\n开机启动服务\r\nchkconfig ntpd on\r\n4、ntpd启动后，客户机要等几分钟再与其进行时间同步，否则会提示“no server suitable for synchronization found”错误。\r\n\r\n二、配置时间同步客户机\r\n手工执行 ntpdate &lt;ntp server&gt; 来同步\r\n或者利用crontab来执行\r\ncrontab -e\r\n0 21 * * * ntpdate 192.168.211.22 &gt;&gt; /root/ntpdate.log 2&gt;&amp;1\r\n每天晚上9点进行同步\r\n附：\r\n当用ntpdate -d 来查询时会发现导致 no server suitable for synchronization found 的错误的信息有以下2个：\r\n错误1.Server dropped: Strata too high\r\n在ntp客户端运行ntpdate serverIP，出现no server suitable for synchronization found的错误。\r\n在ntp客户端用ntpdate –d serverIP查看，发现有“Server dropped: strata too high”的错误，并且显示“stratum 16”。而正常情况下stratum这个值得范围是“0~15”。\r\n这是因为NTP server还没有和其自身或者它的server同步上。\r\n以下的定义是让NTP Server和其自身保持同步，如果在/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端。\r\nserver 127.127.1.0\r\nfudge 127.127.1.0 stratum 8\r\n\r\n在ntp server上重新启动ntp服务后，ntp server自身或者与其server的同步的需要一个时间段，这个过程可能是5分钟，在这个时间之内在客户端运行ntpdate命令时会产生no server suitable for synchronization found的错误。\r\n那么如何知道何时ntp server完成了和自身同步的过程呢？\r\n在ntp server上使用命令：\r\n# watch ntpq -p\r\n出现画面：\r\nEvery 2.0s: ntpq -p                                                                                                             Thu Jul 10 02:28:32 2008\r\n     remote           refid      st t when poll reach   delay   offset jitter\r\n==============================================================================\r\n192.168.30.22   LOCAL(0)         8 u   22   64    1    2.113 179133.   0.001\r\nLOCAL(0)        LOCAL(0)        10 l   21   64    1    0.000   0.000  0.001\r\n注意LOCAL的这个就是与自身同步的ntp server。\r\n注意reach这个值，在启动ntp server服务后，这个值就从0开始不断增加，当增加到17的时候，从0到17是5次的变更，每一次是poll的值的秒数，是64秒*5=320秒的时间。\r\n如果之后从ntp客户端同步ntp server还失败的话，用ntpdate –d来查询详细错误信息，再做判断。\r\n错误2.Server dropped: no data\r\n从客户端执行netdate –d时有错误信息如下：\r\ntransmit(192.168.30.22) transmit(192.168.30.22)\r\ntransmit(192.168.30.22)\r\ntransmit(192.168.30.22)\r\ntransmit(192.168.30.22)\r\n192.168.30.22: Server dropped: no data\r\nserver 192.168.30.22, port 123\r\n.....\r\n28 Jul 17:42:24 ntpdate[14148]: no server suitable for synchronization found出现这个问题的原因可能有2：\r\n1。检查ntp的版本，如果你使用的是ntp4.2（包括4.2）之后的版本，在restrict的定义中使用了notrust的话，会导致以上错误。\r\n使用以下命令检查ntp的版本：\r\n# ntpq -c version\r\n下面是来自ntp官方网站的说明：\r\nThe behavior of notrust changed between versions 4.1 and 4.2.\r\nIn 4.1 (and earlier) notrust meant &quot;Don&#039;t trust this host/subnet for time&quot;.\r\nIn 4.2 (and later) notrust means &quot;Ignore all NTP packets that are not cryptographically authenticated.&quot; This forces remote time servers to authenticate themselves to your (client) ntpd\r\n解决：\r\n把notrust去掉。\r\n2。检查ntp server的防火墙。可能是server的防火墙屏蔽了upd 123端口。\r\n可以用命令\r\n#service iptables stop\r\n\r\n来关掉iptables服务后再尝试从ntp客户端的同步，如果成功，证明是防火墙的问题，需要更改iptables的设置。\r\n',13,0,0,1515155048,0,0,0),(317,1,'Hadoop机架感知','','','Hadoop机架感知\r\n\r\n1.背景\r\n      Hadoop在设计时考虑到数据的安全与高效，数据文件默认在HDFS上存放三份，存储策略为本地一份，同机架内其它某一节点上一份，不同机架的某一节点上一份。这样如果本地数据损坏，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上拿数据要快；同时，如果整个机架的网络出现异常，也能保证在其它机架的节点上找到数据。为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。那么Hadoop是如何确定任意两个节点是位于同一机架，还是跨机架的呢？答案就是机架感知。\r\n     默认情况下，hadoop的机架感知是没有被启用的。所以，在通常情况下，hadoop集群的HDFS在选机器的时候，是随机选择的，也就是说，很有可能在写数据时，hadoop将第一块数据block1写到了rack1上，然后随机的选择下将block2写入到了rack2下，此时两个rack之间产生了数据传输的流量，再接下来，在随机的情况下，又将block3重新又写回了rack1，此时，两个rack之间又产生了一次数据流量。在job处理的数据量非常的大，或者往hadoop推送的数据量非常大的时候，这种情况会造成rack之间的网络流量成倍的上升，成为性能的瓶颈，进而影响作业的性能以至于整个集群的服务\r\n2.配置\r\n\r\n  默认情况下，namenode启动时候日志是这样的：\r\n2013-09-22 17:27:26,423 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node:  /default-rack/ 192.168.147.92:50010\r\n每个IP 对应的机架ID都是 /default-rack ，说明hadoop的机架感知没有被启用。\r\n要将hadoop机架感知的功能启用，配置非常简单，在 NameNode所在节点的/home/bigdata/apps/hadoop/etc/hadoop的core-site.xml配置文件中配置一个选项:\r\n&lt;property&gt;\r\n  &lt;name&gt;topology.script.file.name&lt;/name&gt;\r\n  &lt;value&gt;/home/bigdata/apps/hadoop/etc/hadoop/topology.sh&lt;/value&gt;\r\n&lt;/property&gt;\r\n      这个配置选项的value指定为一个可执行程序，通常为一个脚本，该脚本接受一个参数，输出一个值。接受的参数通常为某台datanode机器的ip地址，而输出的值通常为该ip地址对应的datanode所在的rack，例如”/rack1”。Namenode启动时，会判断该配置选项是否为空，如果非空，则表示已经启用机架感知的配置，此时namenode会根据配置寻找该脚本，并在接收到每一个datanode的heartbeat时，将该datanode的ip地址作为参数传给该脚本运行，并将得到的输出作为该datanode所属的机架ID，保存到内存的一个map中.\r\n      至于脚本的编写，就需要将真实的网络拓朴和机架信息了解清楚后，通过该脚本能够将机器的ip地址和机器名正确的映射到相应的机架上去。一个简单的实现如下：\r\n#!/bin/bash\r\nHADOOP_CONF=/home/bigdata/apps/hadoop/etc/hadoop\r\nwhile [ $# -gt 0 ] ; do\r\n  nodeArg=$1\r\n  exec&lt;${HADOOP_CONF}/topology.data\r\n  result=&quot;&quot;\r\n  while read line ; do\r\n    ar=( $line )\r\n    if [ &quot;${ar[0]}&quot; = &quot;$nodeArg&quot; ]||[ &quot;${ar[1]}&quot; = &quot;$nodeArg&quot; ]; then\r\n      result=&quot;${ar[2]}&quot;\r\n    fi\r\n  done\r\n  shift\r\n  if [ -z &quot;$result&quot; ] ; then\r\n    echo -n &quot;/default-rack&quot;\r\n  else\r\n    echo -n &quot;$result&quot;\r\n  fi\r\n  done\r\ntopology.data,格式为：节点（ip或主机名） /交换机xx/机架xx\r\n192.168.147.91 tbe192168147091 /dc1/rack1\r\n192.168.147.92 tbe192168147092 /dc1/rack1\r\n192.168.147.93 tbe192168147093 /dc1/rack2\r\n192.168.147.94 tbe192168147094 /dc1/rack3\r\n192.168.147.95 tbe192168147095 /dc1/rack3\r\n192.168.147.96 tbe192168147096 /dc1/rack3\r\n需要注意的是，在Namenode上，该文件中的节点必须使用IP，使用主机名无效，而Jobtracker上，该文件中的节点必须使用主机名，使用IP无效,所以，最好ip和主机名都配上。\r\n这样配置后，namenode启动时候日志是这样的：\r\n2013-09-23 17:16:27,272 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node:  /dc1/rack3/  192.168.147.94:50010\r\n说明hadoop的机架感知已经被启用了。\r\n查看HADOOP机架信息命令:  \r\n./hadoop dfsadmin -printTopology \r\nRack: /dc1/rack1\r\n   192.168.147.91:50010 (tbe192168147091)\r\n   192.168.147.92:50010 (tbe192168147092)\r\n\r\nRack: /dc1/rack2\r\n   192.168.147.93:50010 (tbe192168147093)\r\n\r\nRack: /dc1/rack3\r\n   192.168.147.94:50010 (tbe192168147094)\r\n   192.168.147.95:50010 (tbe192168147095)\r\n   192.168.147.96:50010 (tbe192168147096)\r\n3.增加数据节点，不重启NameNode\r\n\r\n 假设Hadoop集群在192.168.147.68上部署了NameNode和DataNode,启用了机架感知，执行bin/hadoop dfsadmin -printTopology看到的结果：\r\nRack: /dc1/rack1\r\n   192.168.147.68:50010 (dbj68)\r\n现在想增加一个物理位置在rack2的数据节点192.168.147.69到集群中，不重启NameNode。 \r\n首先，修改NameNode节点的topology.data的配置，加入:192.168.147.69 dbj69 /dc1/rack2,保存。\r\n192.168.147.68 dbj68 /dc1/rack1\r\n192.168.147.69 dbj69 /dc1/rack2\r\n然后，sbin/hadoop-daemons.sh start datanode启动数据节点dbj69,任意节点执行bin/hadoop dfsadmin -printTopology 看到的结果：\r\nRack: /dc1/rack1\r\n   192.168.147.68:50010 (dbj68)\r\n\r\nRack: /dc1/rack2\r\n   192.168.147.69:50010 (dbj69)\r\n说明hadoop已经感知到了新加入的节点dbj69。 \r\n注意：如果不将dbj69的配置加入到topology.data中，执行sbin/hadoop-daemons.sh start datanode启动数据节点dbj69，datanode日志中会有异常发生，导致dbj69启动不成功。\r\n2013-11-21 10:51:33,502 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-1732631201-192.168.147.68-1385000665316 (storage id DS-878525145-192.168.147.69-50010-1385002292231) service to dbj68/192.168.147.68:9000\r\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.net.NetworkTopology$InvalidTopologyException): Invalid network topology. You cannot have a rack and a non-rack node at the same level of the network topology.\r\n  at org.apache.hadoop.net.NetworkTopology.add(NetworkTopology.java:382)\r\n  at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:746)\r\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerDatanode(FSNamesystem.java:3498)\r\n  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.registerDatanode(NameNodeRpcServer.java:876)\r\n  at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.registerDatanode(DatanodeProtocolServerSideTranslatorPB.java:91)\r\n  at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:20018)\r\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)\r\n  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)\r\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1701)\r\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1697)\r\n  at java.security.AccessController.doPrivileged(Native Method)\r\n  at javax.security.auth.Subject.doAs(Subject.java:415)\r\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)\r\n  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1695)\r\n\r\n  at org.apache.hadoop.ipc.Client.call(Client.java:1231)\r\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)\r\n  at $Proxy10.registerDatanode(Unknown Source)\r\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  at java.lang.reflect.Method.invoke(Method.java:601)\r\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)\r\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)\r\n  at $Proxy10.registerDatanode(Unknown Source)\r\n  at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.registerDatanode(DatanodeProtocolClientSideTranslatorPB.java:149)\r\n  at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:619)\r\n  at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:221)\r\n  at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)\r\n  at java.lang.Thread.run(Thread.java:722)\r\n4.节点间距离计算\r\n\r\n 有了机架感知，NameNode就可以画出下图所示的datanode网络拓扑图。D1,R1都是交换机，最底层是datanode。则H1的rackid=/D1/R1/H1，H1的parent是R1，R1的是D1。这些rackid信息可以通过topology.script.file.name配置。有了这些rackid信息就可以计算出任意两台datanode之间的距离，得到最优的存放策略，优化整个集群的网络带宽均衡以及数据最优分配。\r\ndistance(/D1/R1/H1,/D1/R1/H1)=0  相同的datanode\r\ndistance(/D1/R1/H1,/D1/R1/H2)=2  同一rack下的不同datanode\r\ndistance(/D1/R1/H1,/D1/R2/H4)=4  同一IDC下的不同datanode\r\ndistance(/D1/R1/H1,/D2/R3/H7)=6  不同IDC下的datanode',13,0,0,1515155076,0,0,0),(318,1,'上传文件至hdfs','','','#!/bin/bash\r\n\r\n\r\n#set java env\r\nexport JAVA_HOME=/export/servers/jdk\r\nexport JRE_HOME=${JAVA_HOME}/jre\r\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\r\nexport PATH=${JAVA_HOME}/bin:$PATH\r\n\r\n#set hadoop env\r\nexport HADOOP_HOME=/export/servers/hadoop\r\nexport PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\r\n\r\n\r\n\r\n\r\n#日志文件存放的目录\r\nlog_src_dir=/export/software/\r\n\r\n#日志文件上传到hdfs的根路径\r\nhdfs_root_dir=/data/clickLog/20151226/\r\n\r\n\r\n#读取日志文件的目录，判断是否有需要上传的文件\r\n\r\nls $log_src_dir | while read fileName\r\ndo\r\n        if [ &quot;hadoop.log1&quot; = &quot;$fileName&quot; ];then\r\n                hadoop fs -put $log_src_dir$fileName $hdfs_root_dir\r\n        fi\r\ndone\r\n\r\n\r\n#!/bin/bash\r\n\r\n#set java env\r\nexport JAVA_HOME=/home/hadoop/app/jdk1.7.0_51\r\nexport JRE_HOME=${JAVA_HOME}/jre\r\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\r\nexport PATH=${JAVA_HOME}/bin:$PATH\r\n\r\n#set hadoop env\r\nexport HADOOP_HOME=/home/hadoop/app/hadoop-2.6.4\r\nexport PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\r\n\r\n\r\n#版本1的问题：\r\n#虽然上传到Hadoop集群上了，但是原始文件还在。如何处理？\r\n#日志文件的名称都是xxxx.log1,再次上传文件时，因为hdfs上已经存在了，会报错。如何处理？\r\n\r\n#如何解决版本1的问题\r\n#       1、先将需要上传的文件移动到待上传目录\r\n#	2、在讲文件移动到待上传目录时，将文件按照一定的格式重名名\r\n#		/export/software/hadoop.log1   /export/data/click_log/xxxxx_click_log_{date}\r\n\r\n\r\n#日志文件存放的目录\r\nlog_src_dir=/home/hadoop/logs/log/\r\n\r\n#待上传文件存放的目录\r\nlog_toupload_dir=/home/hadoop/logs/toupload/\r\n\r\n\r\n#日志文件上传到hdfs的根路径\r\nhdfs_root_dir=/data/clickLog/20151226/\r\n\r\n#打印环境变量信息\r\necho &quot;envs: hadoop_home: $HADOOP_HOME&quot;\r\n\r\n\r\n#读取日志文件的目录，判断是否有需要上传的文件\r\necho &quot;log_src_dir:&quot;$log_src_dir\r\nls $log_src_dir | while read fileName\r\ndo\r\n	if [[ &quot;$fileName&quot; == access.log.* ]]; then\r\n	# if [ &quot;access.log&quot; = &quot;$fileName&quot; ];then\r\n		date=`date +%Y_%m_%d_%H_%M_%S`\r\n		#将文件移动到待上传目录并重命名\r\n		#打印信息\r\n		echo &quot;moving $log_src_dir$fileName to $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date&quot;\r\n		mv $log_src_dir$fileName $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date\r\n		#将待上传的文件path写入一个列表文件willDoing\r\n		echo $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date &gt;&gt; $log_toupload_dir&quot;willDoing.&quot;$date\r\n	fi\r\n	\r\ndone\r\n#找到列表文件willDoing\r\nls $log_toupload_dir | grep will |grep -v &quot;_COPY_&quot; | grep -v &quot;_DONE_&quot; | while read line\r\ndo\r\n	#打印信息\r\n	echo &quot;toupload is in file:&quot;$line\r\n	#将待上传文件列表willDoing改名为willDoing_COPY_\r\n	mv $log_toupload_dir$line $log_toupload_dir$line&quot;_COPY_&quot;\r\n	#读列表文件willDoing_COPY_的内容（一个一个的待上传文件名）  ,此处的line 就是列表中的一个待上传文件的path\r\n	cat $log_toupload_dir$line&quot;_COPY_&quot; |while read line\r\n	do\r\n		#打印信息\r\n		echo &quot;puting...$line to hdfs path.....$hdfs_root_dir&quot;\r\n		hadoop fs -put $line $hdfs_root_dir\r\n	done	\r\n	mv $log_toupload_dir$line&quot;_COPY_&quot;  $log_toupload_dir$line&quot;_DONE_&quot;\r\ndone\r\n',13,0,0,1515155204,0,0,0),(319,1,'数据采集','','','数据采集模块项目说明\r\n    1 功能概述\r\n本模块的主要功能是循环采集日志文件服务器上指定目录下的日志文件，并进行分类合并后转移到目标存储位置下的分类文件夹中（包括本地的备份目录和远程hadoop集群等）。\r\n\r\n\r\n    2 运行环境示意图\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    3 功能详细说明\r\n        3.1  配置文件\r\n模块的正确运行需要根据具体运行环境配置3类参数：\r\n    1、 分别是采集数据的源路径\r\n    2、 数据采集分类后需要转移到的源服务器上的备份路径\r\n    3、 数据采集分类后需要转移到的远程HADOOP集群上的HDFS的路径\r\n\r\n\r\n\r\n\r\n参数配置在config.xml文件中\r\n配置示例如下：\r\n	&lt;!-- 数据采集模块开关  --&gt;\r\n	&lt;property key=&quot;data.collect.switch&quot; type=&quot;string&quot;&gt;on&lt;/property&gt;\r\n&lt;!-- 数据采集扫描周期，单位为分钟  --&gt;\r\n	&lt;property key=&quot;data.collect.sycle&quot; type=&quot;string&quot;&gt;1&lt;/property&gt;\r\n&lt;!--每一个分组收满时的文件数  --&gt;\r\n	&lt;property key=&quot;max.file.count&quot; type=&quot;string&quot;&gt;2&lt;/property&gt;\r\n&lt;!--分组的更改状态的时限  --&gt;\r\n	&lt;property key=&quot;max.file.time&quot; type=&quot;string&quot;&gt;1&lt;/property&gt;\r\n&lt;!--划分分组时需要的步骤数  --&gt;\r\n	&lt;property key=&quot;level.group&quot; type=&quot;string&quot;&gt;2&lt;/property&gt;\r\n&lt;!--往分组文件夹传输文件时是否需要将本次传输组的文件合并  --&gt;\r\n	&lt;property key=&quot;data.collect.filecombine&quot; type=&quot;string&quot;&gt;true&lt;/property&gt;\r\n	&lt;!--文件名分类字串截取正则表达式，根据实际抽取需要来配置 --&gt;\r\n	&lt;!-- sca  --&gt;	\r\n	&lt;property key=&quot;whole.regex&quot; type=&quot;string&quot;&gt;\\d{2}-\\d{2}-\\w+-\\w+-\\d{2}_\\d{14}_\\d{2}&lt;/property&gt;\r\n	&lt;property key=&quot;first.level.regex&quot; type=&quot;string&quot;&gt;_(\\d{10})&lt;/property&gt;	\r\n	&lt;property key=&quot;second.level.regex&quot; type=&quot;string&quot;&gt;\\d{2}-\\d{2}-\\w+-(\\w+)-&lt;/property&gt;	\r\n	&lt;!-- 数据采集源数据路径  --&gt;\r\n	&lt;property key=&quot;data.collect.src&quot; type=&quot;string&quot;&gt;/home/hadoop/logsrc&lt;/property&gt;\r\n	&lt;!-- 数据采集历史文件夹扫描路径，用来更改该路径下的过期tmp文件夹状态 --&gt;\r\n	&lt;property key=&quot;data.collect.scandir1&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/in/&lt;/property&gt;\r\n	&lt;property key=&quot;data.collect.scandir2&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/other/&lt;/property&gt;\r\n	\r\n\r\n	&lt;!-- 经分日志采集后本地备份路径  --&gt;\r\n	&lt;property key=&quot;JFen.bak&quot; type=&quot;string&quot;&gt;/home/hadoop/jfbak/&lt;/property&gt;\r\n	&lt;!-- wap1x协议日志采集后本地备份路径  --&gt;\r\n	&lt;property key=&quot;wap1x.bak&quot; type=&quot;string&quot;&gt;/home/hadoop/logbak/&lt;/property&gt;\r\n	&lt;!-- http协议日志采集后本地备份路径  --&gt;\r\n	&lt;property key=&quot;http.bak&quot; type=&quot;string&quot;&gt;/home/hadoop/logbak/&lt;/property&gt;\r\n	&lt;!-- dns协议日志采集后本地备份路径  --&gt;\r\n	&lt;property key=&quot;dns.bak&quot; type=&quot;string&quot;&gt;/home/hadoop/logbak/&lt;/property&gt;\r\n	&lt;!-- mms协议日志采集后本地备份路径  --&gt;\r\n	&lt;property key=&quot;mms.bak&quot; type=&quot;string&quot;&gt;/home/hadoop/logbak/&lt;/property&gt;\r\n	&lt;!-- conn协议日志采集后本地备份路径  --&gt;\r\n	&lt;property key=&quot;conn.bak&quot; type=&quot;string&quot;&gt;/home/hadoop/logbak/&lt;/property&gt;\r\n	&lt;!-- pdp协议日志采集后本地备份路径  --&gt;\r\n	&lt;property key=&quot;pdp.bak&quot; type=&quot;string&quot;&gt;/home/hadoop/logbak/&lt;/property&gt;\r\n	&lt;!-- 不符合分类规则的异常文件的本地转移备份路径  --&gt;\r\n	&lt;property key=&quot;imnormal.bak&quot; type=&quot;string&quot;&gt;/home/hadoop/otherbak/&lt;/property&gt;\r\n	&lt;!-- 经分日志采集后转移到的远程目的  --&gt;\r\n	&lt;property key=&quot;JFen.dest&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/jfen/&lt;/property&gt;	\r\n	&lt;!-- wap1x协议日志采集后转移到的远程目的  --&gt;\r\n	&lt;property key=&quot;wap1x.dest&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/in/&lt;/property&gt;	\r\n	&lt;!-- http协议日志采集后转移到的远程目的  --&gt;\r\n	&lt;property key=&quot;http.dest&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/in/&lt;/property&gt;	\r\n	&lt;!-- dns协议日志采集后转移到的远程目的  --&gt;\r\n	&lt;property key=&quot;dns.dest&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/other/&lt;/property&gt;	\r\n	&lt;!-- mms协议日志采集后转移到的远程目的  --&gt;\r\n	&lt;property key=&quot;mms.dest&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/other/&lt;/property&gt;	\r\n	&lt;!-- conn协议日志采集后转移到的远程目的  --&gt;\r\n	&lt;property key=&quot;conn.dest&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/other/&lt;/property&gt;	\r\n	&lt;!-- pdp协议日志采集后转移到的远程目的  --&gt;\r\n	&lt;property key=&quot;pdp.dest&quot; type=&quot;string&quot;&gt;hdfs://192.168.193.130:9000/sca/bh/other/&lt;/property&gt;	\r\n\r\n\r\n        3.2  源数据说明\r\n    1、 在数据源中的数据为移动公司服务器产生的日志文件，目前共有两种，描述如下：\r\n\r\n    • 第一类数据源文件：\r\nMASA_USER_20130616_001.txt\r\nTB_DIC_AC_ACCT_ITEM_20130616_001.txt\r\nTB_DIC_AREA_CELL_NEW_20130616_001.txt\r\nTB_DIC_AREA_CODE_20130616_001.txt\r\nTB_DIC_BRAND_20130616_001.txt\r\n对此类文件的处理：程序将提取出时间（精确到日期）生成分组名，如:\r\n文件：	TB_DIC_AREA_CODE_20130616_001.txt\r\nTB_DIC_AREA_CODE_20130616_001.txt\r\nMASA_USER_20130616_001.txt\r\n都将被分组到20130616组\r\n\r\n    • 第二类数据源文件：\r\n01-01-gn-wap1x-02_20130605155000_00\r\n01-02-gn-http-00_20130605155000_00\r\n01-03-gn-dns-02_20130605155500_00\r\n01-05-gn-mms-02_20130605155500_00\r\n01-06-gn-conn-01_20130605155000_00\r\n01-08-gn-pdp-00_20130605155500_00\r\n\r\n此类文件的文件名中包含两项关键要素：\r\n    a) 协议名——共有如下类型：wap1x、dns、http、mms、conn、pdp\r\n    b) 时间——如20130605155500，分解即为2013年0605日15时55分00秒\r\n对此类文件的处理：程序将提取出协议名和时间（精确到小时）组成分组名，如:\r\n文件	01-08-gn-pdp-00_20130605155500_00\r\n01-08-gn-pdp-00_20130605155504_00\r\n都将被分到pdp-2013060515组\r\n\r\n\r\n注：以上两类文件不会在一个实际采集系统中同时出现；\r\n程序也不能同时处理这两类文件；\r\n但可通过相关正则表达式的配置，程序可自动识别处理的是哪种类型数据；\r\n	a)经分数据的正则配置：\r\n	&lt;property key=&quot;whole.regex&quot; type=&quot;string&quot;&gt;[\\w_]*?_(\\d{8})_[\\w_]*?.*?&lt;/property&gt;\r\n	&lt;property key=&quot;order.regex&quot; type=&quot;string&quot;&gt;1&lt;/property&gt;\r\n	&lt;property key=&quot;key.regex&quot; type=&quot;string&quot;&gt;(\\d{8})&lt;/property&gt;\r\n	&lt;property key=&quot;tmp.regex&quot; type=&quot;string&quot;&gt;(\\d{8})\\.tmp&lt;/property&gt;\r\n\r\n	b)诺西数据的正则配置：\r\n	&lt;property key=&quot;whole.regex&quot; type=&quot;string&quot;&gt;\\d{2}-\\d{2}-\\w+-(\\w+)-\\d{2}_(\\d{10})\\d{4}_\\d{2}&lt;/property&gt;\r\n	&lt;property key=&quot;order.regex&quot; type=&quot;string&quot;&gt;12&lt;/property&gt;\r\n	&lt;property key=&quot;key.regex&quot; type=&quot;string&quot;&gt;(\\w+)-\\d{10}&lt;/property&gt;\r\n	&lt;property key=&quot;tmp.regex&quot; type=&quot;string&quot;&gt;.*?(\\d{10})\\.tmp&lt;/property&gt;\r\n        3.3  数据处理主体流程：\r\n模块程序将每隔20分钟（可配置”data.collect.sycle”）扫描配置文件中指定的数据源路径下如上图所示的文件，并按照文件名中的协议和时间进行分类；\r\n示例：比如对“01-01-gn-wap1x-02_20130605155500_00”这个文件的处理流程\r\n\r\n    a) 扫描检测配置文件中预设的数据源目录下的所有文件，进行下述遍历分组处理：\r\n    b) 根据配置好的正则表达式截取文件名的协议和时间：wap1x和2013060515\r\n    c) 将上述截取到的两个字串合并成 wap1x-2013060515\r\n    d) 在内存中将本次扫描获取的文件按上述规则分成若干个文件组；\r\n如组wap1x-2013060515\r\nwap1x-2013060516\r\nhttp-2013060515\r\n……\r\n    e) 在采集的目标根路径按 ”组名.tmp” 格式建立分组文件夹\r\n将属于某分组（如wap1x-2013060515）的所有文件：\r\n如01-01-gn-wap1x-02_20130605155500_00\r\n01-01-gn-wap1x-02_20130605155500_02\r\n……\r\n合并存入远程目标目录下的wap1x-2013060515.tmp文件夹内；\r\n生成一个临时统计文件用于计数；\r\n然后将源数据目录的这组文件全部原样转移到本地的备份目录下（可配置）\r\n\r\n    f) 待一组文件处理完毕后，程序会检查分组文件夹wap1x-2013060515.tmp内接收过的文件数量，如果达到配置文件中配置的上限数量，程序会将wap1x-2013060515.tmp更名为wap1x-2013060515.dat\r\n而后，程序继续循环处理下一组文件；\r\n\r\n    g) 在一次扫描处理周期结束前，程序还会检查配置文件中配置的需要做“过期”监控的路径（可配置），找出带.tmp后缀且已经过期（过期时限可配置）的分类文件夹，根据配置对该文件夹内的number-random.count文件进行清理，然后将文件夹后缀名更名为.dat\r\n\r\n        3.4  其他细节处理逻辑\r\n\r\n    a) 当某个分类文件夹如wap1x-2013060515.tmp下收集到的文件已满60（可配置）个，则该文件夹会被程序自动改名为wap1x-2013060515.dat ；\r\n\r\n    b) 在传输一组文件之前，会先判断目标路径下是否已经存在该文件组，如果存在，则将目标文件夹中的同名合并文件删除，然后再重建并写入数据；\r\n（此步处理的来由：一组文件在传输过程中可能出现失败，从而在目标路径下留下一个残留的合并文件。上述处理可以解决这个问题）\r\n\r\n    c) 当某个分类文件夹在一定时限后（可配置）还未能收集满60个文件，则该文件夹也会被改名，即在原来的名字后加上后缀 “.dat” ；\r\n\r\n    d) 当扫描到的某文件所归属的分组所对应的文件夹在此前已被打上“.dat”后缀，该文不会输出到所配置的远程目标和本地备份文件夹内；而是直接转移到一个other（可配置）文件夹中；\r\n\r\n    e) 当扫描到的文件的文件名不符合正常格式，将会被作为异常文件处理，即直接转移到异常文件夹中（可配置）；\r\n\r\n    f) 扫描线程的扫描频率为20分钟一次（可配置），在扫描过程中，如果发生源数据读取异常或目标路径写入异常，程序将重试，重试时间间隔为2秒；\r\n如果重试成功，则又恢复20分钟循环周期；\r\n\r\n    g) 本程序能适应多台采集服务器往同一个目标路径下输出分类结果的需求；\r\n\r\n    h) 本程序有一个总的开关，&quot;data.collect.switch&quot;，\r\n值为OFF时，本模块将不会运作\r\n值为ON时，本模块正常工作\r\n\r\n\r\n        3.5 清扫已完成文件分组文件夹内的临时校验文件\r\na)功能来由：为了统计分组文件夹所接收的文件数量采用的一种处理方法\r\n	b)功能流程详述：\r\n每一台采集服务器往目标路径输送文件的时候，会将一次扫描中扫描到的该组文件合并成一个文件\r\n为了便于统计该分组文件夹中共计接收过的文件数，每台采集服务器在一次扫描传输中会在该分组文件夹下写一个用于统计的临时文件。\r\n该临时文件的命名规则: number-random.cout\r\nnumber：本次传输的文件个数\r\nrandom：随机数\r\n当统计到该分组已接收的文件数量达到配置中指定的文件数量时，程序将清除之前的临时文件（可配置）；\r\n	c)功能预期结果：\r\n在每一个tmp状态分组文件夹中，会存在一些number-random.cout 临时文件\r\n		在每一个dat状态分组文件夹中，这些.count临时文件将不复存在\r\n\r\n	d)该功能有一个配置开关:\r\n	&lt;property key=&quot;data.collect.ifclean&quot; type=&quot;string&quot;&gt;true&lt;/property&gt;\r\n	配为true时，清扫功能开启\r\n	配位false时，清扫功能关闭\r\n\r\n\r\n        3.6 根据测试进度待补充\r\n',13,0,0,1515155347,0,0,0),(320,1,'输出文件规则','','','输出文件规则\r\n\r\n    • 每一台采集机将同一小时时段内的同一类型日志文件合成一个文件输出到目标目录\r\n    • 输出过程中，输出文件夹会带上.tmp后缀，如：\r\n   			hdfs://tascluster/user/sca/conn/20130704-conn.tmp\r\n    • 输出完成后，输出文件夹后缀会变更为.dat，如：\r\n            hdfs://tascluster/user/sca/conn/20130704-conn.dat\r\n    • 输出文件的周期：\r\n每10分钟一次往合并文件中追加内容\r\n每小时完成一个合并文件',13,0,0,1515155416,0,0,0),(321,1,'mapreduce框架','','','1、mapreduce框架的设计思想\r\n\r\n2、mapreduce框架中的程序实体角色：maptask   reducetask   mrappmaster\r\n\r\n3、mapreduce程序运行的整体流程\r\n\r\n4、mapreduce程序中maptask任务切片规划的机制（掌握整体逻辑流程，看day03_word文档中的“maptask并行度”）\r\n\r\n5、mapreduce程序提交的整体流程（看图：一坨  &quot;客户端提交mr程序job的流程&quot;）\r\n\r\n6、编码： \r\n    wordcount\r\n	流量汇总统计（hadoop的序列化实现）\r\n	流量汇总统计并按省份区分\r\n\r\nMapReduce快速入门\r\n如何理解map、reduce计算模型\r\n\r\nMapreudce程序运行演示\r\n\r\nMapreduce编程规范及示例编写\r\n\r\nMapreduce程序运行模式及debug方法\r\nMapReduce高级特性\r\nMapreduce程序的核心机制\r\n\r\nMapReduce的序列化框架\r\n\r\nMapReduce的排序实现\r\n\r\nMapReduce的分区机制及自定义\r\n\r\nMapreduce的数据压缩\r\n\r\nMapreduce与yarn的结合\r\n\r\nMapreduce编程案例\r\n\r\nMapreduce 参数优化\r\n\r\nMapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架；\r\nMapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上；\r\n',13,0,0,1515155579,0,0,0),(322,1,'为什么要MAPREDUCE','','','（1）海量数据在单机上处理因为硬件资源限制，无法胜任\r\n（2）而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度\r\n（3）引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理\r\n\r\n设想一个海量数据场景下的wordcount需求：\r\n单机版：内存受限，磁盘受限，运算能力受限\r\n分布式：\r\n    1、 文件分布式存储（HDFS）\r\n    2、 运算逻辑需要至少分成2个阶段（一个阶段独立并发，一个阶段汇聚）\r\n    3、 运算程序如何分发\r\n    4、 程序如何分配运算任务（切片）\r\n    5、 两阶段的程序如何启动？如何协调？\r\n    6、 整个程序运行过程中的监控？容错？重试？\r\n\r\n可见在程序由单机版扩成分布式时，会引入大量的复杂工作。为了提高开发效率，可以将分布式程序中的公共功能封装成框架，让开发人员可以将精力集中于业务逻辑。\r\n\r\n而mapreduce就是这样一个分布式程序的通用框架，其应对以上问题的整体结构如下：\r\n    1、 MRAppMaster(mapreduce application master)\r\n    2、 MapTask\r\n    3、 ReduceTask\r\n',13,0,0,1515155725,0,0,0),(323,1,'MAPREDUCE框架结构及核心运行机制','','','1.2.1 结构\r\n一个完整的mapreduce程序在分布式运行时有三类实例进程：\r\n1、MRAppMaster：负责整个程序的过程调度及状态协调\r\n2、mapTask：负责map阶段的整个数据处理流程\r\n3、ReduceTask：负责reduce阶段的整个数据处理流程\r\n\r\n1.2.2 MR程序运行流程\r\n1.2.2.1 流程示意图\r\n1.2.2.2 流程解析\r\n    1、 一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程\r\n\r\n    2、 maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为：\r\n        a) 利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对\r\n        b) 将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存\r\n        c) 将缓存中的KV对按照K分区排序后不断溢写到磁盘文件\r\n\r\n    3、 MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区）\r\n\r\n    4、 Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储',13,0,0,1515155766,0,0,0),(324,1,'MapTask并行度决定机制','','','maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度\r\n那么，mapTask并行实例是否越多越好呢？其并行度又是如何决定呢？\r\n\r\n1.3.1 mapTask并行度的决定机制\r\n一个job的map阶段并行度由客户端在提交job时决定\r\n而客户端对map阶段并行度的规划的基本逻辑为：\r\n将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理\r\n\r\n这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成，其过程如下图：\r\n\r\n1.3.2 FileInputFormat切片机制\r\n1、切片定义在InputFormat类中的getSplit()方法\r\n2、FileInputFormat中默认的切片机制：\r\n        a) 简单地按照文件的内容长度进行切片\r\n        b) 切片大小，默认等于block大小\r\n        c) 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片\r\n比如待处理数据有两个文件：\r\nfile1.txt    320M\r\nfile2.txt    10M\r\n\r\n经过FileInputFormat的切片机制运算后，形成的切片信息如下： \r\nfile1.txt.split1--  0~128\r\nfile1.txt.split2--  128~256\r\nfile1.txt.split3--  256~320\r\nfile2.txt.split1--  0~10M\r\n\r\nFileInputFormat中切片的大小的参数配置\r\n通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize));  切片主要由这几个值来运算决定\r\nminsize：默认值：1  \r\n  	配置参数： mapreduce.input.fileinputformat.split.minsize    \r\nmaxsize：默认值：Long.MAXValue  \r\n    配置参数：mapreduce.input.fileinputformat.split.maxsize\r\nblocksize\r\n因此，默认情况下，切片大小=blocksize\r\nmaxsize（切片最大值）：\r\n参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值\r\nminsize （切片最小值）：\r\n参数调的比blockSize大，则可以让切片变得比blocksize还大\r\n\r\n选择并发数的影响因素：\r\n    1、 运算节点的硬件配置\r\n    2、 运算任务的类型：CPU密集型还是IO密集型\r\n    3、 运算任务的数据量',13,0,0,1515155836,0,0,0),(325,1,'map并行度的经验之谈','','','如果硬件配置为2*12core + 64G，恰当的map并行度是大约每个节点20-100个map，最好每个map的执行时间至少一分钟。\r\n    • 如果job的每个map或者 reduce task的运行时间都只有30-40秒钟，那么就减少该job的map或者reduce数，每一个task(map|reduce)的setup和加入到调度器中进行调度，这个中间的过程可能都要花费几秒钟，所以如果每个task都非常快就跑完了，就会在task的开始和结束的时候浪费太多的时间。\r\n配置task的JVM重用可以改善该问题：\r\n（mapred.job.reuse.jvm.num.tasks，默认是1，表示一个JVM上最多可以顺序执行的task\r\n数目（属于同一个Job）是1。也就是说一个task启一个JVM）\r\n\r\n    • 如果input的文件非常的大，比如1TB，可以考虑将hdfs上的每个block size设大，比如设成256MB或者512MB\r\n',13,0,0,1515155857,0,0,0),(326,1,'ReduceTask并行度的决定','','','reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置：\r\n\r\n//默认值是1，手动设置为4\r\njob.setNumReduceTasks(4);\r\n\r\n如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜\r\n注意： reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask\r\n\r\n尽量不要运行太多的reduce task。对大多数job来说，最好rduce的个数最多和集群中的reduce持平，或者比集群的 reduce slots小。这个对于小集群而言，尤其重要。\r\n\r\n\r\n1.6 MAPREDUCE程序运行演示\r\nHadoop的发布包中内置了一个hadoop-mapreduce-example-2.4.1.jar，这个jar包中有各种MR示例程序，可以通过以下步骤运行：\r\n启动hdfs，yarn\r\n然后在集群中的任意一台服务器上启动执行程序（比如运行wordcount）：\r\nhadoop jar hadoop-mapreduce-example-2.4.1.jar wordcount  /wordcount/data /wordcount/out',13,0,0,1515155882,0,0,0),(327,1,'MAPREDUCE 示例编写及编程规范','','','编程规范\r\n    （1） 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端)\r\n    （2） Mapper的输入数据是KV对的形式（KV的类型可自定义）\r\n    （3） Mapper的输出数据是KV对的形式（KV的类型可自定义）\r\n    （4） Mapper中的业务逻辑写在map()方法中\r\n    （5） map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次\r\n    （6） Reducer的输入数据类型对应Mapper的输出数据类型，也是KV\r\n    （7） Reducer的业务逻辑写在reduce()方法中\r\n    （8） Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法\r\n    （9） 用户自定义的Mapper和Reducer都要继承各自的父类\r\n    （10） 整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象\r\n\r\nwordcount示例编写\r\n需求：在一堆给定的文本文件中统计输出每一个单词出现的总次数\r\n\r\n(1)定义一个mapper类\r\n//首先要定义四个泛型的类型\r\n//keyin:  LongWritable    valuein: Text\r\n//keyout: Text            valueout:IntWritable\r\n\r\npublic class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{\r\n	//map方法的生命周期：  框架每传一行数据就被调用一次\r\n	//key :  这一行的起始点在文件中的偏移量\r\n	//value: 这一行的内容\r\n	@Override\r\n	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n		//拿到一行数据转换为string\r\n		String line = value.toString();\r\n		//将这一行切分出各个单词\r\n		String[] words = line.split(&quot; &quot;);\r\n		//遍历数组，输出&lt;单词，1&gt;\r\n		for(String word:words){\r\n			context.write(new Text(word), new IntWritable(1));\r\n		}\r\n	}\r\n}\r\n\r\n(2)定义一个reducer类\r\n	//生命周期：框架每传递进来一个kv 组，reduce方法被调用一次\r\n	@Override\r\n	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\r\n		//定义一个计数器\r\n		int count = 0;\r\n		//遍历这一组kv的所有v，累加到count中\r\n		for(IntWritable value:values){\r\n			count += value.get();\r\n		}\r\n		context.write(key, new IntWritable(count));\r\n	}\r\n}\r\n\r\n(3)定义一个主类，用来描述job并提交job\r\npublic class WordCountRunner {\r\n	//把业务逻辑相关的信息（哪个是mapper，哪个是reducer，要处理的数据在哪里，输出的结果放哪里……）描述成一个job对象\r\n	//把这个描述好的job提交给集群去运行\r\n	public static void main(String[] args) throws Exception {\r\n		Configuration conf = new Configuration();\r\n		Job wcjob = Job.getInstance(conf);\r\n		//指定我这个job所在的jar包\r\n//		wcjob.setJar(&quot;/home/hadoop/wordcount.jar&quot;);\r\n		wcjob.setJarByClass(WordCountRunner.class);\r\n		\r\n		wcjob.setMapperClass(WordCountMapper.class);\r\n		wcjob.setReducerClass(WordCountReducer.class);\r\n		//设置我们的业务逻辑Mapper类的输出key和value的数据类型\r\n		wcjob.setMapOutputKeyClass(Text.class);\r\n		wcjob.setMapOutputValueClass(IntWritable.class);\r\n		//设置我们的业务逻辑Reducer类的输出key和value的数据类型\r\n		wcjob.setOutputKeyClass(Text.class);\r\n		wcjob.setOutputValueClass(IntWritable.class);\r\n		\r\n		//指定要处理的数据所在的位置\r\n		FileInputFormat.setInputPaths(wcjob, &quot;hdfs://hdp-server01:9000/wordcount/data/big.txt&quot;);\r\n		//指定处理完成之后的结果所保存的位置\r\n		FileOutputFormat.setOutputPath(wcjob, new Path(&quot;hdfs://hdp-server01:9000/wordcount/output/&quot;));\r\n		\r\n		//向yarn集群提交这个job\r\n		boolean res = wcjob.waitForCompletion(true);\r\n		System.exit(res?0:1);\r\n	}\r\n',13,0,0,1515155944,0,0,0),(328,1,'MAPREDUCE程序运行模式','','','本地运行模式\r\n    （1） mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行\r\n    （2） 而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上\r\n    （3） 怎样实现本地运行？写一个程序，不要带集群的配置文件（本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数）\r\n    （4） 本地模式非常便于进行业务逻辑的debug，只要在eclipse中打断点即可\r\n\r\n如果在windows下想运行本地模式来测试程序逻辑，需要在windows中配置环境变量：\r\n％HADOOP_HOME％  =  d:/hadoop-2.6.1\r\n%PATH% =  ％HADOOP_HOME％\\bin\r\n并且要将d:/hadoop-2.6.1的lib和bin目录替换成windows平台编译的版本\r\n\r\n\r\n2.2.2 集群运行模式\r\n    （1） 将mapreduce程序提交给yarn集群resourcemanager，分发到很多的节点上并发执行\r\n    （2） 处理的数据和输出结果应该位于hdfs文件系统\r\n    （3） 提交集群的实现步骤：\r\nA、将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动\r\n     $ hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriver inputpath outputpath\r\nB、直接在linux的eclipse中运行main方法\r\n（项目中要带参数：mapreduce.framework.name=yarn以及yarn的两个基本配置）\r\nC、如果要在windows的eclipse中提交job给集群，则要修改YarnRunner类\r\n\r\nmapreduce程序在集群中运行时的大体流程：\r\n\r\n附：在windows平台上访问hadoop时改变自身身份标识的方法之二：',13,0,0,1515155988,0,0,0),(329,1,'MAPREDUCE中的Combiner','','','    （1） combiner是MR程序中Mapper和Reducer之外的一种组件\r\n    （2） combiner组件的父类就是Reducer\r\n    （3） combiner和reducer的区别在于运行的位置：\r\nCombiner是在每一个maptask所在的节点运行\r\nReducer是接收全局所有Mapper的输出结果；\r\n(4) combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量\r\n具体实现步骤：\r\n    1、 自定义一个combiner继承Reducer，重写reduce方法\r\n    2、 在job中设置：  job.setCombinerClass(CustomCombiner.class)\r\n(5) combiner能够应用的前提是不能影响最终的业务逻辑\r\n而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来\r\n',13,0,0,1515156017,0,0,0),(330,1,'mapreduce的shuffle机制','','','概述：\r\n    • mapreduce中，map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle；\r\n    • shuffle: 洗牌、发牌——（核心机制：数据分区，排序，缓存）；\r\n    • 具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序；\r\n\r\n3.1.2 主要流程：\r\nShuffle缓存流程：\r\n\r\nshuffle是MR处理流程中的一个过程，它的每一个处理步骤是分散在各个map task和reduce task节点上完成的，整体来看，分为3个操作：\r\n    1、 分区partition\r\n    2、 Sort根据key排序\r\n    3、 Combiner进行局部value的合并\r\n\r\n3.1.3 详细流程\r\n    1、 maptask收集我们的map()方法输出的kv对，放到内存缓冲区中\r\n    2、 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件\r\n    3、 多个溢出文件会被合并成大的溢出文件\r\n    4、 在溢出过程中，及合并的过程中，都要调用partitoner进行分组和针对key进行排序\r\n    5、 reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据\r\n    6、 reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序）\r\n    7、 合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法）\r\n\r\nShuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快 \r\n缓冲区的大小可以通过参数调整,  参数：io.sort.mb  默认100M',13,0,0,1515156052,0,0,0),(331,1,'MAPREDUCE中的序列化','','','概述\r\nJava的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系。。。。），不便于在网络中高效传输；\r\n所以，hadoop自己开发了一套序列化机制（Writable），精简，高效\r\n\r\n3.2.2 Jdk序列化和MR序列化之间的比较\r\n简单代码验证两种序列化机制的差别：\r\npublic class TestSeri {\r\n	public static void main(String[] args) throws Exception {\r\n		//定义两个ByteArrayOutputStream，用来接收不同序列化机制的序列化结果\r\n		ByteArrayOutputStream ba = new ByteArrayOutputStream();\r\n		ByteArrayOutputStream ba2 = new ByteArrayOutputStream();\r\n\r\n		//定义两个DataOutputStream，用于将普通对象进行jdk标准序列化\r\n		DataOutputStream dout = new DataOutputStream(ba);\r\n		DataOutputStream dout2 = new DataOutputStream(ba2);\r\n		ObjectOutputStream obout = new ObjectOutputStream(dout2);\r\n		//定义两个bean，作为序列化的源对象\r\n		ItemBeanSer itemBeanSer = new ItemBeanSer(1000L, 89.9f);\r\n		ItemBean itemBean = new ItemBean(1000L, 89.9f);\r\n\r\n		//用于比较String类型和Text类型的序列化差别\r\n		Text atext = new Text(&quot;a&quot;);\r\n		// atext.write(dout);\r\n		itemBean.write(dout);\r\n\r\n		byte[] byteArray = ba.toByteArray();\r\n\r\n		//比较序列化结果\r\n		System.out.println(byteArray.length);\r\n		for (byte b : byteArray) {\r\n\r\n			System.out.print(b);\r\n			System.out.print(&quot;:&quot;);\r\n		}\r\n\r\n		System.out.println(&quot;-----------------------&quot;);\r\n\r\n		String astr = &quot;a&quot;;\r\n		// dout2.writeUTF(astr);\r\n		obout.writeObject(itemBeanSer);\r\n\r\n		byte[] byteArray2 = ba2.toByteArray();\r\n		System.out.println(byteArray2.length);\r\n		for (byte b : byteArray2) {\r\n			System.out.print(b);\r\n			System.out.print(&quot;:&quot;);\r\n		}\r\n	}\r\n}\r\n\r\n\r\n\r\n3.2.3 自定义对象实现MR中的序列化接口\r\n如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序,此时，自定义的bean实现的接口应该是：\r\npublic  class  FlowBean  implements  WritableComparable&lt;FlowBean&gt; \r\n需要自己实现的方法是：\r\n	/**\r\n	 * 反序列化的方法，反序列化时，从流中读取到的各个字段的顺序应该与序列化时写出去的顺序保持一致\r\n	 */\r\n	@Override\r\n	public void readFields(DataInput in) throws IOException {\r\n		\r\n		upflow = in.readLong();\r\n		dflow = in.readLong();\r\n		sumflow = in.readLong();\r\n		\r\n\r\n	}\r\n\r\n	/**\r\n	 * 序列化的方法\r\n	 */\r\n	@Override\r\n	public void write(DataOutput out) throws IOException {\r\n\r\n		out.writeLong(upflow);\r\n		out.writeLong(dflow);\r\n		//可以考虑不序列化总流量，因为总流量是可以通过上行流量和下行流量计算出来的\r\n		out.writeLong(sumflow);\r\n\r\n	}\r\n	\r\n	@Override\r\n	public int compareTo(FlowBean o) {\r\n		\r\n		//实现按照sumflow的大小倒序排序\r\n		return sumflow&gt;o.getSumflow()?-1:1;\r\n	}\r\n',13,0,0,1515156076,0,0,0),(332,1,'MapReduce与YARN','','','YARN概述\r\nYarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统之上的应用程序\r\n3.3.2 YARN的重要概念\r\n    1、 yarn并不清楚用户提交的程序的运行机制\r\n    2、 yarn只提供运算资源的调度（用户程序向yarn申请资源，yarn就负责分配资源）\r\n    3、 yarn中的主管角色叫ResourceManager\r\n    4、 yarn中具体提供运算资源的角色叫NodeManager\r\n    5、 这样一来，yarn其实就与运行的用户程序完全解耦，就意味着yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm\r\n\r\n     程序，spark程序，tez ……\r\n    6、 所以，spark、storm等运算框架都可以整合在yarn上运行，只要他们各自的框架中有符合yarn规范的资源请求机制即可\r\n    7、 Yarn就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整合在一个物理集群上，提高资源利用率，方便数据共享\r\n',13,0,0,1515156112,0,0,0),(333,1,'Yarn中运行运算程序的示例','','','mapreduce程序的调度过程，如下图\r\n\r\n\r\n',13,0,0,1515156163,0,0,0),(334,1,'Mapreduce中的排序初步','','','需求\r\n对日志数据中的上下行流量信息汇总，并输出按照总流量倒序排序的结果\r\n数据如下：\r\n1363157985066 	13726230503	00-FD-07-A4-72-B8:CMCC	120.196.100.82             24	27	2481	24681	200\r\n1363157995052 	13826544101	5C-0E-8B-C7-F1-E0:CMCC	120.197.40.4			4	0	264	0	200\r\n1363157991076 	13926435656	20-10-7A-28-CC-0A:CMCC	120.196.100.99			2	4	132	1512	200\r\n1363154400022 	13926251106	5C-0E-8B-8B-B1-50:CMCC	120.197.40.4			4	0	240	0	200\r\n\r\n\r\n4.1.2 分析\r\n基本思路：实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输\r\n\r\nMR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key\r\n所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable\r\n然后重写key的compareTo方法\r\n\r\n4.1.3 实现\r\n    1、 自定义的bean\r\npublic class FlowBean implements WritableComparable&lt;FlowBean&gt;{\r\n	\r\n	long upflow;\r\n	long downflow;\r\n	long sumflow;\r\n	\r\n	//如果空参构造函数被覆盖，一定要显示定义一下，否则在反序列时会抛异常\r\n	public FlowBean(){}\r\n	\r\n	public FlowBean(long upflow, long downflow) {\r\n		super();\r\n		this.upflow = upflow;\r\n		this.downflow = downflow;\r\n		this.sumflow = upflow + downflow;\r\n	}\r\n	\r\n	public long getSumflow() {\r\n		return sumflow;\r\n	}\r\n\r\n	public void setSumflow(long sumflow) {\r\n		this.sumflow = sumflow;\r\n	}\r\n\r\n	public long getUpflow() {\r\n		return upflow;\r\n	}\r\n	public void setUpflow(long upflow) {\r\n		this.upflow = upflow;\r\n	}\r\n	public long getDownflow() {\r\n		return downflow;\r\n	}\r\n	public void setDownflow(long downflow) {\r\n		this.downflow = downflow;\r\n	}\r\n\r\n	//序列化，将对象的字段信息写入输出流\r\n	@Override\r\n	public void write(DataOutput out) throws IOException {\r\n		\r\n		out.writeLong(upflow);\r\n		out.writeLong(downflow);\r\n		out.writeLong(sumflow);\r\n		\r\n	}\r\n\r\n	//反序列化，从输入流中读取各个字段信息\r\n	@Override\r\n	public void readFields(DataInput in) throws IOException {\r\n		upflow = in.readLong();\r\n		downflow = in.readLong();\r\n		sumflow = in.readLong();\r\n		\r\n	}\r\n	\r\n	\r\n	@Override\r\n	public String toString() {\r\n		return upflow + &quot;\\t&quot; + downflow + &quot;\\t&quot; + sumflow;\r\n	}\r\n	@Override\r\n	public int compareTo(FlowBean o) {\r\n		//自定义倒序比较规则\r\n		return sumflow &gt; o.getSumflow() ? -1:1;\r\n	}\r\n}\r\n\r\n\r\n\r\n    2、 mapper 和 reducer\r\npublic class FlowCount {\r\n\r\n	static class FlowCountMapper extends Mapper&lt;LongWritable, Text, FlowBean,Text &gt; {\r\n\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n\r\n			String line = value.toString();\r\n			String[] fields = line.split(&quot;\\t&quot;);\r\n			try {\r\n				String phonenbr = fields[0];\r\n\r\n				long upflow = Long.parseLong(fields[1]);\r\n				long dflow = Long.parseLong(fields[2]);\r\n\r\n				FlowBean flowBean = new FlowBean(upflow, dflow);\r\n\r\n				context.write(flowBean,new Text(phonenbr));\r\n			} catch (Exception e) {\r\n\r\n				e.printStackTrace();\r\n			}\r\n\r\n		}\r\n\r\n	}\r\n\r\n	static class FlowCountReducer extends Reducer&lt;FlowBean,Text,Text, FlowBean&gt; {\r\n\r\n		@Override\r\n		protected void reduce(FlowBean bean, Iterable&lt;Text&gt; phonenbr, Context context) throws IOException, InterruptedException {\r\n\r\n			Text phoneNbr = phonenbr.iterator().next();\r\n\r\n			context.write(phoneNbr, bean);\r\n\r\n		}\r\n\r\n	}\r\n\r\n	public static void main(String[] args) throws Exception {\r\n\r\n		Configuration conf = new Configuration();\r\n\r\n		Job job = Job.getInstance(conf);\r\n\r\n		job.setJarByClass(FlowCount.class);\r\n\r\n		job.setMapperClass(FlowCountMapper.class);\r\n		job.setReducerClass(FlowCountReducer.class);\r\n\r\n		 job.setMapOutputKeyClass(FlowBean.class);\r\n		 job.setMapOutputValueClass(Text.class);\r\n\r\n		job.setOutputKeyClass(Text.class);\r\n		job.setOutputValueClass(FlowBean.class);\r\n\r\n		// job.setInputFormatClass(TextInputFormat.class);\r\n\r\n		FileInputFormat.setInputPaths(job, new Path(args[0]));\r\n		FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n\r\n		job.waitForCompletion(true);\r\n\r\n	}\r\n\r\n}\r\n\r\n\r\n\r\n',13,0,0,1515156231,0,0,0),(335,1,'Mapreduce中的分区Partitioner','','','需求\r\n根据归属地输出流量统计数据结果到不同文件，以便于在查询统计结果时可以定位到省级范围进行\r\n4.2.2 分析\r\nMapreduce中会将map输出的kv对，按照相同key分组，然后分发给不同的reducetask\r\n默认的分发规则为：根据key的hashcode%reducetask数来分发\r\n所以：如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner\r\n自定义一个CustomPartitioner继承抽象类：Partitioner\r\n然后在job对象中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class)\r\n\r\n4.2.3 实现\r\n/**\r\n * 定义自己的从map到reduce之间的数据（分组）分发规则 按照手机号所属的省份来分发（分组）ProvincePartitioner\r\n * 默认的分组组件是HashPartitioner\r\n * \r\n * @author\r\n * \r\n */\r\npublic class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; {\r\n\r\n	static HashMap&lt;String, Integer&gt; provinceMap = new HashMap&lt;String, Integer&gt;();\r\n\r\n	static {\r\n\r\n		provinceMap.put(&quot;135&quot;, 0);\r\n		provinceMap.put(&quot;136&quot;, 1);\r\n		provinceMap.put(&quot;137&quot;, 2);\r\n		provinceMap.put(&quot;138&quot;, 3);\r\n		provinceMap.put(&quot;139&quot;, 4);\r\n\r\n	}\r\n\r\n	@Override\r\n	public int getPartition(Text key, FlowBean value, int numPartitions) {\r\n\r\n		Integer code = provinceMap.get(key.toString().substring(0, 3));\r\n\r\n		return code == null ? 5 : code;\r\n	}\r\n\r\n}\r\n\r\n\r\n\r\n\r\n4.3. mapreduce数据压缩\r\n4.3.1 概述\r\n这是mapreduce的一种优化策略：通过压缩编码对mapper或者reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）\r\n    1、 Mapreduce支持将map输出的结果或者reduce输出的结果进行压缩，以减少网络IO或最终输出数据的体积\r\n    2、 压缩特性运用得当能提高性能，但运用不当也可能降低性能\r\n    3、 基本原则：\r\n运算密集型的job，少用压缩\r\nIO密集型的job，多用压缩\r\n\r\n\r\n\r\n4.3.2 MR支持的压缩编码\r\n\r\n\r\n4.3.3 Reducer输出压缩\r\n在配置参数或在代码中都可以设置reduce的输出压缩\r\n1、在配置参数中设置 \r\nmapreduce.output.fileoutputformat.compress=false\r\nmapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec\r\nmapreduce.output.fileoutputformat.compress.type=RECORD\r\n\r\n2、在代码中设置\r\n		Job job = Job.getInstance(conf);\r\n		FileOutputFormat.setCompressOutput(job, true);\r\n		FileOutputFormat.setOutputCompressorClass(job, (Class&lt;? extends CompressionCodec&gt;) Class.forName(&quot;&quot;));\r\n4.3.4 Mapper输出压缩\r\n在配置参数或在代码中都可以设置reduce的输出压缩\r\n1、在配置参数中设置 \r\nmapreduce.map.output.compress=false\r\nmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec\r\n\r\n2、在代码中设置：\r\nconf.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);\r\nconf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class);\r\n\r\n\r\n\r\n4.3.5 压缩文件的读取\r\nHadoop自带的InputFormat类内置支持压缩文件的读取，比如TextInputformat类，在其initialize方法中：\r\n  public void initialize(InputSplit genericSplit,\r\n                         TaskAttemptContext context) throws IOException {\r\n    FileSplit split = (FileSplit) genericSplit;\r\n    Configuration job = context.getConfiguration();\r\n    this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    start = split.getStart();\r\n    end = start + split.getLength();\r\n    final Path file = split.getPath();\r\n\r\n    // open the file and seek to the start of the split\r\n    final FileSystem fs = file.getFileSystem(job);\r\n    fileIn = fs.open(file);\r\n    //根据文件后缀名创建相应压缩编码的codec\r\n    CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);\r\n    if (null!=codec) {\r\n      isCompressedInput = true;	\r\n      decompressor = CodecPool.getDecompressor(codec);\r\n	  //判断是否属于可切片压缩编码类型\r\n      if (codec instanceof SplittableCompressionCodec) {\r\n        final SplitCompressionInputStream cIn =\r\n          ((SplittableCompressionCodec)codec).createInputStream(\r\n            fileIn, decompressor, start, end,\r\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\r\n		 //如果是可切片压缩编码，则创建一个CompressedSplitLineReader读取压缩数据\r\n        in = new CompressedSplitLineReader(cIn, job,\r\n            this.recordDelimiterBytes);\r\n        start = cIn.getAdjustedStart();\r\n        end = cIn.getAdjustedEnd();\r\n        filePosition = cIn;\r\n      } else {\r\n		//如果是不可切片压缩编码，则创建一个SplitLineReader读取压缩数据，并将文件输入流转换成解压数据流传递给普通SplitLineReader读取\r\n        in = new SplitLineReader(codec.createInputStream(fileIn,\r\n            decompressor), job, this.recordDelimiterBytes);\r\n        filePosition = fileIn;\r\n      }\r\n    } else {\r\n      fileIn.seek(start);\r\n	   //如果不是压缩文件，则创建普通SplitLineReader读取数据\r\n      in = new SplitLineReader(fileIn, job, this.recordDelimiterBytes);\r\n      filePosition = fileIn;\r\n    }\r\n',13,0,0,1515156261,0,0,0),(336,1,'reduce端join算法实现','','','需求：\r\n订单数据表t_order：\r\nid\r\ndate\r\npid\r\namount\r\n1001\r\n20150710\r\nP0001\r\n2\r\n1002\r\n20150710\r\nP0001\r\n3\r\n1002\r\n20150710\r\nP0002\r\n3\r\n\r\n商品信息表t_product\r\nid\r\nname\r\ncategory_id\r\nprice\r\nP0001\r\n小米5\r\nC01\r\n2\r\nP0002\r\n锤子T1\r\nC01\r\n3\r\n\r\n假如数据量巨大，两表的数据是以文件的形式存储在HDFS中，需要用mapreduce程序来实现一下SQL查询运算： \r\nselect  a.id,a.date,b.name,b.category_id,b.price from t_order a join t_product b on a.pid = b.id\r\n\r\n2、实现机制：\r\n通过将关联的条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce task，在reduce中进行数据的串联\r\n\r\n\r\npublic class OrderJoin {\r\n\r\n	static class OrderJoinMapper extends Mapper&lt;LongWritable, Text, Text, OrderJoinBean&gt; {\r\n\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n\r\n			// 拿到一行数据，并且要分辨出这行数据所属的文件\r\n			String line = value.toString();\r\n\r\n			String[] fields = line.split(&quot;\\t&quot;);\r\n\r\n			// 拿到itemid\r\n			String itemid = fields[0];\r\n\r\n			// 获取到这一行所在的文件名（通过inpusplit）\r\n			String name = &quot;你拿到的文件名&quot;;\r\n\r\n			// 根据文件名，切分出各字段（如果是a，切分出两个字段，如果是b，切分出3个字段）\r\n\r\n			OrderJoinBean bean = new OrderJoinBean();\r\n			bean.set(null, null, null, null, null);\r\n			context.write(new Text(itemid), bean);\r\n\r\n		}\r\n\r\n	}\r\n\r\n	static class OrderJoinReducer extends Reducer&lt;Text, OrderJoinBean, OrderJoinBean, NullWritable&gt; {\r\n\r\n		@Override\r\n		protected void reduce(Text key, Iterable&lt;OrderJoinBean&gt; beans, Context context) throws IOException, InterruptedException {\r\n			\r\n			 //拿到的key是某一个itemid,比如1000\r\n			//拿到的beans是来自于两类文件的bean\r\n			//  {1000,amount} {1000,amount} {1000,amount}   ---   {1000,price,name}\r\n			\r\n			//将来自于b文件的bean里面的字段，跟来自于a的所有bean进行字段拼接并输出\r\n		}\r\n	}\r\n}\r\n\r\n\r\n缺点：这种方式中，join的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜\r\n\r\n解决方案： map端join实现方式\r\n\r\n\r\n\r\n\r\n\r\n\r\n4.4.2 map端join算法实现\r\n1、原理阐述\r\n适用于关联表中有小表的情形；\r\n可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果，可以大大提高join操作的并发度，加快处理速度\r\n2、实现示例\r\n--先在mapper类中预先定义好小表，进行join\r\n--引入实际场景中的解决方案：一次加载数据库或者用distributedcache\r\npublic class TestDistributedCache {\r\n	static class TestDistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{\r\n		FileReader in = null;\r\n		BufferedReader reader = null;\r\n		HashMap&lt;String,String&gt; b_tab = new HashMap&lt;String, String&gt;();\r\n		String localpath =null;\r\n		String uirpath = null;\r\n		\r\n		//是在map任务初始化的时候调用一次\r\n		@Override\r\n		protected void setup(Context context) throws IOException, InterruptedException {\r\n			//通过这几句代码可以获取到cache file的本地绝对路径，测试验证用\r\n			Path[] files = context.getLocalCacheFiles();\r\n			localpath = files[0].toString();\r\n			URI[] cacheFiles = context.getCacheFiles();\r\n			\r\n			\r\n			//缓存文件的用法——直接用本地IO来读取\r\n			//这里读的数据是map task所在机器本地工作目录中的一个小文件\r\n			in = new FileReader(&quot;b.txt&quot;);\r\n			reader =new BufferedReader(in);\r\n			String line =null;\r\n			while(null!=(line=reader.readLine())){\r\n				\r\n				String[] fields = line.split(&quot;,&quot;);\r\n				b_tab.put(fields[0],fields[1]);\r\n				\r\n			}\r\n			IOUtils.closeStream(reader);\r\n			IOUtils.closeStream(in);\r\n			\r\n		}\r\n		\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n\r\n			//这里读的是这个map task所负责的那一个切片数据（在hdfs上）\r\n			 String[] fields = value.toString().split(&quot;\\t&quot;);\r\n			 \r\n			 String a_itemid = fields[0];\r\n			 String a_amount = fields[1];\r\n			 \r\n			 String b_name = b_tab.get(a_itemid);\r\n			 \r\n			 // 输出结果  1001	98.9	banan\r\n			 context.write(new Text(a_itemid), new Text(a_amount + &quot;\\t&quot; + &quot;:&quot; + localpath + &quot;\\t&quot; +b_name ));\r\n			 \r\n		}\r\n		\r\n		\r\n	}\r\n	\r\n	\r\n	public static void main(String[] args) throws Exception {\r\n		\r\n		Configuration conf = new Configuration();\r\n		Job job = Job.getInstance(conf);\r\n		\r\n		job.setJarByClass(TestDistributedCache.class);\r\n		\r\n		job.setMapperClass(TestDistributedCacheMapper.class);\r\n		\r\n		job.setOutputKeyClass(Text.class);\r\n		job.setOutputValueClass(LongWritable.class);\r\n		\r\n		//这里是我们正常的需要处理的数据所在路径\r\n		FileInputFormat.setInputPaths(job, new Path(args[0]));\r\n		FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n		\r\n		//不需要reducer\r\n		job.setNumReduceTasks(0);\r\n		//分发一个文件到task进程的工作目录\r\n		job.addCacheFile(new URI(&quot;hdfs://hadoop-server01:9000/cachefile/b.txt&quot;));\r\n		\r\n		//分发一个归档文件到task进程的工作目录\r\n//		job.addArchiveToClassPath(archive);\r\n\r\n		//分发jar包到task节点的classpath下\r\n//		job.addFileToClassPath(jarfile);\r\n		\r\n		job.waitForCompletion(true);\r\n	}\r\n}\r\n4.4.3 web日志预处理\r\n1、需求：\r\n对web访问日志中的各字段识别切分\r\n去除日志中不合法的记录\r\n根据KPI统计需求，生成各类访问请求过滤数据\r\n\r\n2、实现代码：\r\na) 定义一个bean，用来记录日志数据中的各数据字段\r\npublic class WebLogBean {\r\n	\r\n    private String remote_addr;// 记录客户端的ip地址\r\n    private String remote_user;// 记录客户端用户名称,忽略属性&quot;-&quot;\r\n    private String time_local;// 记录访问时间与时区\r\n    private String request;// 记录请求的url与http协议\r\n    private String status;// 记录请求状态；成功是200\r\n    private String body_bytes_sent;// 记录发送给客户端文件主体内容大小\r\n    private String http_referer;// 用来记录从那个页面链接访问过来的\r\n    private String http_user_agent;// 记录客户浏览器的相关信息\r\n\r\n    private boolean valid = true;// 判断数据是否合法\r\n\r\n    \r\n    \r\n	public String getRemote_addr() {\r\n		return remote_addr;\r\n	}\r\n\r\n	public void setRemote_addr(String remote_addr) {\r\n		this.remote_addr = remote_addr;\r\n	}\r\n\r\n	public String getRemote_user() {\r\n		return remote_user;\r\n	}\r\n\r\n	public void setRemote_user(String remote_user) {\r\n		this.remote_user = remote_user;\r\n	}\r\n\r\n	public String getTime_local() {\r\n		return time_local;\r\n	}\r\n\r\n	public void setTime_local(String time_local) {\r\n		this.time_local = time_local;\r\n	}\r\n\r\n	public String getRequest() {\r\n		return request;\r\n	}\r\n\r\n	public void setRequest(String request) {\r\n		this.request = request;\r\n	}\r\n\r\n	public String getStatus() {\r\n		return status;\r\n	}\r\n\r\n	public void setStatus(String status) {\r\n		this.status = status;\r\n	}\r\n\r\n	public String getBody_bytes_sent() {\r\n		return body_bytes_sent;\r\n	}\r\n\r\n	public void setBody_bytes_sent(String body_bytes_sent) {\r\n		this.body_bytes_sent = body_bytes_sent;\r\n	}\r\n\r\n	public String getHttp_referer() {\r\n		return http_referer;\r\n	}\r\n\r\n	public void setHttp_referer(String http_referer) {\r\n		this.http_referer = http_referer;\r\n	}\r\n\r\n	public String getHttp_user_agent() {\r\n		return http_user_agent;\r\n	}\r\n\r\n	public void setHttp_user_agent(String http_user_agent) {\r\n		this.http_user_agent = http_user_agent;\r\n	}\r\n\r\n	public boolean isValid() {\r\n		return valid;\r\n	}\r\n\r\n	public void setValid(boolean valid) {\r\n		this.valid = valid;\r\n	}\r\n    \r\n    \r\n	@Override\r\n	public String toString() {\r\n        StringBuilder sb = new StringBuilder();\r\n        sb.append(this.valid);\r\n        sb.append(&quot;\\001&quot;).append(this.remote_addr);\r\n        sb.append(&quot;\\001&quot;).append(this.remote_user);\r\n        sb.append(&quot;\\001&quot;).append(this.time_local);\r\n        sb.append(&quot;\\001&quot;).append(this.request);\r\n        sb.append(&quot;\\001&quot;).append(this.status);\r\n        sb.append(&quot;\\001&quot;).append(this.body_bytes_sent);\r\n        sb.append(&quot;\\001&quot;).append(this.http_referer);\r\n        sb.append(&quot;\\001&quot;).append(this.http_user_agent);\r\n        return sb.toString();\r\n}\r\n}\r\n\r\nb)定义一个parser用来解析过滤web访问日志原始记录\r\npublic class WebLogParser {\r\n    public static WebLogBean parser(String line) {\r\n        WebLogBean webLogBean = new WebLogBean();\r\n        String[] arr = line.split(&quot; &quot;);\r\n        if (arr.length &gt; 11) {\r\n        	webLogBean.setRemote_addr(arr[0]);\r\n        	webLogBean.setRemote_user(arr[1]);\r\n        	webLogBean.setTime_local(arr[3].substring(1));\r\n        	webLogBean.setRequest(arr[6]);\r\n        	webLogBean.setStatus(arr[8]);\r\n        	webLogBean.setBody_bytes_sent(arr[9]);\r\n        	webLogBean.setHttp_referer(arr[10]);\r\n            \r\n            if (arr.length &gt; 12) {\r\n            	webLogBean.setHttp_user_agent(arr[11] + &quot; &quot; + arr[12]);\r\n            } else {\r\n            	webLogBean.setHttp_user_agent(arr[11]);\r\n            }\r\n            if (Integer.parseInt(webLogBean.getStatus()) &gt;= 400) {// 大于400，HTTP错误\r\n            	webLogBean.setValid(false);\r\n            }\r\n        } else {\r\n        	webLogBean.setValid(false);\r\n        }\r\n        return webLogBean;\r\n    }\r\n   \r\n    public static String parserTime(String time) {\r\n    	\r\n    	time.replace(&quot;/&quot;, &quot;-&quot;);\r\n    	return time;\r\n    	\r\n    }\r\n}\r\n\r\n\r\nc) mapreduce程序\r\npublic class WeblogPreProcess {\r\n\r\n	static class WeblogPreProcessMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {\r\n		Text k = new Text();\r\n		NullWritable v = NullWritable.get();\r\n\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n\r\n			String line = value.toString();\r\n			WebLogBean webLogBean = WebLogParser.parser(line);\r\n			if (!webLogBean.isValid())\r\n				return;\r\n			k.set(webLogBean.toString());\r\n			context.write(k, v);\r\n\r\n		}\r\n\r\n	}\r\n\r\n	public static void main(String[] args) throws Exception {\r\n		\r\n		Configuration conf = new Configuration();\r\n		Job job = Job.getInstance(conf);\r\n		\r\n		job.setJarByClass(WeblogPreProcess.class);\r\n		\r\n		job.setMapperClass(WeblogPreProcessMapper.class);\r\n		\r\n		job.setOutputKeyClass(Text.class);\r\n		job.setOutputValueClass(NullWritable.class);\r\n		\r\n		FileInputFormat.setInputPaths(job, new Path(args[0]));\r\n		FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n		\r\n		job.waitForCompletion(true);\r\n		\r\n	}\r\n}\r\n',13,0,0,1515156286,0,0,0),(337,1,'流量统计相关需求','','','    1、 对流量日志中的用户统计总上、下行流量\r\n技术点： 自定义javaBean用来在mapreduce中充当value\r\n注意： javaBean要实现Writable接口，实现两个方法\r\n	//序列化，将对象的字段信息写入输出流\r\n	@Override\r\n	public void write(DataOutput out) throws IOException {\r\n		\r\n		out.writeLong(upflow);\r\n		out.writeLong(downflow);\r\n		out.writeLong(sumflow);\r\n		\r\n	}\r\n\r\n	//反序列化，从输入流中读取各个字段信息\r\n	@Override\r\n	public void readFields(DataInput in) throws IOException {\r\n		upflow = in.readLong();\r\n		downflow = in.readLong();\r\n		sumflow = in.readLong();\r\n		\r\n	}\r\n\r\n\r\n    2、 统计流量且按照流量大小倒序排序\r\n技术点：这种需求，用一个mapreduce -job 不好实现，需要两个mapreduce -job\r\n第一个job负责流量统计，跟上题相同\r\n第二个job读入第一个job的输出，然后做排序\r\n要将flowBean作为map的key输出，这样mapreduce就会自动排序\r\n     此时，flowBean要实现接口WritableComparable\r\n     要实现其中的compareTo()方法，方法中，我们可以定义倒序比较的逻辑\r\n\r\n\r\n    3、 统计流量且按照手机号的归属地，将结果数据输出到不同的省份文件中\r\n技术点：自定义Partitioner\r\n	@Override\r\n	public int getPartition(Text key, FlowBean value, int numPartitions) {\r\n		\r\n		String prefix = key.toString().substring(0,3);\r\n		Integer partNum = pmap.get(prefix);\r\n		\r\n		return (partNum==null?4:partNum);\r\n	}\r\n\r\n自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce task\r\njob.setNumReduceTasks(5);\r\n\r\n注意：如果reduceTask的数量&gt;= getPartition的结果数  ，则会多产生几个空的输出文件part-r-000xx\r\n如果     1&lt;reduceTask的数量&lt;getPartition的结果数 ，则有一部分分区数据无处安放，会Exception！！！\r\n如果 reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000\r\n\r\n\r\n',13,0,0,1515156334,0,0,0),(338,1,'社交粉丝数据分析','','','以下是qq的好友列表数据，冒号前是一个用，冒号后是该用户的所有好友（数据中的好友关系是单向的）\r\nA:B,C,D,F,E,O\r\nB:A,C,E,K\r\nC:F,A,D,I\r\nD:A,E,F,L\r\nE:B,C,D,M,L\r\nF:A,B,C,D,E,O,M\r\nG:A,C,D,E,F\r\nH:A,C,D,E,O\r\nI:A,O\r\nJ:B,O\r\nK:A,C,D\r\nL:D,E,F\r\nM:E,F,G\r\nO:A,H,I,J\r\n\r\n求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？\r\n解题思路：\r\n第一步  \r\nmap\r\n读一行   A:B,C,D,F,E,O\r\n输出    &lt;B,A&gt;&lt;C,A&gt;&lt;D,A&gt;&lt;F,A&gt;&lt;E,A&gt;&lt;O,A&gt;\r\n在读一行   B:A,C,E,K\r\n输出   &lt;A,B&gt;&lt;C,B&gt;&lt;E,B&gt;&lt;K,B&gt;\r\n\r\n\r\nREDUCE\r\n拿到的数据比如&lt;C,A&gt;&lt;C,B&gt;&lt;C,E&gt;&lt;C,F&gt;&lt;C,G&gt;......\r\n输出：  \r\n&lt;A-B,C&gt;\r\n&lt;A-E,C&gt;\r\n&lt;A-F,C&gt;\r\n&lt;A-G,C&gt;\r\n&lt;B-E,C&gt;\r\n&lt;B-F,C&gt;.....\r\n\r\n\r\n\r\n第二步\r\nmap\r\n读入一行&lt;A-B,C&gt;\r\n直接输出&lt;A-B,C&gt;\r\n\r\nreduce\r\n读入数据  &lt;A-B,C&gt;&lt;A-B,F&gt;&lt;A-B,G&gt;.......\r\n输出： A-B  C,F,G,.....\r\n\r\n扩展：求互粉的人！！！！\r\n\r\n倒排索引建立\r\n需求：有大量的文本（文档、网页），需要建立搜索索引\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    1. 自定义inputFormat\r\n1.1 需求\r\n无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案\r\n\r\n1.2 分析\r\n小文件的优化无非以下几种方式：\r\n    1、 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS\r\n    2、 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并\r\n    3、 在mapreduce处理时，可采用combineInputFormat提高效率\r\n\r\n1.3 实现\r\n本节实现的是上述第二种方式\r\n程序的核心机制：\r\n自定义一个InputFormat\r\n改写RecordReader，实现一次读取一个完整文件封装为KV\r\n在输出时使用SequenceFileOutPutFormat输出合并文件\r\n\r\n代码如下：\r\n自定义InputFromat\r\npublic class WholeFileInputFormat extends\r\n		FileInputFormat&lt;NullWritable, BytesWritable&gt; {\r\n	//设置每个小文件不可分片,保证一个小文件生成一个key-value键值对\r\n	@Override\r\n	protected boolean isSplitable(JobContext context, Path file) {\r\n		return false;\r\n	}\r\n\r\n	@Override\r\n	public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(\r\n			InputSplit split, TaskAttemptContext context) throws IOException,\r\n			InterruptedException {\r\n		WholeFileRecordReader reader = new WholeFileRecordReader();\r\n		reader.initialize(split, context);\r\n		return reader;\r\n	}\r\n}\r\n\r\n\r\n自定义RecordReader\r\nclass WholeFileRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt; {\r\n	private FileSplit fileSplit;\r\n	private Configuration conf;\r\n	private BytesWritable value = new BytesWritable();\r\n	private boolean processed = false;\r\n\r\n	@Override\r\n	public void initialize(InputSplit split, TaskAttemptContext context)\r\n			throws IOException, InterruptedException {\r\n		this.fileSplit = (FileSplit) split;\r\n		this.conf = context.getConfiguration();\r\n	}\r\n\r\n	@Override\r\n	public boolean nextKeyValue() throws IOException, InterruptedException {\r\n		if (!processed) {\r\n			byte[] contents = new byte[(int) fileSplit.getLength()];\r\n			Path file = fileSplit.getPath();\r\n			FileSystem fs = file.getFileSystem(conf);\r\n			FSDataInputStream in = null;\r\n			try {\r\n				in = fs.open(file);\r\n				IOUtils.readFully(in, contents, 0, contents.length);\r\n				value.set(contents, 0, contents.length);\r\n			} finally {\r\n				IOUtils.closeStream(in);\r\n			}\r\n			processed = true;\r\n			return true;\r\n		}\r\n		return false;\r\n	}\r\n\r\n	@Override\r\n	public NullWritable getCurrentKey() throws IOException,\r\n			InterruptedException {\r\n		return NullWritable.get();\r\n	}\r\n\r\n	@Override\r\n	public BytesWritable getCurrentValue() throws IOException,\r\n			InterruptedException {\r\n		return value;\r\n	}\r\n\r\n	@Override\r\n	public float getProgress() throws IOException {\r\n		return processed ? 1.0f : 0.0f;\r\n	}\r\n\r\n	@Override\r\n	public void close() throws IOException {\r\n		// do nothing\r\n	}\r\n}\r\n\r\n定义mapreduce处理流程\r\npublic class SmallFilesToSequenceFileConverter extends Configured implements\r\n		Tool {\r\n	static class SequenceFileMapper extends\r\n			Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt; {\r\n		private Text filenameKey;\r\n\r\n		@Override\r\n		protected void setup(Context context) throws IOException,\r\n				InterruptedException {\r\n			InputSplit split = context.getInputSplit();\r\n			Path path = ((FileSplit) split).getPath();\r\n			filenameKey = new Text(path.toString());\r\n		}\r\n\r\n		@Override\r\n		protected void map(NullWritable key, BytesWritable value,\r\n				Context context) throws IOException, InterruptedException {\r\n			context.write(filenameKey, value);\r\n		}\r\n	}\r\n\r\n	@Override\r\n	public int run(String[] args) throws Exception {\r\n		Configuration conf = new Configuration();\r\n		System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hdfs&quot;);\r\n		String[] otherArgs = new GenericOptionsParser(conf, args)\r\n				.getRemainingArgs();\r\n		if (otherArgs.length != 2) {\r\n			System.err.println(&quot;Usage: combinefiles &lt;in&gt; &lt;out&gt;&quot;);\r\n			System.exit(2);\r\n		}\r\n		\r\n		Job job = Job.getInstance(conf,&quot;combine small files to sequencefile&quot;);\r\n//		job.setInputFormatClass(WholeFileInputFormat.class);\r\n		job.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n		job.setOutputKeyClass(Text.class);\r\n		job.setOutputValueClass(BytesWritable.class);\r\n		job.setMapperClass(SequenceFileMapper.class);\r\n		return job.waitForCompletion(true) ? 0 : 1;\r\n	}\r\n\r\n	public static void main(String[] args) throws Exception {\r\n		int exitCode = ToolRunner.run(new SmallFilesToSequenceFileConverter(),\r\n				args);\r\n		System.exit(exitCode);\r\n		\r\n	}\r\n}\r\n',13,0,0,1515156381,0,0,0),(339,1,'自定义outputFormat','','','需求\r\n现有一些原始日志需要做增强解析处理，流程：\r\n    1、 从原始日志文件中读取数据\r\n    2、 根据日志中的一个URL字段到外部知识库中获取信息增强到原始日志\r\n    3、 如果成功增强，则输出到增强结果目录；如果增强失败，则抽取原始数据中URL字段输出到待爬清单目录\r\n\r\n\r\n2.2 分析\r\n程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现\r\n\r\n2.3 实现\r\n实现要点：\r\n    1、 在mapreduce中访问外部资源\r\n    2、 自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write()\r\n\r\n代码实现如下：\r\n数据库获取数据的工具\r\npublic class DBLoader {\r\n\r\n	public static void dbLoader(HashMap&lt;String, String&gt; ruleMap) {\r\n		Connection conn = null;\r\n		Statement st = null;\r\n		ResultSet res = null;\r\n		\r\n		try {\r\n			Class.forName(&quot;com.mysql.jdbc.Driver&quot;);\r\n			conn = DriverManager.getConnection(&quot;jdbc:mysql://hdp-node01:3306/urlknowledge&quot;, &quot;root&quot;, &quot;root&quot;);\r\n			st = conn.createStatement();\r\n			res = st.executeQuery(&quot;select url,content from urlcontent&quot;);\r\n			while (res.next()) {\r\n				ruleMap.put(res.getString(1), res.getString(2));\r\n			}\r\n		} catch (Exception e) {\r\n			e.printStackTrace();\r\n			\r\n		} finally {\r\n			try{\r\n				if(res!=null){\r\n					res.close();\r\n				}\r\n				if(st!=null){\r\n					st.close();\r\n				}\r\n				if(conn!=null){\r\n					conn.close();\r\n				}\r\n\r\n			}catch(Exception e){\r\n				e.printStackTrace();\r\n			}\r\n		}\r\n	}\r\n	\r\n	\r\n	public static void main(String[] args) {\r\n		DBLoader db = new DBLoader();\r\n		HashMap&lt;String, String&gt; map = new HashMap&lt;String,String&gt;();\r\n		db.dbLoader(map);\r\n		System.out.println(map.size());\r\n	}\r\n}\r\n\r\n\r\n自定义一个outputformat\r\npublic class LogEnhancerOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;{\r\n\r\n	\r\n	@Override\r\n	public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {\r\n\r\n\r\n		FileSystem fs = FileSystem.get(context.getConfiguration());\r\n		Path enhancePath = new Path(&quot;hdfs://hdp-node01:9000/flow/enhancelog/enhanced.log&quot;);\r\n		Path toCrawlPath = new Path(&quot;hdfs://hdp-node01:9000/flow/tocrawl/tocrawl.log&quot;);\r\n		\r\n		FSDataOutputStream enhanceOut = fs.create(enhancePath);\r\n		FSDataOutputStream toCrawlOut = fs.create(toCrawlPath);\r\n		\r\n		\r\n		return new MyRecordWriter(enhanceOut,toCrawlOut);\r\n	}\r\n	\r\n	\r\n	\r\n	static class MyRecordWriter extends RecordWriter&lt;Text, NullWritable&gt;{\r\n		\r\n		FSDataOutputStream enhanceOut = null;\r\n		FSDataOutputStream toCrawlOut = null;\r\n		\r\n		public MyRecordWriter(FSDataOutputStream enhanceOut, FSDataOutputStream toCrawlOut) {\r\n			this.enhanceOut = enhanceOut;\r\n			this.toCrawlOut = toCrawlOut;\r\n		}\r\n\r\n		@Override\r\n		public void write(Text key, NullWritable value) throws IOException, InterruptedException {\r\n			 \r\n			//有了数据，你来负责写到目的地  —— hdfs\r\n			//判断，进来内容如果是带tocrawl的，就往待爬清单输出流中写 toCrawlOut\r\n			if(key.toString().contains(&quot;tocrawl&quot;)){\r\n				toCrawlOut.write(key.toString().getBytes());\r\n			}else{\r\n				enhanceOut.write(key.toString().getBytes());\r\n			}\r\n				\r\n		}\r\n\r\n		@Override\r\n		public void close(TaskAttemptContext context) throws IOException, InterruptedException {\r\n			 \r\n			if(toCrawlOut!=null){\r\n				toCrawlOut.close();\r\n			}\r\n			if(enhanceOut!=null){\r\n				enhanceOut.close();\r\n			}\r\n			\r\n		}\r\n		\r\n		\r\n	}\r\n}\r\n\r\n开发mapreduce处理流程\r\n/**\r\n * 这个程序是对每个小时不断产生的用户上网记录日志进行增强(将日志中的url所指向的网页内容分析结果信息追加到每一行原始日志后面)\r\n * \r\n * @author\r\n * \r\n */\r\npublic class LogEnhancer {\r\n\r\n	static class LogEnhancerMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {\r\n\r\n		HashMap&lt;String, String&gt; knowledgeMap = new HashMap&lt;String, String&gt;();\r\n\r\n		/**\r\n		 * maptask在初始化时会先调用setup方法一次 利用这个机制，将外部的知识库加载到maptask执行的机器内存中\r\n		 */\r\n		@Override\r\n		protected void setup(org.apache.hadoop.mapreduce.Mapper.Context context) throws IOException, InterruptedException {\r\n\r\n			DBLoader.dbLoader(knowledgeMap);\r\n\r\n		}\r\n\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n\r\n			String line = value.toString();\r\n\r\n			String[] fields = StringUtils.split(line, &quot;\\t&quot;);\r\n\r\n			try {\r\n				String url = fields[26];\r\n\r\n				// 对这一行日志中的url去知识库中查找内容分析信息\r\n				String content = knowledgeMap.get(url);\r\n\r\n				// 根据内容信息匹配的结果，来构造两种输出结果\r\n				String result = &quot;&quot;;\r\n				if (null == content) {\r\n					// 输往待爬清单的内容\r\n					result = url + &quot;\\t&quot; + &quot;tocrawl\\n&quot;;\r\n				} else {\r\n					// 输往增强日志的内容\r\n					result = line + &quot;\\t&quot; + content + &quot;\\n&quot;;\r\n				}\r\n\r\n				context.write(new Text(result), NullWritable.get());\r\n			} catch (Exception e) {\r\n\r\n			}\r\n		}\r\n\r\n	}\r\n\r\n	public static void main(String[] args) throws Exception {\r\n\r\n		Configuration conf = new Configuration();\r\n\r\n		Job job = Job.getInstance(conf);\r\n\r\n		job.setJarByClass(LogEnhancer.class);\r\n\r\n		job.setMapperClass(LogEnhancerMapper.class);\r\n\r\n		job.setOutputKeyClass(Text.class);\r\n		job.setOutputValueClass(NullWritable.class);\r\n\r\n		// 要将自定义的输出格式组件设置到job中\r\n		job.setOutputFormatClass(LogEnhancerOutputFormat.class);\r\n\r\n		FileInputFormat.setInputPaths(job, new Path(args[0]));\r\n\r\n		// 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat\r\n		// 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录\r\n		FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n\r\n		job.waitForCompletion(true);\r\n		System.exit(0);\r\n\r\n	}\r\n\r\n}\r\n',13,0,0,1515156413,0,0,0),(340,1,'自定义GroupingComparator','','','需求\r\n有如下订单数据\r\n订单id\r\n商品id\r\n成交金额\r\nOrder_0000001\r\nPdt_01\r\n222.8\r\nOrder_0000001\r\nPdt_05\r\n25.8\r\nOrder_0000002\r\nPdt_03\r\n522.8\r\nOrder_0000002\r\nPdt_04\r\n122.4\r\nOrder_0000003\r\nPdt_01\r\n222.8\r\n\r\n现在需要求出每一个订单中成交金额最大的一笔交易\r\n分析\r\n1、利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce\r\n2、在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值\r\n\r\n\r\n3.3 实现\r\n自定义groupingcomparator\r\n/**\r\n * 用于控制shuffle过程中reduce端对kv对的聚合逻辑\r\n * @author duanhaitao@itcast.cn\r\n *\r\n */\r\npublic class ItemidGroupingComparator extends WritableComparator {\r\n\r\n	protected ItemidGroupingComparator() {\r\n\r\n		super(OrderBean.class, true);\r\n	}\r\n	\r\n\r\n	@Override\r\n	public int compare(WritableComparable a, WritableComparable b) {\r\n		OrderBean abean = (OrderBean) a;\r\n		OrderBean bbean = (OrderBean) b;\r\n		\r\n		//将item_id相同的bean都视为相同，从而聚合为一组\r\n		return abean.getItemid().compareTo(bbean.getItemid());\r\n	}\r\n}\r\n\r\n\r\n定义订单信息bean\r\n/**\r\n * 订单信息bean，实现hadoop的序列化机制\r\n * @author duanhaitao@itcast.cn\r\n *\r\n */\r\npublic class OrderBean implements WritableComparable&lt;OrderBean&gt;{\r\n	private Text itemid;\r\n	private DoubleWritable amount;\r\n\r\n	public OrderBean() {\r\n	}\r\n	public OrderBean(Text itemid, DoubleWritable amount) {\r\n		set(itemid, amount);\r\n	}\r\n\r\n	public void set(Text itemid, DoubleWritable amount) {\r\n\r\n		this.itemid = itemid;\r\n		this.amount = amount;\r\n\r\n	}\r\n\r\n	public Text getItemid() {\r\n		return itemid;\r\n	}\r\n\r\n	public DoubleWritable getAmount() {\r\n		return amount;\r\n	}\r\n\r\n	@Override\r\n	public int compareTo(OrderBean o) {\r\n		int cmp = this.itemid.compareTo(o.getItemid());\r\n		if (cmp == 0) {\r\n\r\n			cmp = -this.amount.compareTo(o.getAmount());\r\n		}\r\n		return cmp;\r\n	}\r\n\r\n	@Override\r\n	public void write(DataOutput out) throws IOException {\r\n		out.writeUTF(itemid.toString());\r\n		out.writeDouble(amount.get());\r\n		\r\n	}\r\n\r\n	@Override\r\n	public void readFields(DataInput in) throws IOException {\r\n		String readUTF = in.readUTF();\r\n		double readDouble = in.readDouble();\r\n		\r\n		this.itemid = new Text(readUTF);\r\n		this.amount= new DoubleWritable(readDouble);\r\n	}\r\n\r\n\r\n	@Override\r\n	public String toString() {\r\n		return itemid.toString() + &quot;\\t&quot; + amount.get();\r\n	}\r\n}\r\n\r\n编写mapreduce处理流程\r\n/**\r\n * 利用secondarysort机制输出每种item订单金额最大的记录\r\n * @author duanhaitao@itcast.cn\r\n *\r\n */\r\npublic class SecondarySort {\r\n	\r\n	static class SecondarySortMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt;{\r\n		\r\n		OrderBean bean = new OrderBean();\r\n		\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n\r\n			String line = value.toString();\r\n			String[] fields = StringUtils.split(line, &quot;\\t&quot;);\r\n			\r\n			bean.set(new Text(fields[0]), new DoubleWritable(Double.parseDouble(fields[1])));\r\n			\r\n			context.write(bean, NullWritable.get());\r\n			\r\n		}\r\n		\r\n	}\r\n	\r\n	static class SecondarySortReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt;{\r\n		\r\n		\r\n		//在设置了groupingcomparator以后，这里收到的kv数据 就是：  &lt;1001 87.6&gt;,null  &lt;1001 76.5&gt;,null  .... \r\n		//此时，reduce方法中的参数key就是上述kv组中的第一个kv的key：&lt;1001 87.6&gt;\r\n		//要输出同一个item的所有订单中最大金额的那一个，就只要输出这个key\r\n		@Override\r\n		protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {\r\n			context.write(key, NullWritable.get());\r\n		}\r\n	}\r\n	\r\n	\r\n	public static void main(String[] args) throws Exception {\r\n		\r\n		Configuration conf = new Configuration();\r\n		Job job = Job.getInstance(conf);\r\n		\r\n		job.setJarByClass(SecondarySort.class);\r\n		\r\n		job.setMapperClass(SecondarySortMapper.class);\r\n		job.setReducerClass(SecondarySortReducer.class);\r\n		\r\n		\r\n		job.setOutputKeyClass(OrderBean.class);\r\n		job.setOutputValueClass(NullWritable.class);\r\n		\r\n		FileInputFormat.setInputPaths(job, new Path(args[0]));\r\n		FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n		//指定shuffle所使用的GroupingComparator类\r\n		job.setGroupingComparatorClass(ItemidGroupingComparator.class);\r\n		//指定shuffle所使用的partitioner类\r\n		job.setPartitionerClass(ItemIdPartitioner.class);\r\n		\r\n		job.setNumReduceTasks(3);\r\n		\r\n		job.waitForCompletion(true);\r\n		\r\n	}\r\n\r\n}\r\n\r\n',13,0,0,1515156474,0,0,0),(341,1,'Map端join案例','','','需求\r\n实现两个“表”的join操作，其中一个表数据量小，一个表很大，这种场景在实际中非常常见，比如“订单日志” join “产品信息”\r\n\r\n\r\n4.1.2 分析\r\n--原理阐述\r\n适用于关联表中有小表的情形；\r\n可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果\r\n可以大大提高join操作的并发度，加快处理速度\r\n\r\n--示例：先在mapper类中预先定义好小表，进行join\r\n--并用distributedcache机制将小表的数据分发到每一个maptask执行节点，从而每一个maptask节点可以从本地加载到小表的数据，进而在本地即可实现join\r\n实现\r\npublic class TestDistributedCache {\r\n	static class TestDistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{\r\n		FileReader in = null;\r\n		BufferedReader reader = null;\r\n		HashMap&lt;String,String&gt; b_tab = new HashMap&lt;String, String&gt;();\r\n		String localpath =null;\r\n		String uirpath = null;\r\n		\r\n		//是在map任务初始化的时候调用一次\r\n		@Override\r\n		protected void setup(Context context) throws IOException, InterruptedException {\r\n			//通过这几句代码可以获取到cache file的本地绝对路径，测试验证用\r\n			Path[] files = context.getLocalCacheFiles();\r\n			localpath = files[0].toString();\r\n			URI[] cacheFiles = context.getCacheFiles();\r\n			\r\n			\r\n			//缓存文件的用法——直接用本地IO来读取\r\n			//这里读的数据是map task所在机器本地工作目录中的一个小文件\r\n			in = new FileReader(&quot;b.txt&quot;);\r\n			reader =new BufferedReader(in);\r\n			String line =null;\r\n			while(null!=(line=reader.readLine())){\r\n				\r\n				String[] fields = line.split(&quot;,&quot;);\r\n				b_tab.put(fields[0],fields[1]);\r\n				\r\n			}\r\n			IOUtils.closeStream(reader);\r\n			IOUtils.closeStream(in);\r\n			\r\n		}\r\n		\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n\r\n			//这里读的是这个map task所负责的那一个切片数据（在hdfs上）\r\n			 String[] fields = value.toString().split(&quot;\\t&quot;);\r\n			 \r\n			 String a_itemid = fields[0];\r\n			 String a_amount = fields[1];\r\n			 \r\n			 String b_name = b_tab.get(a_itemid);\r\n			 \r\n			 // 输出结果  1001	98.9	banan\r\n			 context.write(new Text(a_itemid), new Text(a_amount + &quot;\\t&quot; + &quot;:&quot; + localpath + &quot;\\t&quot; +b_name ));\r\n			 \r\n		}\r\n	}\r\n	public static void main(String[] args) throws Exception {\r\n		\r\n		Configuration conf = new Configuration();\r\n		Job job = Job.getInstance(conf);\r\n		\r\n		job.setJarByClass(TestDistributedCache.class);\r\n		\r\n		job.setMapperClass(TestDistributedCacheMapper.class);\r\n		\r\n		job.setOutputKeyClass(Text.class);\r\n		job.setOutputValueClass(LongWritable.class);\r\n		\r\n		//这里是我们正常的需要处理的数据所在路径\r\n		FileInputFormat.setInputPaths(job, new Path(args[0]));\r\n		FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n		\r\n		//不需要reducer\r\n		job.setNumReduceTasks(0);\r\n		//分发一个文件到task进程的工作目录\r\n		job.addCacheFile(new URI(&quot;hdfs://hadoop-server01:9000/cachefile/b.txt&quot;));\r\n		\r\n		//分发一个归档文件到task进程的工作目录\r\n//		job.addArchiveToClassPath(archive);\r\n\r\n		//分发jar包到task节点的classpath下\r\n//		job.addFileToClassPath(jarfile);\r\n		\r\n		job.waitForCompletion(true);\r\n	}\r\n}\r\n\r\n',13,0,0,1515156524,0,0,0),(342,1,'计数器应用','','','在实际生产代码中，常常需要将数据处理过程中遇到的不合规数据行进行全局计数，类似这种需求可以借助mapreduce框架中提供的全局计数器来实现\r\n示例代码如下：\r\npublic class MultiOutputs {\r\n	//通过枚举形式定义自定义计数器\r\n	enum MyCounter{MALFORORMED,NORMAL}\r\n\r\n	static class CommaMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; {\r\n\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n\r\n			String[] words = value.toString().split(&quot;,&quot;);\r\n\r\n			for (String word : words) {\r\n				context.write(new Text(word), new LongWritable(1));\r\n			}\r\n			//对枚举定义的自定义计数器加1\r\n			context.getCounter(MyCounter.MALFORORMED).increment(1);\r\n			//通过动态设置自定义计数器加1\r\n			context.getCounter(&quot;counterGroupa&quot;, &quot;countera&quot;).increment(1);\r\n		}\r\n\r\n	}\r\n\r\n\r\n\r\n\r\n\r\n\r\n5.2 多job串联\r\n一个稍复杂点的处理逻辑往往需要多个mapreduce程序串联处理，多job的串联可以借助mapreduce框架的JobControl实现\r\n\r\n示例代码：\r\n      ControlledJob cJob1 = new ControlledJob(job1.getConfiguration());\r\n        ControlledJob cJob2 = new ControlledJob(job2.getConfiguration());\r\n        ControlledJob cJob3 = new ControlledJob(job3.getConfiguration());\r\n       \r\n        // 设置作业依赖关系\r\n        cJob2.addDependingJob(cJob1);\r\n        cJob3.addDependingJob(cJob2);\r\n \r\n        JobControl jobControl = new JobControl(&quot;RecommendationJob&quot;);\r\n        jobControl.addJob(cJob1);\r\n        jobControl.addJob(cJob2);\r\n        jobControl.addJob(cJob3);\r\n \r\n        cJob1.setJob(job1);\r\n        cJob2.setJob(job2);\r\n        cJob3.setJob(job3);\r\n \r\n        // 新建一个线程来运行已加入JobControl中的作业，开始进程并等待结束\r\n        Thread jobControlThread = new Thread(jobControl);\r\n        jobControlThread.start();\r\n        while (!jobControl.allFinished()) {\r\n            Thread.sleep(500);\r\n        }\r\n        jobControl.stop();\r\n \r\n        return 0;\r\n\r\n\r\n\r\n\r\n\r\n5.3 Configuration对象高级应用\r\n',13,0,0,1515156589,0,0,0),(343,1,'mapreduce参数优化','','','MapReduce重要配置参数\r\n11.1 资源相关参数\r\n(1) mapreduce.map.memory.mb: 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。\r\n(2) mapreduce.reduce.memory.mb: 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。\r\n(3) mapreduce.map.java.opts: Map Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.\r\n“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc” （@taskid@会被Hadoop框架自动换为相应的taskid）, 默认值: “”\r\n(4) mapreduce.reduce.java.opts: Reduce Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.\r\n“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc”, 默认值: “”\r\n(5) mapreduce.map.cpu.vcores: 每个Map task可使用的最多cpu core数目, 默认值: 1\r\n(6) mapreduce.map.cpu.vcores: 每个Reduce task可使用的最多cpu core数目, 默认值: 1\r\n\r\n11.2 容错相关参数\r\n(1) mapreduce.map.maxattempts: 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。\r\n(2) mapreduce.reduce.maxattempts: 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。\r\n(3) mapreduce.map.failures.maxpercent: 当失败的Map Task失败比例超过该值为，整个作业则失败，默认值为0. 如果你的应用程序允许丢弃部分输入数据，则该该值设为一个大于0的值，比如5，表示如果有低于5%的Map Task失败（如果一个Map Task重试次数超过mapreduce.map.maxattempts，则认为这个Map Task失败，其对应的输入数据将不会产生任何结果），整个作业扔认为成功。\r\n(4) mapreduce.reduce.failures.maxpercent: 当失败的Reduce Task失败比例超过该值为，整个作业则失败，默认值为0.\r\n(5) mapreduce.task.timeout: Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是300000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。\r\n11.3 本地运行mapreduce 作业\r\n设置以下几个参数:\r\nmapreduce.framework.name=local\r\nmapreduce.jobtracker.address=local\r\nfs.defaultFS=local\r\n11.4 效率和稳定性相关参数\r\n(1) mapreduce.map.speculative: 是否为Map Task打开推测执行机制，默认为false\r\n(2) mapreduce.reduce.speculative: 是否为Reduce Task打开推测执行机制，默认为false\r\n(3) mapreduce.job.user.classpath.first &amp; mapreduce.task.classpath.user.precedence：当同一个class同时出现在用户jar包和hadoop jar中时，优先使用哪个jar包中的class，默认为false，表示优先使用hadoop jar中的class。\r\n(4) mapreduce.input.fileinputformat.split.minsize: 每个Map Task处理的数据量（仅针对基于文件的Inputformat有效，比如TextInputFormat，SequenceFileInputFormat），默认为一个block大小，即 134217728。\r\n',13,0,0,1515156644,0,0,0),(344,1,'hadoop集群启动','','','先启动zookeeper集群\r\n再在5/6/7上启动journalnode\r\nhadoop-daemon.sh start journalnode\r\n\r\n\r\n在bi下nn1上  \r\nhdfs namenode -format –clusterID itcast\r\nhdfs zkfc -formatZK\r\n拷贝元数据目录到standby(nn2)\r\n\r\n在dt下nn3上  \r\nhdfs namenode -format –clusterID itcast   ###clusterID必须与bi的相同\r\nhdfs zkfc -formatZK\r\n拷贝元数据目录到standby(nn4)\r\n\r\n在bi下nn1上\r\nsbin/start-dfs.sh\r\n\r\n在resoucemanager配置的主机上启动yarn\r\nsbin/start-yarn.sh',13,0,0,1515156843,0,0,0),(345,1,'HA的运作机制','','','hadoop-HA集群运作机制介绍\r\n所谓HA，即高可用（7*24小时不中断服务）\r\n实现高可用最关键的是消除单点故障\r\nhadoop-ha严格来说应该分成各个组件的HA机制——HDFS的HA、YARN的HA\r\n\r\n（2）HDFS的HA机制详解\r\n通过双namenode消除单点故障\r\n双namenode协调工作的要点：\r\n	A、元数据管理方式需要改变：\r\n	内存中各自保存一份元数据\r\n	Edits日志只能有一份，只有Active状态的namenode节点可以做写操作\r\n	两个namenode都可以读取edits\r\n	共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）\r\n	B、需要一个状态管理功能模块\r\n	实现了一个zkfailover，常驻在每一个namenode所在的节点\r\n	每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识\r\n	当需要进行状态切换时，由zkfailover来负责切换\r\n	切换时需要防止brain split现象的发生\r\n',13,0,0,1515156947,0,0,0),(346,1,'HA集群的安装部署','','','集群节点规划\r\n集群部署节点角色的规划（10节点）：\r\nserver01   namenode   zkfc    &gt; start-dfs.sh\r\nserver02   namenode   zkfc\r\n\r\nserver03   resourcemanager    &gt; start-yarn.sh\r\nserver04   resourcemanager\r\n\r\nserver05   datanode   nodemanager     \r\nserver06   datanode   nodemanager     \r\nserver07   datanode   nodemanager     \r\n\r\nserver08   journal node    zookeeper\r\nserver09   journal node    zookeeper\r\nserver10   journal node    zookeeper\r\n\r\n集群部署节点角色的规划（3节点）\r\nserver01   namenode    resourcemanager  zkfc   nodemanager  datanode   zookeeper   journal node\r\nserver02   namenode    resourcemanager  zkfc   nodemanager  datanode   zookeeper   journal node\r\nserver05   datanode    nodemanager     zookeeper    journal node\r\n 环境准备\r\n1、环境准备\r\na/linux系统准备\r\n	ip地址配置\r\n    hostname配置\r\n    hosts映射配置\r\n	防火墙关闭\r\ninit启动级别修改\r\nsudoers加入hadoop用户\r\nssh免密登陆配置\r\n\r\nb/java环境的配置\r\n    上传jdk，解压，修改/etc/profile\r\n\r\n c/zookeeper集群的部署\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1.2.3 配置文件\r\n\r\ncore-site.xml\r\n				&lt;configuration&gt;\r\n					&lt;!-- 指定hdfs的nameservice为ns1 --&gt;\r\n					&lt;property&gt;\r\n						&lt;name&gt;fs.defaultFS&lt;/name&gt;\r\n						&lt;value&gt;hdfs://ns1/&lt;/value&gt;\r\n					&lt;/property&gt;\r\n					&lt;!-- 指定hadoop临时目录 --&gt;\r\n					&lt;property&gt;\r\n						&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\r\n						&lt;value&gt;/home/hadoop/app/hadoop-2.4.1/tmp&lt;/value&gt;\r\n					&lt;/property&gt;\r\n					\r\n					&lt;!-- 指定zookeeper地址 --&gt;\r\n					&lt;property&gt;\r\n						&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;\r\n						&lt;value&gt;weekend05:2181,weekend06:2181,weekend07:2181&lt;/value&gt;\r\n					&lt;/property&gt;\r\n				&lt;/configuration&gt;\r\n\r\n\r\nhdfs-site.xml\r\nconfiguration&gt;\r\n&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.nameservices&lt;/name&gt;\r\n	&lt;value&gt;ns1&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt;\r\n	&lt;value&gt;nn1,nn2&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- nn1的RPC通信地址 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt;\r\n	&lt;value&gt;weekend01:9000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- nn1的http通信地址 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt;\r\n	&lt;value&gt;weekend01:50070&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- nn2的RPC通信地址 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt;\r\n	&lt;value&gt;weekend02:9000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- nn2的http通信地址 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt;\r\n	&lt;value&gt;weekend02:50070&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定NameNode的edits元数据在JournalNode上的存放位置 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;\r\n	&lt;value&gt;qjournal://weekend05:8485;weekend06:8485;weekend07:8485/ns1&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;\r\n	&lt;value&gt;/home/hadoop/app/hadoop-2.4.1/journaldata&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 开启NameNode失败自动切换 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;\r\n	&lt;value&gt;true&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 配置失败自动切换实现方式 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt;\r\n	&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\r\n	&lt;value&gt;\r\n		sshfence\r\n		shell(/bin/true)\r\n	&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;\r\n	&lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 配置sshfence隔离机制超时时间 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;\r\n	&lt;value&gt;30000&lt;/value&gt;\r\n&lt;/property&gt;\r\n/configuration&gt;\r\n\r\n\r\n1.2.4 集群运维测试\r\n1、Datanode动态上下线\r\nDatanode动态上下线很简单，步骤如下：\r\n    a) 准备一台服务器，设置好环境\r\n    b) 部署hadoop的安装包，并同步集群配置\r\n    c) 联网上线，新datanode会自动加入集群\r\n    d) 如果是一次增加大批datanode，还应该做集群负载重均衡\r\n\r\n\r\n2、Namenode状态切换管理\r\n使用的命令上hdfs  haadmin\r\n可用 hdfs  haadmin –help查看所有帮助信息\r\n\r\n\r\n可以看到，状态操作的命令示例：\r\n查看namenode工作状态   \r\nhdfs haadmin -getServiceState nn1\r\n\r\n将standby状态namenode切换到active\r\nhdfs haadmin –transitionToActive nn1\r\n\r\n将active状态namenode切换到standby\r\nhdfs haadmin –transitionToStandby nn2\r\n\r\n\r\n\r\n3、数据块的balance\r\n启动balancer的命令：\r\nstart-balancer.sh -threshold 8\r\n运行之后，会有Balancer进程出现：\r\n\r\n上述命令设置了Threshold为8%，那么执行balancer命令的时候，首先统计所有DataNode的磁盘利用率的均值，然后判断如果某一个DataNode的磁盘利用率超过这个均值Threshold，那么将会把这个DataNode的block转移到磁盘利用率低的DataNode，这对于新节点的加入来说十分有用。Threshold的值为1到100之间，不显示的进行参数设置的话，默认是10。\r\n',13,0,0,1515157013,0,0,0),(347,1,'HA下hdfs-api变化','','','客户端需要nameservice的配置信息，其他不变\r\n/**\r\n * 如果访问的是一个ha机制的集群\r\n * 则一定要把core-site.xml和hdfs-site.xml配置文件放在客户端程序的classpath下\r\n * 以让客户端能够理解hdfs://ns1/中  “ns1”是一个ha机制中的namenode对——nameservice\r\n * 以及知道ns1下具体的namenode通信地址\r\n * @author\r\n *\r\n */\r\npublic class UploadFile {\r\n	\r\n	public static void main(String[] args) throws Exception  {\r\n		\r\n		Configuration conf = new Configuration();\r\n		conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://ns1/&quot;);\r\n		\r\n		FileSystem fs = FileSystem.get(new URI(&quot;hdfs://ns1/&quot;),conf,&quot;hadoop&quot;);\r\n		\r\n		fs.copyFromLocalFile(new Path(&quot;g:/eclipse-jee-luna-SR1-linux-gtk.tar.gz&quot;), new Path(&quot;hdfs://ns1/&quot;));\r\n		\r\n		fs.close();\r\n		\r\n	}\r\n	\r\n\r\n}\r\n\r\nFederation下 mr程序运行的staging提交目录问题\r\n&lt;property&gt;\r\n  &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;\r\n  &lt;value&gt;/bi/tmp/hadoop-yarn/staging&lt;/value&gt;\r\n  &lt;description&gt;The staging dir used while submitting jobs.\r\n  &lt;/description&gt;\r\n&lt;/property&gt;',13,0,0,1515157049,0,0,0),(348,1,'hadoop ha集群搭建 (七个节点)','','','hadoop2.0已经发布了稳定版本了，增加了很多特性，比如HDFS HA、YARN等。最新的hadoop-2.6.4又增加了YARN HA\r\n\r\n注意：apache提供的hadoop-2.6.4的安装包是在32位操作系统编译的，因为hadoop依赖一些C++的本地库，\r\n所以如果在64位的操作上安装hadoop-2.6.4就需要重新在64操作系统上重新编译\r\n（建议第一次安装用32位的系统，我将编译好的64位的也上传到群共享里了，如果有兴趣的可以自己编译一下）\r\n\r\n前期准备就不详细说了，课堂上都介绍了\r\n1.修改Linux主机名\r\n2.修改IP\r\n3.修改主机名和IP的映射关系 /etc/hosts\r\n	######注意######如果你们公司是租用的服务器或是使用的云主机（如华为用主机、阿里云主机等）\r\n	/etc/hosts里面要配置的是内网IP地址和主机名的映射关系	\r\n4.关闭防火墙\r\n5.ssh免登陆\r\n6.安装JDK，配置环境变量等\r\n\r\n集群规划：\r\n	主机名		IP				安装的软件					运行的进程\r\n	mini1	192.168.1.200	jdk、hadoop					NameNode、DFSZKFailoverController(zkfc)\r\n	mini2	192.168.1.201	jdk、hadoop					NameNode、DFSZKFailoverController(zkfc)\r\n	mini3	192.168.1.202	jdk、hadoop					ResourceManager \r\n	mini4	192.168.1.203	jdk、hadoop					ResourceManager\r\n	mini5	192.168.1.205	jdk、hadoop、zookeeper		DataNode、NodeManager、JournalNode、QuorumPeerMain\r\n	mini6	192.168.1.206	jdk、hadoop、zookeeper		DataNode、NodeManager、JournalNode、QuorumPeerMain\r\n	mini7	192.168.1.207	jdk、hadoop、zookeeper		DataNode、NodeManager、JournalNode、QuorumPeerMain\r\n	\r\n说明：\r\n	1.在hadoop2.0中通常由两个NameNode组成，一个处于active状态，另一个处于standby状态。Active NameNode对外提供服务，而Standby NameNode则不对外提供服务，仅同步active namenode的状态，以便能够在它失败时快速进行切换。\r\n	hadoop2.0官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。这里我们使用简单的QJM。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode\r\n	这里还配置了一个zookeeper集群，用于ZKFC（DFSZKFailoverController）故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode为standby状态\r\n	2.hadoop-2.2.0中依然存在一个问题，就是ResourceManager只有一个，存在单点故障，hadoop-2.6.4解决了这个问题，有两个ResourceManager，一个是Active，一个是Standby，状态由zookeeper进行协调\r\n安装步骤：\r\n	1.安装配置zooekeeper集群（在hadoop05上）\r\n		1.1解压\r\n			tar -zxvf zookeeper-3.4.5.tar.gz -C /home/hadoop/app/\r\n		1.2修改配置\r\n			cd /home/hadoop/app/zookeeper-3.4.5/conf/\r\n			cp zoo_sample.cfg zoo.cfg\r\n			vim zoo.cfg\r\n			修改：dataDir=/home/hadoop/app/zookeeper-3.4.5/tmp\r\n			在最后添加：\r\n			server.1=hadoop05:2888:3888\r\n			server.2=hadoop06:2888:3888\r\n			server.3=hadoop07:2888:3888\r\n			保存退出\r\n			然后创建一个tmp文件夹\r\n			mkdir /home/hadoop/app/zookeeper-3.4.5/tmp\r\n			echo 1 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myid\r\n		1.3将配置好的zookeeper拷贝到其他节点(首先分别在hadoop06、hadoop07根目录下创建一个hadoop目录：mkdir /hadoop)\r\n			scp -r /home/hadoop/app/zookeeper-3.4.5/ hadoop06:/home/hadoop/app/\r\n			scp -r /home/hadoop/app/zookeeper-3.4.5/ hadoop07:/home/hadoop/app/\r\n			\r\n			注意：修改hadoop06、hadoop07对应/hadoop/zookeeper-3.4.5/tmp/myid内容\r\n			hadoop06：\r\n				echo 2 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myid\r\n			hadoop07：\r\n				echo 3 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myid\r\n	\r\n	2.安装配置hadoop集群（在hadoop00上操作）\r\n		2.1解压\r\n			tar -zxvf hadoop-2.6.4.tar.gz -C /home/hadoop/app/\r\n		2.2配置HDFS（hadoop2.0所有的配置文件都在$HADOOP_HOME/etc/hadoop目录下）\r\n			#将hadoop添加到环境变量中\r\n			vim /etc/profile\r\n			export JAVA_HOME=/usr/java/jdk1.7.0_55\r\n			export HADOOP_HOME=/hadoop/hadoop-2.6.4\r\n			export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin\r\n			\r\n			#hadoop2.0的配置文件全部在$HADOOP_HOME/etc/hadoop下\r\n			cd /home/hadoop/app/hadoop-2.6.4/etc/hadoop\r\n			\r\n			2.2.1修改hadoo-env.sh\r\n				export JAVA_HOME=/home/hadoop/app/jdk1.7.0_55\r\n\r\n###############################################################################\r\n				\r\n2.2.2修改core-site.xml\r\n&lt;configuration&gt;\r\n&lt;!-- 指定hdfs的nameservice为ns1 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;fs.defaultFS&lt;/name&gt;\r\n&lt;value&gt;hdfs://bi/&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定hadoop临时目录 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\r\n&lt;value&gt;/home/hadoop/app/hdpdata/&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;!-- 指定zookeeper地址 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;\r\n&lt;value&gt;mini5:2181,mini6:2181,mini7:2181&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n\r\n###############################################################################\r\n				\r\n2.2.3修改hdfs-site.xml\r\n&lt;configuration&gt;\r\n&lt;!--指定hdfs的nameservice为bi，需要和core-site.xml中的保持一致 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.nameservices&lt;/name&gt;\r\n&lt;value&gt;bi&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- bi下面有两个NameNode，分别是nn1，nn2 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.ha.namenodes.bi&lt;/name&gt;\r\n&lt;value&gt;nn1,nn2&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- nn1的RPC通信地址 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.namenode.rpc-address.bi.nn1&lt;/name&gt;\r\n&lt;value&gt;mini1:9000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- nn1的http通信地址 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.namenode.http-address.bi.nn1&lt;/name&gt;\r\n&lt;value&gt;mini1:50070&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- nn2的RPC通信地址 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.namenode.rpc-address.bi.nn2&lt;/name&gt;\r\n&lt;value&gt;mini2:9000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- nn2的http通信地址 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.namenode.http-address.bi.nn2&lt;/name&gt;\r\n&lt;value&gt;mini2:50070&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定NameNode的edits元数据在JournalNode上的存放位置 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;\r\n&lt;value&gt;qjournal://mini5:8485;mini6:8485;mini7:8485/bi&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;\r\n&lt;value&gt;/home/hadoop/journaldata&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 开启NameNode失败自动切换 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;\r\n&lt;value&gt;true&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 配置失败自动切换实现方式 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.client.failover.proxy.provider.bi&lt;/name&gt;\r\n&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\r\n&lt;value&gt;\r\nsshfence\r\nshell(/bin/true)\r\n&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;\r\n&lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 配置sshfence隔离机制超时时间 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;\r\n&lt;value&gt;30000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n\r\n###############################################################################\r\n			\r\n2.2.4修改mapred-site.xml\r\n&lt;configuration&gt;\r\n&lt;!-- 指定mr框架为yarn方式 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapreduce.framework.name&lt;/name&gt;\r\n&lt;value&gt;yarn&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;	\r\n\r\n###############################################################################\r\n			\r\n2.2.5修改yarn-site.xml\r\n&lt;configuration&gt;\r\n&lt;!-- 开启RM高可用 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;\r\n&lt;value&gt;true&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定RM的cluster id --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;\r\n&lt;value&gt;yrc&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定RM的名字 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;\r\n&lt;value&gt;rm1,rm2&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 分别指定RM的地址 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;\r\n&lt;value&gt;mini3&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;\r\n&lt;value&gt;mini4&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定zk集群地址 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;\r\n&lt;value&gt;mini5:2181,mini6:2181,mini7:2181&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\r\n&lt;value&gt;mapreduce_shuffle&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n			\r\n				\r\n2.2.6修改slaves(slaves是指定子节点的位置，因为要在hadoop01上启动HDFS、在hadoop03启动yarn，所以hadoop01上的slaves文件指定的是datanode的位置，hadoop03上的slaves文件指定的是nodemanager的位置)\r\nmini5\r\nmini6\r\nmini7\r\n\r\n			2.2.7配置免密码登陆\r\n				#首先要配置hadoop00到hadoop01、hadoop02、hadoop03、hadoop04、hadoop05、hadoop06、hadoop07的免密码登陆\r\n				#在hadoop01上生产一对钥匙\r\n				ssh-keygen -t rsa\r\n				#将公钥拷贝到其他节点，包括自己\r\n				ssh-coyp-id hadoop00\r\n				ssh-coyp-id hadoop01\r\n				ssh-coyp-id hadoop02\r\n				ssh-coyp-id hadoop03\r\n				ssh-coyp-id hadoop04\r\n				ssh-coyp-id hadoop05\r\n				ssh-coyp-id hadoop06\r\n				ssh-coyp-id hadoop07\r\n				#配置hadoop02到hadoop04、hadoop05、hadoop06、hadoop07的免密码登陆\r\n				#在hadoop02上生产一对钥匙\r\n				ssh-keygen -t rsa\r\n				#将公钥拷贝到其他节点\r\n				ssh-coyp-id hadoop03				\r\n				ssh-coyp-id hadoop04\r\n				ssh-coyp-id hadoop05\r\n				ssh-coyp-id hadoop06\r\n				ssh-coyp-id hadoop07\r\n				#注意：两个namenode之间要配置ssh免密码登陆，别忘了配置hadoop01到hadoop00的免登陆\r\n				在hadoop01上生产一对钥匙\r\n				ssh-keygen -t rsa\r\n				ssh-coyp-id -i hadoop00				\r\n		\r\n		2.4将配置好的hadoop拷贝到其他节点\r\n			scp -r /hadoop/ hadoop02:/\r\n			scp -r /hadoop/ hadoop03:/\r\n			scp -r /hadoop/hadoop-2.6.4/ hadoop@hadoop04:/hadoop/\r\n			scp -r /hadoop/hadoop-2.6.4/ hadoop@hadoop05:/hadoop/\r\n			scp -r /hadoop/hadoop-2.6.4/ hadoop@hadoop06:/hadoop/\r\n			scp -r /hadoop/hadoop-2.6.4/ hadoop@hadoop07:/hadoop/\r\n			\r\n			\r\n			\r\n###注意：严格按照下面的步骤!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n		2.5启动zookeeper集群（分别在mini5、mini6、mini7上启动zk）\r\n			cd /hadoop/zookeeper-3.4.5/bin/\r\n			./zkServer.sh start\r\n			#查看状态：一个leader，两个follower\r\n			./zkServer.sh status\r\n			\r\n		2.6启动journalnode（分别在在mini5、mini6、mini7上执行）\r\n			cd /hadoop/hadoop-2.6.4\r\n			sbin/hadoop-daemon.sh start journalnode\r\n			#运行jps命令检验，hadoop05、hadoop06、hadoop07上多了JournalNode进程\r\n		\r\n		2.7格式化HDFS\r\n			#在mini1上执行命令:\r\n			hdfs namenode -format\r\n			#格式化后会在根据core-site.xml中的hadoop.tmp.dir配置生成个文件，这里我配置的是/hadoop/hadoop-2.6.4/tmp，然后将/hadoop/hadoop-2.6.4/tmp拷贝到hadoop02的/hadoop/hadoop-2.6.4/下。\r\n			scp -r tmp/ hadoop02:/home/hadoop/app/hadoop-2.6.4/\r\n			##也可以这样，建议hdfs namenode -bootstrapStandby\r\n		\r\n		2.8格式化ZKFC(在mini1上执行一次即可)\r\n			hdfs zkfc -formatZK\r\n		\r\n		2.9启动HDFS(在mini1上执行)\r\n			sbin/start-dfs.sh\r\n\r\n		2.10启动YARN(#####注意#####：是在hadoop02上执行start-yarn.sh，把namenode和resourcemanager分开是因为性能问题，因为他们都要占用大量资源，所以把他们分开了，他们分开了就要分别在不同的机器上启动)\r\n			sbin/start-yarn.sh\r\n\r\n		\r\n	到此，hadoop-2.6.4配置完毕，可以统计浏览器访问:\r\n		http://hadoop00:50070\r\n		NameNode &#039;hadoop01:9000&#039; (active)\r\n		http://hadoop01:50070\r\n		NameNode &#039;hadoop02:9000&#039; (standby)\r\n	\r\n	验证HDFS HA\r\n		首先向hdfs上传一个文件\r\n		hadoop fs -put /etc/profile /profile\r\n		hadoop fs -ls /\r\n		然后再kill掉active的NameNode\r\n		kill -9 &lt;pid of NN&gt;\r\n		通过浏览器访问：http://192.168.1.202:50070\r\n		NameNode &#039;hadoop02:9000&#039; (active)\r\n		这个时候hadoop02上的NameNode变成了active\r\n		在执行命令：\r\n		hadoop fs -ls /\r\n		-rw-r--r--   3 root supergroup       1926 2014-02-06 15:36 /profile\r\n		刚才上传的文件依然存在！！！\r\n		手动启动那个挂掉的NameNode\r\n		sbin/hadoop-daemon.sh start namenode\r\n		通过浏览器访问：http://192.168.1.201:50070\r\n		NameNode &#039;hadoop01:9000&#039; (standby)\r\n	\r\n	验证YARN：\r\n		运行一下hadoop提供的demo中的WordCount程序：\r\n		hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar wordcount /profile /out\r\n	\r\n	OK，大功告成！！！\r\n\r\n	\r\n			\r\n		\r\n测试集群工作状态的一些指令 ：\r\nbin/hdfs dfsadmin -report	 查看hdfs的各节点状态信息\r\n\r\n\r\nbin/hdfs haadmin -getServiceState nn1		 获取一个namenode节点的HA状态\r\n\r\nsbin/hadoop-daemon.sh start namenode  单独启动一个namenode进程\r\n\r\n\r\n./hadoop-daemon.sh start zkfc   单独启动一个zkfc进程\r\n			\r\n			\r\n',13,0,0,1515157130,0,0,0),(349,1,'mapreduce总结','','','mapreduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，除了以下几处：\r\n\r\n1、输入数据接口：InputFormat   ---&gt;     FileInputFormat(文件类型数据读取的通用抽象类)  DBInputFormat （数据库数据读取的通用抽象类）\r\n   默认使用的实现类是： TextInputFormat     job.setInputFormatClass(TextInputFormat.class)\r\n   TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回\r\n   \r\n2、逻辑处理接口： Mapper  \r\n   完全需要用户自己去实现其中  map()   setup()   clean()   \r\n   \r\n3、map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义：\r\n     Partitioner\r\n		有默认实现 HashPartitioner，逻辑是  根据key和numReduces来返回一个分区号； key.hashCode()&amp;Integer.MAXVALUE % numReduces\r\n	通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义\r\n	 \r\n	 Comparable\r\n		当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，override其中的compareTo()方法\r\n\r\n4、reduce端的数据分组比较接口 ： Groupingcomparator\r\n	reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数\r\n	\r\n	利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑：\r\n	自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果\r\n	然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）\r\n	这样，我们要取的最大值就是reduce()方法中传进来key\r\n	\r\n\r\n5、逻辑处理接口：Reducer\r\n	完全需要用户自己去实现其中  reduce()   setup()   clean()   \r\n\r\n6、输出数据接口： OutputFormat  ---&gt; 有一系列子类  FileOutputformat  DBoutputFormat  .....\r\n	默认实现类是TextOutputFormat，功能逻辑是：  将每一个KV对向目标文本文件中输出为一行\r\n	\r\n	',13,0,0,1515157209,0,0,0),(350,1,'HQL','','','show databases;\r\nshow tables;\r\ndesc test;\r\n\r\n-------------\r\n分桶表示例：\r\n\r\nset mapreduce.job.reduces=4;\r\n\r\ndrop table stu_buck;\r\ncreate table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)\r\nclustered by(Sno) \r\nsorted by(Sno DESC)\r\ninto 4 buckets\r\nrow format delimited\r\nfields terminated by &#039;,&#039;;\r\n\r\n\r\ninsert overwrite table student_buck\r\nselect * from student cluster by(Sno) sort by(Sage);  报错,cluster 和 sort 不能共存\r\n\r\n\r\n\r\ninsert into table stu_buck\r\nselect Sno,Sname,Sex,Sage,Sdept from student distribute by(Sno) sort by(Sno asc);\r\n\r\ninsert overwrite table stu_buck\r\nselect * from student distribute by(Sno) sort by(Sno asc);\r\n\r\ninsert overwrite table stu_buck\r\nselect * from student cluster by(Sno);\r\n\r\n-------------\r\n多重插入：\r\n\r\nfrom student\r\ninsert into table student_p partition(part=&#039;a&#039;)\r\nselect * where Sno&lt;95011;\r\ninsert into table student_p partition(part=&#039;a&#039;)\r\nselect * where Sno&lt;95011;\r\n\r\n\r\n--------------\r\n导出数据到本地：\r\n\r\ninsert overwrite local directory &#039;/home/hadoop/student.txt&#039;\r\nselect * from student;\r\n\r\n-------------\r\nUDF案例：\r\ncreate table rat_json(line string) row format delimited;\r\nload data local inpath &#039;/home/hadoop/rating.json&#039; into table rat_json;\r\n\r\ndrop table if exists t_rating;\r\ncreate table t_rating(movieid string,rate int,timestring string,uid string)\r\nrow format delimited fields terminated by &#039;\\t&#039;;\r\n\r\ninsert overwrite table t_rating\r\nselect split(parsejson(line),&#039;,&#039;)[0]as movieid,split(parsejson(line),&#039;,&#039;)[1] as rate,split(parsejson(line),&#039;,&#039;)[2] as timestring,split(parsejson(line),&#039;,&#039;)[3] as uid from rat_json limit 10;\r\n\r\n\r\n-------\r\n内置jason函数\r\nselect get_json_object(line,&#039;$.movie&#039;) as moive,get_json_object(line,&#039;$.rate&#039;) as rate  from rat_json limit 10;\r\n\r\n\r\n-----------\r\ntransform案例:\r\n\r\nINSERT OVERWRITE TABLE u_data_new\r\nSELECT\r\n  TRANSFORM (movieid, rate, timestring,uid)\r\n  USING &#039;python weekday_mapper.py&#039;\r\n  AS (movieid, rate, weekday,uid)\r\nFROM t_rating;\r\n\r\nselect distinct(weekday) from u_data_new limit 10;\r\n\r\n\r\n',14,0,0,1515157273,0,0,0),(351,1,'hive安装','','','Hive只在一个节点上安装即可 (apeng1 192.168.179.135)\r\n\r\n1.上传tar包 /opt/download\r\n\r\n2.解压\r\ntar -zxvf hive-0.9.0.tar.gz -C /opt/app\r\n\r\n3.安装mysql数据库 (peng3 192.168.179.129)（切换到root用户）（装在哪里没有限制，只有能联通hadoop集群的节点）\r\nmysql安装仅供参考，不同版本mysql有各自的安装流程\r\n	rpm -qa | grep mysql\r\n	rpm -e mysql-libs-5.1.66-2.el6_3.i686 --nodeps\r\n	rpm -ivh MySQL-server-5.1.73-1.glibc23.i386.rpm \r\n	rpm -ivh MySQL-client-5.1.73-1.glibc23.i386.rpm \r\n修改mysql的密码\r\n/usr/bin/mysql_secure_installation\r\n（注意：删除匿名用户，允许用户远程连接）\r\n登陆mysql\r\nmysql -u root -p\r\n\r\n4.配置hive\r\na.配置HIVE_HOME环境变量  vi conf/hive-env.sh 配置其中的$hadoop_home\r\ncp hive-env.sh.template hive-env.sh\r\nvi hive-env.sh\r\nHADOOP_HOME=/opt/app/hadoop-2.6.4\r\n\r\nb.配置元数据库信息   vi  hive-site.xml \r\n	添加如下内容：\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\r\n&lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;\r\n&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\r\n&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\r\n&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\r\n&lt;value&gt;peng&lt;/value&gt;\r\n&lt;description&gt;username to use against metastore database&lt;/description&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\r\n&lt;value&gt;0418peng&lt;/value&gt;\r\n&lt;description&gt;password to use against metastore database&lt;/description&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n	\r\n5.安装hive和mysq完成后，将mysql的连接jar包拷贝到$HIVE_HOME/lib目录下\r\n	如果出现没有权限的问题，在mysql授权(在安装mysql的机器上执行)\r\n	mysql -uroot -p\r\n	#(执行下面的语句  *.*:所有库下的所有表   %：任何IP地址或主机都可以连接)\r\n	GRANT ALL PRIVILEGES ON *.* TO &#039;root&#039;@&#039;%&#039; IDENTIFIED BY &#039;root&#039; WITH GRANT OPTION;\r\n	FLUSH PRIVILEGES;\r\n\r\n6. Jline包版本不一致的问题，需要拷贝hive的lib目录中jline.2.12.jar的jar包替换掉hadoop中的 \r\n/home/hadoop/app/hadoop-2.6.4/share/hadoop/yarn/lib/jline-0.9.94.jar\r\n[root@apeng1 lib]# cp jline-2.12.jar /opt/app/hadoop-2.6.4/share/hadoop/yarn/lib/\r\n[root@apeng1 lib]# cd /opt/app/hadoop-2.6.4/share/hadoop/yarn/lib/\r\n[root@apeng1 lib]# vi jline-\r\njline-0.9.94.jar  jline-2.12.jar    \r\n[root@apeng1 lib]# rm -rf jline-0.9.94.jar \r\n\r\n7.启动hive\r\nbin/hive\r\n',14,0,0,1515157304,0,0,0),(352,1,'HIVE CLI命令','','','显示当前会话有多少函数可用 \r\nSHOW FUNCTIONS;\r\n显示函数的描述信息 \r\nDESC FUNCTION concat;\r\n\r\n显示函数的扩展描述信息 \r\nDESC FUNCTION EXTENDED concat;\r\n简单函数\r\n函数的计算粒度为单条记录。 \r\n关系运算 \r\n数学运算 \r\n逻辑运算 \r\n数值计算 \r\n类型转换 \r\n日期函数 \r\n条件函数 \r\n字符串函数 \r\n统计函数\r\n聚合函数\r\n函数处理的数据粒度为多条记录。 \r\nsum()—求和 \r\ncount()—求数据量 \r\navg()—求平均直 \r\ndistinct—求不同值数 \r\nmin—求最小值 \r\nmax—求最人值\r\n集合函数\r\n复合类型构建 \r\n复杂类型访问 \r\n复杂类型长度\r\n特殊函数\r\n窗口函数\r\n应用场景 \r\n用于分区排序 \r\n动态Group By \r\nTop N \r\n累计计算 \r\n层次查询\r\nWindowing functions\r\nlead\r\nlag\r\nFIRST_VALUE\r\nLAST_VALUE\r\n分析函数\r\nAnalytics functions\r\nRANK\r\nROW_NUMBER\r\nDENSE_RANK\r\nCUME_DIST\r\nPERCENT_RANK\r\nNTILE\r\n混合函数\r\njava_method(class,method [,arg1 [,arg2])reflect(class,method [,arg1 [,arg2..]])hash(a1 [,a2...])\r\nUDTF\r\nlateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,‘ columnAlias)*  fromClause: FROM baseTable (lateralView)*  \r\nateral view用于和split, explode等UDTF一起使用，它能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。\r\n常用函数Demo：\r\ncreate table employee(\r\nid string,\r\nmoney double,\r\ntype string)row format delimited \r\nfields terminated by ‘\\t‘ \r\nlines terminated by ‘\\n‘ \r\nstored as textfile;load data local inpath ‘/liguodong/hive/data‘ into table employee;select * from employee;\r\n优先级依次为NOT AND ORselect id,money from employee where (id=‘1001‘ or id=‘1002‘) and money=‘100‘;\r\n\r\n\r\ncast类型转换\r\nselect cast(1.5 as int);\r\n\r\n\r\nif判断\r\nif(con,‘‘,‘‘);\r\n\r\nhive (default)&gt; select if(2&gt;1,‘YES‘,‘NO‘);\r\nYES\r\ncase when con then ‘‘ when con then ‘‘ else  ‘‘ end (‘‘里面类型要一样)\r\nselect case when id=‘1001‘ then ‘v1001‘ when id=‘1002‘ then ‘v1002‘ else ‘v1003‘ end from employee;\r\n\r\nget_json_object\r\nget_json_object(json 解析函数，用来处理json，必须是json格式)select get_json_object(‘{&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:&quot;20&quot;}‘,‘$.name‘);\r\n\r\nURL解析函数\r\nparse_url(string urlString, string partToExtract [, string keyToExtract])\r\nselect parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1‘, ‘HOST‘) from\r\nemployee limit 1;\r\n\r\n字符串连接函数： concat \r\n语法: concat(string A, string B…) \r\n返回值: string \r\n说明：返回输入字符串连接后的结果，支持任意个输入字符串 \r\n举例：\r\nhive&gt; select concat(‘abc‘,‘def’,‘gh‘) from lxw_dual;\r\nabcdefgh\r\n带分隔符字符串连接函数： concat_ws \r\n语法: concat_ws(string SEP, string A, string B…) \r\n返回值: string \r\n说明：返回输入字符串连接后的结果， SEP 表示各个字符串间的分隔符\r\nconcat_ws(string SEP, array&lt;string&gt;)\r\n举例：\r\nhive&gt; select concat_ws(‘,‘,‘abc‘,‘def‘,‘gh‘) from lxw_dual;\r\nabc,def,gh\r\n\r\n列出该字段所有不重复的值，相当于去重\r\ncollect_set(id)  //返回的是数组\r\n列出该字段所有的值，列出来不去重 \r\ncollect_list(id)   //返回的是数组\r\nselect collect_set(id) from taborder;\r\n\r\n\r\n求和\r\nsum(money)\r\n统计列数\r\ncount(*)\r\nselect sum(num),count(*) from taborder;\r\n\r\n窗口函数\r\nfirst_value(第一行值)\r\n\r\nfirst_value(money) over (partition by id order by money)\r\nselect ch,num,first_value(num) over (partition by ch order by num) from taborder;\r\n\r\n\r\nrows between 1 preceding and 1 following (当前行以及当前行的前一行与后一行)\r\n\r\n\r\nhive (liguodong)&gt; select ch,num,first_value(num) over (partition by ch order by num ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) from taborder;\r\n\r\nlast_value 最后一行值\r\nhive (liguodong)&gt; select ch,num,last_value(num) over (partition by ch) from taborder;\r\n\r\n lead\r\n 去当前行后面的第二行的值\r\n lead(money,2) over (order by money)\r\n\r\n lag \r\n 去当前行前面的第二行的值\r\n lag(money,2) over (order by money)\r\n ```\r\n\r\n ```\r\n select ch, num, lead(num,2) over (order by num) from taborder;\r\n\r\n select ch, num, lag(num,2) over (order by num) from taborder;\r\n\r\nrank排名\r\nrank() over(partition by id order by money)\r\nselect ch, num, rank() over(partition by ch order by num) as rank from taborder;\r\n\r\nselect ch, num, dense_rank() over(partition by ch order by num) as dense_rank from taborder;\r\n\r\ncume_dist\r\ncume_dist (相同值的最大行号/行数)\r\ncume_dist() over (partition by id order by money)\r\n\r\npercent_rank (相同值的最小行号-1)/(行数-1)\r\n第一个总是从0开始\r\npercent_rank() over (partition by id order by money)\r\n\r\n\r\n select ch,num,cume_dist() over (partition by ch order by num) as cume_dist,\r\n percent_rank() over (partition by ch order by num) as percent_rank\r\n from taborder;\r\n\r\n ntile分片 \r\n ntile(2) over (order by money desc)  分两份 \r\n\r\n select ch,num,ntile(2) over (order by num desc) from taborder;\r\n\r\n混合函数\r\nselect id,java_method(&quot;java.lang,Math&quot;,&quot;sqrt&quot;,cast(id as double)) as sqrt from hiveTest;\r\nUDTF\r\n select id,adid \r\n from employee \r\n lateral view explode(split(type,‘B‘)) tt as adid;\r\n explode 把一列转成多行\r\n\r\nhive (liguodong)&gt;  select id,adid\r\n                &gt;  from hiveDemo\r\n                &gt;  lateral view explode(split(str,‘,‘)) tt as adid;\r\n\r\n正则表达式 \r\n使用正则表达式的函数 \r\nregexp_replace(string subject A,string B,string C) \r\nregexp_extract(string subject,string pattern,int index)\r\nhive&gt; select regexp_replace(‘foobar‘, ‘oo|ar‘, ‘‘) from lxw_dual;\r\nfb\r\n\r\nhive&gt; select regexp_replace(‘979|7.10.80|8684‘, ‘.*\\\\|(.*)‘,1) from hiveDemo limit 1;\r\n\r\nhive&gt; select regexp_replace(‘979|7.10.80|8684‘, ‘(.*?)\\\\|(.*)‘,1) from hiveDemo limit 1;\r\n',13,0,0,1515157481,0,0,0),(353,1,'Hive简介','','','            1.1.1 什么是Hive\r\nHive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。\r\n            1.1.2 为什么使用Hive\r\n    • 直接使用hadoop所面临的问题 \r\n人员学习成本太高 \r\n项目周期要求太短 \r\nMapReduce实现复杂查询逻辑开发难度太大 \r\n\r\n    • 为什么要使用Hive \r\n操作接口采用类SQL语法，提供快速开发的能力。 \r\n避免了去写MapReduce，减少开发人员的学习成本。 \r\n扩展功能很方便。\r\n            1.1.3 Hive的特点\r\n    • 可扩展 \r\nHive可以自由的扩展集群的规模，一般情况下不需要重启服务。\r\n\r\n    • 延展性 \r\nHive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。\r\n\r\n    • 容错 \r\n良好的容错性，节点出现问题SQL仍可完成执行。\r\n\r\n         Hive架构\r\n            1.2.1 架构图\r\n\r\nJobtracker是hadoop1.x中的组件，它的功能相当于： Resourcemanager+AppMaster\r\n\r\nTaskTracker 相当于：  Nodemanager  +  yarnchild\r\n\r\n            1.2.2 基本组成\r\n    • 用户接口：包括 CLI、JDBC/ODBC、WebGUI。\r\n    • 元数据存储：通常是存储在关系数据库如 mysql , derby中。\r\n    • 解释器、编译器、优化器、执行器。\r\n\r\n            1.2.3 各组件的基本功能\r\n    • 用户接口主要由三个：CLI、JDBC/ODBC和WebGUI。其中，CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。\r\n    • 元数据存储：Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。\r\n    • 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。',14,0,0,1515157741,0,0,0),(354,1,'Hive与Hadoop的关系 ','','','Hive利用HDFS存储数据，利用MapReduce查询数据\r\n\r\n\r\n        1.4 Hive与传统数据库对比\r\n\r\n\r\n总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析',14,0,0,1515157798,0,0,0),(355,1,'Hive的数据存储','','','1、Hive中所有的数据都存储在 HDFS 中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，RCFILE等）\r\n2、只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据。\r\n3、Hive 中包含以下数据模型：DB、Table，External Table，Partition，Bucket。\r\n    • db：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹\r\n    • table：在hdfs中表现所属db目录下一个文件夹\r\n    • external table：外部表, 与table类似，不过其数据存放位置可以在任意指定路径\r\n普通表: 删除表后, hdfs上的文件都删了\r\nExternal外部表删除后, hdfs上的文件没有删除, 只是把文件删除了\r\n    • partition：在hdfs中表现为table目录下的子目录\r\n    • bucket：桶, 在hdfs中表现为同一个表目录下根据hash散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中 \r\nHIVE的安装部署\r\n1.6.1 安装\r\n单机版：\r\n元数据库mysql版：\r\n\r\n1.6.2 使用方式\r\nHive交互shell\r\nbin/hive\r\nHive thrift服务\r\n\r\n启动方式，（假如是在hadoop01上）：\r\n启动为前台：bin/hiveserver2\r\n启动为后台：nohup bin/hiveserver2 1&gt;/var/log/hiveserver.log 2&gt;/var/log/hiveserver.err &amp;\r\n\r\n启动成功后，可以在别的节点上用beeline去连接\r\n    • 方式（1）\r\nhive/bin/beeline  回车，进入beeline的命令界面\r\n\r\n输入命令连接hiveserver2\r\nbeeline&gt; !connect jdbc:hive2//mini1:10000\r\n（hadoop01是hiveserver2所启动的那台主机名，端口默认是10000）\r\n    • 方式（2）\r\n或者启动就连接：\r\nbin/beeline -u jdbc:hive2://mini1:10000 -n hadoop\r\n\r\n接下来就可以做正常sql查询了\r\n\r\n\r\nHive命令\r\n[hadoop@hdp-node-02 ~]$ hive  -e  ‘sql’',14,0,0,1515157860,0,0,0),(356,1,' hive 建表','','','1.建表语法\r\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name \r\n   [(col_name data_type [COMMENT col_comment], ...)] \r\n   [COMMENT table_comment] \r\n   [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] \r\n   [CLUSTERED BY (col_name, col_name, ...) \r\n   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] \r\n   [ROW FORMAT row_format] \r\n   [STORED AS file_format] \r\n   [LOCATION hdfs_path]\r\n\r\n说明：\r\na.CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。\r\nb.EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。\r\nc.LIKE 允许用户复制现有的表结构，但是不复制数据。\r\nd.ROW FORMAT \r\nDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] \r\n        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] \r\n   | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]\r\n用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。\r\ne.STORED AS \r\nSEQUENCEFILE|TEXTFILE|RCFILE\r\n如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。\r\nf.CLUSTERED BY\r\n对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 \r\n把表（或者分区）组织成桶（Bucket）有两个理由：\r\n（1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。\r\n（2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。\r\n\r\n2.建表(默认是内部表)\r\ncreate table trade_detail(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#039;\\t&#039;;\r\n建分区表\r\ncreate table td_part(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by &#039;\\t&#039;;\r\n建外部表\r\ncreate external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#039;\\t&#039; location &#039;/td_ext&#039;;\r\n\r\n3.创建分区表\r\n普通表和分区表区别：有大量数据增加的需要建分区表\r\ncreate table book (id bigint, name string) partitioned by (pubdate string) row format delimited fields terminated by &#039;\\t&#039;; \r\n\r\n分区表加载数据\r\nload data local inpath &#039;./book.txt&#039; overwrite into table book partition (pubdate=&#039;2010-08-22&#039;);\r\n	\r\nload data local inpath &#039;/root/data.am&#039; into table beauty partition (nation=&quot;USA&quot;);\r\n	\r\nselect nation, avg(size) from beauties group by nation order by avg(size);\r\n\r\n4.具体实例\r\na.创建内部表mytable （/user/hive/warehouse/mytable）\r\nhive&gt; \r\n    &gt; create table if not exists mytable(sid int, sname string)\r\n    &gt; row format delimited fields terminated by &#039;\\005&#039; stored as textfile;\r\nOK\r\nTime taken: 0.669 seconds\r\nhive&gt; show tables;\r\nOK\r\nmytable\r\nTime taken: 0.116 seconds, Fetched: 1 row(s)\r\n\r\nb.创建外部表pageview\r\nhive&gt; create external table if not exists pageview(pageid int, page_url string comment &#039;the page url&#039;)\r\n    &gt; row format delimited fields terminated by &#039;,&#039;\r\n    &gt; location &#039;hdfs://192.168.179.135:9000/user/hive/warehouse/&#039;;\r\nOK\r\nTime taken: 0.05 seconds\r\nhive&gt; show tables;\r\nOK\r\nmytable\r\npageview\r\nTime taken: 0.03 seconds, Fetched: 2 row(s)\r\n\r\nload data local inpath &#039;/opt/input/t_peng1.data&#039; into table pageview; 导入数据\r\n\r\nc.创建分区表invites。\r\ncreate table student_p(Sno int,Sname string,Sex string,Sage int,Sdept string) partitioned by(part string) row format delimited fields terminated by &#039;,&#039;stored as textfile;\r\nhive&gt; \r\n    &gt; create table t_peng_part(id int, name string)\r\n    &gt; partitioned by (country string)\r\n    &gt; row format delimited\r\n    &gt; fields terminated by &#039;,&#039;;\r\nload data local inpath &#039;/opt/input/china.data&#039; into table t_peng_part partition(country=&#039;china&#039;);\r\nload data local inpath &#039;/opt/input/china.data&#039; into table t_peng_part partition(country=&#039;japan&#039;);\r\n\r\nd.创建带桶的表student。\r\nhive&gt; \r\n    &gt; create table t_bulk(id string, name string)\r\n    &gt; clustered by (id) \r\n    &gt; sorted by (id) \r\n    &gt; into 4 buckets \r\n    &gt; row format delimited fields terminated by &#039;,&#039;;\r\n\r\n',14,0,0,1515157967,0,0,0),(357,1,'hive修改表','','','            2.1.2 修改表\r\n增加/删除分区\r\n    • 语法结构\r\nALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION &#039;location1&#039; ] partition_spec [ LOCATION &#039;location2&#039; ] ...\r\npartition_spec:\r\n: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)\r\n\r\nALTER TABLE table_name DROP partition_spec, partition_spec,...\r\n    • 具体实例\r\nalter table student_p add partition(part=&#039;a&#039;) partition(part=&#039;b&#039;);\r\n\r\n\r\n\r\n重命名表\r\n    • 语法结构\r\nALTER TABLE table_name RENAME TO new_table_name\r\n    • 具体实例\r\n\r\n增加/更新列\r\n    • 语法结构\r\nALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) \r\n\r\n注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。\r\n\r\nALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]\r\n    • 具体实例\r\n\r\n            2.1.3 显示命令\r\nshow tables\r\nshow databases\r\nshow partitions\r\nshow functions\r\ndesc extended t_name;\r\ndesc formatted table_name;',14,0,0,1515158000,0,0,0),(358,1,'Hive操作之DML','','','            2.2.1 Load\r\n    • 语法结构\r\nLOAD DATA [LOCAL] INPATH &#039;filepath&#039; [OVERWRITE] INTO \r\nTABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]\r\n\r\n说明：\r\n    1、 Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。\r\n    2、 filepath：\r\n相对路径，例如：project/data1 \r\n绝对路径，例如：/user/hive/project/data1 \r\n包含模式的完整 URI，列如：\r\nhdfs://namenode:9000/user/hive/project/data1\r\n    3、 LOCAL关键字\r\n如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。\r\n如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件\r\n\r\n\r\n    4、 OVERWRITE 关键字\r\n如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 \r\n如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 \r\n\r\n    • 具体实例\r\n    1、 加载相对路径数据。\r\n\r\n\r\n    2、 加载绝对路径数据。\r\n\r\n\r\n    3、 加载包含模式数据。\r\n\r\n\r\n    4、 OVERWRITE关键字使用。\r\n\r\n            2.2.2 Insert\r\n    • 将查询结果插入Hive表\r\n    • 语法结构\r\nINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement\r\n\r\nMultiple inserts:\r\nFROM from_statement \r\nINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 \r\n[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...\r\n\r\nDynamic partition inserts:\r\nINSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement\r\n\r\n    • 具体实例\r\n1、基本模式插入。\r\n\r\n\r\n多插入模式。\r\n\r\n\r\n3、自动分区模式。\r\n\r\n\r\n    • 导出表数据\r\n    • 语法结构\r\nINSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...\r\n\r\n\r\nmultiple inserts:\r\nFROM from_statement\r\n\r\n[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...\r\n\r\n    • 具体实例\r\n1、导出文件到本地。\r\n\r\n\r\n说明：\r\n数据写入到文件系统时进行文本序列化，且每列用^A来区分，\\n为换行符。用more命令查看时不容易看出分割符，可以使用: sed -e &#039;s/\\x01/|/g&#039; filename来查看。\r\n\r\n2、导出数据到HDFS。\r\n\r\n            2.2.3 SELECT\r\n    • 基本的Select操作\r\n    • 语法结构\r\nSELECT [ALL | DISTINCT] select_expr, select_expr, ... \r\nFROM table_reference\r\n[WHERE where_condition] \r\n[GROUP BY col_list [HAVING condition]] \r\n[CLUSTER BY col_list \r\n  | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list] \r\n] \r\n[LIMIT number]\r\n\r\n注：1、order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。\r\n2、sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。\r\n3、distribute by根据distribute by指定的内容将数据分到同一个reducer。\r\n4、Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by\r\n\r\n\r\n    • 具体实例\r\n1、获取年龄大的3个学生。\r\n\r\n\r\n2、查询学生信息按年龄，降序排序。',14,0,0,1515158100,0,0,0),(359,1,'hive join','','','语法结构\r\njoin_table:\r\n  table_reference JOIN table_factor [join_condition]\r\n  | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition\r\n  | table_reference LEFT SEMI JOIN table_reference join_condition\r\nHive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。\r\n另外，Hive 支持多于 2 个表的连接。\r\n写 join 查询时，需要注意几个关键点：\r\n1. 只支持等值join\r\n例如： \r\n  SELECT a.* FROM a JOIN b ON (a.id = b.id)\r\n  SELECT a.* FROM a JOIN b\r\n    ON (a.id = b.id AND a.department = b.department)\r\n是正确的，然而:\r\n  SELECT a.* FROM a JOIN b ON (a.id&gt;b.id)\r\n是错误的。\r\n\r\n2. 可以 join 多于 2 个表。\r\n例如\r\n  SELECT a.val, b.val, c.val FROM a JOIN b\r\n    ON (a.key = b.key1) JOIN c ON (c.key = b.key2)\r\n如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如：\r\n  SELECT a.val, b.val, c.val FROM a JOIN b\r\n    ON (a.key = b.key1) JOIN c\r\n    ON (c.key = b.key1)\r\n被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。\r\nSELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1)\r\n  JOIN c ON (c.key = b.key2)\r\n而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。\r\n   \r\n3．join 时，每次 map/reduce 任务的逻辑：\r\n    reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如：\r\n SELECT a.val, b.val, c.val FROM a\r\n    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)\r\n所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有：\r\n  SELECT a.val, b.val, c.val FROM a\r\n    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)\r\n这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。\r\n\r\n4．LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况\r\n例如：\r\n  SELECT a.val, b.val FROM \r\na LEFT OUTER  JOIN b ON (a.key=b.key)\r\n对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出:\r\na.val, NULL\r\n所以 a 表中的所有记录都被保留了；\r\n“a RIGHT OUTER JOIN b”会保留所有 b 表的记录。\r\n\r\nJoin 发生在 WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在 join 子句中写。这里面一个容易混淆的问题是表分区的情况：\r\n  SELECT a.val, b.val FROM a\r\n  LEFT OUTER JOIN b ON (a.key=b.key)\r\n  WHERE a.ds=&#039;2009-07-07&#039; AND b.ds=&#039;2009-07-07&#039;\r\n会 join a 表到 b 表（OUTER JOIN），列出 a.val 和 b.val 的记录。WHERE 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 b 表中找不到对应 a 表的记录，b 表的所有列都会列出 NULL，包括 ds 列。也就是说，join 会过滤 b 表中不能找到匹配 a 表 join key 的所有记录。这样的话，LEFT OUTER 就使得查询结果与 WHERE 子句无关了。解决的办法是在 OUTER JOIN 时使用以下语法：\r\n  SELECT a.val, b.val FROM a LEFT OUTER JOIN b\r\n  ON (a.key=b.key AND\r\n      b.ds=&#039;2009-07-07&#039; AND\r\n      a.ds=&#039;2009-07-07&#039;)\r\n这一查询的结果是预先在 join 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 RIGHT 和 FULL 类型的 join 中。\r\n\r\nJoin 是不能交换位置的。无论是 LEFT 还是 RIGHT join，都是左连接的。\r\n  SELECT a.val1, a.val2, b.val, c.val\r\n  FROM a\r\n  JOIN b ON (a.key = b.key)\r\n  LEFT OUTER JOIN c ON (a.key = c.key)\r\n先 join a 表到 b 表，丢弃掉所有 join key 中不匹配的记录，然后用这一中间结果和 c 表做 join。这一表述有一个不太明显的问题，就是当一个 key 在 a 表和 c 表都存在，但是 b 表中不存在的时候：整个记录在第一次 join，即 a JOIN b 的时候都被丢掉了（包括a.val1，a.val2和a.key），然后我们再和 c 表 join 的时候，如果 c.key 与 a.key 或 b.key 相等，就会得到这样的结果：NULL, NULL, NULL, c.val\r\n\r\n    • 具体实例\r\n    1、 获取已经分配班级的学生姓名。\r\n\r\n\r\n    2、 获取尚未分配班级的学生姓名。\r\n\r\n\r\n    3、 LEFT  SEMI  JOIN是IN/EXISTS的高效实现。',14,0,0,1515158124,0,0,0),(360,1,'hive命令行(hive shell参数)','','','1.语法结构\r\nhive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S]\r\n\r\n说明：\r\na. -i 从文件初始化HQL。\r\nb. -e从命令行执行指定的HQL \r\nc. -f 执行HQL脚本 \r\nd. -v 输出执行的HQL语句到控制台 \r\ne. -p &lt;port&gt; connect to Hive Server on port number \r\nf. -hiveconf x=y Use this to set hive/hadoop configuration variables.\r\n\r\n2.具体实例\r\n1、运行一个查询。\r\n2、运行一个文件。\r\n\r\n\r\n3、运行参数文件。\r\n\r\n',14,0,0,1515158162,0,0,0);
INSERT INTO `resource_techn_article` VALUES (361,1,'Hive参数配置方式','','','Hive参数大全：\r\nhttps://cwiki.apache.org/confluence/display/Hive/Configuration+Properties\r\n\r\n开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。\r\n\r\n对于一般参数，有以下三种设定方式：\r\n    • 配置文件 \r\n    • 命令行参数 \r\n    • 参数声明 \r\n\r\n配置文件：Hive的配置文件包括\r\n    • 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml \r\n    • 默认配置文件：$HIVE_CONF_DIR/hive-default.xml \r\n用户自定义配置会覆盖默认配置。\r\n另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。\r\n配置文件的设定对本机启动的所有Hive进程都有效。\r\n\r\n命令行参数：启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：\r\nbin/hive -hiveconf hive.root.logger=INFO,console\r\n这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。\r\n\r\n参数声明：可以在HQL中使用SET关键字设定参数，例如：\r\nset mapred.reduce.tasks=100;\r\n这一设定的作用域也是session级的。\r\n\r\n上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。\r\n',14,0,0,1515158187,0,0,0),(362,1,'Hive函数','','','内置运算符\r\n内容较多，见《Hive官方文档》\r\n\r\n4.2 内置函数\r\n内容较多，见《Hive官方文档》\r\n\r\n\r\n\r\n4.3 Hive自定义函数和Transform\r\n当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。\r\n4.3.1 自定义函数类别\r\nUDF  作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数）\r\nUDAF（用户定义聚集函数）：接收多个输入数据行，并产生一个输出数据行。（count，max）\r\n\r\n4.3.2 UDF开发实例\r\n1、先开发一个java类，继承UDF，并重载evaluate方法\r\npackage cn.itcast.bigdata.udf\r\nimport org.apache.hadoop.hive.ql.exec.UDF;\r\nimport org.apache.hadoop.io.Text;\r\n\r\npublic final class Lower extends UDF{\r\n	public Text evaluate(final Text s){\r\n		if(s==null){return null;}\r\n		return new Text(s.toString().toLowerCase());\r\n	}\r\n}\r\n\r\n2、打成jar包上传到服务器\r\n3、将jar包添加到hive的classpath\r\nhive&gt;add JAR /home/hadoop/udf.jar;\r\n    4、 创建临时函数与开发好的java class关联\r\nHive&gt;create temporary function toprovince as &#039;cn.itcast.bigdata.udf.ToProvince&#039;;\r\n\r\n    5、 即可在hql中使用自定义的函数strip \r\nSelect strip(name),age from t_test;\r\n4.3.3 Transform实现\r\nHive的 TRANSFORM 关键字提供了在SQL中调用自写脚本的功能\r\n适合实现Hive中没有的功能又不想写UDF的情况\r\n\r\n使用示例1：下面这句sql就是借用了weekday_mapper.py对数据进行了处理.\r\nCREATE TABLE u_data_new (\r\n  movieid INT,\r\n  rating INT,\r\n  weekday INT,\r\n  userid INT)\r\nROW FORMAT DELIMITED\r\nFIELDS TERMINATED BY &#039;\\t&#039;;\r\n\r\nadd FILE weekday_mapper.py;\r\n\r\nINSERT OVERWRITE TABLE u_data_new\r\nSELECT\r\n  TRANSFORM (movieid, rating, unixtime,userid)\r\n  USING &#039;python weekday_mapper.py&#039;\r\n  AS (movieid, rating, weekday,userid)\r\nFROM u_data;\r\n\r\n其中weekday_mapper.py内容如下\r\n#!/bin/python\r\nimport sys\r\nimport datetime\r\n\r\nfor line in sys.stdin:\r\n  line = line.strip()\r\n  movieid, rating, unixtime,userid = line.split(&#039;\\t&#039;)\r\n  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()\r\n  print &#039;\\t&#039;.join([movieid, rating, str(weekday),userid])\r\n\r\n使用示例2：下面的例子则是使用了shell的cat命令来处理数据\r\nFROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING &#039;/bin/cat&#039; WHERE a.ds &gt; &#039;2008-08-09&#039;;\r\n',14,0,0,1515158215,0,0,0),(363,1,'Hive实战','','','Hive 实战案例1——数据ETL\r\n需求：\r\n    • 对web点击流日志基础数据表进行etl（按照仓库模型设计）\r\n    • 按各时间维度统计来源域名top10\r\n已有数据表 “t_orgin_weblog” ：\r\n+------------------+------------+----------+--+\r\n|     col_name     | data_type  | comment  |\r\n+------------------+------------+----------+--+\r\n| valid            | string     |          |\r\n| remote_addr      | string     |          |\r\n| remote_user      | string     |          |\r\n| time_local       | string     |          |\r\n| request          | string     |          |\r\n| status           | string     |          |\r\n| body_bytes_sent  | string     |          |\r\n| http_referer     | string     |          |\r\n| http_user_agent  | string     |          |\r\n+------------------+------------+----------+--+\r\n\r\n\r\n\r\n\r\n数据示例：\r\n| true|1.162.203.134| - | 18/Sep/2013:13:47:35| /images/my.jpg                        | 200| 19939 | &quot;http://www.angularjs.cn/A0d9&quot;                      | &quot;Mozilla/5.0 (Windows   |\r\n\r\n| true|1.202.186.37 | - | 18/Sep/2013:15:39:11| /wp-content/uploads/2013/08/windjs.png| 200| 34613 | &quot;http://cnodejs.org/topic/521a30d4bee8d3cb1272ac0f&quot; | &quot;Mozilla/5.0 (Macintosh;|\r\n\r\n\r\n\r\n\r\n实现步骤：\r\n1、对原始数据进行抽取转换\r\n--将来访url分离出host  path  query  query id\r\ndrop table if exists t_etl_referurl;\r\ncreate table t_etl_referurl as\r\nSELECT a.*,b.*\r\nFROM t_orgin_weblog a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, &quot;\\&quot;&quot;, &quot;&quot;), &#039;HOST&#039;, &#039;PATH&#039;,&#039;QUERY&#039;, &#039;QUERY:id&#039;) b as host, path, query, query_id \r\n\r\n\r\n3、从前述步骤进一步分离出日期时间形成ETL明细表“t_etl_detail”    day tm   \r\ndrop table if exists t_etl_detail;\r\ncreate table t_etl_detail as \r\nselect b.*,substring(time_local,0,11) as daystr,\r\nsubstring(time_local,13) as tmstr,\r\nsubstring(time_local,4,3) as month,\r\nsubstring(time_local,0,2) as day,\r\nsubstring(time_local,13,2) as hour\r\nfrom t_etl_referurl b;\r\n\r\n\r\n\r\n3、对etl数据进行分区(包含所有数据的结构化信息)\r\ndrop table t_etl_detail_prt;\r\ncreate table t_etl_detail_prt(\r\nvalid                   string,\r\nremote_addr            string,\r\nremote_user            string,\r\ntime_local               string,\r\nrequest                 string,\r\nstatus                  string,\r\nbody_bytes_sent         string,\r\nhttp_referer             string,\r\nhttp_user_agent         string,\r\nhost                   string,\r\npath                   string,\r\nquery                  string,\r\nquery_id               string,\r\ndaystr                 string,\r\ntmstr                  string,\r\nmonth                  string,\r\nday                    string,\r\nhour                   string) \r\npartitioned by (mm string,dd string);\r\n\r\n\r\n\r\n\r\n导入数据\r\ninsert into table t_etl_detail_prt partition(mm=&#039;Sep&#039;,dd=&#039;18&#039;)\r\nselect * from t_etl_detail where daystr=&#039;18/Sep/2013&#039;;\r\n\r\ninsert into table t_etl_detail_prt partition(mm=&#039;Sep&#039;,dd=&#039;19&#039;)\r\nselect * from t_etl_detail where daystr=&#039;19/Sep/2013&#039;;\r\n\r\n分个时间维度统计各referer_host的访问次数并排序\r\ncreate table t_refer_host_visit_top_tmp as\r\nselect referer_host,count(*) as counts,mm,dd,hh from t_display_referer_counts group by hh,dd,mm,referer_host order by hh asc,dd asc,mm asc,counts desc;\r\n\r\n\r\n4、来源访问次数topn各时间维度URL\r\n取各时间维度的referer_host访问次数topn\r\nselect * from (select referer_host,counts,concat(hh,dd),row_number() over (partition by concat(hh,dd) order by concat(hh,dd) asc) as od from t_refer_host_visit_top_tmp) t where od&lt;=3;\r\n\r\n\r\n\r\n\r\n\r\n\r\nHive 实战案例2——访问时长统计\r\n需求：\r\n从web日志中统计每日访客平均停留时间\r\n实现步骤：\r\n    1、 由于要从大量请求中分辨出用户的各次访问，逻辑相对复杂，通过hive直接实现有困难，因此编写一个mr程序来求出访客访问信息（详见代码）\r\n启动mr程序获取结果：\r\n[hadoop@hdp-node-01 ~]$ hadoop jar weblog.jar cn.itcast.bigdata.hive.mr.UserStayTime /weblog/input /weblog/stayout\r\n\r\n\r\n    2、 将mr的处理结果导入hive表\r\ndrop table t_display_access_info_tmp;\r\ncreate table t_display_access_info_tmp(remote_addr string,firt_req_time string,last_req_time string,stay_long bigint)\r\nrow format delimited fields terminated by &#039;\\t&#039;;\r\n\r\nload data inpath &#039;/weblog/stayout4&#039; into table t_display_access_info_tmp;\r\n\r\n3、得出访客访问信息表 &quot;t_display_access_info&quot;\r\n由于有一些访问记录是单条记录，mr程序处理处的结果给的时长是0，所以考虑给单次请求的停留时间一个默认市场30秒\r\ndrop table t_display_access_info;\r\ncreate table t_display_access_info as\r\nselect remote_addr,firt_req_time,last_req_time,\r\ncase stay_long\r\nwhen 0 then 30000\r\nelse stay_long\r\nend as stay_long\r\nfrom t_display_access_info_tmp;\r\n\r\n\r\n4、统计所有用户停留时间平均值\r\nselect avg(stay_long) from t_display_access_info;\r\n\r\n\r\n\r\n\r\nHive实战案例3——级联求和\r\n需求：\r\n有如下访客访问次数统计表 t_access_times\r\n访客\r\n月份\r\n访问次数\r\nA\r\n2015-01-02\r\n5\r\nA\r\n2015-01-03\r\n15\r\nB\r\n2015-01-01\r\n5\r\nA\r\n2015-01-04\r\n8\r\nB\r\n2015-01-05\r\n25\r\nA\r\n2015-01-06\r\n5\r\nA\r\n2015-02-02\r\n4\r\nA\r\n2015-02-06\r\n6\r\nB\r\n2015-02-06\r\n10\r\nB\r\n2015-02-07\r\n5\r\n……\r\n……\r\n……\r\n\r\n需要输出报表：t_access_times_accumulate\r\n访客\r\n月份\r\n月访问总计\r\n累计访问总计\r\nA\r\n2015-01\r\n33\r\n33\r\nA\r\n2015-02\r\n10\r\n43\r\n…….\r\n…….\r\n…….\r\n…….\r\nB\r\n2015-01\r\n30\r\n30\r\nB\r\n2015-02\r\n15\r\n45\r\n…….\r\n…….\r\n…….\r\n…….\r\n\r\n实现步骤\r\n可以用一个hql语句即可实现：\r\nselect A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulate\r\nfrom \r\n(select username,month,sum(salary) as salary from t_access_times group by username,month) A \r\ninner join \r\n(select username,month,sum(salary) as salary from t_access_times group by username,month) B\r\non\r\nA.username=B.username\r\nwhere B.month &lt;= A.month\r\ngroup by A.username,A.month\r\norder by A.username,A.month;\r\n',14,0,0,1515158238,0,0,0),(364,1,'页面点击流模型','','','    1、 原始数据   access.log.fensi\r\n    2、 清洗上述数据，得到规整的结果：\r\n时间戳\r\nIP地址\r\nCookie\r\nSession\r\n请求URL\r\nReferal\r\n2012-01-01 12:31:12\r\n101.0.0.1\r\nUser01\r\nS001\r\n/a/...\r\nsomesite.com\r\n2012-01-01 12:31:16\r\n201.0.0.2\r\nUser02\r\nS002\r\n/a/...\r\n-\r\n2012-01-01 12:33:06\r\n101.0.0.2\r\nUser03\r\nS002\r\n/b/...\r\nbaidu.com\r\n2012-01-01 15:16:39\r\n234.0.0.3\r\nUser01\r\nS003\r\n/c/...\r\ngoogle.com\r\n2012-01-01 15:17:11\r\n101.0.0.1\r\nUser01\r\nS004\r\n/d/...\r\n/c/...\r\n2012-01-01 15:19:23\r\n101.0.0.1\r\nUser01\r\nS004\r\n/e/...\r\n/d/....\r\n\r\n    3、 从上述清洗结果中梳理出以下两种模型数据\r\n    • 页面点击流模型Pageviews表\r\nSession\r\nuserid\r\n时间\r\n访问页面URL\r\n停留时长\r\n第几步\r\nS001\r\nUser01\r\n2012-01-01 12:31:12\r\n/a/....\r\n30\r\n1\r\nS002\r\nUser02\r\n2012-01-01 12:31:16\r\n/a/....\r\n10\r\n1\r\nS002\r\nUser02\r\n2012-01-01 12:33:06\r\n/b/....\r\n110\r\n2\r\nS002\r\nUser02\r\n2012-01-01 12:35:06\r\n/e/....\r\n30\r\n3\r\n\r\n\r\n    • 点击流模型Visits表\r\nSession\r\n起始时间\r\n结束时间\r\n进入页面\r\n离开页面\r\n访问页面数\r\nIP\r\ncookie\r\nreferal\r\nS001\r\n2012-01-01 12:31:12\r\n2012-01-01 12:31:12\r\n/a/...\r\n/a/...\r\n1\r\n101.0.0.1\r\nUser01\r\nsomesite.com\r\nS002\r\n2012-01-01 12:31:16\r\n2012-01-01 12:35:06\r\n/a/...\r\n/e/...\r\n3\r\n201.0.0.2\r\nUser02\r\n-\r\nS003\r\n2012-01-01 12:35:42\r\n2012-01-01 12:35:42\r\n/c/...\r\n/c/...\r\n1\r\n234.0.0.3\r\nUser03\r\nbaidu.com\r\nS003\r\n2012-01-01 15:16:39\r\n2012-01-01 15:19:23\r\n/c/...\r\n/e/...\r\n3\r\n101.0.0.1\r\nUser01\r\ngoogle.com\r\n……\r\n……\r\n……\r\n……\r\n……\r\n……\r\n……\r\n……\r\n……\r\n',14,0,0,1515160341,0,0,0),(365,1,'Flume介绍','','','1.概述\r\n• Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。\r\n• Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中\r\n• 一般的采集需求，通过对flume的简单配置即可实现\r\n• Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景\r\n\r\n2.运行机制\r\n• Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成\r\n• 每一个agent相当于一个数据传递员，内部有三个组件：\r\n    a) Source：采集源，用于跟数据源对接，以获取数据\r\n    b) Sink：下沉地，采集数据的传送目的，用于往下一级agent传递数据或者往最终存储系统传递数据\r\n    c) Channel：angent内部的数据传输通道，用于从source将数据传递到sink\r\n\r\n3.Flume采集系统结构图\r\n• 简单结构\r\n单个agent采集数据\r\n\r\n• 复杂结构\r\n多级agent之间串联\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n',14,0,0,1515160490,0,0,0),(366,1,'采集案例','','','采集目录到HDFS\r\n采集需求：某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去\r\n根据需求，首先定义以下3大要素\r\n    • 采集源，即source——监控文件目录 :  spooldir\r\n    • 下沉目标，即sink——HDFS文件系统  :  hdfs sink\r\n    • source和sink之间的传递通道——channel，可用file channel 也可以用内存channel\r\n\r\n配置文件编写：\r\n#定义三大组件的名称\r\nagent1.sources = source1\r\nagent1.sinks = sink1\r\nagent1.channels = channel1\r\n\r\n# 配置source组件\r\nagent1.sources.source1.type = spooldir\r\nagent1.sources.source1.spoolDir = /home/hadoop/logs/\r\nagent1.sources.source1.fileHeader = false\r\n\r\n#配置拦截器\r\nagent1.sources.source1.interceptors = i1\r\nagent1.sources.source1.interceptors.i1.type = host\r\nagent1.sources.source1.interceptors.i1.hostHeader = hostname\r\n\r\n# 配置sink组件\r\nagent1.sinks.sink1.type = hdfs\r\nagent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M\r\nagent1.sinks.sink1.hdfs.filePrefix = access_log\r\nagent1.sinks.sink1.hdfs.maxOpenFiles = 5000\r\nagent1.sinks.sink1.hdfs.batchSize= 100\r\nagent1.sinks.sink1.hdfs.fileType = DataStream\r\nagent1.sinks.sink1.hdfs.writeFormat =Text\r\nagent1.sinks.sink1.hdfs.rollSize = 102400\r\nagent1.sinks.sink1.hdfs.rollCount = 1000000\r\nagent1.sinks.sink1.hdfs.rollInterval = 60\r\n#agent1.sinks.sink1.hdfs.round = true\r\n#agent1.sinks.sink1.hdfs.roundValue = 10\r\n#agent1.sinks.sink1.hdfs.roundUnit = minute\r\nagent1.sinks.sink1.hdfs.useLocalTimeStamp = true\r\n# Use a channel which buffers events in memory\r\nagent1.channels.channel1.type = memory\r\nagent1.channels.channel1.keep-alive = 120\r\nagent1.channels.channel1.capacity = 500000\r\nagent1.channels.channel1.transactionCapacity = 600\r\n\r\n# Bind the source and sink to the channel\r\nagent1.sources.source1.channels = channel1\r\nagent1.sinks.sink1.channel = channel1\r\n\r\nChannel参数解释：\r\ncapacity：默认该通道中最大的可以存储的event数量\r\ntrasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量\r\nkeep-alive：event添加到通道中或者移出的允许时间\r\n\r\n2、采集文件到HDFS\r\n采集需求：比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs\r\n\r\n根据需求，首先定义以下3大要素\r\n    • 采集源，即source——监控文件内容更新 :  exec  ‘tail -F file’\r\n    • 下沉目标，即sink——HDFS文件系统  :  hdfs sink\r\n    • Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel\r\n\r\n配置文件编写：\r\nagent1.sources = source1\r\nagent1.sinks = sink1\r\nagent1.channels = channel1\r\n\r\n# Describe/configure tail -F source1\r\nagent1.sources.source1.type = exec\r\nagent1.sources.source1.command = tail -F /home/hadoop/logs/access_log\r\nagent1.sources.source1.channels = channel1\r\n\r\n#configure host for source\r\nagent1.sources.source1.interceptors = i1\r\nagent1.sources.source1.interceptors.i1.type = host\r\nagent1.sources.source1.interceptors.i1.hostHeader = hostname\r\n\r\n# Describe sink1\r\nagent1.sinks.sink1.type = hdfs\r\n#a1.sinks.k1.channel = c1\r\nagent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M\r\nagent1.sinks.sink1.hdfs.filePrefix = access_log\r\nagent1.sinks.sink1.hdfs.maxOpenFiles = 5000\r\nagent1.sinks.sink1.hdfs.batchSize= 100\r\nagent1.sinks.sink1.hdfs.fileType = DataStream\r\nagent1.sinks.sink1.hdfs.writeFormat =Text\r\nagent1.sinks.sink1.hdfs.rollSize = 102400\r\nagent1.sinks.sink1.hdfs.rollCount = 1000000\r\nagent1.sinks.sink1.hdfs.rollInterval = 60\r\nagent1.sinks.sink1.hdfs.round = true\r\nagent1.sinks.sink1.hdfs.roundValue = 10\r\nagent1.sinks.sink1.hdfs.roundUnit = minute\r\nagent1.sinks.sink1.hdfs.useLocalTimeStamp = true\r\n\r\n# Use a channel which buffers events in memory\r\nagent1.channels.channel1.type = memory\r\nagent1.channels.channel1.keep-alive = 120\r\nagent1.channels.channel1.capacity = 500000\r\nagent1.channels.channel1.transactionCapacity = 600\r\n\r\n# Bind the source and sink to the channel\r\nagent1.sources.source1.channels = channel1\r\nagent1.sinks.sink1.channel = channel1\r\n\r\n1.3 更多source和sink组件\r\nFlume支持众多的source和sink类型，详细手册可参考官方文档\r\nhttp://flume.apache.org/FlumeUserGuide.html',14,0,0,1515160538,0,0,0),(367,1,'工作流调度器azkaban','','','2.1 概述\r\n2.1.1为什么需要工作流调度系统\r\n    • 一个完整的数据分析系统通常都是由大量任务单元组成：\r\nshell脚本程序，java程序，mapreduce程序、hive脚本等\r\n    • 各任务单元之间存在时间先后及前后依赖关系\r\n    • 为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行；\r\n\r\n例如，我们可能有这样一个需求，某个业务系统每天产生20G原始数据，我们每天都要对其进行处理，处理步骤如下所示：\r\n    1、 通过Hadoop先将原始数据同步到HDFS上；\r\n    2、 借助MapReduce计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张Hive表中；\r\n    3、 需要对Hive中多个表的数据进行JOIN处理，得到一个明细数据Hive大表；\r\n    4、 将明细数据进行复杂的统计分析，得到结果报表信息；\r\n    5、 需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。\r\n\r\n\r\n2.1.2 工作流调度实现方式\r\n简单的任务调度：直接使用linux的crontab来定义；\r\n复杂的任务调度：开发调度平台\r\n或使用现成的开源调度系统，比如ooize、azkaban等\r\n\r\n\r\n2.1.3 常见工作流调度系统\r\n市面上目前有许多工作流调度器\r\n在hadoop领域，常见的工作流调度器有Oozie, Azkaban,Cascading,Hamake等\r\n\r\n\r\n\r\n2.1.4 各种调度工具特性对比\r\n下面的表格对上述四种hadoop工作流调度器的关键特性进行了比较，尽管这些工作流调度器能够解决的需求场景基本一致，但在设计理念，目标用户，应用场景等方面还是存在显著的区别，在做技术选型的时候，可以提供参考\r\n特性\r\nHamake\r\nOozie\r\nAzkaban\r\nCascading\r\n工作流描述语言\r\nXML\r\nXML (xPDL based)\r\ntext file with key/value pairs\r\nJava API\r\n依赖机制\r\ndata-driven\r\nexplicit\r\nexplicit\r\nexplicit\r\n是否要web容器\r\nNo\r\nYes\r\nYes\r\nNo\r\n进度跟踪\r\nconsole/log messages\r\nweb page\r\nweb page\r\nJava API\r\nHadoop job调度支持\r\nno\r\nyes\r\nyes\r\nyes\r\n运行模式\r\ncommand line utility\r\ndaemon\r\ndaemon\r\nAPI\r\nPig支持\r\nyes\r\nyes\r\nyes\r\nyes\r\n事件通知\r\nno\r\nno\r\nno\r\nyes\r\n需要安装\r\nno\r\nyes\r\nyes\r\nno\r\n支持的hadoop版本\r\n0.18+\r\n0.20+\r\ncurrently unknown\r\n0.18+\r\n重试支持\r\nno\r\nworkflownode evel\r\nyes\r\nyes\r\n运行任意命令\r\nyes\r\nyes\r\nyes\r\nyes\r\nAmazon EMR支持\r\nyes\r\nno\r\ncurrently unknown\r\nyes\r\n2.1.5 Azkaban与Oozie对比\r\n对市面上最流行的两种调度器，给出以下详细对比，以供技术选型参考。总体来说，ooize相比azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器azkaban是很不错的候选对象。\r\n详情如下：\r\n    • 功能\r\n两者均可以调度mapreduce,pig,java,脚本工作流任务\r\n两者均可以定时执行工作流任务\r\n\r\n    • 工作流定义\r\nAzkaban使用Properties文件定义工作流\r\nOozie使用XML文件定义工作流\r\n\r\n    • 工作流传参\r\nAzkaban支持直接传参，例如${input}\r\nOozie支持参数和EL表达式，例如${fs:dirSize(myInputDir)}\r\n\r\n    • 定时执行\r\nAzkaban的定时执行任务是基于时间的\r\nOozie的定时执行任务基于时间和输入数据\r\n\r\n    • 资源管理\r\nAzkaban有较严格的权限控制，如用户对工作流进行读/写/执行等操作\r\nOozie暂无严格的权限控制\r\n\r\n    • 工作流执行\r\nAzkaban有两种运行模式，分别是solo server mode(executor server和web server部署在同一台节点)和multi server mode(executor server和web server可以部署在不同节点)\r\nOozie作为工作流服务器运行，支持多用户和多工作流\r\n\r\n    • 工作流管理\r\nAzkaban支持浏览器以及ajax方式操作工作流\r\nOozie支持命令行、HTTP REST、Java API、浏览器操作工作流\r\n',14,0,0,1515160576,0,0,0),(368,1,'Azkaban介绍','','','Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。\r\n它有如下功能特点：\r\n    • Web用户界面\r\n    • 方便上传工作流\r\n    • 方便设置任务之间的关系\r\n    • 调度工作流\r\n    • 认证/授权(权限的工作)\r\n    • 能够杀死并重新启动工作流\r\n    • 模块化和可插拔的插件机制\r\n    • 项目工作区\r\n    • 工作流和任务的日志记录和审计\r\n',14,0,0,1515160600,0,0,0),(369,1,'Azkaban安装部署','','','准备工作\r\nAzkaban Web服务器\r\nazkaban-web-server-2.5.0.tar.gz\r\nAzkaban执行服务器 \r\nazkaban-executor-server-2.5.0.tar.gz\r\n\r\nMySQL\r\n目前azkaban只支持 mysql,需安装mysql服务器,本文档中默认已安装好mysql服务器,并建立了 root用户,密码 root.\r\n\r\n \r\n下载地址:http://azkaban.github.io/downloads.html\r\n\r\n\r\n\r\n安装\r\n将安装文件上传到集群,最好上传到安装 hive、sqoop的机器上,方便命令的执行\r\n在当前用户目录下新建 azkabantools目录,用于存放源安装文件.新建azkaban目录,用于存放azkaban运行程序\r\nazkaban web服务器安装\r\n解压azkaban-web-server-2.5.0.tar.gz\r\n命令: tar –zxvf azkaban-web-server-2.5.0.tar.gz\r\n将解压后的azkaban-web-server-2.5.0 移动到 azkaban目录中,并重新命名 webserver\r\n命令: mv azkaban-web-server-2.5.0 ../azkaban\r\n        cd ../azkaban\r\n        mv azkaban-web-server-2.5.0  server\r\n\r\nazkaban 执行服器安装\r\n解压azkaban-executor-server-2.5.0.tar.gz\r\n命令:tar –zxvf azkaban-executor-server-2.5.0.tar.gz\r\n将解压后的azkaban-executor-server-2.5.0 移动到 azkaban目录中,并重新命名 executor\r\n命令:mv azkaban-executor-server-2.5.0  ../azkaban\r\ncd ../azkaban\r\nmv azkaban-executor-server-2.5.0  executor\r\n\r\nazkaban脚本导入\r\n解压: azkaban-sql-script-2.5.0.tar.gz\r\n命令:tar –zxvf azkaban-sql-script-2.5.0.tar.gz\r\n将解压后的mysql 脚本,导入到mysql中:\r\n进入mysql\r\nmysql&gt; create database azkaban;\r\nmysql&gt; use azkaban;\r\nDatabase changed\r\nmysql&gt; source /home/hadoop/azkaban-2.5.0/create-all-sql-2.5.0.sql;\r\n\r\n\r\n',14,0,0,1515160634,0,0,0),(370,1,'创建SSL配置','','','参考地址: http://docs.codehaus.org/display/JETTY/How+to+configure+SSL\r\n命令: keytool -keystore keystore -alias jetty -genkey -keyalg RSA\r\n运行此命令后,会提示输入当前生成 keystor的密码及相应信息,输入的密码请劳记,信息如下:\r\n \r\n输入keystore密码： \r\n再次输入新密码:\r\n您的名字与姓氏是什么？\r\n  [Unknown]： \r\n您的组织单位名称是什么？\r\n  [Unknown]： \r\n您的组织名称是什么？\r\n  [Unknown]： \r\n您所在的城市或区域名称是什么？\r\n  [Unknown]： \r\n您所在的州或省份名称是什么？\r\n  [Unknown]： \r\n该单位的两字母国家代码是什么\r\n  [Unknown]：  CN\r\nCN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？\r\n  [否]：  y\r\n \r\n输入&lt;jetty&gt;的主密码\r\n        （如果和 keystore 密码相同，按回车）： \r\n再次输入新密码:\r\n完成上述工作后,将在当前目录生成 keystore 证书文件,将keystore 考贝到 azkaban web服务器根目录中.如:cp keystore azkaban/webserver',14,0,0,1515160665,0,0,0),(371,1,'配置文件','','','注：先配置好服务器节点上的时区\r\n    1、 先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可\r\n    2、 拷贝该时区文件，覆盖系统本地时区配置\r\ncp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  \r\n\r\n\r\nazkaban web服务器配置\r\n进入azkaban web服务器安装目录 conf目录\r\n\r\n    • 修改azkaban.properties文件\r\n命令vi azkaban.properties\r\n内容说明如下:\r\n#Azkaban Personalization Settings\r\nazkaban.name=Test                           #服务器UI名称,用于服务器上方显示的名字\r\nazkaban.label=My Local Azkaban                               #描述\r\nazkaban.color=#FF3601                                                 #UI颜色\r\nazkaban.default.servlet.path=/index                         #\r\nweb.resource.dir=web/                                                 #默认根web目录\r\ndefault.timezone.id=Asia/Shanghai                           #默认时区,已改为亚洲/上海 默认为美国\r\n \r\n#Azkaban UserManager class\r\nuser.manager.class=azkaban.user.XmlUserManager   #用户权限管理默认类\r\nuser.manager.xml.file=conf/azkaban-users.xml              #用户配置,具体配置参加下文\r\n \r\n#Loader for projects\r\nexecutor.global.properties=conf/global.properties    # global配置文件所在位置\r\nazkaban.project.dir=projects                                                #\r\n \r\ndatabase.type=mysql                                                              #数据库类型\r\nmysql.port=3306                                                                       #端口号\r\nmysql.host=hadoop03                                                      #数据库连接IP\r\nmysql.database=azkaban                                                       #数据库实例名\r\nmysql.user=root                                                                 #数据库用户名\r\nmysql.password=root                                                          #数据库密码\r\nmysql.numconnections=100                                                  #最大连接数\r\n \r\n# Velocity dev mode\r\nvelocity.dev.mode=false\r\n# Jetty服务器属性.\r\njetty.maxThreads=25                                                               #最大线程数\r\njetty.ssl.port=8443                                                                   #Jetty SSL端口\r\njetty.port=8081                                                                         #Jetty端口\r\njetty.keystore=keystore                                                          #SSL文件名\r\njetty.password=123456                                                             #SSL文件密码\r\njetty.keypassword=123456                                                      #Jetty主密码 与 keystore文件相同\r\njetty.truststore=keystore                                                                #SSL文件名\r\njetty.trustpassword=123456                                                   # SSL文件密码\r\n \r\n# 执行服务器属性\r\nexecutor.port=12321                                                               #执行服务器端口\r\n \r\n# 邮件设置\r\nmail.sender=xxxxxxxx@163.com                                       #发送邮箱\r\nmail.host=smtp.163.com                                                       #发送邮箱smtp地址\r\nmail.user=xxxxxxxx                                       #发送邮件时显示的名称\r\nmail.password=**********                                                 #邮箱密码\r\njob.failure.email=xxxxxxxx@163.com                              #任务失败时发送邮件的地址\r\njob.success.email=xxxxxxxx@163.com                            #任务成功时发送邮件的地址\r\nlockdown.create.projects=false                                           #\r\ncache.directory=cache                                                            #缓存目录\r\n\r\n \r\n    • azkaban 执行服务器配置\r\n进入执行服务器安装目录conf,修改azkaban.properties\r\nvi azkaban.properties\r\n#Azkaban\r\ndefault.timezone.id=Asia/Shanghai                                              #时区\r\n \r\n# Azkaban JobTypes 插件配置\r\nazkaban.jobtype.plugin.dir=plugins/jobtypes                   #jobtype 插件所在位置\r\n \r\n#Loader for projects\r\nexecutor.global.properties=conf/global.properties\r\nazkaban.project.dir=projects\r\n \r\n#数据库设置\r\ndatabase.type=mysql                                                                       #数据库类型(目前只支持mysql)\r\nmysql.port=3306                                                                                #数据库端口号\r\nmysql.host=192.168.20.200                                                           #数据库IP地址\r\nmysql.database=azkaban                                                                #数据库实例名\r\nmysql.user=azkaban                                                                         #数据库用户名\r\nmysql.password=oracle                                                                   #数据库密码\r\nmysql.numconnections=100                                                           #最大连接数\r\n \r\n# 执行服务器配置\r\nexecutor.maxThreads=50                                                                #最大线程数\r\nexecutor.port=12321                                                               #端口号(如修改,请与web服务中一致)\r\nexecutor.flow.threads=30                                                                #线程数\r\n\r\n\r\n    • 用户配置\r\n进入azkaban web服务器conf目录,修改azkaban-users.xml\r\nvi azkaban-users.xml 增加 管理员用户\r\n&lt;azkaban-users&gt;\r\n        &lt;user username=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; groups=&quot;azkaban&quot; /&gt;\r\n        &lt;user username=&quot;metrics&quot; password=&quot;metrics&quot; roles=&quot;metrics&quot;/&gt;\r\n        &lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot; /&gt;\r\n        &lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot; /&gt;\r\n        &lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&gt;\r\n&lt;/azkaban-users&gt;\r\n',14,0,0,1515160688,0,0,0),(372,1,'启动','','','web服务器\r\n在azkaban web服务器目录下执行启动命令\r\n\r\nbin/azkaban-web-start.sh\r\n注:在web服务器根目录运行\r\n\r\n执行服务器\r\n在执行服务器目录下执行启动命令\r\nbin/azkaban-executor-start.sh ./\r\n注:只能要执行服务器根目录运行\r\n \r\n启动完成后,在浏览器(建议使用谷歌浏览器)中输入https://服务器IP地址:8443 ,即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login',14,0,0,1515160720,0,0,0),(373,1,' Azkaban实战','','','Azkaba内置的任务类型支持command、java\r\n\r\nCommand类型单一job示例\r\n    1、 创建job描述文件\r\nvi command.job\r\n#command.job\r\ntype=command                                                    \r\ncommand=echo &#039;hello&#039;\r\n\r\n\r\n    2、 将job资源文件打包成zip文件\r\nzip command.job\r\n\r\n    3、 通过azkaban的web管理平台创建project并上传job压缩包\r\n首先创建project\r\n\r\n上传zip包\r\n\r\n4、启动执行该job\r\n\r\n\r\nCommand类型多job工作流flow\r\n    1、 创建有依赖关系的多个job描述\r\n第一个job：foo.job\r\n# foo.job\r\ntype=command\r\ncommand=echo foo\r\n第二个job：bar.job依赖foo.job\r\n# bar.job\r\ntype=command\r\ndependencies=foo\r\ncommand=echo bar\r\n\r\n    2、 将所有job资源文件打到一个zip包中\r\n\r\n\r\n    3、 在azkaban的web管理界面创建工程并上传zip包\r\n    4、 启动工作流flow\r\n\r\nHDFS操作任务\r\n    1、 创建job描述文件\r\n# fs.job\r\ntype=command\r\ncommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz\r\n\r\n    2、 将job资源文件打包成zip文件\r\n\r\n3、通过azkaban的web管理平台创建project并上传job压缩包\r\n4、启动执行该job',14,0,0,1515160744,0,0,0),(374,1,'MAPREDUCE任务','','','Mr任务依然可以使用command的job类型来执行\r\n    1、 创建job描述文件，及mr程序jar包（示例中直接使用hadoop自带的example jar）\r\n# mrwc.job\r\ntype=command\r\ncommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop  jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout\r\n\r\n\r\n    2、 将所有job资源文件打到一个zip包中\r\n3、在azkaban的web管理界面创建工程并上传zip包\r\n4、启动job',14,0,0,1515160774,0,0,0),(375,1,'HIVE脚本任务','','','    • 创建job描述文件和hive脚本\r\nHive脚本： test.sql\r\nuse default;\r\ndrop table aztest;\r\ncreate table aztest(id int,name string) row format delimited fields terminated by &#039;,&#039;;\r\nload data inpath &#039;/aztest/hiveinput&#039; into table aztest;\r\ncreate table azres as select * from aztest;\r\ninsert overwrite directory &#039;/aztest/hiveoutput&#039; select count(1) from aztest; \r\nJob描述文件：hivef.job\r\n# hivef.job\r\ntype=command\r\ncommand=/home/hadoop/apps/hive/bin/hive -f &#039;test.sql&#039;\r\n\r\n2、将所有job资源文件打到一个zip包中\r\n3、在azkaban的web管理界面创建工程并上传zip包\r\n4、启动job\r\n',14,0,0,1515160792,0,0,0),(376,1,'sqoop数据迁移','','','3.1 概述\r\nsqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。\r\n导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；\r\n导出数据：从Hadoop的文件系统中导出数据到关系数据库\r\n3.2 工作机制\r\n将导入或导出命令翻译成mapreduce程序来实现\r\n在翻译出的mapreduce中主要是对inputformat和outputformat进行定制\r\n\r\n3.3 sqoop实战及原理\r\n3.3.1 sqoop安装\r\n安装sqoop的前提是已经具备java和hadoop的环境\r\n1、下载并解压\r\n最新版下载地址http://ftp.wayne.edu/apache/sqoop/1.4.6/\r\n\r\n\r\n2、修改配置文件\r\n$ cd $SQOOP_HOME/conf\r\n$ mv sqoop-env-template.sh sqoop-env.sh\r\n打开sqoop-env.sh并编辑下面几行：\r\nexport HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.6.1/ \r\nexport HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.6.1/\r\nexport HIVE_HOME=/home/hadoop/apps/hive-1.2.1\r\n\r\n\r\n    3、 加入mysql的jdbc驱动包\r\ncp  ~/app/hive/lib/mysql-connector-java-5.1.28.jar   $SQOOP_HOME/lib/\r\n4、验证启动\r\n$ cd $SQOOP_HOME/bin\r\n$ sqoop-version\r\n预期的输出：\r\n15/12/17 14:52:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6\r\nSqoop 1.4.6 git commit id 5b34accaca7de251fc91161733f906af2eddbe83\r\nCompiled by abe on Fri Aug 1 11:19:26 PDT 2015\r\n到这里，整个Sqoop安装工作完成。\r\n\r\n\r\n\r\n\r\n3.4 Sqoop的数据导入\r\n“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据） \r\n3.4.1 语法\r\n下面的语法用于将数据导入HDFS。\r\n$ sqoop import (generic-args) (import-args) \r\n\r\n示例\r\n表数据\r\n在mysql中有一个库userdb中三个表：emp, emp_add和emp_contact\r\n表emp:\r\nid\r\nname\r\ndeg\r\nsalary\r\ndept\r\n1201\r\ngopal\r\nmanager\r\n50,000\r\nTP\r\n1202\r\nmanisha\r\nProof reader\r\n50,000\r\nTP\r\n1203\r\nkhalil\r\nphp dev\r\n30,000\r\nAC\r\n1204\r\nprasanth\r\nphp dev\r\n30,000\r\nAC\r\n1205\r\nkranthi\r\nadmin\r\n20,000\r\nTP\r\n表emp_add:\r\nid\r\nhno\r\nstreet\r\ncity\r\n1201\r\n288A\r\nvgiri\r\njublee\r\n1202\r\n108I\r\naoc\r\nsec-bad\r\n1203\r\n144Z\r\npgutta\r\nhyd\r\n1204\r\n78B\r\nold city\r\nsec-bad\r\n1205\r\n720X\r\nhitec\r\nsec-bad\r\n表emp_conn:\r\n\r\nid\r\nphno\r\nemail\r\n1201\r\n2356742\r\ngopal@tp.com\r\n1202\r\n1661663\r\nmanisha@tp.com\r\n1203\r\n8887776\r\nkhalil@ac.com\r\n1204\r\n9988774\r\nprasanth@ac.com\r\n1205\r\n1231231\r\nkranthi@tp.com\r\n导入表表数据到HDFS\r\n下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。\r\n$bin/sqoop import \\\r\n--connect jdbc:mysql://hdp-node-01:3306/test \\\r\n--username root \\\r\n--password root \\\r\n--table emp --m 1\r\n\r\n如果成功执行，那么会得到下面的输出。\r\n14/12/22 15:24:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5\r\n14/12/22 15:24:56 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\r\nINFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/cebe706d23ebb1fd99c1f063ad51ebd7/emp.jar\r\n-----------------------------------------------------\r\nO mapreduce.Job: map 0% reduce 0%\r\n14/12/22 15:28:08 INFO mapreduce.Job: map 100% reduce 0%\r\n14/12/22 15:28:16 INFO mapreduce.Job: Job job_1419242001831_0001 completed successfully\r\n-----------------------------------------------------\r\n-----------------------------------------------------\r\n14/12/22 15:28:17 INFO mapreduce.ImportJobBase: Transferred 145 bytes in 177.5849 seconds (0.8165 bytes/sec)\r\n14/12/22 15:28:17 INFO mapreduce.ImportJobBase: Retrieved 5 records.\r\n\r\n\r\n为了验证在HDFS导入的数据，请使用以下命令查看导入的数据\r\n$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-00000\r\n\r\nemp表的数据和字段之间用逗号(,)表示。\r\n1201, gopal,    manager, 50000, TP\r\n1202, manisha,  preader, 50000, TP\r\n1203, kalil,    php dev, 30000, AC\r\n1204, prasanth, php dev, 30000, AC\r\n1205, kranthi,  admin,   20000, TP\r\n导入关系表到HIVE\r\nbin/sqoop import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import --m 1\r\n导入到HDFS指定目录\r\n在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。\r\n以下是指定目标目录选项的Sqoop导入命令的语法。\r\n--target-dir &lt;new or exist directory in HDFS&gt;\r\n\r\n下面的命令是用来导入emp_add表数据到&#039;/queryresult&#039;目录。\r\nbin/sqoop import \\\r\n--connect jdbc:mysql://hdp-node-01:3306/test \\\r\n--username root \\\r\n--password root \\\r\n--target-dir /queryresult \\\r\n--table emp --m 1\r\n\r\n\r\n下面的命令是用来验证 /queryresult 目录中 emp_add表导入的数据形式。\r\n $HADOOP_HOME/bin/hadoop fs -cat /queryresult/part-m-*\r\n\r\n\r\n它会用逗号（，）分隔emp_add表的数据和字段。\r\n1201, 288A, vgiri,   jublee\r\n1202, 108I, aoc,     sec-bad\r\n1203, 144Z, pgutta,  hyd\r\n1204, 78B,  oldcity, sec-bad\r\n1205, 720C, hitech,  sec-bad\r\n\r\n\r\n\r\n\r\n导入关系表到HIVE\r\nbin/sqoop import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import --m 1\r\n导入到HDFS指定目录\r\n在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。\r\n以下是指定目标目录选项的Sqoop导入命令的语法。\r\n--target-dir &lt;new or exist directory in HDFS&gt;\r\n\r\n下面的命令是用来导入emp_add表数据到&#039;/queryresult&#039;目录。\r\nbin/sqoop import \\\r\n--connect jdbc:mysql://hdp-node-01:3306/test \\\r\n--username root \\\r\n--password root \\\r\n--target-dir /queryresult \\\r\n--table emp --m 1\r\n\r\n\r\n下面的命令是用来验证 /queryresult 目录中 emp_add表导入的数据形式。\r\n $HADOOP_HOME/bin/hadoop fs -cat /queryresult/part-m-*\r\n\r\n\r\n它会用逗号（，）分隔emp_add表的数据和字段。\r\n1201, 288A, vgiri,   jublee\r\n1202, 108I, aoc,     sec-bad\r\n1203, 144Z, pgutta,  hyd\r\n1204, 78B,  oldcity, sec-bad\r\n1205, 720C, hitech,  sec-bad\r\n\r\n\r\n导入表数据子集\r\n我们可以导入表的使用Sqoop导入工具，&quot;where&quot;子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。\r\nwhere子句的语法如下。\r\n--where &lt;condition&gt;\r\n\r\n下面的命令用来导入emp_add表数据的子集。子集查询检索员工ID和地址，居住城市为：Secunderabad\r\nbin/sqoop import \\\r\n--connect jdbc:mysql://hdp-node-01:3306/test \\\r\n--username root \\\r\n--password root \\\r\n--where &quot;city =&#039;sec-bad&#039;&quot; \\\r\n--target-dir /wherequery \\\r\n--table emp_add --m 1\r\n\r\n\r\n\r\n\r\n下面的命令用来验证数据从emp_add表导入/wherequery目录\r\n$HADOOP_HOME/bin/hadoop fs -cat /wherequery/part-m-*\r\n\r\n\r\n它用逗号（，）分隔 emp_add表数据和字段。\r\n1202, 108I, aoc, sec-bad\r\n1204, 78B, oldcity, sec-bad\r\n1205, 720C, hitech, sec-bad\r\n\r\n\r\n\r\n增量导入\r\n增量导入是仅导入新添加的表中的行的技术。\r\n它需要添加‘incremental’, ‘check-column’, 和 ‘last-value’选项来执行增量导入。\r\n下面的语法用于Sqoop导入命令增量选项。\r\n--incremental &lt;mode&gt;\r\n--check-column &lt;column name&gt;\r\n--last value &lt;last check column value&gt;\r\n\r\n\r\n假设新添加的数据转换成emp表如下：\r\n1206, satish p, grp des, 20000, GR\r\n下面的命令用于在EMP表执行增量导入。\r\nbin/sqoop import \\\r\n--connect jdbc:mysql://hdp-node-01:3306/test \\\r\n--username root \\\r\n--password root \\\r\n--table emp --m 1 \\\r\n--incremental append \\\r\n--check-column id \\\r\n--last-value 1205\r\n\r\n\r\n以下命令用于从emp表导入HDFS emp/ 目录的数据验证。\r\n$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-*\r\n它用逗号（，）分隔 emp_add表数据和字段。\r\n1201, gopal,    manager, 50000, TP\r\n1202, manisha,  preader, 50000, TP\r\n1203, kalil,    php dev, 30000, AC\r\n1204, prasanth, php dev, 30000, AC\r\n1205, kranthi,  admin,   20000, TP\r\n1206, satish p, grp des, 20000, GR\r\n\r\n下面的命令是从表emp 用来查看修改或新添加的行\r\n$ $HADOOP_HOME/bin/hadoop fs -cat /emp/part-m-*1\r\n这表示新添加的行用逗号（，）分隔emp表的字段。 \r\n1206, satish p, grp des, 20000, GR\r\n\r\n\r\n\r\n3.5 Sqoop的数据导出\r\n将数据从HDFS导出到RDBMS数据库\r\n导出前，目标表必须存在于目标数据库中。\r\n    • 默认操作是从将文件中的数据使用INSERT语句插入到表中\r\n    • 更新模式下，是生成UPDATE语句更新表数据\r\n\r\n语法\r\n以下是export命令语法。\r\n$ sqoop export (generic-args) (export-args) \r\n\r\n\r\n示例\r\n数据是在HDFS 中“EMP/”目录的emp_data文件中。所述emp_data如下：\r\n1201, gopal,     manager, 50000, TP\r\n1202, manisha,   preader, 50000, TP\r\n1203, kalil,     php dev, 30000, AC\r\n1204, prasanth,  php dev, 30000, AC\r\n1205, kranthi,   admin,   20000, TP\r\n1206, satish p,  grp des, 20000, GR\r\n\r\n1、首先需要手动创建mysql中的目标表\r\n$ mysql\r\nmysql&gt; USE db;\r\nmysql&gt; CREATE TABLE employee ( \r\n   id INT NOT NULL PRIMARY KEY, \r\n   name VARCHAR(20), \r\n   deg VARCHAR(20),\r\n   salary INT,\r\n   dept VARCHAR(10));\r\n\r\n2、然后执行导出命令\r\nbin/sqoop export \\\r\n--connect jdbc:mysql://hdp-node-01:3306/test \\\r\n--username root \\\r\n--password root \\\r\n--table emp2 \\\r\n--export-dir /user/hadoop/emp/\r\n\r\n3、验证表mysql命令行。\r\nmysql&gt;select * from employee;\r\n如果给定的数据存储成功，那么可以找到数据在如下的employee表。\r\n+------+--------------+-------------+-------------------+--------+\r\n| Id   | Name         | Designation | Salary            | Dept   |\r\n+------+--------------+-------------+-------------------+--------+\r\n| 1201 | gopal        | manager     | 50000             | TP     |\r\n| 1202 | manisha      | preader     | 50000             | TP     |\r\n| 1203 | kalil        | php dev     | 30000             | AC     |\r\n| 1204 | prasanth     | php dev     | 30000             | AC     |\r\n| 1205 | kranthi      | admin       | 20000             | TP     |\r\n| 1206 | satish p     | grp des     | 20000             | GR     |\r\n+------+--------------+-------------+-------------------+--------+\r\n\r\n\r\n\r\n\r\n3.6 Sqoop作业\r\n注：Sqoop作业——将事先定义好的数据导入导出任务按照指定流程运行\r\n语法\r\n以下是创建Sqoop作业的语法。\r\n$ sqoop job (generic-args) (job-args)\r\n   [-- [subtool-name] (subtool-args)]\r\n\r\n$ sqoop-job (generic-args) (job-args)\r\n   [-- [subtool-name] (subtool-args)]\r\n\r\n\r\n\r\n\r\n创建作业(--create)\r\n在这里，我们创建一个名为myjob，这可以从RDBMS表的数据导入到HDFS作业。\r\n\r\nbin/sqoop job --create myimportjob -- import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --m 1\r\n该命令创建了一个从db库的employee表导入到HDFS文件的作业。\r\n\r\n验证作业 (--list)\r\n‘--list’ 参数是用来验证保存的作业。下面的命令用来验证保存Sqoop作业的列表。\r\n$ sqoop job --list\r\n它显示了保存作业列表。\r\nAvailable jobs: \r\n   myjob\r\n检查作业(--show)\r\n‘--show’ 参数用于检查或验证特定的工作，及其详细信息。以下命令和样本输出用来验证一个名为myjob的作业。\r\n$ sqoop job --show myjob\r\n它显示了工具和它们的选择，这是使用在myjob中作业情况。\r\nJob: myjob \r\n Tool: import Options:\r\n ---------------------------- \r\n direct.import = true\r\n codegen.input.delimiters.record = 0\r\n hdfs.append.dir = false \r\n db.table = employee\r\n ...\r\n incremental.last.value = 1206\r\n ...\r\n\r\n\r\n\r\n\r\n执行作业 (--exec)\r\n‘--exec’ 选项用于执行保存的作业。下面的命令用于执行保存的作业称为myjob。\r\n$ sqoop job --exec myjob\r\n它会显示下面的输出。\r\n10/08/19 13:08:45 INFO tool.CodeGenTool: Beginning code generation \r\n...\r\n\r\n3.7 Sqoop的原理\r\n概述\r\nSqoop的原理其实就是将导入导出命令转化为mapreduce程序来执行，sqoop在接收到命令后，都要生成mapreduce程序\r\n\r\n使用sqoop的代码生成工具可以方便查看到sqoop所生成的java代码，并可在此基础之上进行深入定制开发\r\n\r\n\r\n\r\n代码定制\r\n以下是Sqoop代码生成命令的语法：\r\n$ sqoop-codegen (generic-args) (codegen-args) \r\n$ sqoop-codegen (generic-args) (codegen-args)\r\n\r\n\r\n示例：以USERDB数据库中的表emp来生成Java代码为例。\r\n下面的命令用来生成导入\r\n$ sqoop-codegen \\\r\n--import\r\n--connect jdbc:mysql://localhost/userdb \\\r\n--username root \\ \r\n--table emp\r\n\r\n\r\n如果命令成功执行，那么它就会产生如下的输出。\r\n14/12/23 02:34:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5\r\n14/12/23 02:34:41 INFO tool.CodeGenTool: Beginning code generation\r\n……………….\r\n14/12/23 02:34:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop\r\nNote: /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/emp.java uses or overrides a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\n14/12/23 02:34:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/emp.jar\r\n\r\n\r\n\r\n验证: 查看输出目录下的文件\r\n$ cd /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/\r\n$ ls\r\nemp.class\r\nemp.jar\r\nemp.java\r\n\r\n如果想做深入定制导出，则可修改上述代码文件\r\n\r\n',14,0,0,1515160978,0,0,0),(377,1,'什么是点击流数据','','',' WEB访问日志\r\n即指用户访问网站时的所有访问、浏览、点击行为数据。比如点击了哪一个链接，在哪个网页停留时间最多，采用了哪个搜索项、总体浏览时间等。而所有这些信息都可被保存在网站日志中。通过分析这些数据，可以获知许多对网站运营至关重要的信息。采集的数据越全面，分析就能越精准。\r\n\r\n    • 日志的生成渠道：\r\n1）是网站的web服务器所记录的web访问日志；\r\n2）是通过在页面嵌入自定义的js代码来获取用户的所有访问行为（比如鼠标悬停的位置，点击的页面组件等），然后通过ajax请求到后台记录日志；这种方式所能采集的信息最全面；\r\n3）通过在页面上埋点1像素的图片，将相关页面访问信息请求到后台记录日志；\r\n\r\n    • 日志数据内容详述：\r\n在实际操作中，有以下几个方面的数据可以被采集：\r\n    1) 访客的系统属性特征。比如所采用的操作系统、浏览器、域名和访问速度等。\r\n    2) 访问特征。包括停留时间、点击的URL等。\r\n    3) 来源特征。包括网络内容信息类型、内容分类和来访URL等。\r\n    4) 产品特征。包括所访问的产品编号、产品类别、产品颜色、产品价格、产品利润、产品数量和特价等级等。\r\n\r\n以电商某东为例，其点击日志格式如下：\r\nGET /log.gif?t=item.010001&amp;m=UA-J2011-1&amp;pin=-&amp;uid=1679790178&amp;sid=1679790178|12&amp;v=je=1$sc=24-bit$sr=1600x900$ul=zh-cn$cs=GBK$dt=【云南白药套装】云南白药 牙膏 180g×3 （留兰香型）【行情 报价 价格 评测】-京东$hn=item.jd.com$fl=16.0 r0$os=win$br=chrome$bv=39.0.2171.95$wb=1437269412$xb=1449548587$yb=1456186252$zb=12$cb=4$usc=direct$ucp=-$umd=none$uct=-$ct=1456186505411$lt=0$tad=-$sku=1326523$cid1=1316$cid2=1384$cid3=1405$brand=20583$pinid=-&amp;ref=&amp;rm=1456186505411 HTTP/1.1\r\n\r\n\r\n\r\n\r\n\r\n\r\n',14,0,0,1515161121,0,0,0),(378,1,'点击流数据模型','','','点击流概念\r\n点击流这个概念更注重用户浏览网站的整个流程，网站日志中记录的用户点击就像是图上的“点”，而点击流更像是将这些“点”串起来形成的“线”。也可以把“点”认为是网站的Page，而“线”则是访问网站的Session。所以点击流数据是由网站日志中整理得到的，它可以比网站日志包含更多的信息，从而使基于点击流数据统计得到的结果更加丰富和高效。\r\n\r\n\r\n\r\n\r\n\r\n点击流模型生成\r\n点击流数据在具体操作上是由散点状的点击日志数据梳理所得，从而，点击数据在数据建模时应该存在两张模型表（Pageviews和visits）：\r\n\r\n    1、 用于生成点击流的访问日志表\r\n时间戳\r\nIP地址\r\nCookie\r\nSession\r\n请求URL\r\nReferal\r\n2012-01-01 12:31:12\r\n101.0.0.1\r\nUser01\r\nS001\r\n/a/...\r\nsomesite.com\r\n2012-01-01 12:31:16\r\n201.0.0.2\r\nUser02\r\nS002\r\n/a/...\r\n-\r\n2012-01-01 12:33:06\r\n101.0.0.2\r\nUser03\r\nS002\r\n/b/...\r\nbaidu.com\r\n2012-01-01 15:16:39\r\n234.0.0.3\r\nUser01\r\nS003\r\n/c/...\r\ngoogle.com\r\n2012-01-01 15:17:11\r\n101.0.0.1\r\nUser01\r\nS004\r\n/d/...\r\n/c/...\r\n2012-01-01 15:19:23\r\n101.0.0.1\r\nUser01\r\nS004\r\n/e/...\r\n/d/....\r\n\r\n\r\n\r\n    2、 页面点击流模型Pageviews表\r\nSession\r\nuserid\r\n时间\r\n访问页面URL\r\n停留时长\r\n第几步\r\nS001\r\nUser01\r\n2012-01-01 12:31:12\r\n/a/....\r\n30\r\n1\r\nS002\r\nUser02\r\n2012-01-01 12:31:16\r\n/a/....\r\n10\r\n1\r\nS002\r\nUser02\r\n2012-01-01 12:33:06\r\n/b/....\r\n110\r\n2\r\nS002\r\nUser02\r\n2012-01-01 12:35:06\r\n/e/....\r\n30\r\n3\r\n\r\n\r\n\r\n    3、 点击流模型Visits表\r\nSession\r\n起始时间\r\n结束时间\r\n进入页面\r\n离开页面\r\n访问页面数\r\nIP\r\ncookie\r\nreferal\r\nS001\r\n2012-01-01 12:31:12\r\n2012-01-01 12:31:12\r\n/a/...\r\n/a/...\r\n1\r\n101.0.0.1\r\nUser01\r\nsomesite.com\r\nS002\r\n2012-01-01 12:31:16\r\n2012-01-01 12:35:06\r\n/a/...\r\n/e/...\r\n3\r\n201.0.0.2\r\nUser02\r\n-\r\nS003\r\n2012-01-01 12:35:42\r\n2012-01-01 12:35:42\r\n/c/...\r\n/c/...\r\n1\r\n234.0.0.3\r\nUser03\r\nbaidu.com\r\nS003\r\n2012-01-01 15:16:39\r\n2012-01-01 15:19:23\r\n/c/...\r\n/e/...\r\n3\r\n101.0.0.1\r\nUser01\r\ngoogle.com\r\n……\r\n……\r\n……\r\n……\r\n……\r\n……\r\n……\r\n……\r\n……\r\n\r\n这就是点击流模型。当WEB日志转化成点击流数据的时候，很多网站分析度量的计算变得简单了，这就是点击流的“魔力”所在。基于点击流数据我们可以统计出许多常见的网站分析度量\r\n\r\n',14,0,0,1515161192,0,0,0),(379,1,'网站流量数据分析的意义','','','网站流量统计分析，可以帮助网站管理员、运营人员、推广人员等实时获取网站流量信息，并从流量来源、网站内容、网站访客特性等多方面提供网站分析的数据依据。从而帮助提高网站流量，提升网站用户体验，让访客更多的沉淀下来变成会员或客户，通过更少的投入获取最大化的收入。\r\n\r\n如下表：\r\n网站的眼睛\r\n网站的神经\r\n网站的大脑\r\n访问者来自哪里？\r\n访问者在寻找什么？\r\n哪些页面最受欢迎？\r\n访问者从哪里进入？\r\n\r\n网页布局合理吗？\r\n网站导航清晰吗？\r\n哪些功能存在问题\r\n网站内容有效吗\r\n转化路径靠谱吗？\r\n如何分解目标？\r\n如何分配广告预算？\r\n如何衡量产品表现？\r\n哪些产品需要优化？\r\n哪些指标需要关注？\r\n\r\n\r\n点击流分析的意义可分为两大方面：\r\n1、技术上\r\n可以合理修改网站结构及适度分配资源，构建后台服务器群组，比如\r\n辅助改进网络的拓扑设计，提高性能\r\n在有高度相关性的节点之间安排快速有效的访问路径\r\n帮助企业更好地设计网站主页和安排网页内容\r\n\r\n\r\n2、业务上\r\n    1) 帮助企业改善市场营销决策，如把广告放在适当的Web页面上。\r\n    2) 优化页面及业务流程设计，提高流量转化率。\r\n    3) 帮助企业更好地根据客户的兴趣来安排内容。\r\n    4) 帮助企业对客户群进行细分，针对不同客户制定个性化的促销策略等。\r\n\r\n终极目标是：改善网站(电商、社交、电影、小说)的运营，获取更高投资回报率（ROI）\r\n\r\n\r\n',14,0,0,1515161227,0,0,0),(380,1,'如何进行网站流量分析','','','流量分析整体来说是一个内涵非常丰富的体系，其整体过程是一个金字塔结构：\r\n\r\n\r\n\r\n\r\n1.3.1 流量分析模型举例\r\n通常有以下几大类的分析需求：\r\n1)网站流量质量分析\r\n流量对于每个网站来说都是很重要，但流量并不是越多越好，应该更加看重流量的质量，换句话来说就是流量可以为我们带来多少收入。\r\n\r\n2)网站流量多维度细分\r\n细分是指通过不同维度对指标进行分割，查看同一个指标在不同维度下的表现，进而找出有问题的那部分指标，对这部分指标进行优化。\r\n\r\n\r\n3)网站内容及导航分析\r\n对于所有网站来说，页面都可以被划分为三个类别：\r\n    • 导航页\r\n    • 功能页\r\n    • 内容页\r\n\r\n首页和列表页都是典型的导航页；\r\n站内搜索页面、注册表单页面和购物车页面都是典型的功能页，\r\n而产品详情页、新闻和文章页都是典型的内容页。\r\n\r\n比如从内容导航分析中，以下两类行为就是网站运营者不希望看到的行为：\r\n第一个问题：访问者从导航页进入，在还没有看到内容页面之前就从导航页离开网站，需要分析导航页造成访问者中途离开的原因。\r\n第二个问题：访问者从导航页进入内容页后，又返回到导航页，说明需要分析内容页的最初设计，并考虑中内容页提供交叉的信息推荐\r\n\r\n4)网站转化及漏斗分析\r\n所谓转化，即网站业务流程中的一个封闭渠道，引导用户按照流程最终实现业务目标（比如商品成交）；而漏斗模型则是指进入渠道的用户在各环节递进过程中逐渐流失的形象描述；\r\n对于转化渠道，主要进行两部分的分析：\r\n访问者的流失和迷失\r\n\r\n\r\n    1、 阻力和流失\r\n造成流失的原因很多，如：\r\n不恰当的商品或活动推荐\r\n对支付环节中专业名词的解释、帮助信息等内容不当\r\n\r\n\r\n\r\n    2、 迷失\r\n\r\n造成迷失的主要原因是转化流量设计不合理，访问者在特定阶段得不到需要的信息，并且不能根据现有的信息作出决策\r\n\r\n总之，网站流量分析是一门内容非常丰富的学科，本课程中主要关注网站分析过程中的技术运用，更多关于网站流量分析的业务知识可学习推荐资料。\r\n1.3.2 流量分析常见指标\r\n课程中涉及的分析指标主要位于以下几大方面：\r\n1)基础分析（PV,IP,UV）\r\n    • 趋势分析：根据选定的时段，提供网站流量数据，通过流量趋势变化形态，为您分析网站访客的访问规律、网站发展状况提供参考。\r\n    • 对比分析：根据选定的两个对比时段，提供网站流量在时间上的纵向对比报表，帮您发现网站发展状况、发展规律、流量变化率等。\r\n    • 当前在线：提供当前时刻站点上的访客量，以及最近15分钟流量、来源、受访、访客变化情况等，方便用户及时了解当前网站流量状况。\r\n    • 访问明细：提供最近7日的访客访问记录，可按每个PV或每次访问行为（访客的每次会话）显示，并可按照来源、搜索词等条件进行筛选。 通过访问明细，用户可以详细了解网站流量的累计过程，从而为用户快速找出流量变动原因提供最原始、最准确的依据。\r\n2)来源分析\r\n    • 来源分类：提供不同来源形式（直接输入、搜索引擎、其他外部链接、站内来源）、不同来源项引入流量的比例情况。通过精确的量化数据，帮助用户分析什么类型的来路产生的流量多、效果好，进而合理优化推广方案。\r\n    • 搜索引擎：提供各搜索引擎以及搜索引擎子产品引入流量的比例情况。从搜索引擎引入流量的的角度，帮助用户了解网站的SEO、SEM效果，从而为制定下一步SEO、SEM计划提供依据。\r\n    • 搜索词：提供访客通过搜索引擎进入网站所使用的搜索词，以及各搜索词引入流量的特征和分布。帮助用户了解各搜索词引入流量的质量，进而了解访客的兴趣关注点、网站与访客兴趣点的匹配度，为优化SEO方案及SEM提词方案提供详细依据。\r\n    • 最近7日的访客搜索记录，可按每个PV或每次访问行为（访客的每次会话）显示，并可按照访客类型、地区等条件进行筛选。为您搜索引擎优化提供最详细的原始数据。\r\n    • 来路域名：提供具体来路域名引入流量的分布情况，并可按“社会化媒体”、“搜索引擎”、“邮箱”等网站类型对来源域名进行分类。 帮助用户了解哪类推广渠道产生的流量多、效果好，进而合理优化网站推广方案。\r\n    • 来路页面：提供具体来路页面引入流量的分布情况。 尤其对于通过流量置换、包广告位等方式从其他网站引入流量的用户，该功能可以方便、清晰地展现广告引入的流量及效果，为优化推广方案提供依据。\r\n    • 来源升降榜：提供开通统计后任意两日的TOP10000搜索词、来路域名引入流量的对比情况，并按照变化的剧烈程度提供排行榜。 用户可通过此功能快速找到哪些来路对网站流量的影响比较大，从而及时排查相应来路问题。\r\n\r\n3)受访分析\r\n    • 受访域名：提供访客对网站中各个域名的访问情况。 一般情况下，网站不同域名提供的产品、内容各有差异，通过此功能用户可以了解不同内容的受欢迎程度以及网站运营成效。\r\n    • 受访页面：提供访客对网站中各个页面的访问情况。 站内入口页面为访客进入网站时浏览的第一个页面，如果入口页面的跳出率较高则需要关注并优化；站内出口页面为访客访问网站的最后一个页面，对于离开率较高的页面需要关注并优化。\r\n    • 受访升降榜：提供开通统计后任意两日的TOP10000受访页面的浏览情况对比，并按照变化的剧烈程度提供排行榜。 可通过此功能验证经过改版的页面是否有流量提升或哪些页面有巨大流量波动，从而及时排查相应问题。\r\n    • 热点图：记录访客在页面上的鼠标点击行为，通过颜色区分不同区域的点击热度；支持将一组页面设置为&quot;关注范围&quot;，并可按来路细分点击热度。 通过访客在页面上的点击量统计，可以了解页面设计是否合理、广告位的安排能否获取更多佣金等。\r\n    • 用户视点：提供受访页面对页面上链接的其他站内页面的输出流量，并通过输出流量的高低绘制热度图，与热点图不同的是，所有记录都是实际打开了下一页面产生了浏览次数（PV）的数据，而不仅仅是拥有鼠标点击行为。\r\n    • 访问轨迹：提供观察焦点页面的上下游页面，了解访客从哪些途径进入页面，又流向了哪里。 通过上游页面列表比较出不同流量引入渠道的效果；通过下游页面列表了解用户的浏览习惯，哪些页面元素、内容更吸引访客点击。',14,0,0,1515161284,0,0,0),(381,1,'访客分析','','','    • 地区运营商：提供各地区访客、各网络运营商访客的访问情况分布。 地方网站、下载站等与地域性、网络链路等结合较为紧密的网站，可以参考此功能数据，合理优化推广运营方案。\r\n    • 终端详情：提供网站访客所使用的浏览终端的配置情况。 参考此数据进行网页设计、开发，可更好地提高网站兼容性，以达到良好的用户交互体验。\r\n    • 新老访客：当日访客中，历史上第一次访问该网站的访客记为当日新访客；历史上已经访问过该网站的访客记为老访客。 新访客与老访客进入网站的途径和浏览行为往往存在差异。该功能可以辅助分析不同访客的行为习惯，针对不同访客优化网站，例如为制作新手导航提供数据支持等。\r\n    • 忠诚度：从访客一天内回访网站的次数（日访问频度）与访客上次访问网站的时间两个角度，分析访客对网站的访问粘性、忠诚度、吸引程度。 由于提升网站内容的更新频率、增强用户体验与用户价值可以有更高的忠诚度，因此该功能在网站内容更新及用户体验方面提供了重要参考。\r\n\r\n    • 活跃度：从访客单次访问浏览网站的时间与网页数两个角度，分析访客在网站上的活跃程度。 由于提升网站内容的质量与数量可以获得更高的活跃度，因此该功能是网站内容分析的关键指标之一。',14,0,0,1515161303,0,0,0),(382,1,'转化路径分析','','','转化定义\r\n·访客在您的网站完成了某项您期望的活动，记为一次转化，如注册或下载。\r\n目标示例\r\n·获得用户目标：在线注册、创建账号等。\r\n·咨询目标：咨询、留言、电话等。\r\n·互动目标：视频播放、加入购物车、分享等。\r\n·收入目标：在线订单、付款等。\r\n转化数据的应用\r\n·在报告的自定义指标中勾选转化指标，实时掌握网站的推广及运营情况。\r\n·结合“全部来源”、“转化路径”、“页面上下游”等报告分析访问漏斗，提高转化率。\r\n·对“转化目标”设置价值，预估转化收益，衡量ROI。\r\n\r\n路径分析：根据设置的特定路线，监测某一流程的完成转化情况，算出每步的转换率和流失率数据，如注册流程，购买流程等。\r\n\r\n转化类型：\r\n    1、 页面\r\n\r\n\r\n    2、 事件\r\n\r\n\r\n\r\n2 整体技术流程及架构\r\n2.1 数据处理流程\r\n该项目是一个纯粹的数据分析项目，其整体流程基本上就是依据数据的处理流程进行，依此有以下几个大的步骤：\r\n    1) 数据采集\r\n首先，通过页面嵌入JS代码的方式获取用户访问行为，并发送到web服务的后台记录日志\r\n然后，将各服务器上生成的点击流日志通过实时或批量的方式汇聚到HDFS文件系统中\r\n\r\n当然，一个综合分析系统，数据源可能不仅包含点击流数据，还有数据库中的业务数据（如用户信息、商品信息、订单信息等）及对分析有益的外部数据。\r\n\r\n    2) 数据预处理\r\n通过mapreduce程序对采集到的点击流数据进行预处理，比如清洗，格式整理，滤除脏数据等\r\n\r\n    3) 数据入库\r\n将预处理之后的数据导入到HIVE仓库中相应的库和表中\r\n\r\n    4) 数据分析\r\n项目的核心内容，即根据需求开发ETL分析语句，得出各种统计结果\r\n\r\n    5) 数据展现\r\n将分析所得数据进行可视化\r\n',14,0,0,1515161338,0,0,0),(383,1,'项目结构','','','由于本项目是一个纯粹数据分析项目，其整体结构亦跟分析流程匹配，并没有特别复杂的结构，如下图：\r\n其中，需要强调的是：\r\n系统的数据分析不是一次性的，而是按照一定的时间频率反复计算，因而整个处理链条中的各个环节需要按照一定的先后依赖关系紧密衔接，即涉及到大量任务单元的管理调度，所以，项目中需要添加一个任务调度模块\r\n\r\n2.3 数据展现\r\n数据展现的目的是将分析所得的数据进行可视化，以便运营决策人员能更方便地获取数据，更快更简单地理解数据\r\n\r\n3 模块开发——数据采集\r\n3.1 需求\r\n数据采集的需求广义上来说分为两大部分。\r\n1）是在页面采集用户的访问行为，具体开发工作：\r\n    1、 开发页面埋点js，采集用户访问行为\r\n    2、 后台接受页面js请求记录日志\r\n此部分工作也可以归属为“数据源”，其开发工作通常由web开发团队负责\r\n\r\n2）是从web服务器上汇聚日志到HDFS，是数据分析系统的数据采集，此部分工作由数据分析平台建设团队负责，具体的技术实现有很多方式：\r\n    • Shell脚本\r\n优点：轻量级，开发简单\r\n缺点：对日志采集过程中的容错处理不便控制\r\n    • Java采集程序\r\n优点：可对采集过程实现精细控制\r\n缺点：开发工作量大\r\n    • Flume日志采集框架\r\n成熟的开源日志采集系统，且本身就是hadoop生态体系中的一员，与hadoop体系中的各种框架组件具有天生的亲和力，可扩展性强\r\n\r\n3.2 技术选型\r\n在点击流日志分析这种场景中，对数据采集部分的可靠性、容错能力要求通常不会非常严苛，因此使用通用的flume日志采集框架完全可以满足需求。\r\n本项目即使用flume来实现日志采集。\r\n\r\n3.3 Flume日志采集系统搭建\r\n    1、 数据源信息\r\n本项目分析的数据用nginx服务器所生成的流量日志，存放在各台nginx服务器上，如：\r\n/var/log/httpd/access_log.2015-11-10-13-00.log\r\n/var/log/httpd/access_log.2015-11-10-14-00.log\r\n/var/log/httpd/access_log.2015-11-10-15-00.log\r\n/var/log/httpd/access_log.2015-11-10-16-00.log\r\n\r\n    2、 数据内容样例\r\n数据的具体内容在采集阶段其实不用太关心。\r\n58.215.204.118 - - [18/Sep/2013:06:51:35 +0000] &quot;GET /wp-includes/js/jquery/jquery.js?ver=1.10.2 HTTP/1.1&quot; 304 0 &quot;http://blog.fens.me/nodejs-socketio-chat/&quot; &quot;Mozilla/5.0 (Windows NT 5.1; rv:23.0) Gecko/20100101 Firefox/23.0&quot;\r\n字段解析：\r\n1、访客ip地址：   58.215.204.118\r\n2、访客用户信息：  - -\r\n3、请求时间：[18/Sep/2013:06:51:35 +0000]\r\n4、请求方式：GET\r\n5、请求的url：/wp-includes/js/jquery/jquery.js?ver=1.10.2\r\n6、请求所用协议：HTTP/1.1\r\n7、响应码：304\r\n8、返回的数据流量：0\r\n9、访客的来源url：http://blog.fens.me/nodejs-socketio-chat/\r\n10、访客所用浏览器：Mozilla/5.0 (Windows NT 5.1; rv:23.0) Gecko/20100101 Firefox/23.0\r\n\r\n\r\n    3、 日志文件生成规律\r\n\r\n基本规律为：\r\n当前正在写的文件为access_log；\r\n文件体积达到64M，或时间间隔达到60分钟，即滚动重命名切换成历史日志文件；\r\n形如： access_log.2015-11-10-13-00.log\r\n\r\n当然，每个公司的web服务器日志策略不同，可在web程序的log4j.properties中定义，如下：\r\nlog4j.appender.logDailyFile = org.apache.log4j.DailyRollingFileAppender \r\nlog4j.appender.logDailyFile.layout = org.apache.log4j.PatternLayout \r\nlog4j.appender.logDailyFile.layout.ConversionPattern = [%-5p][%-22d{yyyy/MM/dd HH:mm:ssS}][%l]%n%m%n \r\nlog4j.appender.logDailyFile.Threshold = DEBUG \r\nlog4j.appender.logDailyFile.ImmediateFlush = TRUE \r\nlog4j.appender.logDailyFile.Append = TRUE \r\nlog4j.appender.logDailyFile.File = /var/logs/access_log \r\nlog4j.appender.logDailyFile.DatePattern = &#039;.&#039;yyyy-MM-dd-HH-mm&#039;.log&#039; \r\nlog4j.appender.logDailyFile.Encoding = UTF-8\r\n',14,0,0,1515161378,0,0,0),(384,1,'Flume采集实现','','','Flume采集系统的搭建相对简单：\r\n    1、 在个web服务器上部署agent节点，修改配置文件\r\n    2、 启动agent节点，将采集到的数据汇聚到指定的HDFS目录中\r\n如下图：\r\n\r\n\r\n    • 版本选择：apache-flume-1.6.0\r\n    • 采集规则设计：\r\n    1、 采集源：nginx服务器日志目录\r\n    2、 存放地：hdfs目录/home/hadoop/weblogs/\r\n\r\n    • 采集规则配置详情\r\nagent1.sources = source1\r\nagent1.sinks = sink1\r\nagent1.channels = channel1\r\n\r\n# Describe/configure spooldir source1\r\n#agent1.sources.source1.type = spooldir\r\n#agent1.sources.source1.spoolDir = /var/logs/nginx/\r\n#agent1.sources.source1.fileHeader = false\r\n\r\n# Describe/configure tail -F source1\r\n#使用exec作为数据源source组件\r\nagent1.sources.source1.type = exec \r\n#使用tail -F命令实时收集新产生的日志数据\r\nagent1.sources.source1.command = tail -F /var/logs/nginx/access_log\r\nagent1.sources.source1.channels = channel1\r\n\r\n#configure host for source\r\n#配置一个拦截器插件\r\nagent1.sources.source1.interceptors = i1\r\nagent1.sources.source1.interceptors.i1.type = host\r\n#使用拦截器插件获取agent所在服务器的主机名\r\nagent1.sources.source1.interceptors.i1.hostHeader = hostname\r\n\r\n#配置sink组件为hdfs\r\nagent1.sinks.sink1.type = hdfs\r\n#a1.sinks.k1.channel = c1\r\n#agent1.sinks.sink1.hdfs.path=hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H%M%S\r\n#指定文件sink到hdfs上的路径\r\nagent1.sinks.sink1.hdfs.path=\r\nhdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M_%hostname\r\n#指定文件名前缀\r\nagent1.sinks.sink1.hdfs.filePrefix = access_log\r\nagent1.sinks.sink1.hdfs.maxOpenFiles = 5000 \r\n#指定每批下沉数据的记录条数\r\nagent1.sinks.sink1.hdfs.batchSize= 100\r\nagent1.sinks.sink1.hdfs.fileType = DataStream\r\nagent1.sinks.sink1.hdfs.writeFormat =Text\r\n#指定下沉文件按1G大小滚动\r\nagent1.sinks.sink1.hdfs.rollSize = 1024*1024*1024\r\n#指定下沉文件按1000000条数滚动\r\nagent1.sinks.sink1.hdfs.rollCount = 1000000\r\n#指定下沉文件按30分钟滚动\r\nagent1.sinks.sink1.hdfs.rollInterval = 30\r\n#agent1.sinks.sink1.hdfs.round = true\r\n#agent1.sinks.sink1.hdfs.roundValue = 10\r\n#agent1.sinks.sink1.hdfs.roundUnit = minute\r\nagent1.sinks.sink1.hdfs.useLocalTimeStamp = true\r\n\r\n# Use a channel which buffers events in memory\r\n#使用memory类型channel\r\nagent1.channels.channel1.type = memory\r\nagent1.channels.channel1.keep-alive = 120\r\nagent1.channels.channel1.capacity = 500000\r\nagent1.channels.channel1.transactionCapacity = 600\r\n\r\n# Bind the source and sink to the channel\r\nagent1.sources.source1.channels = channel1\r\nagent1.sinks.sink1.channel = channel1\r\n\r\n启动采集\r\n在部署了flume的nginx服务器上，启动flume的agent，命令如下：\r\nbin/flume-ng agent --conf ./conf -f ./conf/weblog.properties.2 -n agent\r\n\r\n注意：启动命令中的 -n 参数要给配置文件中配置的agent名称\r\n',14,0,0,1515161404,0,0,0),(385,1,'模块开发——数据预处理','','','主要目的：\r\n过滤“不合规”数据\r\n格式转换和规整\r\n根据后续的统计需求，过滤分离出各种不同主题的基础数据\r\n\r\n4.2 实现方式：\r\n开发一个mr程序WeblogPreProcess(内容太长，见工程代码)\r\npublic class WeblogPreProcess {\r\n	static class WeblogPreProcessMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {\r\n		Text k = new Text();\r\n		NullWritable v = NullWritable.get();\r\n		@Override\r\n		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\r\n			String line = value.toString();\r\n			WebLogBean webLogBean = WebLogParser.parser(line);\r\n//			WebLogBean productWebLog = WebLogParser.parser2(line);\r\n//			WebLogBean bbsWebLog = WebLogParser.parser3(line);\r\n//			WebLogBean cuxiaoBean = WebLogParser.parser4(line);\r\n			if (!webLogBean.isValid())\r\n				return;\r\n			k.set(webLogBean.toString());\r\n			context.write(k, v);\r\n//			k.set(productWebLog);\r\n//			context.write(k, v);\r\n		}\r\n	}\r\n	public static void main(String[] args) throws Exception {\r\n		\r\n		Configuration conf = new Configuration();\r\n		Job job = Job.getInstance(conf);\r\n		job.setJarByClass(WeblogPreProcess.class);\r\n		job.setMapperClass(WeblogPreProcessMapper.class);\r\n		job.setOutputKeyClass(Text.class);\r\n		job.setOutputValueClass(NullWritable.class);\r\n		FileInputFormat.setInputPaths(job, new Path(args[0]));\r\n		FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n		job.waitForCompletion(true);\r\n		\r\n	}\r\n}\r\n\r\n\r\n    • 运行mr对数据进行预处理\r\nhadoop jar weblog.jar  cn.itcast.bigdata.hive.mr.WeblogPreProcess /weblog/input /weblog/preout\r\n\r\n4.3 点击流模型数据梳理\r\n由于大量的指标统计从点击流模型中更容易得出，所以在预处理阶段，可以使用mr程序来生成点击流模型的数据\r\n4.3.1 点击流模型pageviews表\r\nPageviews表模型数据生成\r\n代码见工程\r\n\r\nhadoop jar weblogpreprocess.jar  \\\r\ncn.itcast.bigdata.hive.mr.ClickStreamThree   \\\r\n/user/hive/warehouse/dw_click.db/test_ods_weblog_origin/datestr=2013-09-20/ /test-click/pageviews/\r\n\r\n表结构：\r\n点击流模型visit信息表\r\n注：“一次访问”=“N次连续请求”\r\n直接从原始数据中用hql语法得出每个人的“次”访问信息比较困难，可先用mapreduce程序分析原始数据得出“次”信息数据，然后再用hql进行更多维度统计\r\n\r\n用MR程序从pageviews数据中，梳理出每一次visit的起止时间、页面信息\r\n代码见工程\r\nhadoop jar weblogpreprocess.jar cn.itcast.bigdata.hive.mr.ClickStreamVisit /weblog/sessionout /weblog/visitout\r\n\r\n然后，在hive仓库中建点击流visit模型表\r\ndrop table if exist click_stream_visit;\r\ncreate table click_stream_visit(\r\nsession     string,\r\nremote_addr string,\r\ninTime      string,\r\noutTime     string,\r\ninPage      string,\r\noutPage     string,\r\nreferal     string,\r\npageVisits  int)\r\npartitioned by (datestr string);\r\n\r\n然后，将MR运算得到的visit数据导入visit模型表\r\nload data inpath &#039;/weblog/visitout&#039; into table click_stream_visit partition(datestr=&#039;2013-09-18&#039;);\r\n\r\n5 模块开发——数据仓库设计\r\n注：采用星型模型                                            \r\n5.1 事实表\r\n原始数据表:t_origin_weblog\r\nvalid\r\nstring\r\n是否有效\r\nremote_addr\r\nstring\r\n访客ip\r\nremote_user\r\nstring\r\n访客用户信息\r\ntime_local\r\nstring\r\n请求时间\r\nrequest\r\nstring\r\n请求url\r\nstatus\r\nstring\r\n响应码\r\nbody_bytes_sent\r\nstring\r\n响应字节数\r\nhttp_referer\r\nstring\r\n来源url\r\nhttp_user_agent\r\nstring\r\n访客终端信息\r\n\r\n\r\n\r\nETL中间表：t_etl_referurl\r\nvalid\r\nstring\r\n是否有效\r\nremote_addr\r\nstring\r\n访客ip\r\nremote_user\r\nstring\r\n访客用户信息\r\ntime_local\r\nstring\r\n请求时间\r\nrequest\r\nstring\r\n请求url\r\nrequest_host\r\nstring\r\n请求的域名\r\nstatus\r\nstring\r\n响应码\r\nbody_bytes_sent\r\nstring\r\n响应字节数\r\nhttp_referer\r\nstring\r\n来源url\r\nhttp_user_agent\r\nstring\r\n访客终端信息\r\nvalid\r\nstring\r\n是否有效\r\nremote_addr\r\nstring\r\n访客ip\r\nremote_user\r\nstring\r\n访客用户信息\r\ntime_local\r\nstring\r\n请求时间\r\nrequest\r\nstring\r\n请求url\r\nstatus\r\nstring\r\n响应码\r\nbody_bytes_sent\r\nstring\r\n响应字节数\r\nhttp_referer\r\nstring\r\n外链url\r\nhttp_user_agent\r\nstring\r\n访客终端信息\r\nhost\r\nstring\r\n外链url的域名\r\npath\r\nstring\r\n外链url的路径\r\nquery\r\nstring\r\n外链url的参数\r\nquery_id\r\nstring\r\n外链url的参数值\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n访问日志明细宽表：t_ods_access_detail\r\nvalid\r\nstring\r\n是否有效\r\nremote_addr\r\nstring\r\n访客ip\r\nremote_user\r\nstring\r\n访客用户信息\r\ntime_local\r\nstring\r\n请求时间\r\nrequest\r\nstring\r\n请求url整串\r\nrequest_level1\r\nstring\r\n请求的一级栏目\r\nrequest_level2\r\nstring\r\n请求的二级栏目\r\nrequest_level3\r\nstring\r\n请求的三级栏目\r\nstatus\r\nstring\r\n响应码\r\nbody_bytes_sent\r\nstring\r\n响应字节数\r\nhttp_referer\r\nstring\r\n来源url\r\nhttp_user_agent\r\nstring\r\n访客终端信息\r\nvalid\r\nstring\r\n是否有效\r\nremote_addr\r\nstring\r\n访客ip\r\nremote_user\r\nstring\r\n访客用户信息\r\ntime_local\r\nstring\r\n请求时间\r\nrequest\r\nstring\r\n请求url\r\nstatus\r\nstring\r\n响应码\r\nbody_bytes_sent\r\nstring\r\n响应字节数\r\nhttp_referer\r\nstring\r\n外链url\r\nhttp_user_agent\r\nstring\r\n访客终端信息整串\r\nhttp_user_agent_browser\r\nstring\r\n访客终端浏览器\r\nhttp_user_agent_sys\r\nstring\r\n访客终端操作系统\r\nhttp_user_agent_dev\r\nstring\r\n访客终端设备\r\nhost\r\nstring\r\n外链url的域名\r\npath\r\nstring\r\n外链url的路径\r\nquery\r\nstring\r\n外链url的参数\r\nquery_id\r\nstring\r\n外链url的参数值\r\ndaystr\r\nstring\r\n日期整串\r\ntmstr\r\nstring\r\n时间整串\r\nmonth\r\nstring\r\n月份\r\nday\r\nstring\r\n日\r\nhour\r\nstring\r\n时\r\nminute\r\nstring\r\n分\r\n##\r\n##\r\n##\r\nmm\r\nstring\r\n分区字段--月\r\ndd\r\nstring\r\n分区字段--日\r\n\r\n\r\n\r\n5.2 维度表\r\n\r\n时间维度 v_year_month_date\r\nyear\r\nmonth\r\nday\r\nhour\r\nminute\r\n\r\n\r\n访客地域维度t_dim_area\r\n北京\r\n上海\r\n广州\r\n深圳\r\n河北\r\n河南\r\n\r\n\r\n终端类型维度t_dim_termination\r\nuc\r\nfirefox\r\nchrome\r\nsafari\r\nios\r\nandroid\r\n\r\n\r\n网站栏目维度 t_dim_section\r\n跳蚤市场\r\n房租信息\r\n休闲娱乐\r\n建材装修\r\n本地服务\r\n人才市场\r\n',14,0,0,1515161443,0,0,0),(386,1,'模块开发——ETL','','','该项目的数据分析过程在hadoop集群上实现，主要应用hive数据仓库工具，因此，采集并经过预处理后的数据，需要加载到hive数据仓库中，以进行后续的挖掘分析。\r\n6.1创建原始数据表\r\n--在hive仓库中建贴源数据表\r\ndrop table if exists ods_weblog_origin;\r\ncreate table ods_weblog_origin(\r\nvalid string,\r\nremote_addr string,\r\nremote_user string,\r\ntime_local string,\r\nrequest string,\r\nstatus string,\r\nbody_bytes_sent string,\r\nhttp_referer string,\r\nhttp_user_agent string)\r\npartitioned by (datestr string)\r\nrow format delimited\r\nfields terminated by &#039;\\001&#039;;\r\n\r\n点击流模型pageviews表\r\ndrop table if exists ods_click_pageviews;\r\ncreate table ods_click_pageviews(\r\nSession string,\r\nremote_addr string,\r\ntime_local string,\r\nrequest string,\r\nvisit_step string,\r\npage_staylong string,\r\nhttp_referer string,\r\nhttp_user_agent string,\r\nbody_bytes_sent string,\r\nstatus string)\r\npartitioned by (datestr string)\r\nrow format delimited\r\nfields terminated by &#039;\\001&#039;;\r\n\r\n时间维表创建\r\ndrop table dim_time if exists ods_click_pageviews;\r\ncreate table dim_time(\r\nyear string,\r\nmonth string,\r\nday string,\r\nhour string)\r\nrow format delimited\r\nfields terminated by &#039;,&#039;;\r\n\r\n\r\n6.2导入数据\r\n导入清洗结果数据到贴源数据表ods_weblog_origin\r\nload data inpath &#039;/weblog/preprocessed/16-02-24-16/&#039; overwrite into table ods_weblog_origin partition(datestr=&#039;2013-09-18&#039;);\r\n\r\n0: jdbc:hive2://localhost:10000&gt; show partitions ods_weblog_origin;\r\n+-------------------+--+\r\n|     partition     |\r\n+-------------------+--+\r\n| timestr=20151203  |\r\n+-------------------+--+\r\n\r\n0: jdbc:hive2://localhost:10000&gt; select count(*) from ods_origin_weblog;\r\n+--------+--+\r\n|  _c0   |\r\n+--------+--+\r\n| 11347  |\r\n+--------+--+\r\n\r\n导入点击流模型pageviews数据到ods_click_pageviews表\r\n0: jdbc:hive2://hdp-node-01:10000&gt; load data inpath &#039;/weblog/clickstream/pageviews&#039; overwrite into table ods_click_pageviews partition(datestr=&#039;2013-09-18&#039;);\r\n\r\n0: jdbc:hive2://hdp-node-01:10000&gt; select count(1) from ods_click_pageviews;\r\n+------+--+\r\n| _c0  |\r\n+------+--+\r\n| 66   |\r\n+------+--+\r\n\r\n\r\n导入点击流模型visit数据到ods_click_visit表\r\n\r\n导入时间维表：\r\nload data inpath &#039;/dim_time.txt&#039; into table dim_time;\r\n\r\n6.3 生成ODS层明细宽表 \r\n6.3.1 需求概述\r\n整个数据分析的过程是按照数据仓库的层次分层进行的，总体来说，是从ODS原始数据中整理出一些中间表（比如，为后续分析方便，将原始数据中的时间、url等非结构化数据作结构化抽取，将各种字段信息进行细化，形成明细表），然后再在中间表的基础之上统计出各种指标数据\r\n\r\n6.3.2 ETL实现\r\n建表——明细表   (源：ods_weblog_origin)   （目标：ods_weblog_detail）\r\ndrop table ods_weblog_detail;\r\ncreate table ods_weblog_detail(\r\nvalid           string, --有效标识\r\nremote_addr     string, --来源IP\r\nremote_user     string, --用户标识\r\ntime_local      string, --访问完整时间\r\ndaystr          string, --访问日期\r\ntimestr         string, --访问时间\r\nmonth           string, --访问月\r\nday             string, --访问日\r\nhour            string, --访问时\r\nrequest         string, --请求的url\r\nstatus          string, --响应码\r\nbody_bytes_sent string, --传输字节数\r\nhttp_referer    string, --来源url\r\nref_host        string, --来源的host\r\nref_path        string, --来源的路径\r\nref_query       string, --来源参数query\r\nref_query_id    string, --来源参数query的值\r\nhttp_user_agent string --客户终端标识\r\n)\r\npartitioned by(datestr string);\r\n\r\n--抽取refer_url到中间表  &quot;t_ods_tmp_referurl&quot; \r\n--将来访url分离出host  path  query  query id\r\ndrop table if exists t_ods_tmp_referurl;\r\ncreate table t_ ods _tmp_referurl as\r\nSELECT a.*,b.*\r\nFROM ods_origin_weblog a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, &quot;\\&quot;&quot;, &quot;&quot;), &#039;HOST&#039;, &#039;PATH&#039;,&#039;QUERY&#039;, &#039;QUERY:id&#039;) b as host, path, query, query_id; \r\n\r\n--抽取转换time_local字段到中间表明细表 ”t_ ods _detail”\r\ndrop table if exists t_ods_tmp_detail;\r\ncreate table t_ods_tmp_detail as \r\nselect b.*,substring(time_local,0,10) as daystr,\r\nsubstring(time_local,11) as tmstr,\r\nsubstring(time_local,5,2) as month,\r\nsubstring(time_local,8,2) as day,\r\nsubstring(time_local,11,2) as hour\r\nFrom t_ ods _tmp_referurl b;\r\n\r\n以上语句可以改写成：\r\ninsert into table zs.ods_weblog_detail partition(datestr=&#039;$day_01&#039;)\r\nselect c.valid,c.remote_addr,c.remote_user,c.time_local,\r\nsubstring(c.time_local,0,10) as daystr,\r\nsubstring(c.time_local,12) as tmstr,\r\nsubstring(c.time_local,6,2) as month,\r\nsubstring(c.time_local,9,2) as day,\r\nsubstring(c.time_local,11,3) as hour,\r\nc.request,c.status,c.body_bytes_sent,c.http_referer,c.ref_host,c.ref_path,c.ref_query,c.ref_query_id,c.http_user_agent\r\nfrom\r\n(SELECT \r\na.valid,a.remote_addr,a.remote_user,a.time_local,\r\na.request,a.status,a.body_bytes_sent,a.http_referer,a.http_user_agent,b.ref_host,b.ref_path,b.ref_query,b.ref_query_id \r\nFROM zs.ods_weblog_origin a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, &quot;\\&quot;&quot;, &quot;&quot;), &#039;HOST&#039;, &#039;PATH&#039;,&#039;QUERY&#039;, &#039;QUERY:id&#039;) b as ref_host, ref_path, ref_query, ref_query_id) c\r\n&quot;\r\n0: jdbc:hive2://localhost:10000&gt; show partitions ods_weblog_detail;\r\n+---------------------+--+\r\n|      partition      |\r\n+---------------------+--+\r\n| dd=18%2FSep%2F2013  |\r\n+---------------------+--+\r\n1 row selected (0.134 seconds)\r\n',14,0,0,1515161470,0,0,0),(387,1,'模块开发——统计分析','','','注：每一种统计指标都可以跟各维度表进行叉乘，从而得出各个维度的统计结果  \r\n篇幅限制，叉乘的代码及注释信息详见项目工程代码文件                    \r\n为了在前端展示时速度更快，每一个指标都事先算出各维度结果存入mysql     \r\n\r\n提前准备好维表数据，在hive仓库中创建相应维表，如：\r\n时间维表：\r\ncreate table v_time(year string,month string,day string,hour string)\r\nrow format delimited\r\nfields terminated by &#039;,&#039;;\r\n\r\nload data local inpath &#039;/home/hadoop/v_time.txt&#039; into table v_time;\r\n\r\n在实际生产中，究竟需要哪些统计指标通常由相关数据需求部门人员提出，而且会不断有新的统计需求产生，以下为网站流量分析中的一些典型指标示例。\r\n    1. PV统计\r\n1.1 多维度统计PV总量\r\n1. 时间维度\r\n--计算指定的某个小时pvs\r\nselect count(*),month,day,hour from dw_click.ods_weblog_detail group by month,day,hour;\r\n\r\n\r\n--计算该处理批次（一天）中的各小时pvs\r\ndrop table dw_pvs_hour;\r\ncreate table dw_pvs_hour(month string,day string,hour string,pvs bigint) partitioned by(datestr string);\r\n\r\ninsert into table dw_pvs_hour partition(datestr=&#039;2016-03-18&#039;)\r\nselect a.month as month,a.day as day,a.hour as hour,count(1) as pvs from ods_weblog_detail a\r\nwhere  a.datestr=&#039;2016-03-18&#039;  group by a.month,a.day,a.hour;\r\n\r\n或者用时间维表关联\r\n\r\n\r\n\r\n维度：日\r\ndrop table dw_pvs_day;\r\ncreate table dw_pvs_day(pvs bigint,month string,day string);\r\n\r\ninsert into table dw_pvs_day\r\nselect count(1) as pvs,a.month as month,a.day as day  from dim_time a\r\njoin ods_weblog_detail b \r\non b.dd=&#039;18/Sep/2013&#039; and a.month=b.month and a.day=b.day\r\ngroup by a.month,a.day;\r\n\r\n--或者，从之前算好的小时结果中统计\r\nInsert into table dw_pvs_day\r\nSelect sum(pvs) as pvs,month,day from dw_pvs_hour group by month,day having day=&#039;18&#039;;\r\n\r\n结果如下：\r\n\r\n\r\n维度：月\r\ndrop table t_display_pv_month;\r\ncreate table t_display_pv_month (pvs bigint,month string);\r\ninsert into table t_display_pv_month\r\nselect count(*) as pvs,a.month from t_dim_time a\r\njoin t_ods_detail_prt b on a.month=b.month group by a.month;\r\n\r\n\r\n2. 按终端维度统计pv总量\r\n注：探索数据中的终端类型\r\nselect distinct(http_user_agent) from ods_weblog_detail where http_user_agent like &#039;%Mozilla%&#039; limit 200;\r\n\r\n\r\n终端维度：uc\r\ndrop table t_display_pv_terminal_uc;\r\ncreate table t_display_pv_ terminal_uc (pvs bigint,mm string,dd string,hh string);\r\n\r\n终端维度：chrome\r\ndrop table t_display_pv_terminal_chrome;\r\ncreate table t_display_pv_ terminal_ chrome (pvs bigint,mm string,dd string,hh string);\r\n\r\n终端维度：safari\r\ndrop table t_display_pv_terminal_safari;\r\ncreate table t_display_pv_ terminal_ safari (pvs bigint,mm string,dd string,hh string);\r\n\r\n3. 按栏目维度统计pv总量\r\n栏目维度：job\r\n栏目维度：news\r\n栏目维度：bargin\r\n栏目维度：lane\r\n\r\n\r\n1.2 人均浏览页数\r\n需求描述：比如，今日所有来访者，平均请求的页面数\r\n\r\n--总页面请求数/去重总人数\r\ndrop table dw_avgpv_user_d;\r\ncreate table dw_avgpv_user_d(\r\nday string,\r\navgpv string);\r\n\r\ninsert into table dw_avgpv_user_d\r\nselect &#039;2013-09-18&#039;,sum(b.pvs)/count(b.remote_addr) from\r\n(select remote_addr,count(1) as pvs from ods_weblog_detail where datestr=&#039;2013-09-18&#039; group by remote_addr) b;\r\n\r\n\r\n\r\n1.3 按referer维度统计pv总量\r\n需求：按照来源及时间维度统计PVS，并按照PV大小倒序排序\r\n\r\n-- 按照小时粒度统计，查询结果存入：( &quot;dw_pvs_referer_h&quot; )\r\ndrop table dw_pvs_referer_h;\r\ncreate table dw_pvs_referer_h(referer_url string,referer_host string,month string,day string,hour string,pv_referer_cnt bigint) partitioned by(datestr string);\r\n\r\ninsert into table dw_pvs_referer_h partition(datestr=&#039;2016-03-18&#039;)\r\nselect http_referer,ref_host,month,day,hour,count(1) as pv_referer_cnt\r\nfrom ods_weblog_detail \r\ngroup by http_referer,ref_host,month,day,hour \r\nhaving ref_host is not null\r\norder by hour asc,day asc,month asc,pv_referer_cnt desc;\r\n\r\n\r\n按天粒度统计各来访域名的访问次数并排序\r\ndrop table dw_ref_host_visit_cnts_h;\r\ncreate table dw_ref_host_visit_cnts_h(ref_host string,month string,day string,hour string,ref_host_cnts bigint) partitioned by(datestr string);\r\n\r\ninsert into table dw_ref_host_visit_cnts_h partition(datestr=&#039;2016-03-18&#039;)\r\nselect ref_host,month,day,hour,count(1) as ref_host_cnts\r\nfrom ods_weblog_detail \r\ngroup by ref_host,month,day,hour \r\nhaving ref_host is not null\r\norder by hour asc,day asc,month asc,ref_host_cnts desc;\r\n注：还可以按来源地域维度、访客终端维度等计算\r\n\r\n1.4 统计pv总量最大的来源TOPN\r\n需求描述：按照时间维度，比如，统计一天内产生最多pvs的来源topN\r\n\r\n需要用到row_number函数\r\n以下语句对每个小时内的来访host次数倒序排序标号，\r\nselect ref_host,ref_host_cnts,concat(month,hour,day),\r\nrow_number() over (partition by concat(month,hour,day) order by ref_host_cnts desc) as od \r\nfrom dw_ref_host_visit_cnts_h\r\n\r\n根据上述row_number的功能，可编写Hql取各小时的ref_host访问次数topn\r\ndrop table dw_pvs_refhost_topn_h;\r\ncreate table dw_pvs_refhost_topn_h(\r\nhour string,\r\ntoporder string,\r\nref_host string,\r\nref_host_cnts string\r\n) partitioned by(datestr string);\r\n\r\ninsert into table zs.dw_pvs_refhost_topn_h partition(datestr=&#039;2016-03-18&#039;)\r\nselect t.hour,t.od,t.ref_host,t.ref_host_cnts from\r\n (select ref_host,ref_host_cnts,concat(month,day,hour) as hour,\r\nrow_number() over (partition by concat(month,day,hour) order by ref_host_cnts desc) as od \r\nfrom zs.dw_ref_host_visit_cnts_h) t where od&lt;=3;\r\n\r\n\r\n结果如下：',14,0,0,1515161514,0,0,0),(388,1,'受访分析','','','统计每日最热门的页面top10\r\ndrop table dw_pvs_d;\r\ncreate table dw_pvs_d(day string,url string,pvs string);\r\n\r\ninsert into table dw_pvs_d\r\nselect &#039;2013-09-18&#039;,a.request,a.request_counts from\r\n(select request as request,count(request) as request_counts from ods_weblog_detail where datestr=&#039;2013-09-18&#039; group by request having request is not null) a\r\norder by a.request_counts desc limit 10;\r\n\r\n结果如下：\r\n\r\n\r\n注：还可继续得出各维度交叉结果\r\n\r\n',14,0,0,1515161550,0,0,0),(389,1,'访客分析','','','独立访客\r\n需求描述：按照时间维度比如小时来统计独立访客及其产生的pvCnts\r\n对于独立访客的识别，如果在原始日志中有用户标识，则根据用户标识即很好实现;\r\n此处，由于原始日志中并没有用户标识，以访客IP来模拟，技术上是一样的，只是精确度相对较低\r\n\r\n时间维度：时\r\ndrop table dw_user_dstc_ip_h;\r\ncreate table dw_user_dstc_ip_h(\r\nremote_addr string,\r\npvs      bigint,\r\nhour     string);\r\n\r\ninsert into table dw_user_dstc_ip_h \r\nselect remote_addr,count(1) as pvs,concat(month,day,hour) as hour \r\nfrom ods_weblog_detail\r\nWhere datestr=&#039;2013-09-18&#039;\r\ngroup by concat(month,day,hour),remote_addr;\r\n\r\n在此结果表之上，可以进一步统计出，每小时独立访客总数，每小时请求次数topn访客等\r\n如每小时独立访客总数：\r\nselect count(1) as dstc_ip_cnts,hour from dw_user_dstc_ip_h group by hour;\r\n\r\n练习：\r\n统计每小时请求次数topn的独立访客\r\n时间维度：月\r\nselect remote_addr,count(1) as counts,month \r\nfrom ods_weblog_detail\r\ngroup by month,remote_addr;\r\n\r\n时间维度：日\r\nselect remote_addr,count(1) as counts,concat(month,day) as day\r\nfrom ods_weblog_detail\r\nWhere dd=&#039;18/Sep/2013&#039;\r\ngroup by concat(month,day),remote_addr;\r\n\r\n\r\n\r\n注：还可以按来源地域维度、访客终端维度等计算\r\n',14,0,0,1515161587,0,0,0),(390,1,'每日新访客','','','需求描述：将每天的新访客统计出来\r\n实现思路：创建一个去重访客累积表，然后将每日访客对比累积表\r\n\r\n时间维度：日\r\n--历日去重访客累积表\r\ndrop table dw_user_dsct_history;\r\ncreate table dw_user_dsct_history(\r\nday string,\r\nip string\r\n) \r\npartitioned by(datestr string);\r\n\r\n--每日新用户追加到累计表\r\ndrop table dw_user_dsct_history;\r\ncreate table dw_user_dsct_history(\r\nday string,\r\nip string\r\n) \r\npartitioned by(datestr string);\r\n\r\n--每日新用户追加到累计表\r\ninsert into table dw_user_dsct_history partition(datestr=&#039;2013-09-19&#039;)\r\nselect tmp.day as day,tmp.today_addr as new_ip from\r\n(\r\nselect today.day as day,today.remote_addr as today_addr,old.ip as old_addr \r\nfrom \r\n(select distinct remote_addr as remote_addr,&quot;2013-09-19&quot; as day from ods_weblog_detail where datestr=&quot;2013-09-19&quot;) today\r\nleft outer join \r\ndw_user_dsct_history old\r\non today.remote_addr=old.ip\r\n) tmp\r\nwhere tmp.old_addr is null;\r\n\r\n验证：\r\nselect count(distinct remote_addr) from ods_weblog_detail;\r\n-- 1005\r\n\r\nselect count(1) from dw_user_dsct_history where prtflag_day=&#039;18/Sep/2013&#039;;\r\n--845\r\n\r\nselect count(1) from dw_user_dsct_history where prtflag_day=&#039;19/Sep/2013&#039;;\r\n--160\r\n\r\n\r\n时间维度：月\r\n类似日粒度算法\r\n\r\n注：还可以按来源地域维度、访客终端维度等计算',14,0,0,1515161609,0,0,0),(391,1,'Visit分析（点击流模型）','','','回头/单次访客统计\r\n需求描述：查询今日所有回头访客及其访问次数\r\n\r\n实现思路：上表中出现次数&gt;1的访客，即回头访客；反之，则为单次访客\r\n\r\n\r\ndrop table dw_user_returning;\r\ncreate table dw_user_returning(\r\nday string,\r\nremote_addr string,\r\nacc_cnt string)\r\npartitioned by (datestr string);\r\n\r\ninsert overwrite table dw_user_returning partition(datestr=&#039;2013-09-18&#039;)\r\n\r\nselect tmp.day,tmp.remote_addr,tmp.acc_cnt\r\nfrom\r\n(select &#039;2013-09-18&#039; as day,remote_addr,count(session) as acc_cnt from click_stream_visit group by remote_addr) tmp\r\nwhere tmp.acc_cnt&gt;1;\r\n\r\n4.3 人均访问频次\r\n需求：统计出每天所有用户访问网站的平均次数（visit）\r\n总visit数/去重总用户数\r\nselect sum(pagevisits)/count(distinct remote_addr) from click_stream_visit partition(datestr=&#039;2013-09-18&#039;);\r\n\r\nVisit分析另一种实现方式\r\n5.1 mr程序识别出访客的每次访问\r\na.) 首先开发MAPREDUCE程序：UserStayTime\r\n注：代码略长，见项目工程代码 \r\n                                                   \r\n\r\nb.) 提交MAPREDUCE程序进行运算\r\n[hadoop@hdp-node-01 ~]$ hadoop jar weblog.jar cn.itcast.bigdata.hive.mr.UserStayTime /weblog/input /weblog/stayout4\r\n--导入hive表(&quot;t_display_access_info&quot;)中\r\ndrop table ods_access_info;\r\ncreate table ods_access_info(remote_addr string,firt_req_time string,last_req_time string,stay_long string)\r\npartitioned by(prtflag_day string)\r\nrow format delimited\r\nfields terminated by &#039;\\t&#039;;\r\n\r\nload data inpath &#039;/weblog/stayout4&#039; into table ods_access_info partition(prtflag_day=&#039;18/Sep/2013&#039;);\r\n创建表时stay_long使用的string类型，但是在后续过程中发现还是用bigint更好，进行表修改\r\nalter table ods_access_info change column stay_long stay_long bigint;\r\n\r\n5.2 将mr结果导入访客访问信息表 &quot;t_display_access_info&quot;\r\n由于有一些访问记录是单条记录，mr程序处理处的结果给的时长是0，所以考虑给单次请求的停留时间一个默认市场30秒\r\ndrop table dw_access_info;\r\ncreate table dw_access_info(remote_addr string,firt_req_time string,last_req_time string,stay_long string)\r\npartitioned by(prtflag_day string);\r\n\r\ninsert into table dw_access_info partition(prtflag_day=&#039;19/Sep/2013&#039;)\r\nselect remote_addr,firt_req_time,last_req_time,\r\ncase stay_long\r\nwhen 0 then 30000\r\nelse stay_long\r\nend as stay_long\r\nfrom ods_access_info\r\nwhere prtflag_day=&#039;18/Sep/2013&#039;;\r\n\r\n\r\n\r\n在访问信息表的基础之上，可以实现更多指标统计，如：\r\n统计所有用户停留时间平均值，观察用户在站点停留时长的变化走势\r\nselect prtflag_day as dt,avg(stay_long) as avg_staylong \r\nfrom dw_access_info group by prtflag_day;\r\n\r\n\r\n\r\n\r\n5.3 回头/单次访客统计\r\n注：从上一步骤得到的访问信息统计表“dw_access_info”中查询\r\n\r\n--回头访客访问信息表 &quot;dw_access_info_htip&quot;\r\ndrop table dw_access_info_htip;\r\ncreate table dw_access_info_htip(remote_addr string, firt_req_time string, last_req_time string, stay_long string,acc_counts string)\r\npartitioned by(prtflag_day string);\r\n\r\ninsert into table dw_access_info_htip partition(prtflag_day=&#039;18/Sep/2013&#039;)\r\nselect b.remote_addr,b.firt_req_time,b.last_req_time,b.stay_long,a.acc_counts from \r\n(select remote_addr,count(remote_addr) as acc_counts from dw_access_info where prtflag_day=&#039;18/Sep/2013&#039; group by remote_addr having acc_counts&gt;1) a\r\njoin \r\ndw_access_info b\r\non a.remote_addr = b.remote_addr;\r\n\r\n\r\n\r\n--单次访客访问信息表 &quot;dw_access_info_dcip&quot;\r\ndrop table dw_access_info_dcip;\r\ncreate table dw_access_info_dcip(remote_addr string, firt_req_time string, last_req_time string, stay_long string,acc_counts string)\r\npartitioned by(prtflag_day string);\r\n\r\ninsert into table dw_access_dcip partition(prtflag_day=&#039;18/Sep/2013&#039;)\r\nselect b.remote_addr,b.firt_req_time,b.last_req_time,b.stay_long,a.acc_counts from \r\n(select remote_addr,count(remote_addr) as acc_counts from dw_access_info where prtflag_day=&#039;18/Sep/2013&#039; group by remote_addr having acc_counts&lt;2) a\r\njoin \r\ndw_access_info b\r\non a.remote_addr = b.remote_addr;\r\n\r\n\r\n\r\n\r\n\r\n在回头/单词访客信息表的基础之上，可以实现更多统计指标，如：\r\n\r\n--当日回头客占比\r\ndrop table dw_htpercent_d;\r\ncreate table dw_htpercent_d(day string,ht_percent float);\r\n\r\nInsert into table dw_htpercent_d\r\nselect &#039;18/Sep/2013&#039;,(tmp_ht.ht/tmp_all.amount)*100 from \r\n(select count( distinct a.remote_addr) as ht from dw_access_info_htip a where prtflag_day=&#039;18/Sep/2013&#039;) tmp_ht \r\nJoin \r\n(select count(distinct b.remote_addr) as amount from dw_access_info b where prtflag_day=&#039;18/Sep/2013&#039;) tmp_all;\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n5.4 人均访问频度\r\n--总访问次数/去重总人数，从访客次数汇总表中查询\r\nselect avg(user_times.counts) as user_access_freq\r\nfrom\r\n(select remote_addr,counts from t_display_htip \r\nunion all\r\nselect remote_addr,counts from t_display_access_dcip) user_times;\r\n\r\n--或直接从 访问信息表 t_display_access_info 中查询\r\nselect avg(a.acc_cts) from \r\n(select remote_addr,count(*) as acc_cts from dw_access_info group by remote_addr) a; \r\n',14,0,0,1515161660,0,0,0),(392,1,'关键路径转化率分析——漏斗模型','','','转化：在一条指定的业务流程中，各个步骤的完成人数及相对上一个步骤的百分比\r\n6.1 需求分析\r\n\r\n\r\n\r\n6.2 模型设计\r\n定义好业务流程中的页面标识，下例中的步骤为：\r\nStep1、  /item%\r\nStep2、  /category\r\nStep3、  /order\r\nStep4、  /index\r\n\r\n6.3 开发实现\r\n分步骤开发：\r\n\r\n    1、 查询每一个步骤的总访问人数\r\ncreate table route_numbs as \r\nselect &#039;step1&#039; as step,count(distinct remote_addr)  as numbs from ods_click_pageviews where request like &#039;/item%&#039;\r\nunion\r\nselect &#039;step2&#039; as step,count(distinct remote_addr) as numbs from ods_click_pageviews where request like &#039;/category%&#039;\r\nunion\r\nselect &#039;step3&#039; as step,count(distinct remote_addr) as numbs from ods_click_pageviews where request like &#039;/order%&#039;\r\nunion\r\nselect &#039;step4&#039; as step,count(distinct remote_addr)  as numbs from ods_click_pageviews where request like &#039;/index%&#039;;\r\n\r\n\r\n    2、 查询每一步骤相对于路径起点人数的比例\r\n\r\n思路：利用join\r\nselect rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs  from route_num rn\r\ninner join \r\nroute_num rr\r\n\r\n\r\n\r\n\r\n\r\nselect tmp.rnstep,tmp.rnnumbs/tmp.rrnumbs as ratio\r\nfrom\r\n(\r\nselect rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs  from route_num rn\r\ninner join \r\nroute_num rr) tmp\r\nwhere tmp.rrstep=&#039;step1&#039;;\r\n\r\n\r\n    3、 查询每一步骤相对于上一步骤的漏出率\r\nselect tmp.rrstep as rrstep,tmp.rrnumbs/tmp.rnnumbs as ration\r\nfrom\r\n(\r\nselect rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs  from route_num rn\r\ninner join \r\nroute_num rr) tmp\r\nwhere cast(substr(tmp.rnstep,5,1) as int)=cast(substr(tmp.rrstep,5,1) as int)-1\r\n\r\n\r\n\r\n    4、 汇总以上两种指标\r\nselect abs.step,abs.numbs,abs.ratio as abs_ratio,rel.ratio as rel_ratio\r\nfrom \r\n(\r\nselect tmp.rnstep as step,tmp.rnnumbs as numbs,tmp.rnnumbs/tmp.rrnumbs as ratio\r\nfrom\r\n(\r\nselect rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs  from route_num rn\r\ninner join \r\nroute_num rr) tmp\r\nwhere tmp.rrstep=&#039;step1&#039;\r\n) abs\r\nleft outer join\r\n(\r\nselect tmp.rrstep as step,tmp.rrnumbs/tmp.rnnumbs as ratio\r\nfrom\r\n(\r\nselect rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs  from route_num rn\r\ninner join \r\nroute_num rr) tmp\r\nwhere cast(substr(tmp.rnstep,5,1) as int)=cast(substr(tmp.rrstep,5,1) as int)-1\r\n) rel\r\non abs.step=rel.step\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n',14,0,0,1515161690,0,0,0),(393,1,'模块开发——结果导出','','','报表统计结果，由sqoop从hive表中导出，示例如下，详见工程代码\r\nsqoop export \\\r\n--connect jdbc:mysql://hdp-node-01:3306/webdb --username root --password root  \\\r\n--table click_stream_visit  \\\r\n--export-dir /user/hive/warehouse/dw_click.db/click_stream_visit/datestr=2013-09-18 \\\r\n--input-fields-terminated-by &#039;\\001&#039;\r\n\r\n9 模块开发——工作流调度 \r\n注：将整个项目的数据处理过程，从数据采集到数据分析，再到结果数据的导出，一系列的任务分割成若干个oozie的工作流，并用coordinator进行协调\r\n\r\n工作流定义示例\r\nOoize配置片段示例，详见项目工程\r\n1、日志预处理mr程序工作流定义\r\n&lt;workflow-app name=&quot;weblogpreprocess&quot; xmlns=&quot;uri:oozie:workflow:0.4&quot;&gt;\r\n&lt;start to=&quot;firstjob&quot;/&gt;\r\n&lt;action name=&quot;firstjob&quot;&gt;\r\n&lt;map-reduce&gt;\r\n&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\r\n&lt;name-node&gt;${nameNode}&lt;/name-node&gt;\r\n&lt;prepare&gt;\r\n&lt;delete path=&quot;${nameNode}/${outpath}&quot;/&gt;\r\n&lt;/prepare&gt;\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapreduce.job.map.class&lt;/name&gt;\r\n&lt;value&gt;cn.itcast.bigdata.hive.mr.WeblogPreProcess$WeblogPreProcessMapper&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n&lt;name&gt;mapreduce.job.output.key.class&lt;/name&gt;\r\n&lt;value&gt;org.apache.hadoop.io.Text&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapreduce.job.output.value.class&lt;/name&gt;\r\n&lt;value&gt;org.apache.hadoop.io.NullWritable&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n&lt;name&gt;mapreduce.input.fileinputformat.inputdir&lt;/name&gt;\r\n&lt;value&gt;${inpath}&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapreduce.output.fileoutputformat.outputdir&lt;/name&gt;\r\n&lt;value&gt;${outpath}&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapred.mapper.new-api&lt;/name&gt;\r\n&lt;value&gt;true&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapred.reducer.new-api&lt;/name&gt;\r\n&lt;value&gt;true&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;/configuration&gt;\r\n&lt;/map-reduce&gt;\r\n&lt;ok to=&quot;end&quot;/&gt;\r\n&lt;error to=&quot;kill&quot;/&gt;\r\n\r\n2、数据加载etl工作流定义：\r\n&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.5&quot; name=&quot;hive2-wf&quot;&gt;\r\n&lt;start to=&quot;hive2-node&quot;/&gt;\r\n\r\n&lt;action name=&quot;hive2-node&quot;&gt;\r\n&lt;hive2 xmlns=&quot;uri:oozie:hive2-action:0.1&quot;&gt;\r\n&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\r\n&lt;name-node&gt;${nameNode}&lt;/name-node&gt;\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapred.job.queue.name&lt;/name&gt;\r\n&lt;value&gt;${queueName}&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n&lt;jdbc-url&gt;jdbc:hive2://hdp-node-01:10000&lt;/jdbc-url&gt;\r\n&lt;script&gt;script.q&lt;/script&gt;\r\n&lt;param&gt;input=/weblog/outpre2&lt;/param&gt;\r\n&lt;/hive2&gt;\r\n&lt;ok to=&quot;end&quot;/&gt;\r\n&lt;error to=&quot;fail&quot;/&gt;\r\n&lt;/action&gt;\r\n\r\n&lt;kill name=&quot;fail&quot;&gt;\r\n&lt;message&gt;Hive2 (Beeline) action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\r\n&lt;/kill&gt;\r\n&lt;end name=&quot;end&quot;/&gt;\r\n&lt;/workflow-app&gt;\r\n',14,0,0,1515161715,0,0,0),(394,1,'数据加载工作流所用hive脚本','','','create database if not exists dw_weblog;\r\nuse dw_weblog;\r\ndrop table if exists t_orgin_weblog;\r\ncreate table t_orgin_weblog(valid string,remote_addr string,\r\nremote_user string,\r\ntime_local string,\r\nrequest string,\r\nstatus string,\r\nbody_bytes_sent string,\r\nhttp_referer string,\r\nhttp_user_agent string)\r\nrow format delimited\r\nfields terminated by &#039;\\001&#039;;\r\nload data inpath &#039;/weblog/preout&#039; overwrite into table t_orgin_weblog;\r\n\r\ndrop table if exists t_ods_detail_tmp_referurl;\r\ncreate table t_ods_detail_tmp_referurl as\r\nSELECT a.*,b.*\r\nFROM t_orgin_weblog a \r\nLATERAL VIEW parse_url_tuple(regexp_replace(http_referer, &quot;\\&quot;&quot;, &quot;&quot;), &#039;HOST&#039;, &#039;PATH&#039;,&#039;QUERY&#039;, &#039;QUERY:id&#039;) b as host, path, query, query_id;\r\n\r\ndrop table if exists t_ods_detail;\r\ncreate table t_ods_detail as \r\nselect b.*,substring(time_local,0,11) as daystr,\r\nsubstring(time_local,13) as tmstr,\r\nsubstring(time_local,4,3) as month,\r\nsubstring(time_local,0,2) as day,\r\nsubstring(time_local,13,2) as hour\r\nfrom t_ods_detail_tmp_referurl b;\r\n\r\ndrop table t_ods_detail_prt;\r\ncreate table t_ods_detail_prt(\r\nvalid                  string,\r\nremote_addr            string,\r\nremote_user            string,\r\ntime_local             string,\r\nrequest                string,\r\nstatus                 string,\r\nbody_bytes_sent        string,\r\n\r\nhttp_referer           string,\r\nhttp_user_agent        string,\r\nhost                   string,\r\npath                   string,\r\nquery                  string,\r\nquery_id               string,\r\ndaystr                 string,\r\ntmstr                  string,\r\nmonth                  string,\r\nday                    string,\r\nhour                   string) \r\npartitioned by (mm string,dd string);\r\n\r\n\r\ninsert into table t_ods_detail_prt partition(mm=&#039;Sep&#039;,dd=&#039;18&#039;)\r\nselect * from t_ods_detail where daystr=&#039;18/Sep/2013&#039;;\r\ninsert into table t_ods_detail_prt partition(mm=&#039;Sep&#039;,dd=&#039;19&#039;)\r\nselect * from t_ods_detail where daystr=&#039;19/Sep/2013&#039;;\r\n更多工作流及hql脚本定义详见项目工程',14,0,0,1515161760,0,0,0),(395,1,'工作流定义配置上传','','','[hadoop@hdp-node-01 wf-oozie]$ hadoop fs -put hive2-etl /user/hadoop/oozie/myapps/\r\n[hadoop@hdp-node-01 wf-oozie]$ hadoop fs -put hive2-dw /user/hadoop/oozie/myapps/ \r\n[hadoop@hdp-node-01 wf-oozie]$ ll\r\ntotal 12\r\ndrwxrwxr-x. 2 hadoop hadoop 4096 Nov 23 16:32 hive2-dw\r\ndrwxrwxr-x. 2 hadoop hadoop 4096 Nov 23 16:32 hive2-etl\r\ndrwxrwxr-x. 3 hadoop hadoop 4096 Nov 23 11:24 weblog\r\n[hadoop@hdp-node-01 wf-oozie]$ export OOZIE_URL=http://localhost:11000/oozie\r\n\r\n工作流单元提交启动\r\noozie job -D inpath=/weblog/input -D outpath=/weblog/outpre -config weblog/job.properties  -run\r\n\r\n启动etl的hive工作流\r\noozie job -config hive2-etl/job.properties  -run\r\n启动pvs统计的hive工作流\r\noozie job -config hive2-dw/job.properties  -run\r\n\r\n\r\n3、工作流coordinator配置（片段）\r\n多个工作流job用coordinator组织协调：\r\n[hadoop@hdp-node-01 hive2-etl]$ ll\r\ntotal 28\r\n-rw-rw-r--. 1 hadoop hadoop  265 Nov 13 16:39 config-default.xml\r\n-rw-rw-r--. 1 hadoop hadoop  512 Nov 26 16:43 coordinator.xml\r\n-rw-rw-r--. 1 hadoop hadoop  382 Nov 26 16:49 job.properties\r\ndrwxrwxr-x. 2 hadoop hadoop 4096 Nov 27 11:26 lib\r\n-rw-rw-r--. 1 hadoop hadoop 1910 Nov 23 17:49 script.q\r\n-rw-rw-r--. 1 hadoop hadoop  687 Nov 23 16:32 workflow.xml\r\n\r\n\r\n\r\n    • config-default.xml\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;jobTracker&lt;/name&gt;\r\n&lt;value&gt;hdp-node-01:8032&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;nameNode&lt;/name&gt;\r\n&lt;value&gt;hdfs://hdp-node-01:9000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;queueName&lt;/name&gt;\r\n&lt;value&gt;default&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n\r\n\r\n    • job.properties\r\nuser.name=hadoop\r\noozie.use.system.libpath=true\r\noozie.libpath=hdfs://hdp-node-01:9000/user/hadoop/share/lib\r\noozie.wf.application.path=hdfs://hdp-node-01:9000/user/hadoop/oozie/myapps/hive2-etl/\r\n    • workflow.xml\r\n&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.5&quot; name=&quot;hive2-wf&quot;&gt;\r\n&lt;start to=&quot;hive2-node&quot;/&gt;\r\n\r\n&lt;action name=&quot;hive2-node&quot;&gt;\r\n&lt;hive2 xmlns=&quot;uri:oozie:hive2-action:0.1&quot;&gt;\r\n&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\r\n&lt;name-node&gt;${nameNode}&lt;/name-node&gt;\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;mapred.job.queue.name&lt;/name&gt;\r\n&lt;value&gt;${queueName}&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n&lt;jdbc-url&gt;jdbc:hive2://hdp-node-01:10000&lt;/jdbc-url&gt;\r\n&lt;script&gt;script.q&lt;/script&gt;\r\n&lt;param&gt;input=/weblog/outpre2&lt;/param&gt;\r\n&lt;/hive2&gt;\r\n&lt;ok to=&quot;end&quot;/&gt;\r\n&lt;error to=&quot;fail&quot;/&gt;\r\n&lt;/action&gt;\r\n\r\n&lt;kill name=&quot;fail&quot;&gt;\r\n&lt;message&gt;Hive2 (Beeline) action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\r\n&lt;/kill&gt;\r\n&lt;end name=&quot;end&quot;/&gt;\r\n&lt;/workflow-app&gt;\r\n\r\n    • coordinator.xml\r\n&lt;coordinator-app name=&quot;cron-coord&quot; frequency=&quot;${coord:minutes(5)}&quot; start=&quot;${start}&quot; end=&quot;${end}&quot; timezone=&quot;Asia/Shanghai&quot; xmlns=&quot;uri:oozie:coordinator:0.2&quot;&gt;\r\n&lt;action&gt;\r\n&lt;workflow&gt;\r\n&lt;app-path&gt;${workflowAppUri}&lt;/app-path&gt;\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;jobTracker&lt;/name&gt;\r\n&lt;value&gt;${jobTracker}&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;nameNode&lt;/name&gt;\r\n&lt;value&gt;${nameNode}&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;queueName&lt;/name&gt;\r\n&lt;value&gt;${queueName}&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n&lt;/workflow&gt;\r\n&lt;/action&gt;\r\n&lt;/coordinator-app&gt;\r\n',14,0,0,1515161805,0,0,0),(396,1,'模块开发——数据展示','','','在企业的数据分析系统中，前端展现工具有很多，\r\n    • 独立部署专门系统的方式：以Business Objects(BO,Crystal Report),Heperion(Brio),Cognos等国外产品为代表的，它们的服务器是单独部署的，与应用程序之间通过某种协议沟通信息\r\n\r\n    • 有WEB程序展现方式：通过独立的或者嵌入式的java web系统来读取报表统计结果，以网页的形式对结果进行展现，如，100%纯Java的润乾报表\r\n\r\n本日志分析项目采用自己开发web程序展现的方式\r\n    • Web展现程序采用的技术框架：\r\nJquery + Echarts + springmvc + spring + mybatis + mysql\r\n    • 展现的流程：\r\n    1. 使用ssh从mysql中读取要展现的数据\r\n    2. 使用json格式将读取到的数据返回给页面\r\n    3. 在页面上用echarts对json解析并形成图标\r\n\r\nWeb程序工程结构\r\n采用maven管理工程，引入SSH框架依赖及jquery+echarts的js库\r\n\r\nWeb程序的实现代码\r\n采用典型的MVC架构实现\r\n页面\r\nHTML + JQUERY + ECHARTS\r\nController\r\nSpringMVC\r\nService\r\nService\r\nDAO\r\nMybatis\r\n数据库\r\nMysql\r\n代码详情见项目工程\r\n\r\n\r\n代码示例：ChartServiceImpl \r\n@Service(&quot;chartService&quot;)\r\npublic class ChartServiceImpl implements IChartService {\r\n	@Autowired\r\n	IEchartsDao iEchartsDao;\r\n\r\n	public EchartsData getChartsData() {\r\n		List&lt;Integer&gt; xAxiesList = iEchartsDao.getXAxiesList(&quot;&quot;);\r\n		List&lt;Integer&gt; pointsDataList = iEchartsDao.getPointsDataList(&quot;&quot;);\r\n		\r\n		\r\n		EchartsData data = new EchartsData();\r\n		ToolBox toolBox = EchartsOptionUtil.getToolBox();\r\n		Serie serie = EchartsOptionUtil.getSerie(pointsDataList);\r\n		ArrayList&lt;Serie&gt; series = new ArrayList&lt;Serie&gt;();\r\n		series.add(serie);\r\n		\r\n		\r\n		List&lt;XAxi&gt; xAxis = EchartsOptionUtil.getXAxis(xAxiesList);\r\n		List&lt;YAxi&gt; yAxis = EchartsOptionUtil.getYAxis();\r\n		\r\n		HashMap&lt;String, String&gt; title = new HashMap&lt;String, String&gt;();\r\n		title.put(&quot;text&quot;, &quot;pvs&quot;);\r\n		title.put(&quot;subtext&quot;, &quot;超级pvs&quot;);\r\n		HashMap&lt;String, String&gt; tooltip = new HashMap&lt;String, String&gt;();\r\n		tooltip.put(&quot;trigger&quot;, &quot;axis&quot;);\r\n		\r\n		HashMap&lt;String, String[]&gt; legend = new HashMap&lt;String, String[]&gt;();\r\n		legend.put(&quot;data&quot;, new String[]{&quot;pv统计&quot;});\r\n		\r\n		\r\n		data.setTitle(title);\r\n		data.setTooltip(tooltip);\r\n		data.setLegend(legend);\r\n		data.setToolbox(toolBox);\r\n		data.setCalculable(true);\r\n		data.setxAxis(xAxis);\r\n		data.setyAxis(yAxis);\r\n		data.setSeries(series);\r\n		return data;\r\n	}\r\n	\r\n	public List&lt;HashMap&lt;String, Integer&gt;&gt; getGaiKuangList(String date) throws ParseException{\r\n		\r\n		HashMap&lt;String, Integer&gt; gaiKuangToday = iEchartsDao.getGaiKuang(date);\r\n		SimpleDateFormat sf = new SimpleDateFormat(&quot;MMdd&quot;);\r\n		Date parse = sf.parse(date);\r\n		Calendar calendar = Calendar.getInstance();\r\n		calendar.setTime(parse);\r\n		calendar.add(Calendar.DAY_OF_MONTH, -1);\r\n		Date before = calendar.getTime();\r\n		String beforeString = sf.format(before);\r\n		System.out.println(beforeString);\r\n		\r\n		HashMap&lt;String, Integer&gt; gaiKuangBefore = iEchartsDao.getGaiKuang(beforeString);\r\n		\r\n		ArrayList&lt;HashMap&lt;String, Integer&gt;&gt; gaiKuangList = new ArrayList&lt;HashMap&lt;String, Integer&gt;&gt;();\r\n		gaiKuangList.add(gaiKuangToday);\r\n		gaiKuangList.add(gaiKuangBefore);\r\n		\r\n		return gaiKuangList;\r\n		\r\n	}\r\n	\r\n	public static void main(String[] args) {\r\n		ChartServiceImpl chartServiceImpl = new ChartServiceImpl();\r\n		EchartsData chartsData = chartServiceImpl.getChartsData();\r\n		Gson gson = new Gson();\r\n		String json = gson.toJson(chartsData);\r\n		System.out.println(json);\r\n		\r\n	}\r\n}\r\n\r\n\r\n\r\n\r\nWeb程序的展现效果\r\n网站概况\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n流量分析\r\n\r\n\r\n\r\n\r\n\r\n来源分析\r\n\r\n\r\n\r\n\r\n\r\n访客分析\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n',14,0,0,1515161836,0,0,0),(397,1,'hive分桶表','','','#创建分桶表\r\ncreate table stu_buck(sno int,sname string,sex string,sage int,sdept string)\r\nclustered by(sno) \r\nsorted by(sno DESC)\r\ninto 4 buckets\r\nrow format delimited\r\nfields terminated by &#039;,&#039;;\r\n\r\n#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数\r\nset hive.enforce.bucketing = true;\r\nset mapreduce.job.reduces=4;\r\n\r\n#开会往创建的分通表插入数据(插入数据需要是已分桶, 且排序的)\r\n#可以使用distribute by(sno) sort by(sno asc)   或是排序和分桶的字段相同的时候使用Cluster by(字段)\r\n#注意使用cluster by  就等同于分桶+排序(sort)\r\ninsert into table stu_buck\r\nselect sno,sname,sex,sage,sdept from student distribute by(sno) sort by(sno asc);\r\n',14,0,0,1515162065,0,0,0),(398,1,'nc命令使用','','','功能说明：功能强大的网络工具\r\n语　　法：nc [-hlnruz][-g&lt;网关...&gt;][-G&lt;指向器数目&gt;][-i&lt;延迟秒数&gt;][-o&lt;输出文件&gt;][-p&lt;通信端口&gt;][-s&lt;来源位址&gt;][-v...][-w&lt;超时秒数&gt;][主机名称][通信端口...]\r\n参　　数：\r\n -g&lt;网关&gt;  设置路由器跃程通信网关，最丢哦可设置8个。\r\n -G&lt;指向器数目&gt;  设置来源路由指向器，其数值为4的倍数。\r\n -h  在线帮助。\r\n -i&lt;延迟秒数&gt;  设置时间间隔，以便传送信息及扫描通信端口。\r\n -l  使用监听模式，管控传入的资料。\r\n -n  直接使用IP地址，而不通过域名服务器。\r\n -o&lt;输出文件&gt;  指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存。\r\n -p&lt;通信端口&gt;  设置本地主机使用的通信端口。\r\n -r  乱数指定本地与远端主机的通信端口。\r\n -s&lt;来源位址&gt;  设置本地主机送出数据包的IP地址。\r\n -u  使用UDP传输协议。\r\n-v 详细输出--用两个-v可得到更详细的内容\r\n -w&lt;超时秒数&gt;  设置等待连线的时间。\r\n -z  使用0输入/输出模式，只在扫描通信端口时使用。\r\n \r\nnc使用示例\r\n \r\n1. 端口扫描\r\n# nc -v -w 2 192.168.2.34 -z 21-24\r\nnc: connect to 192.168.2.34 port 21 (tcp) failed: Connection refused\r\nConnection to 192.168.2.34 22 port [tcp/ssh] succeeded!\r\nnc: connect to 192.168.2.34 port 23 (tcp) failed: Connection refused\r\nnc: connect to 192.168.2.34 port 24 (tcp) failed: Connection refused\r\n \r\n2. 从192.168.2.33拷贝文件到192.168.2.34\r\n在192.168.2.34上： nc -l 1234 &gt; test.txt\r\n在192.168.2.33上： nc 192.168.2.34 &lt; test.txt\r\n \r\n3. 简单聊天工具\r\n在192.168.2.34上： nc -l 1234\r\n在192.168.2.33上： nc 192.168.2.34 1234\r\n这样，双方就可以相互交流了。使用ctrl+C(或D）退出。\r\n \r\n4. 用nc命令操作memcached\r\n1）存储数据：printf “set key 0 10 6rnresultrn” |nc 192.168.2.34 11211\r\n2）获取数据：printf “get keyrn” |nc 192.168.2.34 11211\r\n3）删除数据：printf “delete keyrn” |nc 192.168.2.34 11211\r\n4）查看状态：printf “statsrn” |nc 192.168.2.34 11211\r\n5）模拟top命令查看状态：watch “echo stats” |nc 192.168.2.34 11211\r\n6）清空缓存：printf “flush_allrn” |nc 192.168.2.34 11211 (小心操作，清空了缓存就没了）\r\n \r\n5. nc -p 1234 -w 5 host.example.com 80\r\n建立从本地1234端口到host.example.com的80端口连接，5秒超时\r\nnc -u host.example.com 53\r\nu为UDP连接\r\n \r\n6. echo -n &quot;GET / HTTP/1.0&quot;r&quot;n&quot;r&quot;n&quot; | nc host.example.com 80\r\n连接到主机并执行\r\n \r\n7. nc -v -z host.example.com 70-80\r\n扫描端口(70到80)，可指定范围。-v输出详细信息。\r\n \r\n \r\n8. 远程拷贝文件\r\n从server1拷贝文件到server2上。需要先在server2上，用nc激活监听，\r\n \r\nserver2上运行：\r\n \r\nroot@hatest2 tmp]# nc -lp 1234 &gt; install.log\r\n \r\nserver1上运行：\r\n \r\n[root@hatest1 ~]# ll install.log\r\n-rw-r–r–  1 root root 39693 12月 20  2007 install.log\r\n[root@hatest1 ~]# nc -w 1 192.168.228.222 1234 &lt; install.log\r\n \r\n9. 克隆硬盘或分区\r\n操作与上面的拷贝是雷同的，只需要由dd获得硬盘或分区的数据，然后传输即可。\r\n克隆硬盘或分区的操作，不应在已经mount的的系统上进行。所以，需要使用安装光盘引导后，进入拯救模式（或使用Knoppix工 具光盘）启动系统后，在server2上进行类似的监听动作：\r\n# nc -l -p 1234 | dd of=/dev/sda\r\n \r\nserver1上执行传输，即可完成从server1克隆sda硬盘到server2的任务：\r\n# dd if=/dev/sda | nc 192.168.228.222 1234\r\n \r\n※ 完成上述工作的前提，是需要落实光盘的拯救模式支持服务器上的网卡，并正确配置IP。\r\n \r\n \r\n \r\n10. 保存Web页面\r\n# while true; do nc -l -p 80 -q 1 &lt; somepage.html; done\r\n \r\n11. 模拟HTTP Headers,获取网页源代码和返回头部信息\r\n \r\n[root@hatest1 ~]# nc 80\r\nGET / HTTP/1.1\r\nHost: ispconfig.org\r\nReferrer: mypage.com\r\nUser-Agent: my-browser\r\n \r\nHTTP/1.1 200 OK\r\nDate: Tue, 16 Dec 2008 07:23:24 GMT\r\nServer: Apache/2.2.6 (Unix) DAV/2 mod_mono/1.2.1 mod_python/3.2.8 Python/2.4.3 mod_perl/2.0.2 Perl/v5.8.8\r\nSet-Cookie: PHPSESSID=bbadorbvie1gn037iih6lrdg50; path=/\r\nExpires: 0\r\nCache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nPragma: no-cache\r\nCache-Control: private, post-check=0, pre-check=0, max-age=0\r\nSet-Cookie: oWn_sid=xRutAY; expires=Tue, 23-Dec-2008 07:23:24 GMT; path=/\r\nVary: Accept-Encoding\r\nTransfer-Encoding: chunked\r\nContent-Type: text/html\r\n[......]\r\n在nc命令后，输入红色部分的内容，然后按两次回车，即可从对方获得HTTP Headers内容。\r\n \r\n \r\n12. 传输目录\r\n从server1拷贝nginx-0.6.34目录内容到server2上。需要先在server2上，用nc激活监听，server2上运行：\r\n引用\r\n \r\n[root@hatest2 tmp]# nc -l 1234 |tar xzvf -\r\nserver1上运行：\r\n引用\r\n \r\n[root@hatest1 ~]# ll -d nginx-0.6.34\r\ndrwxr-xr-x 8 1000 1000 4096 12-23 17:25 nginx-0.6.34\r\n[root@hatest1 ~]# tar czvf – nginx-0.6.34|nc 192.168.228.222 1234\r\n \r\n \r\n13.REMOTE主机绑定SHELL\r\n例子：\r\n \r\n格式：nc -l -p 5354 -t -e c:\\winnt\\system32\\cmd.exe\r\n讲解：绑定REMOTE主机的CMDSHELL在REMOTE主机的TCP5354端口\r\n \r\n \r\n14.REMOTE主机绑定SHELL并反向连接\r\n例子：\r\n \r\n格式：nc -t -e c:\\winnt\\system32\\cmd.exe 192.168.x.x 5354\r\n讲解：绑定REMOTE主机的CMDSHELL并反向连接到192.168.x.x的TCP5354端口\r\n以上为最基本的几种用法（其实NC的用法还有很多，\r\n当配合管道命令&quot;|&quot;与重定向命令&quot;&lt;&quot;、&quot;&gt;&quot;等等命令功能更强大......）。\r\n   ',14,0,0,1515162133,0,0,0),(399,1,'zk的安装(三台主机)','','','安装三台主机(apeng1, apeng2, apeng3)\r\n\r\napeng1 内存:2G 硬盘:20G 网络连接方式:NAT\r\napeng2 内存:2G 硬盘:20G 网络连接方式:NAT\r\napeng3 内存:2G 硬盘:20G 网络连接方式:NAT\r\n\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\nONBOOT=yes\r\n\r\nservice network restart\r\napeng1 192.168.179.135\r\napeng2 192.168.179.136\r\napeng3 192.168.179.137 \r\n\r\n配置三台主机无密码登录\r\nssh-keygen -t dsa -P &#039;&#039; -f ~/.ssh/id_dsa (apeng1,apeng2,apeng3三台主机都执行)\r\n\r\napeng1 vi /etc/hosts #配置主机\r\n192.168.179.135 apeng1\r\n192.168.179.136 apeng2\r\n192.168.179.137 apeng3\r\n\r\ncd /root/.ssh/\r\nscp id_dsa.pub root@apeng3:~\r\nscp id_dsa.pub root@apeng3:~\r\n\r\napeng2\r\ncat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys\r\n\r\napeng3\r\ncat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys\r\n\r\napeng1\r\nssh apeng2\r\nssh apeng3\r\n\r\n安装zookeeper集群 (检测是否安装JDK)\r\n[root@apeng1 jdk1.7.0_45]# scp -r /usr/local/jdk1.7.0_45/ root@apeng2:/usr/local/jdk1.7.0_45\r\n[root@apeng1 jdk1.7.0_45]# scp -r /usr/local/jdk1.7.0_45/ root@apeng3:/usr/local/jdk1.7.0_45\r\n\r\n[root@apeng1 jdk1.7.0_45]# scp -r /etc/profile root@apeng2:/etc/\r\nprofile                                                                                                100% 1902     1.9KB/s   00:00    \r\n[root@apeng1 jdk1.7.0_45]# scp -r /etc/profile root@apeng3:/etc/\r\n\r\n[root@apeng1 app]# scp -r /opt/app/zookeeper/ root@apeng2:/opt/app/\r\n[root@apeng1 app]# scp -r /opt/app/zookeeper/ root@apeng3:/opt/app/\r\n\r\n[root@apeng1 app]# vi /etc/profile\r\nexport ZOOKEEPER_HOME=/opt/app/zookeeper\r\nexport PATH=$PATH:$ZOOKEEPER_HOME/bin\r\n\r\n[root@apeng1 app]# scp -r /etc/profile root@apeng2:/etc/\r\nprofile                                                                                                100% 1987     1.9KB/s   00:00    \r\n[root@apeng1 app]# scp -r /etc/profile root@apeng3:/etc/\r\nprofile         \r\n\r\n',11,0,0,1515221749,0,0,0),(400,1,'zk服务停止','','','启动服务 zkServer.sh start\r\n停止服务 zkServer.sh stop\r\n\r\n[root@apeng1 app]# vi zk_stop.sh\r\n[root@apeng1 app]# cat zk_stop.sh\r\n#!/bin/bash\r\n\r\necho &quot;停止 zookeeper 服务 ...&quot;\r\n\r\nfor i in 1 2 3\r\ndo\r\nssh apeng$i &quot;source /etc/profile;/opt/app/zookeeper/bin/zkServer.sh stop&quot;\r\ndone\r\n\r\n[root@apeng1 app]# chmod 755 zk_stop.sh\r\n[root@apeng1 app]# ./zk_stop.sh\r\n停止 zookeeper 服务 ...\r\nroot@apeng1&#039;s password: \r\nJMX enabled by default\r\nUsing config: /opt/app/zookeeper/bin/../conf/zoo.cfg\r\nStopping zookeeper ... STOPPED\r\nJMX enabled by default\r\nUsing config: /opt/app/zookeeper/bin/../conf/zoo.cfg\r\nStopping zookeeper ... STOPPED\r\nJMX enabled by default\r\nUsing config: /opt/app/zookeeper/bin/../conf/zoo.cfg\r\nStopping zookeeper ... STOPPED\r\n',11,0,0,1515236935,0,0,0),(401,1,'sshpass的使用','','','sshpass的安装\r\n\r\nhttps://sourceforge.net/projects/sshpass/ 下载\r\n\r\n安装包上传至 apeng1(192.168.179.135)\r\ntar -zxvf sshpass-1.06.tar.gz -C /opt/app\r\n\r\ncd /opt/app/sshpass-1.06\r\n\r\n./configure --prefix=/opt/sshpass\r\n\r\nmake\r\n\r\nmake install\r\n\r\nln -s /opt/app/sshpass-1.06/sshpass /usr/bin/sshpass\r\n\r\nsshpass\r\n\r\nzk停止服务脚本\r\n#!/bin/bash\r\n\r\necho &quot;停止 zookeeper 服务 ...&quot;\r\n\r\nfor i in 1 2 3\r\ndo\r\nsshpass -p &#039;123456&#039; ssh apeng$i &quot;source /etc/profile;/opt/app/zookeeper/bin/zkServer.sh stop&quot;\r\ndone\r\n\r\nzk启动服务脚本\r\n#!/bin/bash\r\n\r\necho &quot;启动 zookeeper 服务 ...&quot;\r\n\r\nfor i in 1 2 3\r\ndo\r\nsshpass -p &#039;123456&#039; ssh apeng$i &quot;source /etc/profile;/opt/app/zookeeper/bin/zkServer.sh start&quot;\r\ndone',9,0,0,1515297711,0,0,0),(402,1,'hadoop高可用集群(四节点)','','','四台安装linux虚拟机(并初始化)\r\napeng 服务名\r\napeng1 192.168.179.135\r\napeng2 192.168.179.136\r\napeng3 192.168.179.137\r\napeng4 192.168.179.138\r\n\r\n四台都配置无密码登录\r\n\r\n确定四台都安装了java\r\n\r\n       NN  DN  ZK  ZKFC  JN  RM   DM\r\npeng1  1       1   1         1    \r\npeng2  1   1   1   1     1        1\r\npeng3      1   1         1        1\r\npeng4      1             1        1\r\n\r\nNN namenode\r\nDN datanode\r\nZK zookeeper\r\nZKFC zookeeper failover controller\r\nJN journalnode\r\nRM resource manager\r\nDM node manager\r\n\r\n上传hadoop包至 /opt/download 目录 (apeng1 192.168.179.135)\r\ncd /opt/download\r\ntar -zxvf hadoop-2.... -C /opt/app/\r\ncd /opt/app/hadoop-2.6.0\r\n\r\n修改配置文件\r\ncd /opt/app/hadoop-2.6.4/etc/hadoop\r\n1.[root@peng1 hadoop]# vi hadoop-env.sh\r\nexport JAVA_HOME=/usr/local/java/jdk1.8.0_73\r\n\r\n2.core-site.xml\r\n&lt;configuration&gt;\r\n&lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;fs.defaultFS&lt;/name&gt;\r\n&lt;!-- hostname:9000 --&gt;\r\n&lt;value&gt;hdfs://apeng1:9000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;\r\n&lt;property&gt;\r\n&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\r\n&lt;value&gt;/opt/app/hadoop-2.6.4/tmp&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;\r\n    &lt;value&gt;apeng1:2181,apeng2:2181,apeng3:2181&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n\r\n3.hdfs-site.xml\r\n&lt;configuration&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.nameservices&lt;/name&gt;\r\n    &lt;value&gt;apeng&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.ha.namenodes.apeng&lt;/name&gt;\r\n    &lt;value&gt;nn1,nn2&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.namenode.rpc-address.apeng.nn1&lt;/name&gt;\r\n    &lt;value&gt;apeng1:8020&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.namenode.rpc-address.apeng.nn2&lt;/name&gt;\r\n    &lt;value&gt;apeng2:8020&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.namenode.http-address.apeng.nn1&lt;/name&gt;\r\n    &lt;value&gt;apeng1:50070&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.namenode.http-address.apeng.nn2&lt;/name&gt;\r\n    &lt;value&gt;apeng2:50070&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;\r\n    &lt;value&gt;qjournal://apeng2:8485;apeng3:8485;apeng4:8485/apeng&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n  &lt;name&gt;dfs.client.failover.proxy.provider.apeng&lt;/name&gt;\r\n  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\r\n    &lt;value&gt;sshfence&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;\r\n    &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;\r\n    &lt;value&gt;/opt/jn/data&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;\r\n    &lt;value&gt;true&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;/configuration&gt;\r\n\r\n4.mapred-site.xml (mv mapred-site.xml.template mapred-site.xml)\r\nmv mapred-site.xml.template mapred-site.xml\r\nvim mapred-site.xml\r\n&lt;!-- 指定mr运行在yarn上 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;mapreduce.framework.name&lt;/name&gt;\r\n	&lt;value&gt;yarn&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n5.yarn-site.xml\r\n&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\r\n	&lt;value&gt;apeng1&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- reducer获取数据的方式 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\r\n	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n6.将hadoop添加到环境变量\r\nvim /etc/proflie\r\nexport JAVA_HOME=/usr/java/jdk1.7.0_65\r\nexport HADOOP_HOME=/itcast/hadoop-2.4.1\r\nexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\r\n\r\nsource /etc/profile\r\n\r\n7.在apeng2,apeng3,apeng4节点上安装\r\nscp -r /opt/app/hadoop-2.6.4/ root@apeng2:/opt/app/\r\nscp -r /opt/app/hadoop-2.6.4/ root@apeng3:/opt/app/\r\nscp -r /opt/app/hadoop-2.6.4/ root@apeng4:/opt/app/\r\n\r\nscp -r /etc/profile root@apeng2:/etc/\r\nscp -r /etc/profile root@apeng3:/etc/\r\nscp -r /etc/profile root@apeng4:/etc/\r\n\r\n登录apeng2 apeng3 apeng4\r\nsource /etc/profile\r\n\r\n8.启动dfs\r\n登录apeng1\r\nsbin/start-dfs.sh\r\n\r\n9.格式化namenode（是对namenode进行初始化）\r\nbin/hdfs namenode -format (hadoop namenode -format)\r\n\r\n10.再启动YARN\r\nsbin/start-yarn.sh\r\n\r\n[root@apeng1 hadoop-2.6.4]# sbin/start-all.sh \r\nThis script is Deprecated. Instead use start-dfs.sh and start-yarn.sh\r\nStarting namenodes on [apeng1 apeng2]\r\napeng2: starting namenode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-namenode-apeng2.out\r\napeng1: starting namenode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-namenode-apeng1.out\r\napeng2: starting datanode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-datanode-apeng2.out\r\napeng3: starting datanode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-datanode-apeng3.out\r\napeng4: starting datanode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-datanode-apeng4.out\r\nStarting journal nodes [apeng2 apeng3 apeng4]\r\napeng2: journalnode running as process 36240. Stop it first.\r\napeng3: journalnode running as process 36619. Stop it first.\r\napeng4: journalnode running as process 33563. Stop it first.\r\nStarting ZK Failover Controllers on NN hosts [apeng1 apeng2]\r\napeng1: starting zkfc, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-zkfc-apeng1.out\r\napeng2: starting zkfc, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-zkfc-apeng2.out\r\nstarting yarn daemons\r\nstarting resourcemanager, logging to /opt/app/hadoop-2.6.4/logs/yarn-root-resourcemanager-apeng1.out\r\napeng4: starting nodemanager, logging to /opt/app/hadoop-2.6.4/logs/yarn-root-nodemanager-apeng4.out\r\napeng2: starting nodemanager, logging to /opt/app/hadoop-2.6.4/logs/yarn-root-nodemanager-apeng2.out\r\napeng3: starting nodemanager, logging to /opt/app/hadoop-2.6.4/logs/yarn-root-nodemanager-apeng3.out\r\n\r\n11.jps验证 (如果没有NameNode则重新执行一下格式化namenode bin/hdfs namenode -format)\r\n[root@apeng1 hadoop-2.6.4]# jps\r\n42051 SecondaryNameNode\r\n41875 NameNode\r\n42194 ResourceManager\r\n42388 Jps\r\n\r\n[root@apeng2 app]# jps\r\n37922 NodeManager\r\n36240 JournalNode\r\n38049 Jps\r\n37818 DataNode\r\n\r\n[root@apeng3 hadoop-2.6.4]# jps\r\n38108 NodeManager\r\n38004 DataNode\r\n38236 Jps\r\n36619 JournalNode\r\n\r\n[root@apeng4 ~]# jps\r\n34979 NodeManager\r\n34875 DataNode\r\n33563 JournalNode\r\n35107 Jps\r\n\r\nhttp://192.168.179.135:50070 （HDFS管理界面）\r\nhttp://192.168.179/135:8088 （MR管理界面）\r\n\r\n./hadoop-daemon.sh start journalnode (JN) #单独启动\r\n\r\ncd /opt/hadoop2/dfs/current/... 元数据文件\r\n复制元数据到第二个 nodename\r\n\r\n./hadoop-daemon.sh start namenode 单独启动namenode\r\n\r\n./stop-dfs.sh #停止所有dfs 所有的服务\r\n\r\n初始化 zkfc\r\n./hdfs zkfc -formatZK\r\n./start-dfs.sh \r\n',13,0,0,1515303138,0,0,0),(403,1,'hadoop集群(四节点)','','','四台安装linux虚拟机(并初始化)\r\napeng1 192.168.179.135\r\napeng2 192.168.179.136\r\napeng3 192.168.179.135\r\napeng4 192.168.179.135\r\n\r\n四台时间同步(使用date查看当前时间)\r\n\r\n四台都配置无密码登录\r\n\r\n确定四台都安装了java\r\n		\r\n上传hadoop包至 /opt/download 目录 (apeng1 192.168.179.135)\r\ncd /opt/download\r\ntar -zxvf hadoop-2.... -C /opt/app/\r\ncd /opt/app/hadoop-2.6.0\r\n\r\n修改配置文件\r\ncd /opt/app/hadoop-2.6.4/etc/hadoop\r\n1.[root@peng1 hadoop]# vi hadoop-env.sh\r\nexport JAVA_HOME=/usr/local/java/jdk1.8.0_73\r\n		\r\n2.core-site.xml\r\n&lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;fs.defaultFS&lt;/name&gt;\r\n	&lt;value&gt;hdfs://apeng1:9000&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\r\n	&lt;value&gt;/opt/app/hadoop-2.6.4/tmp&lt;/value&gt;\r\n&lt;/property&gt;\r\n		\r\n3.hdfs-site.xml   \r\n&lt;!-- 指定HDFS副本的数量 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.replication&lt;/name&gt;\r\n	&lt;value&gt;1&lt;/value&gt;\r\n&lt;/property&gt;\r\n		\r\n&lt;property&gt;\r\n	&lt;name&gt;dfs.secondary.http.address&lt;/name&gt;\r\n	&lt;value&gt;192.168.179.135:50090&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n4.mapred-site.xml (mv mapred-site.xml.template mapred-site.xml)\r\nmv mapred-site.xml.template mapred-site.xml\r\nvim mapred-site.xml\r\n&lt;!-- 指定mr运行在yarn上 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;mapreduce.framework.name&lt;/name&gt;\r\n	&lt;value&gt;yarn&lt;/value&gt;\r\n&lt;/property&gt;\r\n		\r\n5.yarn-site.xml\r\n&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\r\n	&lt;value&gt;apeng1&lt;/value&gt;\r\n&lt;/property&gt;\r\n&lt;!-- reducer获取数据的方式 --&gt;\r\n&lt;property&gt;\r\n	&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\r\n	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;\r\n&lt;/property&gt;\r\n     	\r\n6.将hadoop添加到环境变量\r\nvim /etc/proflie\r\nexport HADOOP_HOME=/opt/app/hadoop-2.6.0\r\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\r\n\r\nsource /etc/profile\r\n\r\n7.在apeng2,apeng3,apeng4节点上安装\r\nscp -r /opt/app/hadoop-2.6.4/ root@apeng2:/opt/app/\r\nscp -r /opt/app/hadoop-2.6.4/ root@apeng3:/opt/app/\r\nscp -r /opt/app/hadoop-2.6.4/ root@apeng4:/opt/app/\r\n\r\nscp -r /etc/profile root@apeng2:/etc/\r\nscp -r /etc/profile root@apeng3:/etc/\r\nscp -r /etc/profile root@apeng4:/etc/\r\n\r\n登录apeng2 apeng3 apeng4\r\nsource /etc/profile\r\n\r\n8.启动dfs\r\n登录apeng1\r\nsbin/start-dfs.sh\r\n\r\n9.格式化namenode（是对namenode进行初始化）\r\nbin/hdfs namenode -format (hadoop namenode -format)\r\n\r\n10.再启动YARN\r\nsbin/start-yarn.sh\r\n\r\n[root@apeng1 hadoop-2.6.4]# sbin/start-all.sh \r\nThis script is Deprecated. Instead use start-dfs.sh and start-yarn.sh\r\nStarting namenodes on [apeng1]\r\napeng1: starting namenode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-namenode-apeng1.out\r\napeng2: starting datanode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-datanode-apeng2.out\r\napeng3: starting datanode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-datanode-apeng3.out\r\napeng4: starting datanode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-datanode-apeng4.out\r\nStarting secondary namenodes [apeng1]\r\napeng1: starting secondarynamenode, logging to /opt/app/hadoop-2.6.4/logs/hadoop-root-secondarynamenode-apeng1.out\r\nstarting yarn daemons\r\nstarting resourcemanager, logging to /opt/app/hadoop-2.6.4/logs/yarn-root-resourcemanager-apeng1.out\r\napeng2: starting nodemanager, logging to /opt/app/hadoop-2.6.4/logs/yarn-root-nodemanager-apeng2.out\r\napeng3: starting nodemanager, logging to /opt/app/hadoop-2.6.4/logs/yarn-root-nodemanager-apeng3.out\r\napeng4: starting nodemanager, logging to /opt/app/hadoop-2.6.4/logs/yarn-root-nodemanager-apeng4.out\r\n\r\n11.jps验证 (如果没有NameNode则重新执行一下格式化namenode bin/hdfs namenode -format)\r\n[root@apeng1 hadoop-2.6.4]# jps\r\n42051 SecondaryNameNode\r\n41875 NameNode\r\n42194 ResourceManager\r\n42388 Jps\r\n\r\n[root@apeng2 app]# jps\r\n37922 NodeManager\r\n36240 JournalNode\r\n38049 Jps\r\n37818 DataNode\r\n\r\n[root@apeng3 hadoop-2.6.4]# jps\r\n38108 NodeManager\r\n38004 DataNode\r\n38236 Jps\r\n36619 JournalNode\r\n\r\n[root@apeng4 ~]# jps\r\n34979 NodeManager\r\n34875 DataNode\r\n33563 JournalNode\r\n35107 Jps\r\n\r\nhttp://192.168.179.135:50070 （HDFS管理界面）\r\nhttp://192.168.179/135:8088 （MR管理界面）',13,0,0,1515303912,0,0,0),(404,1,'java输出详细日志','','','log4j.properties 复制到项目/src目录下\r\n\r\n# Configure logging for testing: optionally with log file\r\n#可以设置级别：debug&gt;info&gt;error\r\n#debug:可以显式debug,info,error\r\n#info:可以显式info,error\r\n#error:可以显式error\r\n\r\nlog4j.rootLogger=debug,appender1\r\n#log4j.rootLogger=info,appender1\r\n#log4j.rootLogger=error,appender1\r\n\r\n#输出到控制台\r\nlog4j.appender.appender1=org.apache.log4j.ConsoleAppender\r\n#样式为TTCCLayout\r\nlog4j.appender.appender1.layout=org.apache.log4j.TTCCLayout',12,0,0,1515357946,0,0,0),(405,1,'hdfs shell (代码实现)','','','项目名 --- 右键 --- Build Path --- All Libraries --- User Library --- user lib... --- New --- Add ...\r\ncommon/hadoop-common-2.6.4.jar 及相关依赖lib\r\nhdfs/hadoop-hdfs-2.6.4.jar 及相关依赖lib\r\n\r\nimport java.net.URI;\r\nimport java.util.Iterator;\r\nimport java.util.Map.Entry;\r\n\r\nimport org.apache.hadoop.conf.Configuration;\r\nimport org.apache.hadoop.fs.FileStatus;\r\nimport org.apache.hadoop.fs.FileSystem;\r\nimport org.apache.hadoop.fs.LocatedFileStatus;\r\nimport org.apache.hadoop.fs.Path;\r\nimport org.apache.hadoop.fs.RemoteIterator;\r\nimport org.junit.Before;\r\nimport org.junit.Test;\r\n/**\r\n * \r\n * 客户端去操作hdfs时，是有一个用户身份的\r\n * 默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=hadoop\r\n * \r\n * 也可以在构造客户端fs对象时，通过参数传递进去\r\n * @author\r\n *\r\n */\r\npublic class HdfsClientDemo {\r\n	FileSystem fs = null;\r\n	Configuration conf = null;\r\n	@Before\r\n	public void init() throws Exception{\r\n		\r\n		conf = new Configuration();\r\n		conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://apeng1:9000&quot;);\r\n// 处理错误HADOOP_HOME or hadoop.home.dir are not set.\r\nSystem.setProperty(&quot;hadoop.home.dir&quot;, &quot;d:/files/hadoop-2.6.4&quot;);\r\n		\r\n		//拿到一个文件系统操作的客户端实例对象\r\n		/*fs = FileSystem.get(conf);*/\r\n		//可以直接传入 uri和用户身份\r\n		fs = FileSystem.get(new URI(&quot;hdfs://apeng:9000&quot;),conf,&quot;hadoop&quot;); //最后一个参数为用户名\r\n	}\r\n\r\n	@Test\r\n	public void testUpload() throws Exception {\r\n		\r\n		Thread.sleep(2000);\r\n		fs.copyFromLocalFile(new Path(&quot;G:/access.log&quot;), new Path(&quot;/access.log.copy&quot;));\r\n		fs.close();\r\n	}\r\n	\r\n	\r\n	@Test\r\n	public void testDownload() throws Exception {\r\n		\r\n		fs.copyToLocalFile(new Path(&quot;/access.log.copy&quot;), new Path(&quot;d:/&quot;));\r\n		fs.close();\r\n	}\r\n	\r\n	@Test\r\n	public void testConf(){\r\n		Iterator&lt;Entry&lt;String, String&gt;&gt; iterator = conf.iterator();\r\n		while (iterator.hasNext()) {\r\n			Entry&lt;String, String&gt; entry = iterator.next();\r\n			System.out.println(entry.getValue() + &quot;--&quot; + entry.getValue());//conf加载的内容\r\n		}\r\n	}\r\n	\r\n	/**\r\n	 * 创建目录\r\n	 */\r\n	@Test\r\n	public void makdirTest() throws Exception {\r\n		boolean mkdirs = fs.mkdirs(new Path(&quot;/aaa/bbb&quot;));\r\n		System.out.println(mkdirs);\r\n	}\r\n	\r\n	/**\r\n	 * 删除\r\n	 */\r\n	@Test\r\n	public void deleteTest() throws Exception{\r\n		boolean delete = fs.delete(new Path(&quot;/aaa&quot;), true);//true， 递归删除\r\n		System.out.println(delete);\r\n	}\r\n	\r\n	@Test\r\n	public void listTest() throws Exception{\r\n		\r\n		FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;));\r\n		for (FileStatus fileStatus : listStatus) {\r\n			System.err.println(fileStatus.getPath()+&quot;=================&quot;+fileStatus.toString());\r\n		}\r\n		//会递归找到所有的文件\r\n		RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true);\r\n		while(listFiles.hasNext()){\r\n			LocatedFileStatus next = listFiles.next();\r\n			String name = next.getPath().getName();\r\n			Path path = next.getPath();\r\n			System.out.println(name + &quot;---&quot; + path.toString());\r\n		}\r\n	}\r\n	\r\n	public static void main(String[] args) throws Exception {\r\n		Configuration conf = new Configuration();\r\n		conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://master:9000&quot;);\r\n		//拿到一个文件系统操作的客户端实例对象\r\n		FileSystem fs = FileSystem.get(conf);\r\n		\r\n		fs.copyFromLocalFile(new Path(&quot;G:/access.log&quot;), new Path(&quot;/access.log.copy&quot;));\r\n		fs.close();\r\n	}\r\n	\r\n}\r\n\r\nwin10 native 没解决\r\n会出现空指针错误',14,0,0,1515358187,0,0,0),(406,1,'hive基本操作','','','bin/hive\r\n首次执行时(fs)会创建tmp目录\r\nhttp://apeng1:50070/explorer.html#/\r\nPermission	Owner	Group	Size	Replication	Block Size	Name\r\ndrwx-wx-wx	root	supergroup	0 B	0	0 B	        tmp\r\n\r\n执行 create database caopeng;时(fs)会创建user目录\r\nhttp://apeng1:50070/explorer.html#/\r\nPermission	Owner	Group	Size	Replication	Block Size	Name\r\ndrwxr-xr-x	root	supergroup	0 B	0	0 B	        user\r\n\r\n/user/hive/warehouse/caopeng.db\r\n\r\nhive&gt; create table test_peng(id int, name string);\r\n/user/hive/warehouse/caopeng.db/test_peng\r\n\r\nhive&gt; create table t_peng1(id int, name string)\r\n    &gt; row format delimited\r\n    &gt; fields terminated by &#039;,&#039;;\r\n\r\n导入数据\r\n[root@apeng1 input]# vi t_peng1.data \r\n[root@apeng1 input]# cat t_peng1.data \r\n1,peng1\r\n2,peng2\r\n3,peng3\r\n4,peng4\r\n5,peng5\r\n[root@apeng1 input]# hadoop fs -put t_peng1.data /user/hive/warehouse/caopeng.db/t_peng1\r\n\r\nhive&gt; select * from t_peng1;\r\nOK\r\n1	peng1\r\n2	peng2\r\n3	peng3\r\n4	peng4\r\n5	peng5\r\n\r\n',14,0,0,1515365391,0,0,0),(407,1,'ntp同步时间(四节点)','','','apeng1 192.168.179.135\r\napeng2 192.168.179.136\r\napeng3 192.168.179.137\r\napeng4 192.168.179.138\r\n\r\n[root@apeng1 config]# ntpdate cn.pool.ntp.org\r\n15 Jan 14:13:13 ntpdate[17579]: step time server 85.199.214.100 offset 629800.794531 sec\r\n[root@peng1 config]# crontab -e\r\n[root@peng1 config]# crontab -l\r\n0 1 * * * /usr/sbin/ntpdate cn.pool.ntp.org\r\n\r\napeng2 apeng3 apeng4 三节点同上操作\r\n',9,0,0,1515996977,0,0,0),(408,1,'hive相关问题','','','select id,name from t_peng1 order by id;\r\n运行时报错\r\n\r\nhttp://192.168.179.135:8088/cluster\r\n单击 ID可以看错具体错误信息\r\n\r\n Unauthorized request to start container.\r\nThis token is expired. current time is 1515766023233 found 1515764591820 （集群节点时间不同步)\r\n\r\ndate 查看时间\r\nntpdate cn.pool.ntp.org （设置当前时间同步)\r\n[root@apeng1 hadoop-2.6.4]# crontab -e\r\n[root@apeng1 hadoop-2.6.4]# crontab -l\r\n0 1 * * * /usr/sbin/ntpdate cn.pool.ntp.org\r\n',14,0,0,1515997194,0,0,0),(409,1,'beeline(hive客户端)','','','bin/hiveserver2\r\ncat /tmp/root/hive.log 查看相关日志\r\n\r\nbin/beeline\r\n\r\n!connect jdbc:hive2://localhost:10000\r\n\r\nsql操作',14,0,0,1516003320,0,0,0),(410,1,'grant命令(mysql)','','','    mysql 的 grant 命令\r\n    本文实例 运行于 mysql 5.0 及以上版本\r\n    mysql 赋予用户权限命令的简单格式可概括为\r\n    grant 权限 on 数据库对象 to 用户\r\n    grant 普通数据用户 查询 插入 更新 删除 数据库中所有表数据的权利\r\n    grant select on testdb.* to common_user@&#039;%&#039;\r\n    grant insert on testdb.* to common_user@&#039;%&#039;\r\n    grant update on testdb.* to common_user@&#039;%&#039;\r\n    grant delete on testdb.* to common_user@&#039;%&#039;\r\n    或者 用一条 mysql 命令来替代\r\n    grant select,insert,update,delete on testdb.* to common_user@&#039;%&#039;\r\n\r\n    grant 数据库开发人员 创建表 索引 视图 存储过程 函数 ... 等权限\r\n    grant 创建 修改 删除 mysql 数据表结构权限\r\n    grant create on testdb.* to developer@&#039;192.168.0.%&#039;\r\n    grant alter on testdb.* to developer@&#039;192.168.0.%&#039;\r\n    grant drop on testdb.* to developer@&#039;192.168.0.%&#039;\r\n\r\n    grant 操作 mysql 外键权限\r\n    grant references on testdb.* to developer@&#039;192.168.0.%&#039;\r\n\r\n    grant 操作 mysql 临时表权限\r\n    grant create temporary tables on testdb.* to developer@&#039;192.168.0.%&#039;\r\n\r\n    grant 操作 mysql 索引权限\r\n    grant index on testdb.* to developer@&#039;192.168.0.%&#039;\r\n\r\n    grant 操作 mysql 视图 查看视图源代码权限\r\n    grant create view on testdb.* to developer@&#039;192.168.0.%&#039;\r\n    grant show view on testdb.* to developer@&#039;192.168.0.%&#039;\r\n\r\n    grant 操作 mysql 存储过程 函数 权限\r\n    grant create routine on testdb.* to developer@&#039;192.168.0.%&#039; -- now, can show procedure status\r\n    grant alter routine on testdb.* to developer@&#039;192.168.0.%&#039; -- now, you can drop a procedure\r\n    grant execute on testdb.* to developer@&#039;192.168.0.%&#039;\r\n\r\n    grant 普通 DBA 管理某个 mysql 数据库的权限\r\n    grant all privileges on testdb.* to developer@&#039;192.168.0.%&#039;\r\n    其中 关键字 privileges 可以省略\r\n\r\n    grant 高级 DBA 管理 mysql 中所有数据库的权限\r\n    grant all on testdb.* to dba@&#039;localhost&#039;\r\n\r\n    mysql grant 权限 分别可以作用在多个层次上\r\n    grant 作用在整个 mysql 服务器上\r\n    grant select on *.* to dba@&#039;localhost&#039; -- dba 可以查询 mysql 中所有数据库中的表\r\n    grant all on *.* to dba@&#039;localhost&#039; -- dba 可以管理 mysql 中的所有数据库\r\n\r\n    grant 作用在单个数据库上\r\n    grant select on testdb.* to dba@localhost -- dba 可以查询 testdb 中的表\r\n\r\n    grant 作用在单个数据表上\r\n    grant select insert update delete on testdb.orders to dba@localhost\r\n\r\n    这里在给一个用户授权多张表时 可以多次执行以上语句 例如\r\n    grant select (user_id, username) on smp.users to mo_user@&#039;%&#039; identified by &quot;12345&quot;\r\n    grant select on smp.mo_sms to mo_user@&#039;%&#039; identified by &#039;12345&#039;\r\n\r\n    grant 作用在表中的列上\r\n    grant select(id,se,rank) on testdb.apache_log to dba@localhost\r\n\r\n    grant all on *.* to root@&#039;%&#039; identified by &#039;123456&#039;',16,0,0,1516003609,0,0,0),(411,1,'flume安装部署','','','1. Flume的安装非常简单，只需要解压即可，当然，前提是已有hadoop环境\r\n上传安装包到数据源所在节点上 (apeng1 192.168.179.135)\r\n然后解压 tar -zxvf apache-flume-1.6.0-bin.tar.gz -C /opt/app\r\ncd /opt/app\r\nmv apache-flume-1.6.0-bin.tar.gz flume-1.6.0\r\n然后进入flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME\r\n\r\n2.根据数据采集的需求配置采集方案，描述在配置文件中(文件名可任意自定义)\r\n\r\n3.指定采集方案配置文件，在相应的节点上启动flume agent\r\n\r\n4.先用一个最简单的例子来测试一下程序环境是否正常\r\n先在flume的conf目录下新建一个文件\r\nvi conf/netcat-logger.conf\r\n# 定义这个agent中各组件的名字\r\na1.sources = r1\r\na1.sinks = k1\r\na1.channels = c1\r\n\r\n# 描述和配置source组件：r1\r\na1.sources.r1.type = netcat\r\na1.sources.r1.bind = localhost\r\na1.sources.r1.port = 44444\r\n\r\n# 描述和配置sink组件：k1\r\na1.sinks.k1.type = logger\r\n\r\n# 描述和配置channel组件，此处使用是内存缓存的方式\r\na1.channels.c1.type = memory\r\na1.channels.c1.capacity = 1000\r\na1.channels.c1.transactionCapacity = 100\r\n\r\n# 描述和配置source channel sink之间的连接关系\r\na1.sources.r1.channels = c1\r\na1.sinks.k1.channel = c1\r\n\r\n5.启动agent去采集数据\r\nbin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1 -Dflume.root.logger=INFO,console\r\n-c conf 指定flume自身的配置文件所在目录\r\n-f conf/netcat-logger.con 指定我们所描述的采集方案\r\n-n a1 指定我们这个agent的名字\r\n\r\n6.测试\r\n先要往agent采集监听的端口上发送数据，让agent有数据可采\r\n随便在一个能跟agent节点联网的机器上\r\ntelnet anget-hostname port （telnet localhost 44444） \r\ntelnet的安装(yum -y install telnet)\r\n\r\n[root@apeng1 input]# telnet localhost 44444\r\nTrying ::1...\r\ntelnet: connect to address ::1: Connection refused\r\nTrying 127.0.0.1...\r\nConnected to localhost.\r\nEscape character is &#039;^]&#039;.\r\ntest flume #向flume发送数据\r\nOK\r\n\r\nflume接收内容\r\n2018-01-15 22:37:44,392 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: { headers:{} body: 74 65 73 74 20 66 6C 75 6D 65 0D                test flume. }\r\n',13,0,0,1516027433,0,0,0),(412,1,'flume采集文件夹','','','监视文件夹\r\n\r\n1.编写配置文件\r\nvi conf/spool-logger.conf\r\n# Name the components on this agent\r\na1.sources = r1\r\na1.sinks = k1\r\na1.channels = c1\r\n\r\n# Describe/configure the source\r\n#监听目录,spoolDir指定目录, fileHeader要不要给文件夹前坠名\r\na1.sources.r1.type = spooldir\r\na1.sources.r1.spoolDir = /home/hadoop/flumespool\r\na1.sources.r1.fileHeader = true\r\n\r\n# Describe the sink\r\na1.sinks.k1.type = logger\r\n\r\n# Use a channel which buffers events in memory\r\na1.channels.c1.type = memory\r\na1.channels.c1.capacity = 1000\r\na1.channels.c1.transactionCapacity = 100\r\n\r\n# Bind the source and sink to the channel\r\na1.sources.r1.channels = c1\r\na1.sinks.k1.channel = c1\r\n\r\n\r\n2.启动命令：  \r\nbin/flume-ng agent -c ./conf -f ./conf/spool-logger.conf -n a1 -Dflume.root.logger=INFO,console\r\n\r\n3.测试： 往/home/hadoop/flumeSpool放文件（mv ././xxxFile /home/hadoop/flumeSpool），但是不要在里面生成文件\r\n',13,0,0,1516028107,0,0,0),(413,1,'mysqldump备份','','','vi dump.sh\r\n\r\n#!/bin/bash\r\n\r\n#备份resource库\r\nmysqldump -uroot -p123456 resource &gt; res.sql\r\n\r\n\r\nvi dump_table.sh\r\n\r\n#!/bin/bash\r\n\r\n#备份单个表\r\nmysqldump -uroot -p123456 resource --tables resource_techn_article &gt; res.sql\r\n',16,0,0,1516028957,0,0,0),(414,1,'hadoop编译(linux)','','','1.下载maven（apache-maven-3.3.3-bin.tar.gz）\r\nhttp://archive.apache.org/dist/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz\r\n2.安装maven\r\ntar -zxvf apache-maven-3.3.3-bin.tar.gz -C /usr/local\r\n3.添加环境变量\r\nvim /etc/profile\r\n\r\nexport JAVA_HOME=/usr/local/jdk1.7.0_45\r\nexport MAVEN_HOME=/usr/local/apache-maven-3.3.3\r\nexport PATH=$PATH:$JAVA_HOME/bin:$MAVEN_HOME/bin\r\n\r\n4.加载环境变量\r\nsource /etc/proflie\r\nmvn -version\r\n\r\n5.下载protobuf（https://code.google.com/p/protobuf/downloads/list 或 https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz）\r\n\r\n6.安装protobuf编译依赖，为了编译安装protoc，需要使用YUM下载几个依赖的工具\r\nyum install -y gcc gcc-c++ make\r\n\r\n7.解压安装protobuf\r\ntar -zxvf  protobuf-2.5.0.tar.gz -C /usr/local/src\r\ncd /usr/local/src/protobuf-2.5.0  \r\n./configure --prefix=/usr/local/protobuf \r\nmake &amp;&amp; make install\r\n\r\n8将protobuf添加到环境变量\r\nvim /etc/profile\r\n\r\nexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$MAVEN_HOME/bin:/usr/local/protobuf/bin\r\n\r\nsource /etc/profile\r\n\r\nprotoc --version\r\n\r\n9.安装其他编译依赖\r\nyum install -y cmake openssl-devel ncurses-devel zlib-devel\r\nyum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop openssl openssl-devel\r\n\r\n10.编译hadoop（进入文件夹里面，里面有一个文件BUILDINT.txt，打开即可看见里面关于编译hadoop的一些环境要求）\r\ntar -zxvf hadoop-2.6.4-src.tar.gz -C /usr/local/src/\r\ncd /usr/local/src/hadoop-2.6.4-src\r\n\r\n\r\nmvn package -Pdist,native -DskipTests -Dtar  \r\n\r\n12.查看编译好的安装包\r\ncd /usr/local/src/hadoop-2.6.4-src/hadoop-dist/target\r\n\r\n\r\n#出现问题\r\nINFO util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n\r\n在/etc/profile中添加\r\nexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib/native&quot;\r\n\r\n#查看可用的本地库\r\nhadoop checknative -a\r\n',13,0,0,1516192893,0,0,0),(415,1,'idea的安装(linux)','','','下载 https://www.jetbrains.com/idea/download/#section=windows\r\n\r\n上传 bash(192.168.179.139) hadoop用户\r\nsysytem/software\r\n\r\ntar -zxvf ideaIC-2017.3.3.tar.gz -C /home/hadoop/system/app/\r\n\r\ncd system/app/cd idea-IC-173.4301.25/\r\n\r\nbin/idea.sh\r\n\r\n\r\ncardula --- next ---\r\n\r\n\r\n安装scala插件 File --- Settings --- scala\r\n\r\n// idea的使用\r\n// 下载idea https://www.jetbrains.com/idea/download/#section=windows 安装idea\r\n// 安装scala 插件  File --- Settings --- Plugins --- install ... --- Scala --- install\r\n// 修改编码 File --- Settings --- File encoding --- UTF-8\r\n// 修改背景色 File --- Settings --- Color Scheam Font --- Darcula --- Apply --- OK\r\n// 修改字体大小 File --- Setttings --- Font --- 修改size\r\n// 编码 创建scala项目 src/main/scala --- new --- Scala Class --- 编码\r\nsbt --- NewProject\r\nName: wel\r\nLocation: ~/IdeaProjects/wel\r\nJdk: 1.7\r\nScala 2.11.8\r\n\r\n// 运行 run\r\nobject wel {\r\n  def main(args:Array[String]): Unit = {\r\n    println(&quot;welcome to scala world&quot;)\r\n  }\r\n}\r\n',14,0,0,1516284352,0,0,0),(416,1,'环境(日志分析)','','','imooc (192.168.179.140)\r\n\r\n环境参数\r\nLinux版本 CentOS(6.4)\r\nHadoop版本 CDH(hadoop-2.6.0-cdh5.7.0)\r\nHive版本 CDH(hive-1.1.0-cdh5.7.0)\r\nScala版本 2.11.8\r\nSpark版本 spark-2.1.0\r\n开发工具 IDEA\r\n\r\nsoftware #所有软件\r\napp #所有应用\r\ndata #所有测试数据\r\nsource 所有源代码\r\nshell 所有脚本\r\n[hadoop@imooc ~]$ mkdir software\r\n[hadoop@imooc ~]$ mkdir app\r\n[hadoop@imooc ~]$ mkdir data\r\n[hadoop@imooc ~]$ mkdir source\r\n[hadoop@imooc ~]$ mkdir shell\r\n\r\n[hadoop@base bin]$ sudo vi /etc/hosts (192.168.179.140)\r\n[hadoop@base bin]$ cat /etc/hosts\r\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\r\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\r\n\r\n192.168.179.140 imooc\r\n\r\n安装jdk\r\n[hadoop@imooc jdk1.7.0_51]$ pwd\r\n/home/hadoop/app/jdk1.7.0_51\r\n[hadoop@imooc jdk1.7.0_51]$ cd\r\n[hadoop@imooc ~]$ vi .bashrc \r\n[hadoop@imooc ~]$ source .bashrc \r\n[hadoop@imooc ~]$ cat .bashrc \r\nexport JAVA_HOME=/home/hadoop/app/jdk1.7.0_51\r\nexport PATH=$JAVA_HOME/bin:$PATH\r\n\r\n[hadoop@imooc ~]$ sudo vi /etc/sysconfig/network\r\n[hadoop@imooc ~]$ cat /etc/sysconfig/network\r\nNETWORKING=yes\r\nHOSTNAME=imooc\r\n\r\n[hadoop@imooc ~]$ sudo vi /etc/hosts\r\n[hadoop@imooc ~]$ cat /etc/hosts\r\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\r\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\r\n\r\n192.168.179.140 imooc\r\n\r\n免密登录\r\nssh-keygen -t rsa\r\n139 --- 登录至 --- 140(imooc)\r\n\r\n139主机上执行\r\n[hadoop@base ~]$ cd .ssh\r\n[hadoop@base .ssh]$ \r\n[hadoop@base .ssh]$ ls\r\nid_rsa  id_rsa.pub\r\n[hadoop@base .ssh]$ scp id_rsa.pub hadoop@imooc:~\r\n\r\n140主机上执行\r\ncat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys\r\n\r\nhadoop-env.sh\r\ncore-site.xml\r\nhdfs-site.xml\r\n\r\n解压到指定目录 -C\r\ncd software\r\ntar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app\r\nHadoop配置文件修改 ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop\r\n[hadoop@hadoop001 ~]$ cd app/hadoop-2.6.0-cdh5.7.0/etc/hadoop\r\n[hadoop@hadoop001 hadoop]$ echo $JAVA_HOME\r\n/home/hadoop/app/jdk1.7.0_51\r\n[hadoop@hadoop001 hadoop]$ vi hadoop-env.sh \r\nexport JAVA_HOME=/home/hadoop/app/jdk1.7.0_51 #修改JAVA_HOME\r\n\r\nhttp://hadoop.apache.org/docs/current/ 查看hadoop当前文档\r\ncore-site.xml\r\n&lt;property&gt;\r\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\r\n	&lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\r\n	&lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\nhdfs-site.xml\r\n&lt;property&gt;\r\n    &lt;name&gt;dfs.replication&lt;/name&gt;\r\n	&lt;value&gt;1&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n格式化HDFS\r\n注意 这一步操作 只是在第一次时执行 每次如果都有格式化的话 那么HDFS上的数据就会被清空\r\nhdfs namenode -format\r\n\r\n启停HDFS\r\nsbin/start-dfs.sh\r\n\r\n验证是否启动成功\r\njps\r\nDataNode\r\nSecondaryNameNode\r\nNameNode\r\n\r\n',17,0,0,1516286818,0,0,0),(417,1,'hadoop配置(日志分析)','','','cp mapred-site.xml.template mapred-site.xml\r\nvi mapred-site.xml\r\n&lt;property&gt;\r\n    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\r\n	&lt;value&gt;yarn&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\nyarn-site.xml\r\n&lt;property&gt;\r\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\r\n	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;\r\n&lt;/property&gt;\r\n启停YARN\r\n\r\n启动yarn sbin/start-yarn.sh\r\n验证是否启动成功 \r\njps\r\nResourceManager\r\nNodeManager\r\n\r\nhttp://192.168.17.128:8088/cluster\r\n\r\n停止yarn sbin/stop-yarn.sh',17,0,0,1516288034,0,0,0),(418,1,'hive安装(日志分析)','','','Hive环境搭建\r\n1.Hive下载 http://archive.cloudera.com/cdh5/cdh/5/\r\nwget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz\r\n\r\n2.解压\r\ntar -zxvf hive-1.1.0-cdh5.7.0.tar.gz -C ~/app/\r\n\r\n3.配置\r\n系统环境变量(~/.bash_profile)\r\nexport HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0\r\nexport PATH=$HIVE_HOME/bin:$PATH\r\n\r\nvi conf/hive-env.sh\r\nexport HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0\r\n\r\nvi hive-site.xml\r\n&lt;property&gt;\r\n    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\r\n	&lt;value&gt;jdbc:mysql://192.168.179.129:3306/sparksql?createDatabaseIfNotExist=true&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\r\n	&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\r\n	&lt;value&gt;peng&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n&lt;property&gt;\r\n    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\r\n	&lt;value&gt;0418peng&lt;/value&gt;\r\n&lt;/property&gt;\r\n\r\n4.复制mysql驱动包到$HIVE_HOME/lib/\r\n[hadoop@imooc lib]$ cp jline-2.12.jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/\r\n\r\n5.启动hive $HIVE_HOME/bin/hive\r\n\r\n问题 Specified key was too long; max key length is 767 bytes\r\nmysql&gt;alter database hive character set latin1;\r\n',17,0,0,1516288737,0,0,0),(419,1,'编译spark','','','前置要求\r\n1.maven 3.3.9 版本  jdk 7版本\r\n2.export MAVEN_OPTS=&quot;-Xmx2g -Xx:ReservedCodeCacheSize=512m&quot;\r\n\r\n下载源码包 (2.\r\nChoose a Spark release:  2.2.1\r\n\r\nChoose a package type:  source code\r\n\r\nDownload Spark: spark-2.2.1.tgz\r\n\r\n[root@apeng1 download]# tar -xvf spark-2.2.1.tgz -C /opt/app/\r\n[root@apeng1 download]# cd /opt/app/spark-2.2.1/\r\n\r\nhttp://spark.apache.org/docs/latest/\r\n点击 More --- Build spark\r\n\r\necho $HADOP_HOME 查看hadoop版本\r\n\r\n./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz  -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0\r\n\r\nt/app/spark-2.1.0/build/mvn help:evaluate -Dexpression=project.version -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0\r\n[root@apeng1 spark-2.1.0]# \r\n[root@apeng1 spark-2.1.0]# ./build/mvn -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0 -DskipTests clean package\r\nexec: curl --progress-bar -L https://downloads.typesafe.com/zinc/0.3.9/zinc-0.3.9.tgz\r\n################################                                          44.8%\r\n',17,0,0,1516292989,0,0,0),(420,1,'SQLContextApp ','','','创建maven项目\r\nFile --- New --- Project --- 勾选create from archetype --- org.scala-tools.archetype:scala-archetype-simple\r\nGroupId com.peng.wel\r\nArtifactid sql\r\n\r\n删除无用 main目录 ... App文件 test/scala目录下所有文件\r\n添加相关依赖 pom.xml\r\n  &lt;properties&gt;\r\n    &lt;scala.version&gt;2.11.0&lt;/scala.version&gt;\r\n    &lt;spark.version&gt;2.1.0&lt;/spark.version&gt;\r\n  &lt;/properties&gt;\r\n  \r\n  &lt;dependencies&gt;\r\n    &lt;!-- scala 依赖 --&gt;\r\n    &lt;dependency&gt;\r\n      &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;\r\n      &lt;artifactId&gt;scala-library&lt;/artifactId&gt;\r\n      &lt;version&gt;${scala.version}&lt;/version&gt;\r\n    &lt;/dependency&gt;\r\n\r\n    &lt;!-- sparkSQL 依赖 --&gt;\r\n    &lt;dependency&gt;\r\n      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\r\n      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;\r\n      &lt;version&gt;${spark.version}&lt;/version&gt;\r\n    &lt;/dependency&gt;\r\n  &lt;/dependencies&gt;\r\n\r\n代码\r\npackage com.peng.spark\r\n\r\nimport org.apache.spark.SparkContext\r\nimport org.apache.spark.sql.SQLContext\r\n\r\nimport org.apache.spark.SparkConf\r\n\r\n/**\r\n * SQLContext的使用:\r\n * 注意：IDEA是在本地，而测试数据是在服务器上 ，能不能在本地进行开发测试的？\r\n *\r\n */\r\nobject SQLContextApp {\r\n\r\n  def main(args: Array[String]) {\r\n\r\n    val path = args(0)\r\n\r\n    //1)创建相应的Context\r\n    val sparkConf = new SparkConf()\r\n\r\n    //在测试或者生产中，AppName和Master我们是通过脚本进行指定\r\n    //sparkConf.setAppName(&quot;SQLContextApp&quot;).setMaster(&quot;local[2]&quot;)\r\n\r\n    val sc = new SparkContext(sparkConf)\r\n    val sqlContext = new SQLContext(sc)\r\n\r\n    //2)相关的处理: json\r\n    val people = sqlContext.read.format(&quot;json&quot;).load(path)\r\n    people.printSchema()\r\n    people.show()\r\n\r\n\r\n\r\n    //3)关闭资源\r\n    sc.stop()\r\n  }\r\n\r\n}',14,0,0,1516371653,0,0,0);
/*!40000 ALTER TABLE `resource_techn_article` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2018-01-19 22:51:43
