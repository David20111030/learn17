日志分析实战项目
用户行为日志概述
离线数据处理架构
项目需求
功能实现
Spark on YARN
性能调优


用户行为日志：用户每次访问网站时所有的行为数据（访问、浏览、搜索、点击...）
	用户行为轨迹、流量日志


日志数据内容：
1）访问的系统属性： 操作系统、浏览器等等
2）访问特征：点击的url、从哪个url跳转过来的(referer)、页面上的停留时间等
3）访问信息：session_id、访问ip(访问城市)等

2013-05-19 13:00:00     http://www.taobao.com/17/?tracker_u=1624169&type=1      B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1        http://hao.360.cn/      1.196.34.243   

用户行为日志分析的意义
网站的眼睛
网站的神经
网站的大脑

数据处理流程
1）数据采集
	Flume： web日志写入到HDFS

2）数据清洗
	脏数据
	Spark、Hive、MapReduce 或者是其他的一些分布式计算框架  
	清洗完之后的数据可以存放在HDFS(Hive/Spark SQL)

3）数据处理
	按照我们的需要进行相应业务的统计和分析
	Spark、Hive、MapReduce 或者是其他的一些分布式计算框架

4）处理结果入库
	结果可以存放到RDBMS、NoSQL

5）数据的可视化
	通过图形化展示的方式展现出来：饼图、柱状图、地图、折线图
	ECharts、HUE、Zeppelin

项目需求
需求一 统计imooc主站最受欢迎的课程/手记的Top N访问次数
需求二 按地市统计imooc主站最受欢迎的Top N课程
根据IP地址提取出城市信息
窗口函数在Spark SQL中的使用
按流量统计imooc主站最受欢迎的TopN课程

功能实现
imooc网主站日志介绍
访问时间
访问过程耗费流量
访问URL
访问IP地址

解压本地日志文件 (access.20161111.log)
上传至测试机 /home/hadoop/data
head -10000 access.20161111.log >> 10000_access.log
一般的日志处理方式，我们是需要进行分区的，
按照日志中的访问时间进行相应的分区，比如：d,h,m5(每5分钟一个分区)


输入：访问时间、访问URL、耗费的流量、访问IP地址信息
输出：URL、cmsType(video/article)、cmsId(编号)、流量、ip、城市信息、访问时间、天


import org.apache.spark.sql.SparkSession

/**
  * 第一步清洗 抽取出我们所需要的指定列的数据
  */
object SparkStatFormatJob {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder().appName("SparkStatFormatJob")
      .master("local[2]").getOrCreate()

    //spark.read.format("json").load("file:///G:/spark_json/peo

    val access = spark.sparkContext.textFile("file:///G:/spark_json/10000_access.log");

    //access.take(10).foreach(println) // 从文件中读出十行

    access.map(line => {
      val splits = line.split(" ");
      val ip = splits(0)

      /**
        * 原如日志的第三个和第四个字段拼接起来就是完整的访问时间
        * [10/Nov/2016:00:01:02 +0800] ==> yyyy-MM-dd HH:mm:ss
        */
      val time = splits(3) + " " + splits(4)
      val url = splits(11).replaceAll("\"","")
      val traffic = splits(9)

      (ip, DataUtils.parse(time), url, traffic)
      // 测试输出内容
//      (183.162.52.7,2016-11-10 00:01:02,-,813)
//      (10.100.0.1,2016-11-10 00:01:02,-,0)
//      (117.35.88.11,2016-11-10 00:01:02,http://www.imooc.com/code/1852,2345)
//      (182.106.215.93,2016-11-10 00:01:02,-,94)
//      (10.100.0.1,2016-11-10 00:01:02,-,0)
//      (183.162.52.7,2016-11-10 00:01:02,-,19501)
//      (10.100.0.1,2016-11-10 00:01:02,-,0)
//      (114.248.161.26,2016-11-10 00:01:02,-,2510)
//      (120.52.94.105,2016-11-10 00:01:02,-,633)
//      (10.100.0.1,2016-11-10 00:01:02,-,0)
      //DataUtils.parse(time) + "\t" + url + "\t" + traffic + "\t" + ip

    }).take(10).foreach(println);
    // 不允许向本地写,会报空指针异常
    //.saveAsTextFile("file:///G:/spark_json/acc_output")//.take(10).foreach(println);

    spark.stop()
  }
}


import java.text.SimpleDateFormat
import java.util.{Date, Locale}

import org.apache.commons.lang3.time.FastDateFormat


/**
  * 日期时间解析工具
  * Alt + Enter补全import
  * 注意 SimpleDateFormat是线程不安全
  */
object DataUtils {

  //输入文件日期时间格式
  //10/Nov/2016:00:01:02 +0800
  //val YYYYMMDDHHMM_TIME_FORMAT = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.ENGLISH)
  val YYYYMMDDHHMM_TIME_FORMAT = FastDateFormat.getInstance("dd/MMM/yyyy:HH:mm:ss Z", Locale.ENGLISH)

  //目标日期格式
  val TARGET_FORMAT = FastDateFormat.getInstance("yyyy-MM-dd HH:mm:ss")

  /**
    * 获取时间 yyyy-MM-dd HH:mm:ss
    */
  def parse(time:String) = {
    TARGET_FORMAT.format(new Date(getTime(time)))
  }

  /**
    * 获取输入日志时间 long类型
    *
    * time: [10/Nov/2016:00:01:02 +0800]
    */
  def getTime(time: String) = {
    try {
      YYYYMMDDHHMM_TIME_FORMAT.parse(time.substring(time.indexOf("[") + 1,
        time.lastIndexOf("]"))).getTime
    } catch {
      case e: Exception => {
        0l
      }
    }
  }

  def main(args: Array[String]): Unit = {
    // 测试 --- 2016-11-10 00:01:02
    println(parse("[10/Nov/2016:00:01:02 +0800]"))
  }

}

数据清洗
使用Spark SQL 解析访问文件
解析出课程编号 类型
根据IP解析出城市信息
使用Spark SQL将访问时间按天进行分区输出

一般的日志处理方式 我们是需要进行分区的
按照日志中的访问时间进行相应的分区 比如 d,h,m5(每5分钟一个分区)

输入 访问时间 访问URL 耗费的流量、访问IP地址信息
输出：URL、cmsType(video/article) cmsId(编号) 流量 ip 城市信息 访问时间 天


使用github上已有的开源项目
1）git clone https://github.com/wzhe06/ipdatabase.git
2）编译下载的项目：mvn clean package -DskipTests
3）安装jar包到自己的maven仓库
mvn install:install-file -Dfile=/Users/rocky/source/ipdatabase/target/ipdatabase-1.0-SNAPSHOT.jar -DgroupId=com.ggstar -DartifactId=ipdatabase -Dversion=1.0 -Dpackaging=jar
添加 (pom.xml)
<dependency>
    <groupId>com.ggstar</groupId>
	<artifactId>ipdatabase</artifactId>
	<version>1.0</version>
</dependency>

java.io.FileNotFoundException: 
file:/Users/rocky/maven_repos/com/ggstar/ipdatabase/1.0/ipdatabase-1.0.jar!/ipRegion.xlsx (No such file or directory)


调优点：
1) 控制文件输出的大小： coalesce
2) 分区字段的数据类型调整：spark.sql.sources.partitionColumnTypeInference.enabled
3) 批量插入数据库数据，提交使用batch操作

按照需求完成统计信息并将统计结果入库
使用DataFrame API完成统计分析
使用SQL API完成统计分析
将统计分析结果写入到MySQL数据库

package com.peng.log

/**
  * IP解析工具类
  */
object IpUtils {

  def getCity(ip:String) = {
    IPHelper.findRegionByIP(ip)
  }

  def main(args: Array[String]): Unit = {
    // 测试前 复制poi依赖至pom.xml 复制资源文件至resources
    println("218.75.35.226")
  }
}


package com.peng.log


import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StringType, StructField, StructType}

/**
  * 访问日志转换(输入===>输出)工具类
  */
object AccessConvertUtil {

  //定义的输出的字段
  val struct = StructType(
    Array(
      StructField("url",StringType),
      StructField("cmsType",StringType),
      StructField("cmsId",StringType),
      StructField("traffic",StringType),
      StructField("ip",StringType),
      StructField("city",StringType),
      StructField("time",StringType),
      StructField("day",StringType)
    )
  )

  /**
    * 根据输入的每一行信息转换成输出的样式
    */
  def parseLog(log:String) = {

    try {
      val splits = log.split("\t")

      val url = splits(1)
      val traffic = splits(2).toLong
      val ip = splits(3)

      val domain = "http://www.imooc.com"
      val cms = url.substring(url.indexOf(domain) + domain.length)
      val cmsTypeId = cms.split("/")

      var cmsType = ""
      var cmsId = 0l
      if(cmsTypeId.length > 1) {

        cmsType = cmsTypeId(0)
        cmsId = cmsTypeId(1).toLong
      }

      val city = IpUtils.getCity(ip)
      val time = splits(0)
      val day = time.substring(0,10).replaceAll("-","")

      //row里面的字段要和struct中的字段对应上
      Row(url, cmsType, cmsId, traffic, ip, city, time, day)
    } catch {
      case e:Exception => Row(0)
    }

  }
}

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._

/**
  * TopN统计Spark作业
  */
object TopNStatJob {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("TopNStatJob")
        .config("spark.sql.sources.partitonColumnTypeInference.enabled", "false")
      .master("local[2]").getOrCreate()

    val accessDF = spark.read.format("parquet").load("")

    accessDF.printSchema()
    accessDF.show(false)

    videoAccessTopNStat(spark, accessDF)

    spark.stop()
  }

  /**
    * 最受欢迎的TopN课程
    * @param accessDF
    */
  def videoAccessTopNStat(spark: SparkSession, accessDF:DataFrame): Unit = {
    /**
      * 使用DataFrame的方式进行统计
      */
    //    val videoAccessTopNDF = accessDF.filter($"day" === "20170511" && $"cmsType" === "video")
//      .groupBy("day", "cmsId").agg(count("cmsId").as("times")).orderBy($"times".desc)
//
//    videoAccessTopNStat.show(false)

    /**
      * 使用SQL的方式进行统计
      */
    accessDF.createOrReplaceTempView("access_logs")
    val videoAccessTopNDF = spark.sql("select day,cmsId,count(1) as times from access_logs " +
      "where day='20170511' and cmsType='video' " +
      "group by day,cmsId order by times desc")

    videoAccessTopNDF.show(false)
  }
}

create table day_video_access_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null,
times bigint(10) not null,
primary key (day, cms_id)
);


create table day_video_city_access_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null,
city varchar(20) not null,
times bigint(10) not null,
times_rank int not null,
primary key (day, cms_id, city)
);

create table day_video_traffics_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null,
traffics bigint(20) not null,
primary key (day, cms_id)
);


数据可视化：一副图片最伟大的价值莫过于它能够使得我们实际看到的比我们期望看到的内容更加丰富

常见的可视化框架
1）echarts
2）highcharts
3）D3.js
4）HUE 
5）Zeppelin

在Spark中，支持4种运行模式：
1）Local：开发时使用
2）Standalone： 是Spark自带的，如果一个集群是Standalone的话，那么就需要在多台机器上同时部署Spark环境
3）YARN：建议大家在生产上使用该模式，统一使用YARN进行整个集群作业(MR、Spark)的资源调度
4）Mesos

不管使用什么模式，Spark应用程序的代码是一模一样的，只需要在提交的时候通过--master参数来指定我们的运行模式即可

Client
	Driver运行在Client端(提交Spark作业的机器)
	Client会和请求到的Container进行通信来完成作业的调度和执行，Client是不能退出的
	日志信息会在控制台输出：便于我们测试

Cluster
	Driver运行在ApplicationMaster中
	Client只要提交完作业之后就可以关掉，因为作业已经在YARN上运行了
	日志是在终端看不到的，因为日志是在Driver上，只能通过yarn logs -applicationIdapplication_id


./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--executor-memory 1G \
--num-executors 1 \
/home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.1.0.jar \
4


此处的yarn就是我们的yarn client模式
如果是yarn cluster模式的话，yarn-cluster


Exception in thread "main" java.lang.Exception: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.

如果想运行在YARN之上，那么就必须要设置HADOOP_CONF_DIR或者是YARN_CONF_DIR

1） export HADOOP_CONF_DIR=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop
2) $SPARK_HOME/conf/spark-env.sh


./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn-cluster \
--executor-memory 1G \
--num-executors 1 \
/home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.1.0.jar \
4


yarn logs -applicationId application_1495632775836_0002



打包时要注意，pom.xml中需要添加如下plugin
<plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
        <archive>
            <manifest>
                <mainClass></mainClass>
            </manifest>
        </archive>
        <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
        </descriptorRefs>
    </configuration>
</plugin>

mvn assembly:assembly



./bin/spark-submit \
--class com.imooc.log.SparkStatCleanJobYARN \
--name SparkStatCleanJobYARN \
--master yarn \
--executor-memory 1G \
--num-executors 1 \
--files /home/hadoop/lib/ipDatabase.csv,/home/hadoop/lib/ipRegion.xlsx \
/home/hadoop/lib/sql-1.0-jar-with-dependencies.jar \
hdfs://hadoop001:8020/imooc/input/* hdfs://hadoop001:8020/imooc/clean

注意：--files在spark中的使用

spark.read.format("parquet").load("/imooc/clean/day=20170511/part-00000-71d465d1-7338-4016-8d1a-729504a9f95e.snappy.parquet").show(false)


./bin/spark-submit \
--class com.imooc.log.TopNStatJobYARN \
--name TopNStatJobYARN \
--master yarn \
--executor-memory 1G \
--num-executors 1 \
/home/hadoop/lib/sql-1.0-jar-with-dependencies.jar \
hdfs://hadoop001:8020/imooc/clean 20170511 

存储格式的选择：http://www.infoq.com/cn/articles/bigdata-store-choose/
压缩格式的选择：https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-compression-analysis/

调整并行度
./bin/spark-submit \
--class com.imooc.log.TopNStatJobYARN \
--name TopNStatJobYARN \
--master yarn \
--executor-memory 1G \
--num-executors 1 \
--conf spark.sql.shuffle.partitions=100 \
/home/hadoop/lib/sql-1.0-jar-with-dependencies.jar \
hdfs://hadoop001:8020/imooc/clean 20170511


val spark = SparkSession.builder().appName("SparkStat
CleanJob").master("local[2]").getOrCreate()
val accessRDD = spark.sparkContext.
textFile("/home/hadoop/data/access.log")
accessRDD.take(10).foreach(println)

val accessDF = spark.createDataFrame(accessRDD.map(
x => AccessConvertUtil.parseLog(x)

accessDF.printSchema()

accessDF.show(false)




<dependency>
    <groupId>org.apache.poi</groupId>
	<artifactId>poi-ooxml</artifactId>
	<version>3.14</version>
</dependency>

<dependency>
    <groupId>org.apache.poi</groupId>
	<artifactId>poi</artifactId>
	<version>3.14</version>
</dependency>

def main(args: Array[String]): Unit = {
    println(parseLog("2017-05-11 14:09:14\thttp://www.imooc.com/video/4500\t304\t218.75.35.226"))
}

MySQLUtils

// MySQL操作工具类

// 每天课程访问次数实体类
case class DayVideoAccessStat(day: String, cmsId: Long, times: Long)

// 各个维度统计的DAO操作
// 批量保存DayVideoAccessStat到数据库
// 设置手动提交
// 执行批量处理
// 手工提交

按照地市进行统计TopN课程
cityAccessTopNStat(spark, accessDF)


可视化
=======================================
数据可视化 一副图片最伟大的价值莫过于它能够使得我们实际看到的比我们期望看到的内容更加丰富

数据可视化展示概述
什么是数据可视化
使用SQL API完成统计分析为什么要可视化
可视化常用框架介绍
echarts
highcharts