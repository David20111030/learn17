写更少的代码

计算平均值
hadoop 代码
private IntWritable one = new IntWritable(1)
private IntWritable output = new IntWritable
proctected void map(LongWritable key, Text value, Context context) {
    String[] fields = value.split("\t")
	output.set(Integer.parseInt(fields[1))
	context.write(one, output)
}

IntWritable one = new IntWritable(1)
DoubleWritable average = new DoubleWritable()

proctected void reduce(IntWritable key, Iterable<IntWritable> values, Context context) {
    int sum = 0
	int count = 0
	for (IntWritable value : values) {
	    sum += value.get()
		count++
	}
	average.set(sum / (double) count)
	context.Write(key, average)
}

spark 代码
data = sc.textFile(...).split("\t")
data.map(lambda x:(x[0], [x.[1], 1])) \
.reduceByKey(lambda x,y:[x[0] + y[0],x[1] + y[1]]) \
.map(lambda x:[x[0], x[1][0] / x[1][1]]) \
.collect()

Using SQL
select name,avg(age) from people group by name

Using DataFrames
sqlCtx.table("people").groupBy("name").agg("name", avg("age")).collect()

写更少的代码(统一访问操作接口)
df = sqlContext.read.format("json").option("samplingRatio","0.1").load("/home/michael/data.json")

df.write.format("parquet").mode("append").partitionBy("year").saveAsTable("fasterData")

events = sqlCtx.load("/data/events","parquet")
training_data = events.where("city='New York' and year=2015").select("timestamp").collect()

joined = users.join(events, users.id == events.uid)
filtered = joined.filter(events.date >= "2015-01-01")

