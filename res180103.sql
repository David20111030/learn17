-- MySQL dump 10.13  Distrib 5.5.56, for Linux (i686)
--
-- Host: localhost    Database: resource
-- ------------------------------------------------------
-- Server version	5.5.56-log

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `resource_assoc`
--

DROP TABLE IF EXISTS `resource_assoc`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_assoc` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(30) NOT NULL DEFAULT '' COMMENT '表名',
  `struct` text COMMENT '表结构',
  `note` varchar(255) NOT NULL DEFAULT '' COMMENT '备注',
  `pid` tinyint(1) unsigned DEFAULT '0' COMMENT '项目标志',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8 COMMENT='逻辑表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_assoc`
--

LOCK TABLES `resource_assoc` WRITE;
/*!40000 ALTER TABLE `resource_assoc` DISABLE KEYS */;
INSERT INTO `resource_assoc` VALUES (6,'','','',0),(7,'逻辑表','ddd\r\ndd','dd',1),(8,'','','',0),(9,'','','',0),(10,'','','',0),(11,'','','',0);
/*!40000 ALTER TABLE `resource_assoc` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_assoc_page`
--

DROP TABLE IF EXISTS `resource_assoc_page`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_assoc_page` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(255) NOT NULL DEFAULT '' COMMENT '页面名',
  `des` text COMMENT '功能描述',
  `techn` text COMMENT '涉及技术',
  `pid` int(10) unsigned DEFAULT NULL COMMENT '所属分类',
  `assoc` varchar(255) DEFAULT NULL COMMENT '涉及表ids',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='逻辑页';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_assoc_page`
--

LOCK TABLES `resource_assoc_page` WRITE;
/*!40000 ALTER TABLE `resource_assoc_page` DISABLE KEYS */;
INSERT INTO `resource_assoc_page` VALUES (1,'留言浏览页 (index.php)','按照留言先后顺序 分页显示留言信息','PHP',9,'2'),(2,'',NULL,NULL,NULL,NULL);
/*!40000 ALTER TABLE `resource_assoc_page` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_auth`
--

DROP TABLE IF EXISTS `resource_auth`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_auth` (
  `id` smallint(6) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(20) NOT NULL DEFAULT '' COMMENT '权限名',
  `pid` smallint(6) NOT NULL DEFAULT '0' COMMENT '父id',
  `controller` varchar(32) NOT NULL DEFAULT '' COMMENT '控制器',
  `action` varchar(32) NOT NULL DEFAULT '' COMMENT '操作方法',
  `path` varchar(32) NOT NULL DEFAULT '' COMMENT '全路径',
  `level` tinyint(4) NOT NULL DEFAULT '0' COMMENT '权限级别',
  `image` varchar(255) NOT NULL DEFAULT '' COMMENT '权限关联图',
  `isShow` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否显示 0 显示 1 不显示',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `mid` int(10) unsigned DEFAULT '0' COMMENT '修改者',
  `updatetime` int(10) unsigned DEFAULT '0' COMMENT '修改时间',
  `uid` int(10) unsigned DEFAULT '0' COMMENT '创建者',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=8 DEFAULT CHARSET=utf8 COMMENT='后台权限表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_auth`
--

LOCK TABLES `resource_auth` WRITE;
/*!40000 ALTER TABLE `resource_auth` DISABLE KEYS */;
INSERT INTO `resource_auth` VALUES (1,'权限管理',0,'','','1',0,'upload/2016-09-17/57dc49e4c5653.png',0,1474034557,1,1474054628,1),(2,'权限列表',1,'Auth','index','1-2',1,'',0,1474034557,0,0,1),(3,'管理员列表',1,'Manager','index','1-3',1,'upload/2016-09-17/57dc490cb5879.png',0,1474054412,0,0,1),(4,'角色列表',1,'Role','index','1-4',1,'',0,1474093955,0,0,1),(5,'资料管理',0,'','','5',0,'upload/2016-09-17/57dd4c5a36b9e.png',0,1474120794,0,0,1),(6,'技术文章分类',5,'Technarticlecate','index','5-6',1,'',0,1474120842,0,0,1),(7,'技术文章',5,'Technarticle','index','5-7',1,'',0,1474120875,0,0,1);
/*!40000 ALTER TABLE `resource_auth` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_bug`
--

DROP TABLE IF EXISTS `resource_bug`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_bug` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `mid` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '报告者id',
  `name` varchar(255) NOT NULL DEFAULT '' COMMENT 'bug 名',
  `content` varchar(255) NOT NULL DEFAULT '' COMMENT 'bug 描述',
  `did` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '修改者id',
  `image` varchar(255) NOT NULL DEFAULT '' COMMENT 'bug图',
  `img` varchar(255) NOT NULL DEFAULT '' COMMENT '修改好的截图',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_bug`
--

LOCK TABLES `resource_bug` WRITE;
/*!40000 ALTER TABLE `resource_bug` DISABLE KEYS */;
INSERT INTO `resource_bug` VALUES (1,0,'权限关联图','在模板文件中 进行非空处理',1,'Uploads/2015-09-20/55fe4f11247d2.png','Uploads/2015-09-20/55fe56df0e996.png');
/*!40000 ALTER TABLE `resource_bug` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_business_logic`
--

DROP TABLE IF EXISTS `resource_business_logic`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_business_logic` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(255) NOT NULL DEFAULT '' COMMENT '逻辑名',
  `image` varchar(50) NOT NULL DEFAULT '' COMMENT '逻辑图',
  `flag` tinyint(1) unsigned NOT NULL DEFAULT '0' COMMENT '导航标志 0 无导航 1 有导航',
  `des` text COMMENT '描述',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_business_logic`
--

LOCK TABLES `resource_business_logic` WRITE;
/*!40000 ALTER TABLE `resource_business_logic` DISABLE KEYS */;
INSERT INTO `resource_business_logic` VALUES (2,'后台首页','Uploads/2015-09-26/560648beb39b1.png',1,'进入后台首页时 立即显示'),(3,'前台个人设置','',1,'个人中心设置页 导航'),(4,'前台个人中心','',1,'前台个人中心 包括会员所有信息的入口');
/*!40000 ALTER TABLE `resource_business_logic` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_categorys`
--

DROP TABLE IF EXISTS `resource_categorys`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_categorys` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `catename` varchar(100) NOT NULL COMMENT '分类名',
  `pid` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '父id',
  `rid` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '所属方向',
  `did` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '所属难度',
  `path` varchar(255) NOT NULL DEFAULT '0,' COMMENT '全路径',
  `image` varchar(255) NOT NULL COMMENT '分类总结图',
  `addtime` int(10) unsigned DEFAULT NULL COMMENT '添加时间',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_categorys`
--

LOCK TABLES `resource_categorys` WRITE;
/*!40000 ALTER TABLE `resource_categorys` DISABLE KEYS */;
INSERT INTO `resource_categorys` VALUES (1,'课程',0,0,0,'0,','',1442640181),(2,'php',1,0,0,'0,1,','',1442640225),(3,'php 基础',2,3,1,'0,1,2,','',1442640256);
/*!40000 ALTER TABLE `resource_categorys` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_develop`
--

DROP TABLE IF EXISTS `resource_develop`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_develop` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(255) NOT NULL DEFAULT '' COMMENT '开发人员名',
  `flag` tinyint(1) unsigned NOT NULL DEFAULT '0' COMMENT '开发人员职位',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_develop`
--

LOCK TABLES `resource_develop` WRITE;
/*!40000 ALTER TABLE `resource_develop` DISABLE KEYS */;
INSERT INTO `resource_develop` VALUES (2,'admin',0),(3,'apeng1',1);
/*!40000 ALTER TABLE `resource_develop` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_gold`
--

DROP TABLE IF EXISTS `resource_gold`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_gold` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `amount` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '总金额',
  `revenue` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '收益',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `uid` int(10) unsigned DEFAULT '0' COMMENT '所属用户id',
  `hold_gold` decimal(15,4) NOT NULL DEFAULT '0.0000' COMMENT '持有黄金',
  `type` tinyint(1) unsigned NOT NULL DEFAULT '3' COMMENT '类型 0 买入 1 卖出 3非转入转出',
  `money` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '买入卖出金额',
  `cost_price` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '成本',
  `current_price` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '现价',
  `note` varchar(255) NOT NULL DEFAULT '' COMMENT '说明',
  `total_revenue` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '总收益',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='黄金收益表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_gold`
--

LOCK TABLES `resource_gold` WRITE;
/*!40000 ALTER TABLE `resource_gold` DISABLE KEYS */;
/*!40000 ALTER TABLE `resource_gold` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_index_fund`
--

DROP TABLE IF EXISTS `resource_index_fund`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_index_fund` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(50) NOT NULL DEFAULT '' COMMENT '指基名',
  `revenue` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '收益',
  `amount` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '总金额',
  `total_revenue` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '总收益',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `uid` int(10) unsigned DEFAULT '0' COMMENT '所属用户id',
  `now_worth` decimal(15,4) NOT NULL DEFAULT '0.0000' COMMENT '最新净值',
  `hold_lot` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '持有份额',
  `type` tinyint(1) unsigned NOT NULL DEFAULT '3' COMMENT '类型 0 买入 1 卖出 3非转入转出',
  `money` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '买入卖出金额',
  `note` varchar(255) NOT NULL DEFAULT '' COMMENT '说明',
  `account_revenue` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '账户总收益',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='指基表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_index_fund`
--

LOCK TABLES `resource_index_fund` WRITE;
/*!40000 ALTER TABLE `resource_index_fund` DISABLE KEYS */;
/*!40000 ALTER TABLE `resource_index_fund` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_log`
--

DROP TABLE IF EXISTS `resource_log`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_log` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `content` varchar(255) NOT NULL DEFAULT '' COMMENT '日志内容',
  `controller` varchar(20) NOT NULL DEFAULT '' COMMENT '操作控制器名',
  `action` varchar(20) NOT NULL DEFAULT '' COMMENT '操作方法名',
  `flag` tinyint(1) NOT NULL DEFAULT '0' COMMENT '前后台标志 0前台 1后台',
  `rectime` int(10) unsigned DEFAULT '0' COMMENT '日志记录时间',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `uid` int(10) unsigned DEFAULT '0' COMMENT '创建者',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='操作日志记录表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_log`
--

LOCK TABLES `resource_log` WRITE;
/*!40000 ALTER TABLE `resource_log` DISABLE KEYS */;
/*!40000 ALTER TABLE `resource_log` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_manager`
--

DROP TABLE IF EXISTS `resource_manager`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_manager` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `username` varchar(30) NOT NULL DEFAULT '' COMMENT '管理名称',
  `password` char(32) NOT NULL DEFAULT '' COMMENT '管理密码',
  `regtime` int(10) unsigned DEFAULT '0' COMMENT '注册时间',
  `logtime` int(10) unsigned DEFAULT '0' COMMENT '登录时间',
  `email` varchar(30) DEFAULT NULL COMMENT '邮箱',
  `rid` tinyint(3) unsigned NOT NULL DEFAULT '0' COMMENT '角色id',
  `display_name` varchar(30) NOT NULL DEFAULT '' COMMENT '显示名',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='管理员表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_manager`
--

LOCK TABLES `resource_manager` WRITE;
/*!40000 ALTER TABLE `resource_manager` DISABLE KEYS */;
INSERT INTO `resource_manager` VALUES (1,'admin','e10adc3949ba59abbe56e057f20f883e',1473254891,0,'caopeng8787@163.com',0,'后台管理员'),(2,'apeng','e10adc3949ba59abbe56e057f20f883e',1473254891,0,'411104493@qq.com',1,'apeng');
/*!40000 ALTER TABLE `resource_manager` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_pay`
--

DROP TABLE IF EXISTS `resource_pay`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_pay` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `amount` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '总金额',
  `revenue` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '收益',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `uid` int(10) unsigned DEFAULT '0' COMMENT '所属用户id',
  `type` tinyint(1) unsigned NOT NULL DEFAULT '3' COMMENT '类型 0 转入 1 转出 3非转入转出',
  `money` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '转入转出金额',
  `note` varchar(255) NOT NULL DEFAULT '' COMMENT '说明',
  `total_revenue` decimal(15,2) NOT NULL DEFAULT '0.00' COMMENT '总收益',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='支付宝收益表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_pay`
--

LOCK TABLES `resource_pay` WRITE;
/*!40000 ALTER TABLE `resource_pay` DISABLE KEYS */;
/*!40000 ALTER TABLE `resource_pay` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_recycle`
--

DROP TABLE IF EXISTS `resource_recycle`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_recycle` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `uid` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '操作者id',
  `image` varchar(255) NOT NULL DEFAULT '' COMMENT '删除的图片',
  `flag` tinyint(1) NOT NULL DEFAULT '0' COMMENT '删除分类 0 图片 1 资源包',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `mid` int(10) unsigned DEFAULT '0' COMMENT '删除者',
  `deltime` int(10) unsigned DEFAULT '0' COMMENT '删除时间',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_recycle`
--

LOCK TABLES `resource_recycle` WRITE;
/*!40000 ALTER TABLE `resource_recycle` DISABLE KEYS */;
/*!40000 ALTER TABLE `resource_recycle` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_role`
--

DROP TABLE IF EXISTS `resource_role`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_role` (
  `id` tinyint(3) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(20) NOT NULL DEFAULT '' COMMENT '角色名',
  `aids` varchar(128) NOT NULL DEFAULT '' COMMENT '权限ids',
  `aac` text COMMENT '控制器-操作方法',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `mid` int(10) unsigned DEFAULT '0' COMMENT '修改者',
  `updatetime` int(10) unsigned DEFAULT '0' COMMENT '修改时间',
  `uid` int(10) unsigned DEFAULT '0' COMMENT '创建者',
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COMMENT='后台角色表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_role`
--

LOCK TABLES `resource_role` WRITE;
/*!40000 ALTER TABLE `resource_role` DISABLE KEYS */;
INSERT INTO `resource_role` VALUES (1,'管理专员','',NULL,1474098936,0,0,1);
/*!40000 ALTER TABLE `resource_role` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_techn_article`
--

DROP TABLE IF EXISTS `resource_techn_article`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_techn_article` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `uid` int(11) NOT NULL DEFAULT '0' COMMENT '用户id',
  `title` varchar(30) NOT NULL DEFAULT '' COMMENT '文章标题',
  `summary` varchar(200) DEFAULT '' COMMENT '文章摘要',
  `img` varchar(255) NOT NULL DEFAULT '' COMMENT '文章图片',
  `content` text NOT NULL COMMENT '文章内容',
  `pid` int(11) NOT NULL DEFAULT '0' COMMENT '文章所属分类id',
  `state` tinyint(1) DEFAULT '0' COMMENT '添加状态 0 后台添加 1 前后添加',
  `views` int(11) DEFAULT '0' COMMENT '浏览量',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `mid` int(10) unsigned DEFAULT '0' COMMENT '修改者',
  `updatetime` int(10) unsigned DEFAULT '0' COMMENT '修改时间',
  `uuid` int(10) unsigned DEFAULT '0' COMMENT '创建者',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=241 DEFAULT CHARSET=utf8 COMMENT='技术文章表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_techn_article`
--

LOCK TABLES `resource_techn_article` WRITE;
/*!40000 ALTER TABLE `resource_techn_article` DISABLE KEYS */;
INSERT INTO `resource_techn_article` VALUES (1,1,'什么是搜索','','','百度：我们比如说想找寻任何的信息的时候，就会上百度去搜索一下，比如说找一部自己\r\n喜欢的电影，或者说找一本喜欢的书，或者找一条感兴趣的新闻（提到搜索的第一印象）\r\n百度 != 搜索，这是不对的\r\n\r\n垂直搜索（站内搜索）\r\n\r\n互联网的搜索：电商网站，招聘网站，新闻网站，各种app\r\nIT系统的搜索：OA软件，办公自动化软件，会议管理，日程管理，项目管理，员工管理，\r\n搜索“张三”，“张三儿”，“张小三”；有个电商网站，卖家，后台管理系统，搜索“牙膏”，\r\n订单，“牙膏相关的订单”\r\n\r\n搜索，就是在任何场景下，找寻你想要的信息，这个时候，会输入一段你要搜索的关键字，\r\n然后就期望找到这个关键字相关的有些信息',8,0,0,1514601542,0,0,0),(2,1,'如果用数据库做搜索会怎么样','','','做软件开发的话，或者对IT、计算机有一定的了解的话，都知道，数据都是存储在数据库里面的，比如说电商网站的商品信息，招聘网站的职位信息，新闻网站的新闻信息，等等吧。所以说，很自然的一点，如果说从技术的角度去考虑，如何实现如说，电商网站内部的搜索功能的话，就可以考虑，去使用数据库去进行搜索。\r\n\r\n1、比方说，每条记录的指定字段的文本，可能会很长，比如说“商品描述”字段的长度，有长达数千个，甚至数万个字符，这个时候，每次都要对每条记录的所有文本进行扫描，懒判断说，你包不包含我指定的这个关键词（比如说“牙膏”）\r\n2、还不能将搜索词拆分开来，尽可能去搜索更多的符合你的期望的结果，比如输入“生化机”，就搜索不出来“生化危机”\r\n\r\n用数据库来实现搜索，是不太靠谱的。通常来说，性能会很差的',8,0,0,1514601743,0,0,0),(3,1,'什么是全文检索(.png)和Lucene','','','（1）全文检索，倒排索引\r\n数据库里的数据 一共有100万条 按照之前的思路，其实就要扫描100万次,而且每次扫描，都需要匹配那个文本所有的字符，确认是否包含搜索的关键词，而且还不能将搜索词拆解开来进行检索\r\n利用倒排索引的话,假设100万条数据，拆分出来的词语，假设有1000成个词语，那么在倒排索引中，就有1000万行,我们可能并不需要搜索1000万次，很可能说，在搜索到第一次的时候，我们就可以找到这个搜索词对应的数据，也可能是第100次，或者第1000次\r\n（2）lucene，就是一个jar包，里面包含了封装好的各种建立倒排索引，以及进行搜索的代码，包括各种算法。我们就用java开发的时候，引入lucene jar，然后基于lucene的api进行去进行开发就可以了。用lucene，我们就可以去将已有的数据建立索引，lucene会在本地磁盘上面，给我们组织索引的数据结构。另外的话，我们也可以用lucene提供的一些功能和api来针对磁盘上额\r\n',8,0,0,1514602341,0,0,0),(4,1,'什么是Elasticsearch','','','Elasticsearch，分布式，高性能，高可用，可伸缩的搜索和分析系统\r\n\r\n1.自动维护数据的分布到多个节点的索引的建立，还有搜索请求分布到多个节点的执行\r\n2.自动维护数据的冗余副本，保证说，一些机器宕机了，不会丢失任何的数据\r\n3.封装了更多的高级功能，以给我们提供更多高级的支持，让我们快速的开发应用，开发更加复杂的应用，复杂性的搜索功能，聚合分析的功能，基于地理位置的搜索(距离我当前位置1公里以内的烤肉店)\r\n',8,0,0,1514602804,0,0,0),(5,1,'Elasticsearch的功能','','','（1）分布式的搜索引擎和数据分析引擎\r\n\r\n搜索：百度，网站的站内搜索，IT系统的检索\r\n数据分析：电商网站，最近7天牙膏这种商品销量排名前10的商家有哪些；新闻网站，最近1个月访问量排名前3的新闻版块是哪些\r\n分布式，搜索，数据分析\r\n\r\n（2）全文检索，结构化检索，数据分析\r\n\r\n全文检索：我想搜索商品名称包含牙膏的商品，select * from products where product_name like &quot;%牙膏%&quot;\r\n结构化检索：我想搜索商品分类为日化用品的商品都有哪些，select * from products where category_id=&#039;日化用品&#039;\r\n部分匹配、自动完成、搜索纠错、搜索推荐\r\n数据分析：我们分析每一个商品分类下有多少个商品，select category_id,count(*) from products group by category_id\r\n\r\n（3）对海量数据进行近实时的处理\r\n\r\n分布式：ES自动可以将海量数据分散到多台服务器上去存储和检索\r\n海联数据的处理：分布式以后，就可以采用大量的服务器去存储和检索数据，自然而然就可以实现海量数据的处理了\r\n近实时：检索个数据要花费1小时（这就不要近实时，离线批处理，batch-processing）；在秒级别对数据进行搜索和分析\r\n\r\n跟分布式/海量数据相反的：lucene，单机应用，只能在单台服务器上使用，最多只能处理单台服务器可以处理的数据量\r\n',8,0,0,1514603029,0,0,0),(6,1,'Elasticsearch的适用场景','','','国外\r\n\r\n（1）维基百科，类似百度百科，牙膏，牙膏的维基百科，全文检索，高亮，搜索推荐\r\n（2）The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击，浏览，收藏，评论）+社交网络数据（对某某新闻的相关看法），数据分析，给到每篇新闻文章的作者，让他知道他的文章的公众反馈（好，坏，热门，垃圾，鄙视，崇拜）\r\n（3）Stack Overflow（国外的程序异常讨论论坛），IT问题，程序的报错，提交上去，有人会跟你讨论和回答，全文检索，搜索相关问题和答案，程序报错了，就会将报错信息粘贴到里面去，搜索有没有对应的答案\r\n（4）GitHub（开源代码管理），搜索上千亿行代码\r\n（5）电商网站，检索商品\r\n（6）日志数据分析，logstash采集日志，ES进行复杂的数据分析（ELK技术，elasticsearch+logstash+kibana）\r\n（7）商品价格监控网站，用户设定某商品的价格阈值，当低于该阈值的时候，发送通知消息给用户，比如说订阅牙膏的监控，如果高露洁牙膏的家庭套装低于50块钱，就通知我，我就去买\r\n（8）BI系统，商业智能，Business Intelligence。比如说有个大型商场集团，BI，分析一下某某区域最近3年的用户消费金额的趋势以及用户群体的组成构成，产出相关的数张报表，**区，最近3年，每年消费金额呈现100%的增长，而且用户群体85%是高级白领，开一个新商场。ES执行数据分析和挖掘，Kibana进行数据可视化\r\n\r\n国内\r\n\r\n（9）国内：站内搜索（电商，招聘，门户，等等），IT系统搜索（OA，CRM，ERP，等等），数据分析（ES热门的一个使用场景）\r\n',8,0,0,1514603083,0,0,0),(7,1,'Elasticsearch的特点','','','（1）可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上，服务小公司\r\n（2）Elasticsearch不是什么新技术，主要是将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的ES；lucene（全文检索），商用的数据分析软件（也是有的），分布式数据库（mycat）\r\n（3）对用户而言，是开箱即用的，非常简单，作为中小型的应用，直接3分钟部署一下ES，就可以作为生产环境的系统来使用了，数据量不大，操作不是太复杂\r\n（4）数据库的功能面对很多领域是不够用的（事务，还有各种联机事务型的操作）；特殊的功能，比如全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理；Elasticsearch作为传统数据库的一个补充，提供了数据库所不不能提供的很多功能\r\n\r\n',8,0,0,1514603107,0,0,0),(8,1,'lucene和elasticsearch的前世今生','','','lucene，最先进、功能最强大的搜索库，直接基于lucene开发，非常复杂，api复杂（实现一些简单的功能，写大量的java代码），需要深入理解原理（各种索引结构）\r\n\r\nelasticsearch，基于lucene，隐藏复杂性，提供简单易用的restful api接口、java api接口（还有其他语言的api接口）\r\n（1）分布式的文档存储引擎\r\n（2）分布式的搜索引擎和分析引擎\r\n（3）分布式，支持PB级数据\r\n\r\n开箱即用，优秀的默认参数，不需要任何额外设置，完全开源\r\n\r\n关于elasticsearch的一个传说，有一个程序员失业了，陪着自己老婆去英国伦敦学习厨师课程。程序员在失业期间想给老婆写一个菜谱搜索引擎，觉得lucene实在太复杂了，就开发了一个封装了lucene的开源项目，compass。后来程序员找到了工作，是做分布式的高性能项目的，觉得compass不够，就写了elasticsearch，让lucene变成分布式的系统。\r\n',8,0,0,1514603267,0,0,0),(9,1,'elasticsearch的核心概念','','','（1）Near Realtime（NRT）：近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级\r\n\r\n（2）Cluster：集群，包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常\r\n（3）Node：节点，集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为“elasticsearch”的集群，如果直接启动一堆节点，那么它们会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群\r\n\r\n（4）Document&amp;field：文档，es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。一个document里面有多个field，每个field就是一个数据字段。\r\n\r\nproduct document\r\n\r\n{\r\n  &quot;product_id&quot;: &quot;1&quot;,\r\n  &quot;product_name&quot;: &quot;高露洁牙膏&quot;,\r\n  &quot;product_desc&quot;: &quot;高效美白&quot;,\r\n  &quot;category_id&quot;: &quot;2&quot;,\r\n  &quot;category_name&quot;: &quot;日化用品&quot;\r\n}\r\n\r\n（5）Index：索引，包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。\r\n（6）Type：类型，每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type。\r\n\r\n商品index，里面存放了所有的商品数据，商品document\r\n\r\n但是商品分很多种类，每个种类的document的field可能不太一样，比如说电器商品，可能还包含一些诸如售后时间范围这样的特殊field；生鲜商品，还包含一些诸如生鲜保质期之类的特殊field\r\n\r\ntype，日化商品type，电器商品type，生鲜商品type\r\n\r\n日化商品type：product_id，product_name，product_desc，category_id，category_name\r\n电器商品type：product_id，product_name，product_desc，category_id，category_name，service_period\r\n生鲜商品type：product_id，product_name，product_desc，category_id，category_name，eat_period\r\n\r\n每一个type里面，都会包含一堆document\r\n\r\n\r\n{\r\n  &quot;product_id&quot;: &quot;2&quot;,\r\n  &quot;product_name&quot;: &quot;长虹电视机&quot;,\r\n  &quot;product_desc&quot;: &quot;4k高清&quot;,\r\n  &quot;category_id&quot;: &quot;3&quot;,\r\n  &quot;category_name&quot;: &quot;电器&quot;,\r\n  &quot;service_period&quot;: &quot;1年&quot;\r\n}\r\n\r\n\r\n{\r\n  &quot;product_id&quot;: &quot;3&quot;,\r\n  &quot;product_name&quot;: &quot;基围虾&quot;,\r\n  &quot;product_desc&quot;: &quot;纯天然，冰岛产&quot;,\r\n  &quot;category_id&quot;: &quot;4&quot;,\r\n  &quot;category_name&quot;: &quot;生鲜&quot;,\r\n  &quot;eat_period&quot;: &quot;7天&quot;\r\n}\r\n\r\n（7）shard：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个shard都是一个lucene index。\r\n（8）replica：任何一个服务器随时可能故障或宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据不丢失，多个replica还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认5个），replica shard（随时修改数量，默认1个），默认每个索引10个shard，5个primary shard，5个replica shard，最小的高可用配置，是2台服务器。\r\n',8,0,0,1514603492,0,0,0),(10,1,'elasticsearch核心概念 vs. 数据库核心概念','','','Elasticsearch			数据库\r\n\r\n-----------------------------------------\r\n\r\nDocument			行\r\nType				表\r\nIndex				库',8,0,0,1514603531,0,0,0),(11,1,'Elasticsearch的安装','','','1、安装JDK，至少1.8.0以上版本，java -version\r\ncmd\r\nC:\\Users\\caopeng&gt;java -version\r\njava version &quot;1.8.0_151&quot;\r\nJava(TM) SE Runtime Environment (build 1.8.0_151-b12)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)\r\n\r\n2、下载和解压缩Elasticsearch安装包，目录结构\r\nbin 命令脚本\r\nconfig 配置文件,日志\r\nlib 依赖的jar (主要依赖lucene)\r\nmodules 功能模块\r\nplugins 插件\r\n\r\n3、启动Elasticsearch：bin\\elasticsearch.bat，es本身特点之一就是开箱即用，如果是中小型应用，数据量少，操作不是很复杂，直接启动就可以用了\r\ne:\r\ncd es\\elasticsearch-5.2.0\\bin\\elasticsearch.bat\r\n\r\n4、检查ES是否启动成功：http://localhost:9200/?pretty\r\n\r\nname: node名称\r\ncluster_name: 集群名称（默认的集群名称就是elasticsearch）\r\nversion.number: 5.2.0，es版本号\r\n\r\n{\r\n  &quot;name&quot; : &quot;4onsTYV&quot;,\r\n  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,\r\n  &quot;cluster_uuid&quot; : &quot;nKZ9VK_vQdSQ1J0Dx9gx1Q&quot;,\r\n  &quot;version&quot; : {\r\n    &quot;number&quot; : &quot;5.2.0&quot;,\r\n    &quot;build_hash&quot; : &quot;24e05b9&quot;,\r\n    &quot;build_date&quot; : &quot;2017-01-24T19:52:35.800Z&quot;,\r\n    &quot;build_snapshot&quot; : false,\r\n    &quot;lucene_version&quot; : &quot;6.4.0&quot;\r\n  },\r\n  &quot;tagline&quot; : &quot;You Know, for Search&quot;\r\n}\r\n\r\n5、修改集群名称：elasticsearch.yml\r\n6、下载和解压缩Kibana安装包，使用里面的开发界面，去操作elasticsearch，作为我们学习es知识点的一个主要的界面入口\r\n7、启动Kibana：bin\\kibana.bat\r\n8、进入Dev Tools界面\r\n9、GET _cluster/health (Console)\r\n{\r\n  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,\r\n  &quot;status&quot;: &quot;yellow&quot;,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;number_of_nodes&quot;: 1,\r\n  &quot;number_of_data_nodes&quot;: 1,\r\n  &quot;active_primary_shards&quot;: 1,\r\n  &quot;active_shards&quot;: 1,\r\n  &quot;relocating_shards&quot;: 0,\r\n  &quot;initializing_shards&quot;: 0,\r\n  &quot;unassigned_shards&quot;: 1,\r\n  &quot;delayed_unassigned_shards&quot;: 0,\r\n  &quot;number_of_pending_tasks&quot;: 0,\r\n  &quot;number_of_in_flight_fetch&quot;: 0,\r\n  &quot;task_max_waiting_in_queue_millis&quot;: 0,\r\n  &quot;active_shards_percent_as_number&quot;: 50\r\n}\r\n\r\n---------------------------------------\r\nlinux系统上安装\r\nelastic 安装\r\n-----------------------------------------------\r\nhttps://www.elastic.co/downloads/elasticsearch 下载 tar.gz包\r\n\r\nmkdir /opt/elastic\r\n将tar.gz包上传到/opt/elastic目录下\r\ncd  /opt/elastic\r\ntar -zxvf elasticsearch-5.1.1.tar.gz\r\ncd elasticsearch-5.1.1\r\n\r\n新建用户 (使用root用户会报错 can not run elasticsearch as root)\r\nuseradd apeng\r\npasswd apeng\r\n\r\n组目录添加用户权限 (不然会报错 could not register mbeans : access denied)\r\ncd /opt/elastic\r\nsudo chown -R apeng elasticsearch-5.1.1\r\nsudo chgrp -R apeng elasticsearch-5.1.1\r\ncd /opt/\r\nsudo chown -R apeng es\r\nsudo chgrp -R apeng es\r\n\r\nmax file descriptors[4096] for elasticsearch likely to low, increase to al least [65536]\r\n无法创建本地文件问题\r\n\r\nvi /etc/security/limits.conf #添加以下内容 (* 指所有用户)\r\n* soft nofile 65536\r\n* hard nofile 131072\r\n* soft nproc 2048\r\n* hard nproc 4096\r\n\r\nmax number of threads [1024] for user [es] likely too low, increase to at least[2048]\r\n无法创建本要线程问题\r\nvi /etc/security/limits.d/90-nproc.conf\r\n* soft nproc 1024 ---&gt; * soft nproc 2048\r\n\r\nmax virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]\r\n最大虚拟内存大小\r\nvi /etc/sysctl.conf #添加以下内容\r\nvm.max_map_count=655360\r\n修改后并执行命令\r\nsysctl -p\r\n\r\nunable to install syscall filter\r\n警告 (linux系统版本过低)\r\n\r\nsu apeng\r\ncd /opt/elastic/elasticsearch\r\nvim config/el.yml\r\npath.data: /opt/es/data\r\npath.logs: /opt/es/logs\r\nnetwork.host: 192.168.233.132\r\nhttp.port: 9200\r\n\r\nbin/elasticsearch\r\n页面访问 192.168.233.132:9200',8,0,0,1514603823,0,0,0),(12,1,'document数据格式','','','面向文档的搜索分析引擎\r\n\r\n（1）应用系统的数据结构都是面向对象的，复杂的\r\n（2）对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦\r\n（3）ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能\r\n（4）es的document用json数据格式来表达\r\n\r\npublic class Employee {\r\n\r\n  private String email;\r\n  private String firstName;\r\n  private String lastName;\r\n  private EmployeeInfo info;\r\n  private Date joinDate;\r\n\r\n}\r\n\r\nprivate class EmployeeInfo {\r\n  \r\n  private String bio; // 性格\r\n  private Integer age;\r\n  private String[] interests; // 兴趣爱好\r\n\r\n}\r\n\r\nEmployeeInfo info = new EmployeeInfo();\r\ninfo.setBio(&quot;curious and modest&quot;);\r\ninfo.setAge(30);\r\ninfo.setInterests(new String[]{&quot;bike&quot;, &quot;climb&quot;});\r\n\r\nEmployee employee = new Employee();\r\nemployee.setEmail(&quot;zhangsan@sina.com&quot;);\r\nemployee.setFirstName(&quot;san&quot;);\r\nemployee.setLastName(&quot;zhang&quot;);\r\nemployee.setInfo(info);\r\nemployee.setJoinDate(new Date());\r\n\r\nemployee对象：里面包含了Employee类自己的属性，还有一个EmployeeInfo对象\r\n\r\n两张表：employee表，employee_info表，将employee对象的数据重新拆开来，变成Employee数据和EmployeeInfo数据\r\nemployee表：email，first_name，last_name，join_date，4个字段\r\nemployee_info表：bio，age，interests，3个字段；此外还有一个外键字段，比如employee_id，关联着employee表\r\n\r\n{\r\n    &quot;email&quot;:      &quot;zhangsan@sina.com&quot;,\r\n    &quot;first_name&quot;: &quot;san&quot;,\r\n    &quot;last_name&quot;: &quot;zhang&quot;,\r\n    &quot;info&quot;: {\r\n        &quot;bio&quot;:         &quot;curious and modest&quot;,\r\n        &quot;age&quot;:         30,\r\n        &quot;interests&quot;: [ &quot;bike&quot;, &quot;climb&quot; ]\r\n    },\r\n    &quot;join_date&quot;: &quot;2017/01/01&quot;\r\n}\r\n\r\n我们就明白了es的document数据格式和数据库的关系型数据格式的区别',8,0,0,1514603992,0,0,0),(13,1,'电商网站商品管理案例背景介绍','','','有一个电商网站，需要为其基于ES构建一个后台系统，提供以下功能：\r\n\r\n（1）对商品信息进行CRUD（增删改查）操作\r\n（2）执行简单的结构化查询\r\n（3）可以执行简单的全文检索，以及复杂的phrase（短语）检索\r\n（4）对于全文检索的结果，可以进行高亮显示\r\n（5）对数据进行简单的聚合分析',8,0,0,1514604016,0,0,0),(14,1,'简单的集群管理','','','（1）快速检查集群的健康状况\r\n\r\nes提供了一套api，叫做cat api，可以查看es中各种各样的数据\r\n\r\nGET /_cat/health?v\r\n\r\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\r\n1488006741 15:12:21  elasticsearch yellow          1         1      1   1    0    0        1             0                  -                 50.0%\r\n\r\n再解压一个 启动之后会自动加入集群 会在9301端口\r\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\r\n1488007113 15:18:33  elasticsearch green           2         2      2   1    0    0        0             0                  -                100.0%\r\n\r\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\r\n1488007216 15:20:16  elasticsearch yellow          1         1      1   1    0    0        1             0                  -                 50.0%\r\n\r\n如何快速了解集群的健康状况？green、yellow、red？\r\n\r\ngreen：每个索引的primary shard和replica shard都是active状态的\r\nyellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态\r\nred：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了\r\n\r\n为什么现在会处于一个yellow状态？\r\n\r\n我们现在就一个笔记本电脑，就启动了一个es进程，相当于就只有一个node。现在es中有一个index，就是kibana自己内置建立的index。由于默认的配置是给每个index分配5个primary shard和5个replica shard，而且primary shard和replica shard不能在同一台机器上（为了容错）。现在kibana自己建立的index是1个primary shard和1个replica shard。当前就一个node，所以只有1个primary shard被分配了和启动了，但是一个replica shard没有第二台机器去启动。\r\n\r\n做一个小实验：此时只要启动第二个es进程，就会在es集群中有2个node，然后那1个replica shard就会自动分配过去，然后cluster status就会变成green状态。\r\n\r\n（2）快速查看集群中有哪些索引\r\n\r\nGET /_cat/indices?v\r\n\r\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\r\nyellow open   .kibana rUm9n9wMRQCCrRDEhqneBg   1   1          1            0      3.1kb          3.1kb\r\n\r\n（3）简单的索引操作\r\n\r\n创建索引：PUT /test_index?pretty\r\n\r\nhealth status index      uuid                   pri rep docs.count docs.deleted store.size pri.store.size\r\nyellow open   test_index XmS9DTAtSkSZSwWhhGEKkQ   5   1          0            0       650b           650b\r\nyellow open   .kibana    rUm9n9wMRQCCrRDEhqneBg   1   1          1            0      3.1kb          3.1kb\r\n\r\n删除索引：DELETE /test_index?pretty\r\n\r\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\r\nyellow open   .kibana rUm9n9wMRQCCrRDEhqneBg   1   1          1            0      3.1kb          3.1kb\r\n',8,0,0,1514604059,0,0,0),(15,1,'商品的CRUD操作','','','（1）新增商品：新增文档，建立索引\r\n\r\nPUT /index/type/id\r\n{\r\n  &quot;json数据&quot;\r\n}\r\n\r\nPUT /ecommerce/product/1\r\n{\r\n    &quot;name&quot; : &quot;gaolujie yagao&quot;,\r\n    &quot;desc&quot; :  &quot;gaoxiao meibai&quot;,\r\n    &quot;price&quot; :  30,\r\n    &quot;producer&quot; :      &quot;gaolujie producer&quot;,\r\n    &quot;tags&quot;: [ &quot;meibai&quot;, &quot;fangzhu&quot; ]\r\n}\r\n创建返回值\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,  #索引\r\n  &quot;_type&quot;: &quot;product&quot;,     #类型\r\n  &quot;_id&quot;: &quot;1&quot;,             #id\r\n  &quot;_version&quot;: 1,          #初始版本\r\n  &quot;result&quot;: &quot;created&quot;,    #结果类型\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nPUT /ecommerce/product/2\r\n{\r\n    &quot;name&quot; : &quot;jiajieshi yagao&quot;,\r\n    &quot;desc&quot; :  &quot;youxiao fangzhu&quot;,\r\n    &quot;price&quot; :  25,\r\n    &quot;producer&quot; :      &quot;jiajieshi producer&quot;,\r\n    &quot;tags&quot;: [ &quot;fangzhu&quot; ]\r\n}\r\n\r\nPUT /ecommerce/product/3\r\n{\r\n    &quot;name&quot; : &quot;zhonghua yagao&quot;,\r\n    &quot;desc&quot; :  &quot;caoben zhiwu&quot;,\r\n    &quot;price&quot; :  40,\r\n    &quot;producer&quot; :      &quot;zhonghua producer&quot;,\r\n    &quot;tags&quot;: [ &quot;qingxin&quot; ]\r\n}\r\n\r\nes会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索\r\n\r\n（2）查询商品：检索文档\r\n\r\nGET /index/type/id\r\nGET /ecommerce/product/1\r\n\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;name&quot;: &quot;gaolujie yagao&quot;,\r\n    &quot;desc&quot;: &quot;gaoxiao meibai&quot;,\r\n    &quot;price&quot;: 30,\r\n    &quot;producer&quot;: &quot;gaolujie producer&quot;,\r\n    &quot;tags&quot;: [\r\n      &quot;meibai&quot;,\r\n      &quot;fangzhu&quot;\r\n    ]\r\n  }\r\n}\r\n\r\n（3）修改商品：替换文档\r\n\r\nPUT /ecommerce/product/1\r\n{\r\n    &quot;name&quot; : &quot;jiaqiangban gaolujie yagao&quot;, #更新name\r\n    &quot;desc&quot; :  &quot;gaoxiao meibai&quot;,\r\n    &quot;price&quot; :  30,\r\n    &quot;producer&quot; :      &quot;gaolujie producer&quot;,\r\n    &quot;tags&quot;: [ &quot;meibai&quot;, &quot;fangzhu&quot; ]\r\n}\r\n// 初始返回值(第一次执行)\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n// 第二次执行返回值 _version有改变 result由created ---&gt; updated created:true ---&gt; false\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}\r\n\r\n\r\nPUT /ecommerce/product/1 #执行之后只有name字段\r\n{\r\n    &quot;name&quot; : &quot;jiaqiangban gaolujie yagao&quot;\r\n}\r\n\r\n替换方式有一个不好，即使必须带上所有的field，才能去进行信息的修改\r\n\r\n（4）修改商品：更新文档\r\n\r\nPOST /ecommerce/product/1/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;name&quot;: &quot;jiaqiangban gaolujie yagao1&quot;\r\n  }\r\n}\r\n注意有修改result的值是updated没修改返回noop\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 8,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\n我的风格，其实有选择的情况下，不太喜欢念ppt，或者照着文档做，或者直接粘贴写好的代码，尽量是纯手敲代码\r\n\r\n（5）删除商品：删除文档\r\n\r\nDELETE /ecommerce/product/1\r\n\r\n{\r\n  &quot;found&quot;: true,\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 9,\r\n  &quot;result&quot;: &quot;deleted&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;ecommerce&quot;,\r\n  &quot;_type&quot;: &quot;product&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;found&quot;: false\r\n}\r\n',8,0,0,1514604713,0,0,0),(16,1,'query string search','','','query string search GET /索引/类型/_search\r\n\r\n搜索全部商品：GET /ecommerce/product/_search\r\n\r\ntook：耗费了几毫秒\r\ntimed_out：是否超时，这里是没有\r\n_shards：数据拆成了5个分片，所以对于搜索请求，会打到所有的primary shard（或者是它的某个replica shard也可以）\r\nhits.total：查询结果的数量，3个document\r\nhits.max_score：score的含义，就是document对于一个search的相关度的匹配分数，越相关，就越匹配，分数也高\r\nhits.hits：包含了匹配搜索的document的详细数据\r\n\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;jiajieshi yagao&quot;,\r\n          &quot;desc&quot;: &quot;youxiao fangzhu&quot;,\r\n          &quot;price&quot;: 25,\r\n          &quot;producer&quot;: &quot;jiajieshi producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;fangzhu&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;gaolujie yagao&quot;,\r\n          &quot;desc&quot;: &quot;gaoxiao meibai&quot;,\r\n          &quot;price&quot;: 30,\r\n          &quot;producer&quot;: &quot;gaolujie producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;meibai&quot;,\r\n            &quot;fangzhu&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nquery string search的由来，因为search参数都是以http请求的query string来附带的\r\n\r\n搜索商品名称中包含yagao的商品，而且按照售价降序排序：GET /ecommerce/product/_search?q=name:yagao&amp;sort=price:desc\r\n\r\n适用于临时的在命令行使用一些工具，比如curl，快速的发出请求，来检索想要的信息；但是如果查询请求很复杂，是很难去构建的\r\n在生产环境中，几乎很少使用query string search',8,0,0,1514604958,0,0,0),(17,1,'query DSL','','','query DSL\r\n\r\nDSL：Domain Specified Language，特定领域的语言\r\nhttp request body：请求体，可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法，比query string search肯定强大多了\r\n\r\n查询所有的商品\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;query&quot;: { &quot;match_all&quot;: {} }\r\n}\r\n\r\n查询名称包含yagao的商品，同时按照价格降序排序\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match&quot; : {\r\n            &quot;name&quot; : &quot;yagao&quot;\r\n        }\r\n    },\r\n    &quot;sort&quot;: [\r\n        { &quot;price&quot;: &quot;desc&quot; }\r\n    ]\r\n}\r\n\r\n分页查询商品，总共3条商品，假设每页就显示1条商品，现在显示第2页，所以就查出来第2个商品\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;query&quot;: { &quot;match_all&quot;: {} },\r\n  &quot;from&quot;: 1,\r\n  &quot;size&quot;: 1\r\n}\r\n\r\n指定要查询出来商品的名称和价格就可以\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;query&quot;: { &quot;match_all&quot;: {} },\r\n  &quot;_source&quot;: [&quot;name&quot;, &quot;price&quot;]\r\n}\r\n\r\n更加适合生产环境的使用，可以构建复杂的查询',8,0,0,1514605008,0,0,0),(18,1,'query filter','','','query filter\r\n\r\n搜索商品名称包含yagao，而且售价大于25元的商品\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;bool&quot; : {\r\n            &quot;must&quot; : {\r\n                &quot;match&quot; : {\r\n                    &quot;name&quot; : &quot;yagao&quot; \r\n                }\r\n            },\r\n            &quot;filter&quot; : {\r\n                &quot;range&quot; : {\r\n                    &quot;price&quot; : { &quot;gt&quot; : 25 } \r\n                }\r\n            }\r\n        }\r\n    }\r\n}',8,0,0,1514605036,0,0,0),(19,1,'full-text search','','','full-text search（全文检索）\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match&quot; : {\r\n            &quot;producer&quot; : &quot;yagao producer&quot;\r\n        }\r\n    }\r\n}\r\n\r\n尽量，无论是学什么技术，比如说你当初学java，学linux，学shell，学javascript，学hadoop。。。。一定自己动手，特别是手工敲各种命令和代码，切记切记，减少复制粘贴的操作。只有自己动手手工敲，学习效果才最好。\r\n\r\nproducer这个字段，会先被拆解，建立倒排索引\r\n\r\nspecial		4\r\nyagao		4\r\nproducer	1,2,3,4\r\ngaolujie	1\r\nzhognhua	3\r\njiajieshi	2\r\n\r\nyagao producer ---&gt; yagao和producer\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 4,\r\n    &quot;max_score&quot;: 0.70293105,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 0.70293105,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;special yagao&quot;,\r\n          &quot;desc&quot;: &quot;special meibai&quot;,\r\n          &quot;price&quot;: 50,\r\n          &quot;producer&quot;: &quot;special yagao producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;meibai&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.25811607,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;gaolujie yagao&quot;,\r\n          &quot;desc&quot;: &quot;gaoxiao meibai&quot;,\r\n          &quot;price&quot;: 30,\r\n          &quot;producer&quot;: &quot;gaolujie producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;meibai&quot;,\r\n            &quot;fangzhu&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514605087,0,0,0),(20,1,'phrase search','','','phrase search\r\n\r\n跟全文检索相对应，相反，全文检索会将输入的搜索串拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回\r\nphrase search，要求输入的搜索串，必须在指定的字段文本中，完全包含一模一样的，才可以算匹配，才能作为结果返回\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match_phrase&quot; : {\r\n            &quot;producer&quot; : &quot;yagao producer&quot;\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 11,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 0.70293105,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;ecommerce&quot;,\r\n        &quot;_type&quot;: &quot;product&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 0.70293105,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;special yagao&quot;,\r\n          &quot;desc&quot;: &quot;special meibai&quot;,\r\n          &quot;price&quot;: 50,\r\n          &quot;producer&quot;: &quot;special yagao producer&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;meibai&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514605143,0,0,0),(21,1,'highlight search（高亮搜索结果）','','','GET /ecommerce/product/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match&quot; : {\r\n            &quot;producer&quot; : &quot;producer&quot;\r\n        }\r\n    },\r\n    &quot;highlight&quot;: {\r\n        &quot;fields&quot; : {\r\n            &quot;producer&quot; : {}\r\n        }\r\n    }\r\n}',8,0,0,1514605160,0,0,0),(22,1,'聚合计算一','','','第一个分析需求：计算每个tag下的商品数量\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_tags&quot;: {\r\n      &quot;terms&quot;: { &quot;field&quot;: &quot;tags&quot; }\r\n    }\r\n  }\r\n}\r\n\r\n将文本field的fielddata属性设置为true\r\n\r\nPUT /ecommerce/_mapping/product\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;tags&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;fielddata&quot;: true\r\n    }\r\n  }\r\n}\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;all_tags&quot;: {\r\n      &quot;terms&quot;: { &quot;field&quot;: &quot;tags&quot; }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 20,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 4,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_tags&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;fangzhu&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;meibai&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;qingxin&quot;,\r\n          &quot;doc_count&quot;: 1\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\n第二个聚合分析的需求：对名称中包含yagao的商品，计算每个tag下的商品数量\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;name&quot;: &quot;yagao&quot;\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;all_tags&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;tags&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\n第三个聚合分析的需求：先分组，再算每组的平均值，计算每个tag下的商品的平均价格\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;size&quot;: 0,\r\n    &quot;aggs&quot; : {\r\n        &quot;group_by_tags&quot; : {\r\n            &quot;terms&quot; : { &quot;field&quot; : &quot;tags&quot; },\r\n            &quot;aggs&quot; : {\r\n                &quot;avg_price&quot; : {\r\n                    &quot;avg&quot; : { &quot;field&quot; : &quot;price&quot; }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 8,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 4,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_tags&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;fangzhu&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 27.5\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;meibai&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 40\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;qingxin&quot;,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 40\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\n第四个数据分析需求：计算每个tag下的商品的平均价格，并且按照平均价格降序排序\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n    &quot;size&quot;: 0,\r\n    &quot;aggs&quot; : {\r\n        &quot;all_tags&quot; : {\r\n            &quot;terms&quot; : { &quot;field&quot; : &quot;tags&quot;, &quot;order&quot;: { &quot;avg_price&quot;: &quot;desc&quot; } },\r\n            &quot;aggs&quot; : {\r\n                &quot;avg_price&quot; : {\r\n                    &quot;avg&quot; : { &quot;field&quot; : &quot;price&quot; }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n我们现在全部都是用es的restful api在学习和讲解es的所欲知识点和功能点，但是没有使用一些编程语言去讲解（比如java），原因有以下：\r\n\r\n1、es最重要的api，让我们进行各种尝试、学习甚至在某些环境下进行使用的api，就是restful api。如果你学习不用es restful api，比如我上来就用java api来讲es，也是可以的，但是你根本就漏掉了es知识的一大块，你都不知道它最重要的restful api是怎么用的\r\n2、讲知识点，用es restful api，更加方便，快捷，不用每次都写大量的java代码，能加快讲课的效率和速度，更加易于同学们关注es本身的知识和功能的学习\r\n3、我们通常会讲完es知识点后，开始详细讲解java api，如何用java api执行各种操作\r\n4、我们每个篇章都会搭配一个项目实战，项目实战是完全基于java去开发的真实项目和系统\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\n第五个数据分析需求：按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格\r\n\r\nGET /ecommerce/product/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_price&quot;: {\r\n      &quot;range&quot;: {\r\n        &quot;field&quot;: &quot;price&quot;,\r\n        &quot;ranges&quot;: [\r\n          {\r\n            &quot;from&quot;: 0,\r\n            &quot;to&quot;: 20\r\n          },\r\n          {\r\n            &quot;from&quot;: 20,\r\n            &quot;to&quot;: 40\r\n          },\r\n          {\r\n            &quot;from&quot;: 40,\r\n            &quot;to&quot;: 50\r\n          }\r\n        ]\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_tags&quot;: {\r\n          &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;tags&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;average_price&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;price&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514605416,0,0,0),(23,1,'Elasticsearch对复杂分布式机制的透明隐藏特性','','','Elasticsearch是一套分布式的系统，分布式是为了应对大数据量\r\n隐藏了复杂的分布式机制\r\n\r\n分片机制（我们之前随随便便就将一些document插入到es集群中去了，我们有没有care过数据怎么进行分片的，数据到哪个shard中去）\r\n\r\ncluster discovery（集群发现机制，我们之前在做那个集群status从yellow转green的实验里，直接启动了第二个es进程，那个进程作为一个node自动就发现了集群，并且加入了进去，还接受了部分数据，replica shard）\r\n\r\nshard负载均衡（举例，假设现在有3个节点，总共有25个shard要分配到3个节点上去，es会自动进行均匀分配，以保持每个节点的均衡的读写负载请求）\r\n\r\nshard副本，请求路由，集群扩容，shard重分配',8,0,0,1514605567,0,0,0),(24,1,'Elasticsearch的垂直扩容与水平扩容','','','垂直扩容：采购更强大的服务器，成本非常高昂，而且会有瓶颈，假设世界上最强大的服务器容量就是10T，但是当你的总数据量达到5000T的时候，你要采购多少台最强大的服务器啊\r\n\r\n水平扩容：业界经常采用的方案，采购越来越多的普通服务器，性能比较一般，但是很多普通服务器组织在一起，就能构成强大的计算和存储能力\r\n\r\n普通服务器：1T，1万，100万\r\n强大服务器：10T，50万，500万\r\n\r\n扩容对应用程序的透明性',8,0,0,1514605594,0,0,0),(25,1,'节点介绍','','','1.增减或减少节点时的数据rebalance\r\n\r\n保持负载均衡\r\n\r\n2.master节点\r\n\r\n（1）创建或删除索引\r\n（2）增加或删除节点\r\n\r\n3.节点平等的分布式架构\r\n\r\n（1）节点对等，每个节点都能接收所有的请求\r\n（2）自动请求路由\r\n（3）响应收集',8,0,0,1514605655,0,0,0),(26,1,'kibana安装','','','https://www.elastic.co/downloads/elasticsearch 下载 tar.gz包\r\n\r\nuname -a #查看linux的版本 (下载对应的版本kibana)\r\nmkdir /opt/kibana\r\n将tar.gz包上传到/opt/kibana\r\ncd /opt/kibana\r\ntar -zxvf kibana-5.1.1.tar.gz\r\ncd kibana-5.1.1\r\n\r\n修改配置\r\nvi config/kibana.yml\r\nserver.host 在windows中不用指定IP,访问使用 http://localhost:5601\r\n在vm虚拟机上，使用IP来访问，这时需要指定IP 访问使用http://IP:5601\r\nserver.host: &quot;192.168.233.132&quot;\r\nelasticsearch.url: &quot;http://192.168.233.132:9200&quot;\r\nkibana.index: &quot;.kibana&quot;\r\n\r\nbin/kibana\r\nhttp://192.168.233.132:5601/server.port: 5601',8,0,0,1514605909,0,0,0),(27,1,'shard&replica机制再次梳理','','','（1）index包含多个shard\r\n（2）每个shard都是一个最小工作单元，承载部分数据，lucene实例，完整的建立索引和处理请求的能力\r\n（3）增减节点时，shard会自动在nodes中负载均衡\r\n（4）primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard\r\n（5）replica shard是primary shard的副本，负责容错，以及承担读请求负载\r\n（6）primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改\r\n（7）primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard\r\n（8）primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上\r\n',8,0,0,1514605958,0,0,0),(28,1,'图解单node环境下创建index是什么样子的','','','（1）单node环境下，创建一个index，有3个primary shard，3个replica shard\r\n（2）集群status是yellow\r\n（3）这个时候，只会将3个primary shard分配到仅有的一个node上去，另外3个replica shard是无法分配的\r\n（4）集群可以正常工作，但是一旦出现节点宕机，数据全部丢失，而且集群不可用，无法承接任何请求\r\n\r\nPUT /test_index\r\n{\r\n   &quot;settings&quot; : {\r\n      &quot;number_of_shards&quot; : 3,\r\n      &quot;number_of_replicas&quot; : 1\r\n   }\r\n}',8,0,0,1514605992,0,0,0),(29,1,'图解2个node环境下replica shard是如何分配的','','','（1）replica shard分配：3个primary shard，3个replica shard，1 node\r\n（2）primary ---&gt; replica同步\r\n（3）读请求：primary/replica',8,0,0,1514606330,0,0,0),(30,1,'图解横向扩容过程，如何超出扩容极限，以及如何提升容错性','','','（1）primary&amp;replica自动负载均衡，6个shard，3 primary，3 replica\r\n（2）每个node有更少的shard，IO/CPU/Memory资源给每个shard分配更多，每个shard性能更好\r\n（3）扩容的极限，6个shard（3 primary，3 replica），最多扩容到6台机器，每个shard可以占用单台服务器的所有资源，性能最好\r\n（4）超出扩容极限，动态修改replica数量，9个shard（3primary，6 replica），扩容到9台机器，比3台机器时，拥有3倍的读吞吐量\r\n（5）3台机器下，9个shard（3 primary，6 replica），资源更少，但是容错性更好，最多容纳2台机器宕机，6个shard只能容纳0台机器宕机\r\n（6）这里的这些知识点，你综合起来看，就是说，一方面告诉你扩容的原理，怎么扩容，怎么提升系统整体吞吐量；另一方面要考虑到系统的容错性，怎么保证提高容错性，让尽可能多的服务器宕机，保证数据不丢失\r\n',8,0,0,1514606434,0,0,0),(31,1,'图解Elasticsearch容错机制：master选举，r','','','（1）9 shard，3 node\r\n（2）master node宕机，自动master选举，red\r\n（3）replica容错：新master将replica提升为primary shard，yellow\r\n（4）重启宕机node，master copy replica到该node，使用原有的shard并同步宕机后的修改，green\r\n',8,0,0,1514606508,0,0,0),(32,1,'es元数据','','','{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_content&quot;: &quot;test test&quot;\r\n  }\r\n}\r\n\r\n------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n1、_index元数据 (index如何创建的反例分析.png)\r\n\r\n（1）代表一个document存放在哪个index中\r\n（2）类似的数据放在一个索引，非类似的数据放不同索引：product index（包含了所有的商品），sales index（包含了所有的商品销售数据），inventory index（包含了所有库存相关的数据）。如果你把比如product，sales，human resource（employee），全都放在一个大的index里面，比如说company index，不合适的。\r\n（3）index中包含了很多类似的document：类似是什么意思，其实指的就是说，这些document的fields很大一部分是相同的，你说你放了3个document，每个document的fields都完全不一样，这就不是类似了，就不太适合放到一个index里面去了。\r\n（4）索引名称必须是小写的，不能用下划线开头，不能包含逗号：product，website，blog\r\n\r\n2、_type元数据\r\n\r\n（1）代表document属于index中的哪个类别（type）\r\n（2）一个索引通常会划分为多个type，逻辑上对index中有些许不同的几类数据进行分类：因为一批相同的数据，可能有很多相同的fields，但是还是可能会有一些轻微的不同，可能会有少数fields是不一样的，举个例子，就比如说，商品，可能划分为电子商品，生鲜商品，日化商品，等等。\r\n（3）type名称可以是大写或者小写，但是同时不能用下划线开头，不能包含逗号\r\n\r\n3、_id元数据\r\n\r\n（1）代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document\r\n（2）我们可以手动指定document的id（put /index/type/id），也可以不指定，由es自动为我们创建一个id\r\n',8,0,0,1514606589,0,0,0),(33,1,'指定document id','','','1、手动指定document id\r\n\r\n（1）根据应用情况来说，是否满足手动指定document id的前提：\r\n\r\n一般来说，是从某些其他的系统中，导入一些数据到es时，会采取这种方式，就是使用系统中已有数据的唯一标识，作为es中document的id。举个例子，比如说，我们现在在开发一个电商网站，做搜索功能，或者是OA系统，做员工检索功能。这个时候，数据首先会在网站系统或者IT系统内部的数据库中，会先有一份，此时就肯定会有一个数据库的primary key（自增长，UUID，或者是业务编号）。如果将数据导入到es中，此时就比较适合采用数据在数据库中已有的primary key。\r\n\r\n如果说，我们是在做一个系统，这个系统主要的数据存储就是es一种，也就是说，数据产生出来以后，可能就没有id，直接就放es一个存储，那么这个时候，可能就不太适合说手动指定document id的形式了，因为你也不知道id应该是什么，此时可以采取下面要讲解的让es自动生成id的方式。\r\n\r\n（2）put /index/type/id\r\n\r\nPUT /test_index/test_type/2\r\n{\r\n  &quot;test_content&quot;: &quot;my test&quot;\r\n}\r\n\r\n2、自动生成document id\r\n\r\n（1）post /index/type\r\n\r\nPOST /test_index/test_type\r\n{\r\n  &quot;test_content&quot;: &quot;my test&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;AVp4RN0bhjxldOOnBxaE&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n（2）自动生成的id，长度为20个字符，URL安全，base64编码，GUID，分布式系统并行生成时不可能会发生冲突\r\n',8,0,0,1514606739,0,0,0),(34,1,'es source','','','1、_source元数据\r\n\r\nput /test_index/test_type/1\r\n{\r\n  &quot;test_field1&quot;: &quot;test field1&quot;,\r\n  &quot;test_field2&quot;: &quot;test field2&quot;\r\n}\r\n\r\nget /test_index/test_type/1\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field1&quot;: &quot;test field1&quot;,\r\n    &quot;test_field2&quot;: &quot;test field2&quot;\r\n  }\r\n}\r\n\r\n_source元数据：就是说，我们在创建一个document的时候，使用的那个放在request body中的json串，默认情况下，在get的时候，会原封不动的给我们返回回来。\r\n\r\n------------------------------------------------------------------------------------------------------------------\r\n\r\n2、定制返回结果\r\n\r\n定制返回的结果，指定_source中，返回哪些field\r\n\r\nGET /test_index/test_type/1?_source=test_field1,test_field2\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field2&quot;: &quot;test field2&quot;\r\n  }\r\n}',8,0,0,1514606831,0,0,0),(35,1,'es document','','','1、document的全量替换\r\n\r\n（1）语法与创建文档是一样的，如果document id不存在，那么就是创建；如果document id已经存在，那么就是全量替换操作，替换document的json串内容\r\n（2）document是不可变的，如果要修改document的内容，第一种方式就是全量替换，直接对document重新建立索引，替换里面所有的内容\r\n（3）es会将老的document标记为deleted，然后新增我们给定的一个document，当我们创建越来越多的document的时候，es会在适当的时机在后台自动删除标记为deleted的document\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n\r\n2、document的强制创建\r\n\r\n（1）创建文档与全量替换的语法是一样的，有时我们只是想新建文档，不想替换文档，如果强制进行创建呢？\r\n（2）PUT /index/type/id?op_type=create，PUT /index/type/id/_create\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n\r\n3、document的删除\r\n\r\n（1）DELETE /index/type/id\r\n（2）不会理解物理删除，只会将其标记为deleted，当数据越来越多的时候，在后台自动删除\r\n',8,0,0,1514607024,0,0,0),(36,1,'深度图解剖析Elasticsearch并发冲突问题','','','深度图解剖析Elasticsearch并发冲突问题',8,0,0,1514607395,0,0,0),(37,1,'深度图解剖析悲观锁与乐观锁两种并发控制方案','','','深度图解剖析悲观锁与乐观锁两种并发控制方案',8,0,0,1514607438,0,0,0),(38,1,'图解Elasticsearch内部如何基于_version进','','','（1）_version元数据\r\n\r\nPUT /test_index/test_type/6\r\n{\r\n  &quot;test_field&quot;: &quot;test test&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;6&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n第一次创建一个document的时候，它的_version内部版本号就是1；以后，每次对这个document执行修改或者删除操作，都会对这个_version版本号自动加1；哪怕是删除，也会对这条数据的版本号加1\r\n\r\n{\r\n  &quot;found&quot;: true,\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;6&quot;,\r\n  &quot;_version&quot;: 4,\r\n  &quot;result&quot;: &quot;deleted&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\n我们会发现，在删除一个document之后，可以从一个侧面证明，它不是立即物理删除掉的，因为它的一些版本号等信息还是保留着的。先删除一条document，再重新创建这条document，其实会在delete version基础之上，再把version号加1\r\n',8,0,0,1514607485,0,0,0),(39,1,'上机动手实战演练基于_version进行乐观锁并发控制','','','1、上机动手实战演练基于_version进行乐观锁并发控制\r\n\r\n（1）先构造一条数据出来\r\n\r\nPUT /test_index/test_type/7\r\n{\r\n  &quot;test_field&quot;: &quot;test test&quot;\r\n}\r\n\r\n（2）模拟两个客户端，都获取到了同一条数据\r\n\r\nGET test_index/test_type/7\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;7&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field&quot;: &quot;test test&quot;\r\n  }\r\n}\r\n\r\n（3）其中一个客户端，先更新了一下这个数据\r\n\r\n同时带上数据的版本号，确保说，es中的数据的版本号，跟客户端中的数据的版本号是相同的，才能修改\r\n\r\nPUT /test_index/test_type/7?version=1 \r\n{\r\n  &quot;test_field&quot;: &quot;test client 1&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;7&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}\r\n\r\n（4）另外一个客户端，尝试基于version=1的数据去进行修改，同样带上version版本号，进行乐观锁的并发控制\r\n\r\nPUT /test_index/test_type/7?version=1 \r\n{\r\n  &quot;test_field&quot;: &quot;test client 2&quot;\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[test_type][7]: version conflict, current version [2] is different than the one provided [1]&quot;,\r\n        &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n        &quot;shard&quot;: &quot;3&quot;,\r\n        &quot;index&quot;: &quot;test_index&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[test_type][7]: version conflict, current version [2] is different than the one provided [1]&quot;,\r\n    &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n    &quot;shard&quot;: &quot;3&quot;,\r\n    &quot;index&quot;: &quot;test_index&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n（5）在乐观锁成功阻止并发问题之后，尝试正确的完成更新\r\n\r\nGET /test_index/test_type/7\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;7&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field&quot;: &quot;test client 1&quot;\r\n  }\r\n}\r\n\r\n基于最新的数据和版本号，去进行修改，修改后，带上最新的版本号，可能这个步骤会需要反复执行好几次，才能成功，特别是在多线程并发更新同一条数据很频繁的情况下\r\n\r\nPUT /test_index/test_type/7?version=2 \r\n{\r\n  &quot;test_field&quot;: &quot;test client 2&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;7&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}',8,0,0,1514607581,0,0,0),(40,1,'上机动手实战演练基于external version进行乐观','','','1、上机动手实战演练基于external version进行乐观锁并发控制\r\n\r\nexternal version\r\n\r\nes提供了一个feature，就是说，你可以不用它提供的内部_version版本号来进行并发控制，可以基于你自己维护的一个版本号来进行并发控制。举个列子，加入你的数据在mysql里也有一份，然后你的应用系统本身就维护了一个版本号，无论是什么自己生成的，程序控制的。这个时候，你进行乐观锁并发控制的时候，可能并不是想要用es内部的_version来进行控制，而是用你自己维护的那个version来进行控制。\r\n\r\n?version=1\r\n?version=1&amp;version_type=external\r\n\r\nversion_type=external，唯一的区别在于，_version，只有当你提供的version与es中的_version一模一样的时候，才可以进行修改，只要不一样，就报错；当version_type=external的时候，只有当你提供的version比es中的_version大的时候，才能完成修改\r\n\r\nes，_version=1，?version=1，才能更新成功\r\nes，_version=1，?version&gt;1&amp;version_type=external，才能成功，比如说?version=2&amp;version_type=external\r\n\r\n（1）先构造一条数据\r\n\r\nPUT /test_index/test_type/8\r\n{\r\n  &quot;test_field&quot;: &quot;test&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n（2）模拟两个客户端同时查询到这条数据\r\n\r\nGET /test_index/test_type/8\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field&quot;: &quot;test&quot;\r\n  }\r\n}\r\n\r\n（3）第一个客户端先进行修改，此时客户端程序是在自己的数据库中获取到了这条数据的最新版本号，比如说是2\r\n\r\nPUT /test_index/test_type/8?version=2&amp;version_type=external\r\n{\r\n  &quot;test_field&quot;: &quot;test client 1&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}\r\n\r\n（4）模拟第二个客户端，同时拿到了自己数据库中维护的那个版本号，也是2，同时基于version=2发起了修改\r\n\r\nPUT /test_index/test_type/8?version=2&amp;version_type=external\r\n{\r\n  &quot;test_field&quot;: &quot;test client 2&quot;\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[test_type][8]: version conflict, current version [2] is higher or equal to the one provided [2]&quot;,\r\n        &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n        &quot;shard&quot;: &quot;1&quot;,\r\n        &quot;index&quot;: &quot;test_index&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[test_type][8]: version conflict, current version [2] is higher or equal to the one provided [2]&quot;,\r\n    &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n    &quot;shard&quot;: &quot;1&quot;,\r\n    &quot;index&quot;: &quot;test_index&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n（5）在并发控制成功后，重新基于最新的版本号发起更新\r\n\r\nGET /test_index/test_type/8\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;test_field&quot;: &quot;test client 1&quot;\r\n  }\r\n}\r\n\r\nPUT /test_index/test_type/8?version=3&amp;version_type=external\r\n{\r\n  &quot;test_field&quot;: &quot;test client 2&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;8&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: false\r\n}',8,0,0,1514607647,0,0,0),(41,1,'什么是partial update','','','1、什么是partial update？\r\n\r\nPUT /index/type/id，创建文档&amp;替换文档，就是一样的语法\r\n\r\n一般对应到应用程序中，每次的执行流程基本是这样的：\r\n\r\n（1）应用程序先发起一个get请求，获取到document，展示到前台界面，供用户查看和修改\r\n（2）用户在前台界面修改数据，发送到后台\r\n（3）后台代码，会将用户修改的数据在内存中进行执行，然后封装好修改后的全量数据\r\n（4）然后发送PUT请求，到es中，进行全量替换\r\n（5）es将老的document标记为deleted，然后重新创建一个新的document\r\n\r\npartial update\r\n\r\npost /index/type/id/_update \r\n{\r\n   &quot;doc&quot;: {\r\n      &quot;要修改的少数几个field即可，不需要全量的数据&quot;\r\n   }\r\n}\r\n\r\n看起来，好像就比较方便了，每次就传递少数几个发生修改的field即可，不需要将全量的document数据发送过去\r\n\r\n2、图解partial update实现原理以及其优点\r\n\r\npartial update，看起来很方便的操作，实际内部的原理是什么样子的，然后它的优点是什么\r\n\r\n3、上机动手实战演练partial update\r\n\r\nPUT /test_index/test_type/10\r\n{\r\n  &quot;test_field1&quot;: &quot;test1&quot;,\r\n  &quot;test_field2&quot;: &quot;test2&quot;\r\n}\r\n\r\nPOST /test_index/test_type/10/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;test_field2&quot;: &quot;updated test2&quot;\r\n  }\r\n}',8,0,0,1514607714,0,0,0),(42,1,'groovy脚本','','','es，其实是有个内置的脚本支持的，可以基于groovy脚本实现各种各样的复杂操作\r\n基于groovy脚本，如何执行partial update\r\nes scripting module，我们会在高手进阶篇去讲解，这里就只是初步讲解一下\r\n\r\nPUT /test_index/test_type/11\r\n{\r\n  &quot;num&quot;: 0,\r\n  &quot;tags&quot;: []\r\n}\r\n\r\n（1）内置脚本\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n   &quot;script&quot; : &quot;ctx._source.num+=1&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;test_index&quot;,\r\n  &quot;_type&quot;: &quot;test_type&quot;,\r\n  &quot;_id&quot;: &quot;11&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;num&quot;: 1,\r\n    &quot;tags&quot;: []\r\n  }\r\n}\r\n\r\n（2）外部脚本 (config/scripts)\r\n\r\nctx._source.tags+=new_tag\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;, \r\n    &quot;file&quot;: &quot;test-add-tags&quot;,\r\n    &quot;params&quot;: {\r\n      &quot;new_tag&quot;: &quot;tag1&quot;\r\n    }\r\n  }\r\n}\r\n\r\n（3）用脚本删除文档\r\n\r\nctx.op = ctx._source.num == count ? &#039;delete&#039; : &#039;none&#039;\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;test-delete-document&quot;,\r\n    &quot;params&quot;: {\r\n      &quot;count&quot;: 1\r\n    }\r\n  }\r\n}\r\n\r\n（4）upsert操作\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;num&quot;: 1\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;document_missing_exception&quot;,\r\n        &quot;reason&quot;: &quot;[test_type][11]: document missing&quot;,\r\n        &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n        &quot;shard&quot;: &quot;4&quot;,\r\n        &quot;index&quot;: &quot;test_index&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;document_missing_exception&quot;,\r\n    &quot;reason&quot;: &quot;[test_type][11]: document missing&quot;,\r\n    &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n    &quot;shard&quot;: &quot;4&quot;,\r\n    &quot;index&quot;: &quot;test_index&quot;\r\n  },\r\n  &quot;status&quot;: 404\r\n}\r\n\r\n如果指定的document不存在，就执行upsert中的初始化操作；如果指定的document存在，就执行doc或者script指定的partial update操作\r\n\r\nPOST /test_index/test_type/11/_update\r\n{\r\n   &quot;script&quot; : &quot;ctx._source.num+=1&quot;,\r\n   &quot;upsert&quot;: {\r\n       &quot;num&quot;: 0,\r\n       &quot;tags&quot;: []\r\n   }\r\n}',8,0,0,1514607816,0,0,0),(43,1,'partial update','','','（1）partial update内置乐观锁并发控制\r\n（2）retry_on_conflict\r\n（3）_version\r\n\r\npost /index/type/id/_update?retry_on_conflict=5&amp;version=6',8,0,0,1514607877,0,0,0),(44,1,'批量查询mget','','','1、批量查询的好处\r\n\r\n就是一条一条的查询，比如说要查询100条数据，那么就要发送100次网络请求，这个开销还是很大的\r\n如果进行批量查询的话，查询100条数据，就只要发送1次网络请求，网络请求的性能开销缩减100倍\r\n\r\n2、mget的语法\r\n\r\n（1）一条一条的查询\r\n\r\nGET /test_index/test_type/1\r\nGET /test_index/test_type/2\r\n\r\n（2）mget批量查询\r\n\r\nGET /_mget\r\n{\r\n   &quot;docs&quot; : [\r\n      {\r\n         &quot;_index&quot; : &quot;test_index&quot;,\r\n         &quot;_type&quot; :  &quot;test_type&quot;,\r\n         &quot;_id&quot; :    1\r\n      },\r\n      {\r\n         &quot;_index&quot; : &quot;test_index&quot;,\r\n         &quot;_type&quot; :  &quot;test_type&quot;,\r\n         &quot;_id&quot; :    2\r\n      }\r\n   ]\r\n}\r\n\r\n{\r\n  &quot;docs&quot;: [\r\n    {\r\n      &quot;_index&quot;: &quot;test_index&quot;,\r\n      &quot;_type&quot;: &quot;test_type&quot;,\r\n      &quot;_id&quot;: &quot;1&quot;,\r\n      &quot;_version&quot;: 2,\r\n      &quot;found&quot;: true,\r\n      &quot;_source&quot;: {\r\n        &quot;test_field1&quot;: &quot;test field1&quot;,\r\n        &quot;test_field2&quot;: &quot;test field2&quot;\r\n      }\r\n    },\r\n    {\r\n      &quot;_index&quot;: &quot;test_index&quot;,\r\n      &quot;_type&quot;: &quot;test_type&quot;,\r\n      &quot;_id&quot;: &quot;2&quot;,\r\n      &quot;_version&quot;: 1,\r\n      &quot;found&quot;: true,\r\n      &quot;_source&quot;: {\r\n        &quot;test_content&quot;: &quot;my test&quot;\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\n（3）如果查询的document是一个index下的不同type种的话\r\n\r\nGET /test_index/_mget\r\n{\r\n   &quot;docs&quot; : [\r\n      {\r\n         &quot;_type&quot; :  &quot;test_type&quot;,\r\n         &quot;_id&quot; :    1\r\n      },\r\n      {\r\n         &quot;_type&quot; :  &quot;test_type&quot;,\r\n         &quot;_id&quot; :    2\r\n      }\r\n   ]\r\n}\r\n\r\n（4）如果查询的数据都在同一个index下的同一个type下，最简单了\r\n\r\nGET /test_index/test_type/_mget\r\n{\r\n   &quot;ids&quot;: [1, 2]\r\n}\r\n\r\n3、mget的重要性\r\n\r\n可以说mget是很重要的，一般来说，在进行查询的时候，如果一次性要查询多条数据的话，那么一定要用batch批量操作的api\r\n尽可能减少网络开销次数，可能可以将性能提升数倍，甚至数十倍，非常非常之重要\r\n',8,0,0,1514607958,0,0,0),(45,1,'bulk语法','','','1、bulk语法\r\n\r\nPOST /_bulk\r\n{ &quot;delete&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;3&quot; }} \r\n{ &quot;create&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;12&quot; }}\r\n{ &quot;test_field&quot;:    &quot;test12&quot; }\r\n{ &quot;index&quot;:  { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;test_field&quot;:    &quot;replaced test2&quot; }\r\n{ &quot;update&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_retry_on_conflict&quot; : 3} }\r\n{ &quot;doc&quot; : {&quot;test_field2&quot; : &quot;bulk test1&quot;} }\r\n\r\n每一个操作要两个json串，语法如下：\r\n\r\n{&quot;action&quot;: {&quot;metadata&quot;}}\r\n{&quot;data&quot;}\r\n\r\n举例，比如你现在要创建一个文档，放bulk里面，看起来会是这样子的：\r\n\r\n{&quot;index&quot;: {&quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;, &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;}}\r\n{&quot;test_field1&quot;: &quot;test1&quot;, &quot;test_field2&quot;: &quot;test2&quot;}\r\n\r\n有哪些类型的操作可以执行呢？\r\n（1）delete：删除一个文档，只要1个json串就可以了\r\n（2）create：PUT /index/type/id/_create，强制创建\r\n（3）index：普通的put操作，可以是创建文档，也可以是全量替换文档\r\n（4）update：执行的partial update操作\r\n\r\nbulk api对json的语法，有严格的要求，每个json串不能换行，只能放一行，同时一个json串和一个json串之间，必须有一个换行\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;json_e_o_f_exception&quot;,\r\n        &quot;reason&quot;: &quot;Unexpected end-of-input: expected close marker for Object (start marker at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@5a5932cd; line: 1, column: 1])\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@5a5932cd; line: 1, column: 3]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;json_e_o_f_exception&quot;,\r\n    &quot;reason&quot;: &quot;Unexpected end-of-input: expected close marker for Object (start marker at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@5a5932cd; line: 1, column: 1])\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@5a5932cd; line: 1, column: 3]&quot;\r\n  },\r\n  &quot;status&quot;: 500\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 41,\r\n  &quot;errors&quot;: true,\r\n  &quot;items&quot;: [\r\n    {\r\n      &quot;delete&quot;: {\r\n        &quot;found&quot;: true,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;10&quot;,\r\n        &quot;_version&quot;: 3,\r\n        &quot;result&quot;: &quot;deleted&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;status&quot;: 200\r\n      }\r\n    },\r\n    {\r\n      &quot;create&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_version&quot;: 1,\r\n        &quot;result&quot;: &quot;created&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;created&quot;: true,\r\n        &quot;status&quot;: 201\r\n      }\r\n    },\r\n    {\r\n      &quot;create&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;status&quot;: 409,\r\n        &quot;error&quot;: {\r\n          &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n          &quot;reason&quot;: &quot;[test_type][2]: version conflict, document already exists (current version [1])&quot;,\r\n          &quot;index_uuid&quot;: &quot;6m0G7yx7R1KECWWGnfH1sw&quot;,\r\n          &quot;shard&quot;: &quot;2&quot;,\r\n          &quot;index&quot;: &quot;test_index&quot;\r\n        }\r\n      }\r\n    },\r\n    {\r\n      &quot;index&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_version&quot;: 1,\r\n        &quot;result&quot;: &quot;created&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;created&quot;: true,\r\n        &quot;status&quot;: 201\r\n      }\r\n    },\r\n    {\r\n      &quot;index&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_version&quot;: 2,\r\n        &quot;result&quot;: &quot;updated&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;created&quot;: false,\r\n        &quot;status&quot;: 200\r\n      }\r\n    },\r\n    {\r\n      &quot;update&quot;: {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_version&quot;: 3,\r\n        &quot;result&quot;: &quot;updated&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;status&quot;: 200\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\nbulk操作中，任意一个操作失败，是不会影响其他的操作的，但是在返回结果里，会告诉你异常日志\r\n\r\nPOST /test_index/_bulk\r\n{ &quot;delete&quot;: { &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;3&quot; }} \r\n{ &quot;create&quot;: { &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;12&quot; }}\r\n{ &quot;test_field&quot;:    &quot;test12&quot; }\r\n{ &quot;index&quot;:  { &quot;_type&quot;: &quot;test_type&quot; }}\r\n{ &quot;test_field&quot;:    &quot;auto-generate id test&quot; }\r\n{ &quot;index&quot;:  { &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;test_field&quot;:    &quot;replaced test2&quot; }\r\n{ &quot;update&quot;: { &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_retry_on_conflict&quot; : 3} }\r\n{ &quot;doc&quot; : {&quot;test_field2&quot; : &quot;bulk test1&quot;} }\r\n\r\nPOST /test_index/test_type/_bulk\r\n{ &quot;delete&quot;: { &quot;_id&quot;: &quot;3&quot; }} \r\n{ &quot;create&quot;: { &quot;_id&quot;: &quot;12&quot; }}\r\n{ &quot;test_field&quot;:    &quot;test12&quot; }\r\n{ &quot;index&quot;:  { }}\r\n{ &quot;test_field&quot;:    &quot;auto-generate id test&quot; }\r\n{ &quot;index&quot;:  { &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;test_field&quot;:    &quot;replaced test2&quot; }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;, &quot;_retry_on_conflict&quot; : 3} }\r\n{ &quot;doc&quot; : {&quot;test_field2&quot; : &quot;bulk test1&quot;} }\r\n\r\n2、bulk size最佳大小\r\n\r\nbulk request会加载到内存里，如果太大的话，性能反而会下降，因此需要反复尝试一个最佳的bulk size。一般从1000~5000条数据开始，尝试逐渐增加。另外，如果看大小的话，最好是在5~15MB之间。\r\n',8,0,0,1514608013,0,0,0),(46,1,'distributed document store','','','到目前为止，你觉得你在学什么东西，给大家一个直观的感觉，好像已经知道了es是分布式的，包括一些基本的原理，然后花了不少时间在学习document本身相关的操作，增删改查。一句话点出来，给大家归纳总结一下，其实我们应该思考一下，es的一个最最核心的功能，已经被我们相对完整的讲完了。\r\n\r\nElasticsearch在跑起来以后，其实起到的第一个最核心的功能，就是一个分布式的文档数据存储系统。ES是分布式的。文档数据存储系统。文档数据，存储系统。\r\n文档数据：es可以存储和操作json文档类型的数据，而且这也是es的核心数据结构。\r\n存储系统：es可以对json文档类型的数据进行存储，查询，创建，更新，删除，等等操作。其实已经起到了一个什么样的效果呢？其实ES满足了这些功能，就可以说已经是一个NoSQL的存储系统了。\r\n\r\n围绕着document在操作，其实就是把es当成了一个NoSQL存储引擎，一个可以存储文档类型数据的存储系统，在操作里面的document。\r\n\r\nes可以作为一个分布式的文档存储系统，所以说，我们的应用系统，是不是就可以基于这个概念，去进行相关的应用程序的开发了。\r\n\r\n什么类型的应用程序呢？\r\n\r\n（1）数据量较大，es的分布式本质，可以帮助你快速进行扩容，承载大量数据\r\n（2）数据结构灵活多变，随时可能会变化，而且数据结构之间的关系，非常复杂，如果我们用传统数据库，那是不是很坑，因为要面临大量的表\r\n（3）对数据的相关操作，较为简单，比如就是一些简单的增删改查，用我们之前讲解的那些document操作就可以搞定\r\n（4）NoSQL数据库，适用的也是类似于上面的这种场景\r\n\r\n举个例子，比如说像一些网站系统，或者是普通的电商系统，博客系统，面向对象概念比较复杂，但是作为终端网站来说，没什么太复杂的功能，就是一些简单的CRUD操作，而且数据量可能还比较大。这个时候选用ES这种NoSQL型的数据存储，比传统的复杂的功能务必强大的支持SQL的关系型数据库，更加合适一些。无论是性能，还是吞吐量，可能都会更好。\r\n',8,0,0,1514608228,0,0,0),(47,1,'路由','','','（1）document路由到shard上是什么意思？\r\n\r\n（2）路由算法：shard = hash(routing) % number_of_primary_shards\r\n\r\n举个例子，一个index有3个primary shard，P0，P1，P2\r\n\r\n每次增删改查一个document的时候，都会带过来一个routing number，默认就是这个document的_id（可能是手动指定，也可能是自动生成）\r\nrouting = _id，假设_id=1\r\n\r\n会将这个routing值，传入一个hash函数中，产出一个routing值的hash值，hash(routing) = 21\r\n然后将hash函数产出的值对这个index的primary shard的数量求余数，21 % 3 = 0\r\n就决定了，这个document就放在P0上。\r\n\r\n决定一个document在哪个shard上，最重要的一个值就是routing值，默认是_id，也可以手动指定，相同的routing值，每次过来，从hash函数中，产出的hash值一定是相同的\r\n\r\n无论hash值是几，无论是什么数字，对number_of_primary_shards求余数，结果一定是在0~number_of_primary_shards-1之间这个范围内的。0,1,2。\r\n\r\n（3）_id or custom routing value\r\n\r\n默认的routing就是_id\r\n也可以在发送请求的时候，手动指定一个routing value，比如说put /index/type/id?routing=user_id\r\n\r\n手动指定routing value是很有用的，可以保证说，某一类document一定被路由到一个shard上去，那么在后续进行应用级别的负载均衡，以及提升批量读取的性能的时候，是很有帮助的\r\n\r\n（4）primary shard数量不可变的谜底',8,0,0,1514608295,0,0,0),(48,1,'node','','','（1）客户端选择一个node发送请求过去，这个node就是coordinating node（协调节点）\r\n（2）coordinating node，对document进行路由，将请求转发给对应的node（有primary shard）\r\n（3）实际的node上的primary shard处理请求，然后将数据同步到replica node\r\n（4）coordinating node，如果发现primary node和所有replica node都搞定之后，就返回响应结果给客户端\r\n',8,0,0,1514608345,0,0,0),(49,1,'quorum','','','（1）consistency，one（primary shard），all（all shard），quorum（default）\r\n\r\n我们在发送任何一个增删改操作的时候，比如说put /index/type/id，都可以带上一个consistency参数，指明我们想要的写一致性是什么？\r\nput /index/type/id?consistency=quorum\r\n\r\none：要求我们这个写操作，只要有一个primary shard是active活跃可用的，就可以执行\r\nall：要求我们这个写操作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作\r\nquorum：默认的值，要求所有的shard中，必须是大部分的shard都是活跃的，可用的，才可以执行这个写操作\r\n\r\n（2）quorum机制，写之前必须确保大多数shard都可用，int( (primary + number_of_replicas) / 2 ) + 1，当number_of_replicas&gt;1时才生效\r\n\r\nquroum = int( (primary + number_of_replicas) / 2 ) + 1\r\n举个例子，3个primary shard，number_of_replicas=1，总共有3 + 3 * 1 = 6个shard\r\nquorum = int( (3 + 1) / 2 ) + 1 = 3\r\n所以，要求6个shard中至少有3个shard是active状态的，才可以执行这个写操作\r\n\r\n（3）如果节点数少于quorum数量，可能导致quorum不齐全，进而导致无法执行任何写操作\r\n\r\n3个primary shard，replica=1，要求至少3个shard是active，3个shard按照之前学习的shard&amp;replica机制，必须在不同的节点上，如果说只有2台机器的话，是不是有可能出现说，3个shard都没法分配齐全，此时就可能会出现写操作无法执行的情况\r\n\r\nes提供了一种特殊的处理场景，就是说当number_of_replicas&gt;1时才生效，因为假如说，你就一个primary shard，replica=1，此时就2个shard\r\n\r\n(1 + 1 / 2) + 1 = 2，要求必须有2个shard是活跃的，但是可能就1个node，此时就1个shard是活跃的，如果你不特殊处理的话，导致我们的单节点集群就无法工作\r\n\r\n（4）quorum不齐全时，wait，默认1分钟，timeout，100，30s\r\n\r\n等待期间，期望活跃的shard数量可以增加，最后实在不行，就会timeout\r\n我们其实可以在写操作的时候，加一个timeout参数，比如说put /index/type/id?timeout=30，这个就是说自己去设定quorum不齐全的时候，es的timeout时长，可以缩短，也可以增长\r\n\r\n',8,0,0,1514608428,0,0,0),(50,1,'coordinate node','','','1、客户端发送请求到任意一个node，成为coordinate node\r\n2、coordinate node对document进行路由，将请求转发到对应的node，此时会使用round-robin随机轮询算法，在primary shard以及其所有replica中随机选择一个，让读请求负载均衡\r\n3、接收请求的node返回document给coordinate node\r\n4、coordinate node返回document给客户端\r\n5、特殊情况：document如果还在建立索引过程中，可能只有primary shard有，任何一个replica shard都没有，此时可能会导致无法读取到document，但是document完成索引建立之后，primary shard和replica shard就都有了\r\n',8,0,0,1514608562,0,0,0),(51,1,'bulk api','','','bulk api奇特的json格式\r\n\r\n{&quot;action&quot;: {&quot;meta&quot;}}\\n\r\n{&quot;data&quot;}\\n\r\n{&quot;action&quot;: {&quot;meta&quot;}}\\n\r\n{&quot;data&quot;}\\n\r\n\r\n[{\r\n  &quot;action&quot;: {\r\n \r\n  },\r\n  &quot;data&quot;: {\r\n\r\n  }\r\n}]\r\n\r\n1、bulk中的每个操作都可能要转发到不同的node的shard去执行\r\n\r\n2、如果采用比较良好的json数组格式\r\n\r\n允许任意的换行，整个可读性非常棒，读起来很爽，es拿到那种标准格式的json串以后，要按照下述流程去进行处理\r\n\r\n（1）将json数组解析为JSONArray对象，这个时候，整个数据，就会在内存中出现一份一模一样的拷贝，一份数据是json文本，一份数据是JSONArray对象\r\n（2）解析json数组里的每个json，对每个请求中的document进行路由\r\n（3）为路由到同一个shard上的多个请求，创建一个请求数组\r\n（4）将这个请求数组序列化\r\n（5）将序列化后的请求数组发送到对应的节点上去\r\n\r\n3、耗费更多内存，更多的jvm gc开销\r\n\r\n我们之前提到过bulk size最佳大小的那个问题，一般建议说在几千条那样，然后大小在10MB左右，所以说，可怕的事情来了。假设说现在100个bulk请求发送到了一个节点上去，然后每个请求是10MB，100个请求，就是1000MB = 1GB，然后每个请求的json都copy一份为jsonarray对象，此时内存中的占用就会翻倍，就会占用2GB的内存，甚至还不止。因为弄成jsonarray之后，还可能会多搞一些其他的数据结构，2GB+的内存占用。\r\n\r\n占用更多的内存可能就会积压其他请求的内存使用量，比如说最重要的搜索请求，分析请求，等等，此时就可能会导致其他请求的性能急速下降\r\n另外的话，占用内存更多，就会导致java虚拟机的垃圾回收次数更多，跟频繁，每次要回收的垃圾对象更多，耗费的时间更多，导致es的java虚拟机停止工作线程的时间更多\r\n\r\n4、现在的奇特格式\r\n\r\n{&quot;action&quot;: {&quot;meta&quot;}}\\n\r\n{&quot;data&quot;}\\n\r\n{&quot;action&quot;: {&quot;meta&quot;}}\\n\r\n{&quot;data&quot;}\\n\r\n\r\n（1）不用将其转换为json对象，不会出现内存中的相同数据的拷贝，直接按照换行符切割json\r\n（2）对每两个一组的json，读取meta，进行document路由\r\n（3）直接将对应的json发送到node上去\r\n\r\n5、最大的优势在于，不需要将json数组解析为一个JSONArray对象，形成一份大数据的拷贝，浪费内存空间，尽可能地保证性能\r\n',8,0,0,1514608624,0,0,0),(52,1,'took','','','1、我们如果发出一个搜索请求的话，会拿到一堆搜索结果，本节课，我们来讲解一下，这个搜索结果里的各种数据，都代表了什么含义\r\n2、我们来讲解一下，搜索的timeout机制，底层的原理，画图讲解\r\n\r\nGET /_search\r\n\r\n{\r\n  &quot;took&quot;: 6,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 6,\r\n    &quot;successful&quot;: 6,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 10,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;.kibana&quot;,\r\n        &quot;_type&quot;: &quot;config&quot;,\r\n        &quot;_id&quot;: &quot;5.2.0&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;buildNum&quot;: 14695\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\ntook：整个搜索请求花费了多少毫秒\r\n\r\nhits.total：本次搜索，返回了几条结果\r\nhits.max_score：本次搜索的所有结果中，最大的相关度分数是多少，每一条document对于search的相关度，越相关，_score分数越大，排位越靠前\r\nhits.hits：默认查询前10条数据，完整数据，_score降序排序\r\n\r\nshards：shards fail的条件（primary和replica全部挂掉），不影响其他shard。默认情况下来说，一个搜索请求，会打到一个index的所有primary shard上去，当然了，每个primary shard都可能会有一个或多个replic shard，所以请求也可以到primary shard的其中一个replica shard上去。\r\n\r\ntimeout：默认无timeout，latency平衡completeness，手动指定timeout，timeout查询执行机制\r\n\r\ntimeout=10ms，timeout=1s，timeout=1m\r\nGET /_search?timeout=10m\r\n',8,0,0,1514608672,0,0,0),(53,1,'multi','','','1、multi-index和multi-type搜索模式\r\n\r\n告诉你如何一次性搜索多个index和多个type下的数据\r\n\r\n/_search：所有索引，所有type下的所有数据都搜索出来\r\n/index1/_search：指定一个index，搜索其下所有type的数据\r\n/index1,index2/_search：同时搜索两个index下的数据\r\n/*1,*2/_search：按照通配符去匹配多个索引\r\n/index1/type1/_search：搜索一个index下指定的type的数据\r\n/index1/type1,type2/_search：可以搜索一个index下多个type的数据\r\n/index1,index2/type1,type2/_search：搜索多个index下的多个type的数据\r\n/_all/type1,type2/_search：_all，可以代表搜索所有index下的指定type的数据\r\n\r\n2、初步图解一下简单的搜索原理\r\n\r\n搜索原理初步图解',8,0,0,1514608732,0,0,0),(54,1,'分页搜索','','','1、讲解如何使用es进行分页搜索的语法\r\n\r\nsize，from\r\n\r\nGET /_search?size=10\r\nGET /_search?size=10&amp;from=0\r\nGET /_search?size=10&amp;from=20\r\n\r\n分页的上机实验\r\n\r\nGET /test_index/test_type/_search\r\n\r\n&quot;hits&quot;: {\r\n    &quot;total&quot;: 9,\r\n    &quot;max_score&quot;: 1,\r\n\r\n我们假设将这9条数据分成3页，每一页是3条数据，来实验一下这个分页搜索的效果\r\n\r\nGET /test_index/test_type/_search?from=0&amp;size=3\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 9,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;8&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test client 2&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;6&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;tes test&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test4&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n第一页：id=8,6,4\r\n\r\nGET /test_index/test_type/_search?from=3&amp;size=3\r\n\r\n第二页：id=2,自动生成,7\r\n\r\nGET /test_index/test_type/_search?from=6&amp;size=3\r\n\r\n第三页：id=1,11,3\r\n\r\n2、什么是deep paging问题？为什么会产生这个问题，它的底层原理是什么？\r\n\r\ndeep paging性能问题，以及原理深度图解揭秘，很高级的知识点\r\n',8,0,0,1514608838,0,0,0),(55,1,'query string','','','1、query string基础语法\r\n\r\nGET /test_index/test_type/_search?q=test_field:test\r\nGET /test_index/test_type/_search?q=+test_field:test\r\nGET /test_index/test_type/_search?q=-test_field:test\r\n\r\n一个是掌握q=field:search content的语法，还有一个是掌握+和-的含义\r\n\r\n2、_all metadata的原理和作用\r\n\r\nGET /test_index/test_type/_search?q=test\r\n\r\n直接可以搜索所有的field，任意一个field包含指定的关键字就可以搜索出来。我们在进行中搜索的时候，难道是对document中的每一个field都进行一次搜索吗？不是的\r\n\r\nes中的_all元数据，在建立索引的时候，我们插入一条document，它里面包含了多个field，此时，es会自动将多个field的值，全部用字符串的方式串联起来，变成一个长的字符串，作为_all field的值，同时建立索引\r\n\r\n后面如果在搜索的时候，没有对某个field指定搜索，就默认搜索_all field，其中是包含了所有field的值的\r\n\r\n举个例子\r\n\r\n{\r\n  &quot;name&quot;: &quot;jack&quot;,\r\n  &quot;age&quot;: 26,\r\n  &quot;email&quot;: &quot;jack@sina.com&quot;,\r\n  &quot;address&quot;: &quot;guamgzhou&quot;\r\n}\r\n\r\n&quot;jack 26 jack@sina.com guangzhou&quot;，作为这一条document的_all field的值，同时进行分词后建立对应的倒排索引\r\n\r\n生产环境不使用',8,0,0,1514608884,0,0,0),(56,1,'mapping','','','插入几条数据，让es自动为我们建立一个索引\r\n\r\nPUT /website/article/1\r\n{\r\n  &quot;post_date&quot;: &quot;2017-01-01&quot;,\r\n  &quot;title&quot;: &quot;my first article&quot;,\r\n  &quot;content&quot;: &quot;this is my first article in this website&quot;,\r\n  &quot;author_id&quot;: 11400\r\n}\r\n\r\nPUT /website/article/2\r\n{\r\n  &quot;post_date&quot;: &quot;2017-01-02&quot;,\r\n  &quot;title&quot;: &quot;my second article&quot;,\r\n  &quot;content&quot;: &quot;this is my second article in this website&quot;,\r\n  &quot;author_id&quot;: 11400\r\n}\r\n\r\nPUT /website/article/3\r\n{\r\n  &quot;post_date&quot;: &quot;2017-01-03&quot;,\r\n  &quot;title&quot;: &quot;my third article&quot;,\r\n  &quot;content&quot;: &quot;this is my third article in this website&quot;,\r\n  &quot;author_id&quot;: 11400\r\n}\r\n\r\n尝试各种搜索\r\n\r\nGET /website/article/_search?q=2017			3条结果             \r\nGET /website/article/_search?q=2017-01-01        	3条结果\r\nGET /website/article/_search?q=post_date:2017-01-01   	1条结果\r\nGET /website/article/_search?q=post_date:2017         	1条结果\r\n\r\n查看es自动建立的mapping，带出什么是mapping的知识点\r\n自动或手动为index中的type建立的一种数据结构和相关配置，简称为mapping\r\ndynamic mapping，自动为我们建立index，创建type，以及type对应的mapping，mapping中包含了每个field对应的数据类型，以及如何分词等设置\r\n我们当然，后面会讲解，也可以手动在创建数据之前，先创建index和type，以及type对应的mapping\r\n\r\nGET /website/_mapping/article\r\n\r\n{\r\n  &quot;website&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;article&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;author_id&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          },\r\n          &quot;content&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          },\r\n          &quot;post_date&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          },\r\n          &quot;title&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n搜索结果为什么不一致，因为es自动建立mapping的时候，设置了不同的field不同的data type。不同的data type的分词、搜索等行为是不一样的。所以出现了_all field和post_date field的搜索表现完全不一样。\r\n',8,0,0,1514608990,0,0,0),(57,1,'exact value','','','1、exact value\r\n\r\n2017-01-01，exact value，搜索的时候，必须输入2017-01-01，才能搜索出来\r\n如果你输入一个01，是搜索不出来的\r\n\r\n2、full text\r\n（1）缩写 vs. 全程：cn vs. china\r\n（2）格式转化：like liked likes\r\n（3）大小写：Tom vs tom\r\n（4）同义词：like vs love\r\n\r\n2017-01-01，2017 01 01，搜索2017，或者01，都可以搜索出来\r\nchina，搜索cn，也可以将china搜索出来\r\nlikes，搜索like，也可以将likes搜索出来\r\nTom，搜索tom，也可以将Tom搜索出来\r\nlike，搜索love，同义词，也可以将like搜索出来\r\n\r\n就不是说单纯的只是匹配完整的一个值，而是可以对值进行拆分词语后（分词）进行匹配，也可以通过缩写、时态、大小写、同义词等进行匹配',8,0,0,1514609054,0,0,0),(58,1,'分词初识','','','doc1：I really liked my small dogs, and I think my mom also liked them.\r\ndoc2：He never liked any dogs, so I hope that my mom will not expect me to liked him.\r\n\r\n分词，初步的倒排索引的建立\r\n\r\nword		doc1			doc2\r\n\r\nI		*			*\r\nreally		*\r\nliked		*			*\r\nmy		*			*\r\nsmall		*	\r\ndogs		*\r\nand		*\r\nthink		*\r\nmom		*			*\r\nalso		*\r\nthem		*	\r\nHe					*\r\nnever					*\r\nany					*\r\nso					*\r\nhope					*\r\nthat					*\r\nwill					*\r\nnot					*\r\nexpect					*\r\nme					*\r\nto					*\r\nhim					*\r\n\r\n演示了一下倒排索引最简单的建立的一个过程\r\n\r\n搜索\r\n\r\nmother like little dog，不可能有任何结果\r\n\r\nmother\r\nlike\r\nlittle\r\ndog\r\n\r\n这个是不是我们想要的搜索结果？？？绝对不是，因为在我们看来，mother和mom有区别吗？同义词，都是妈妈的意思。like和liked有区别吗？没有，都是喜欢的意思，只不过一个是现在时，一个是过去时。little和small有区别吗？同义词，都是小小的。dog和dogs有区别吗？狗，只不过一个是单数，一个是复数。\r\n\r\nnormalization，建立倒排索引的时候，会执行一个操作，也就是说对拆分出的各个单词进行相应的处理，以提升后面搜索的时候能够搜索到相关联的文档的概率\r\n\r\n时态的转换，单复数的转换，同义词的转换，大小写的转换\r\n\r\nmom —&gt; mother\r\nliked —&gt; like\r\nsmall —&gt; little\r\ndogs —&gt; dog\r\n\r\n重新建立倒排索引，加入normalization，再次用mother liked little dog搜索，就可以搜索到了\r\n\r\nword		doc1			doc2\r\n\r\nI		*			*\r\nreally		*\r\nlike		*			*			liked --&gt; like\r\nmy		*			*\r\nlittle		*						small --&gt; little\r\ndog		*			*			dogs --&gt; dog						\r\nand		*\r\nthink		*\r\nmom		*			*\r\nalso		*\r\nthem		*	\r\nHe					*\r\nnever					*\r\nany					*\r\nso					*\r\nhope					*\r\nthat					*\r\nwill					*\r\nnot					*\r\nexpect					*\r\nme					*\r\nto					*\r\nhim					*\r\n\r\nmother like little dog，分词，normalization\r\n\r\nmother	--&gt; mom\r\nlike	--&gt; like\r\nlittle	--&gt; little\r\ndog	--&gt; dog\r\n\r\ndoc1和doc2都会搜索出来\r\n\r\ndoc1：I really liked my small dogs, and I think my mom also liked them.\r\ndoc2：He never liked any dogs, so I hope that my mom will not expect me to liked him.\r\n',8,0,0,1514609108,0,0,0),(59,1,'分词器','','','1、什么是分词器\r\n\r\n切分词语，normalization（提升recall召回率）\r\n\r\n给你一段句子，然后将这段句子拆分成一个一个的单个的单词，同时对每个单词进行normalization（时态转换，单复数转换），分瓷器\r\nrecall，召回率：搜索的时候，增加能够搜索到的结果的数量\r\n\r\ncharacter filter：在一段文本进行分词之前，先进行预处理，比如说最常见的就是，过滤html标签（&lt;span&gt;hello&lt;span&gt; --&gt; hello），&amp; --&gt; and（I&amp;you --&gt; I and you）\r\ntokenizer：分词，hello you and me --&gt; hello, you, and, me\r\ntoken filter：lowercase，stop word，synonymom，dogs --&gt; dog，liked --&gt; like，Tom --&gt; tom，a/the/an --&gt; 干掉，mother --&gt; mom，small --&gt; little\r\n\r\n一个分词器，很重要，将一段文本进行各种处理，最后处理好的结果才会拿去建立倒排索引\r\n\r\n2、内置分词器的介绍\r\n\r\nSet the shape to semi-transparent by calling set_trans(5)\r\n\r\nstandard analyzer：set, the, shape, to, semi, transparent, by, calling, set_trans, 5（默认的是standard）\r\nsimple analyzer：set, the, shape, to, semi, transparent, by, calling, set, trans\r\nwhitespace analyzer：Set, the, shape, to, semi-transparent, by, calling, set_trans(5)\r\nlanguage analyzer（特定的语言的分词器，比如说，english，英语分词器）：set, shape, semi, transpar, call, set_tran, 5\r\n',8,0,0,1514609194,0,0,0),(60,1,'query string分词','','','1、query string分词\r\n\r\nquery string必须以和index建立时相同的analyzer进行分词\r\nquery string对exact value和full text的区别对待\r\n\r\ndate：exact value\r\n_all：full text\r\n\r\n比如我们有一个document，其中有一个field，包含的value是：hello you and me，建立倒排索引\r\n我们要搜索这个document对应的index，搜索文本是hell me，这个搜索文本就是query string\r\nquery string，默认情况下，es会使用它对应的field建立倒排索引时相同的分词器去进行分词，分词和normalization，只有这样，才能实现正确的搜索\r\n\r\n我们建立倒排索引的时候，将dogs --&gt; dog，结果你搜索的时候，还是一个dogs，那不就搜索不到了吗？所以搜索的时候，那个dogs也必须变成dog才行。才能搜索到。\r\n\r\n知识点：不同类型的field，可能有的就是full text，有的就是exact value\r\n\r\npost_date，date：exact value\r\n_all：full text，分词，normalization\r\n\r\n2、mapping引入案例遗留问题大揭秘\r\n\r\nGET /_search?q=2017\r\n\r\n搜索的是_all field，document所有的field都会拼接成一个大串，进行分词\r\n\r\n2017-01-02 my second article this is my second article in this website 11400\r\n\r\n		doc1		doc2		doc3\r\n2017		*		*		*\r\n01		* 		\r\n02				*\r\n03						*\r\n\r\n_all，2017，自然会搜索到3个docuemnt\r\n\r\nGET /_search?q=2017-01-01\r\n\r\n_all，2017-01-01，query string会用跟建立倒排索引一样的分词器去进行分词\r\n\r\n2017\r\n01\r\n01\r\n\r\nGET /_search?q=post_date:2017-01-01\r\n\r\ndate，会作为exact value去建立索引\r\n\r\n		doc1		doc2		doc3\r\n2017-01-01	*		\r\n2017-01-02			* 		\r\n2017-01-03					*\r\n\r\npost_date:2017-01-01，2017-01-01，doc1一条document\r\n\r\nGET /_search?q=post_date:2017，这个在这里不讲解，因为是es 5.2以后做的一个优化\r\n\r\n\r\n\r\n3、测试分词器\r\n\r\nGET /_analyze\r\n{\r\n  &quot;analyzer&quot;: &quot;standard&quot;,\r\n  &quot;text&quot;: &quot;Text to analyze&quot;\r\n}',8,0,0,1514609247,0,0,0),(61,1,'mapping 介绍','','','（1）往es里面直接插入数据，es会自动建立索引，同时建立type以及对应的mapping\r\n（2）mapping中就自动定义了每个field的数据类型\r\n（3）不同的数据类型（比如说text和date），可能有的是exact value，有的是full text\r\n（4）exact value，在建立倒排索引的时候，分词的时候，是将整个值一起作为一个关键词建立到倒排索引中的；full text，会经历各种各样的处理，分词，normaliztion（时态转换，同义词转换，大小写转换），才会建立到倒排索引中\r\n（5）同时呢，exact value和full text类型的field就决定了，在一个搜索过来的时候，对exact value field或者是full text field进行搜索的行为也是不一样的，会跟建立倒排索引的行为保持一致；比如说exact value搜索的时候，就是直接按照整个值进行匹配，full text query string，也会进行分词和normalization再去倒排索引中去搜索\r\n（6）可以用es的dynamic mapping，让其自动建立mapping，包括自动设置数据类型；也可以提前手动创建index和type的mapping，自己对各个field进行设置，包括数据类型，包括索引行为，包括分词器，等等\r\n\r\nmapping，就是index的type的元数据，每个type都有一个自己的mapping，决定了数据类型，建立倒排索引的行为，还有进行搜索的行为',8,0,0,1514609361,0,0,0),(62,1,'数据类型','','','1、核心的数据类型\r\n\r\nstring\r\nbyte，short，integer，long\r\nfloat，double\r\nboolean\r\ndate\r\n\r\n2、dynamic mapping\r\n\r\ntrue or false	--&gt;	boolean\r\n123		--&gt;	long\r\n123.45		--&gt;	double\r\n2017-01-01	--&gt;	date\r\n&quot;hello world&quot;	--&gt;	string/text\r\n\r\n3、查看mapping\r\n\r\nGET /index/_mapping/type',8,0,0,1514609399,0,0,0),(63,1,'如何建立索引','','','1、如何建立索引\r\n\r\nanalyzed\r\nnot_analyzed\r\nno\r\n\r\n2、修改mapping\r\n\r\n只能创建index时手动建立mapping，或者新增field mapping，但是不能update field mapping\r\n\r\nPUT /website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;article&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;author_id&quot;: {\r\n          &quot;type&quot;: &quot;long&quot;\r\n        },\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;english&quot;\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        },\r\n        &quot;post_date&quot;: {\r\n          &quot;type&quot;: &quot;date&quot;\r\n        },\r\n        &quot;publisher_id&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;index&quot;: &quot;not_analyzed&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;article&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;author_id&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;index_already_exists_exception&quot;,\r\n        &quot;reason&quot;: &quot;index [website/co1dgJ-uTYGBEEOOL8GsQQ] already exists&quot;,\r\n        &quot;index_uuid&quot;: &quot;co1dgJ-uTYGBEEOOL8GsQQ&quot;,\r\n        &quot;index&quot;: &quot;website&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;index_already_exists_exception&quot;,\r\n    &quot;reason&quot;: &quot;index [website/co1dgJ-uTYGBEEOOL8GsQQ] already exists&quot;,\r\n    &quot;index_uuid&quot;: &quot;co1dgJ-uTYGBEEOOL8GsQQ&quot;,\r\n    &quot;index&quot;: &quot;website&quot;\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\nPUT /website/_mapping/article\r\n{\r\n  &quot;properties&quot; : {\r\n    &quot;new_field&quot; : {\r\n      &quot;type&quot; :    &quot;string&quot;,\r\n      &quot;index&quot;:    &quot;not_analyzed&quot;\r\n    }\r\n  }\r\n}\r\n\r\n3、测试mapping\r\n\r\nGET /website/_analyze\r\n{\r\n  &quot;field&quot;: &quot;content&quot;,\r\n  &quot;text&quot;: &quot;my-dogs&quot; \r\n}\r\n\r\nGET website/_analyze\r\n{\r\n  &quot;field&quot;: &quot;new_field&quot;,\r\n  &quot;text&quot;: &quot;my dogs&quot;\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;remote_transport_exception&quot;,\r\n        &quot;reason&quot;: &quot;[4onsTYV][127.0.0.1:9300][indices:admin/analyze[s]]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n    &quot;reason&quot;: &quot;Can&#039;t process field [new_field], Analysis requests are only supported on tokenized fields&quot;\r\n  },\r\n  &quot;status&quot;: 400\r\n}',8,0,0,1514609462,0,0,0),(64,1,'multivalue field','','','1、multivalue field\r\n\r\n{ &quot;tags&quot;: [ &quot;tag1&quot;, &quot;tag2&quot; ]}\r\n\r\n建立索引时与string是一样的，数据类型不能混\r\n\r\n2、empty field\r\n\r\nnull，[]，[null]\r\n\r\n3、object field\r\n\r\nPUT /company/employee/1\r\n{\r\n  &quot;address&quot;: {\r\n    &quot;country&quot;: &quot;china&quot;,\r\n    &quot;province&quot;: &quot;guangdong&quot;,\r\n    &quot;city&quot;: &quot;guangzhou&quot;\r\n  },\r\n  &quot;name&quot;: &quot;jack&quot;,\r\n  &quot;age&quot;: 27,\r\n  &quot;join_date&quot;: &quot;2017-01-01&quot;\r\n}\r\n\r\naddress：object类型\r\n\r\n{\r\n  &quot;company&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;employee&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;address&quot;: {\r\n            &quot;properties&quot;: {\r\n              &quot;city&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              },\r\n              &quot;country&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              },\r\n              &quot;province&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          },\r\n          &quot;age&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          },\r\n          &quot;join_date&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          },\r\n          &quot;name&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;address&quot;: {\r\n    &quot;country&quot;: &quot;china&quot;,\r\n    &quot;province&quot;: &quot;guangdong&quot;,\r\n    &quot;city&quot;: &quot;guangzhou&quot;\r\n  },\r\n  &quot;name&quot;: &quot;jack&quot;,\r\n  &quot;age&quot;: 27,\r\n  &quot;join_date&quot;: &quot;2017-01-01&quot;\r\n}\r\n\r\n{\r\n    &quot;name&quot;:            [jack],\r\n    &quot;age&quot;:          [27],\r\n    &quot;join_date&quot;:      [2017-01-01],\r\n    &quot;address.country&quot;:         [china],\r\n    &quot;address.province&quot;:   [guangdong],\r\n    &quot;address.city&quot;:  [guangzhou]\r\n}\r\n\r\n{\r\n    &quot;authors&quot;: [\r\n        { &quot;age&quot;: 26, &quot;name&quot;: &quot;Jack White&quot;},\r\n        { &quot;age&quot;: 55, &quot;name&quot;: &quot;Tom Jones&quot;},\r\n        { &quot;age&quot;: 39, &quot;name&quot;: &quot;Kitty Smith&quot;}\r\n    ]\r\n}\r\n\r\n{\r\n    &quot;authors.age&quot;:    [26, 55, 39],\r\n    &quot;authors.name&quot;:   [jack, white, tom, jones, kitty, smith]\r\n}',8,0,0,1514609501,0,0,0),(65,1,'search api的基本语法','','','1、search api的基本语法\r\n\r\nGET /search\r\n{}\r\n\r\nGET /index1,index2/type1,type2/search\r\n{}\r\n\r\nGET /_search\r\n{\r\n  &quot;from&quot;: 0,\r\n  &quot;size&quot;: 10\r\n}\r\n\r\n2、http协议中get是否可以带上request body\r\n\r\nHTTP协议，一般不允许get请求带上request body，但是因为get更加适合描述查询数据的操作，因此还是这么用了\r\n\r\nGET /_search?from=0&amp;size=10\r\n\r\nPOST /_search\r\n{\r\n  &quot;from&quot;:0,\r\n  &quot;size&quot;:10\r\n}\r\n\r\n碰巧，很多浏览器，或者是服务器，也都支持GET+request body模式\r\n\r\n如果遇到不支持的场景，也可以用POST /_search',8,0,0,1514609537,0,0,0),(66,1,'Query DSL的基本语法','','','1、一个例子让你明白什么是Query DSL\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_all&quot;: {}\r\n    }\r\n}\r\n\r\n2、Query DSL的基本语法\r\n\r\n{\r\n    QUERY_NAME: {\r\n        ARGUMENT: VALUE,\r\n        ARGUMENT: VALUE,...\r\n    }\r\n}\r\n\r\n{\r\n    QUERY_NAME: {\r\n        FIELD_NAME: {\r\n            ARGUMENT: VALUE,\r\n            ARGUMENT: VALUE,...\r\n        }\r\n    }\r\n}\r\n\r\n示例：\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;test_field&quot;: &quot;test&quot;\r\n    }\r\n  }\r\n}\r\n\r\n3、如何组合多个搜索条件\r\n\r\n搜索需求：title必须包含elasticsearch，content可以包含elasticsearch也可以不包含，author_id必须不为111\r\n\r\n{\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;my hadoop article&quot;,\r\n          &quot;content&quot;: &quot;hadoop is very bad&quot;,\r\n          &quot;author_id&quot;: 111\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;my elasticsearch article&quot;,\r\n          &quot;content&quot;: &quot;es is very bad&quot;,\r\n          &quot;author_id&quot;: 110\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;my elasticsearch article&quot;,\r\n          &quot;content&quot;: &quot;es is very goods&quot;,\r\n          &quot;author_id&quot;: 111\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /website/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;must_not&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;author_id&quot;: 111\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nGET /test_index/_search\r\n{\r\n    &quot;query&quot;: {\r\n            &quot;bool&quot;: {\r\n                &quot;must&quot;: { &quot;match&quot;:   { &quot;name&quot;: &quot;tom&quot; }},\r\n                &quot;should&quot;: [\r\n                    { &quot;match&quot;:       { &quot;hired&quot;: true }},\r\n                    { &quot;bool&quot;: {\r\n                        &quot;must&quot;:      { &quot;match&quot;: { &quot;personality&quot;: &quot;good&quot; }},\r\n                        &quot;must_not&quot;:  { &quot;match&quot;: { &quot;rude&quot;: true }}\r\n                    }}\r\n                ],\r\n                &quot;minimum_should_match&quot;: 1\r\n            }\r\n    }\r\n}',8,0,0,1514609591,0,0,0),(67,1,'filter与query示例','','','1、filter与query示例\r\n\r\nPUT /company/employee/2\r\n{\r\n  &quot;address&quot;: {\r\n    &quot;country&quot;: &quot;china&quot;,\r\n    &quot;province&quot;: &quot;jiangsu&quot;,\r\n    &quot;city&quot;: &quot;nanjing&quot;\r\n  },\r\n  &quot;name&quot;: &quot;tom&quot;,\r\n  &quot;age&quot;: 30,\r\n  &quot;join_date&quot;: &quot;2016-01-01&quot;\r\n}\r\n\r\nPUT /company/employee/3\r\n{\r\n  &quot;address&quot;: {\r\n    &quot;country&quot;: &quot;china&quot;,\r\n    &quot;province&quot;: &quot;shanxi&quot;,\r\n    &quot;city&quot;: &quot;xian&quot;\r\n  },\r\n  &quot;name&quot;: &quot;marry&quot;,\r\n  &quot;age&quot;: 35,\r\n  &quot;join_date&quot;: &quot;2015-01-01&quot;\r\n}\r\n\r\n搜索请求：年龄必须大于等于30，同时join_date必须是2016-01-01\r\n\r\nGET /company/employee/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;join_date&quot;: &quot;2016-01-01&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;gte&quot;: 30\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2、filter与query对比大解密\r\n\r\nfilter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响\r\nquery，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序\r\n\r\n一般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter\r\n除非是你的这些搜索条件，你希望越符合这些搜索条件的document越排在前面返回，那么这些搜索条件要放在query中；如果你不希望一些搜索条件来影响你的document排序，那么就放在filter中即可\r\n\r\n3、filter与query性能\r\n\r\nfilter，不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据\r\nquery，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果\r\n\r\n',8,0,0,1514609622,0,0,0),(68,1,'match all','','','1、match all\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_all&quot;: {}\r\n    }\r\n}\r\n\r\n2、match\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;my elasticsearch article&quot; }}\r\n}\r\n\r\n3、multi match\r\n\r\nGET /test_index/test_type/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;multi_match&quot;: {\r\n      &quot;query&quot;: &quot;test&quot;,\r\n      &quot;fields&quot;: [&quot;test_field&quot;, &quot;test_field1&quot;]\r\n    }\r\n  }\r\n}\r\n\r\n4、range query\r\n\r\nGET /company/employee/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;range&quot;: {\r\n      &quot;age&quot;: {\r\n        &quot;gte&quot;: 30\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n5、term query\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;test_field&quot;: &quot;test hello&quot;\r\n    }\r\n  }\r\n}\r\n\r\n6、terms query\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot;: { &quot;terms&quot;: { &quot;tag&quot;: [ &quot;search&quot;, &quot;full_text&quot;, &quot;nosql&quot; ] }}\r\n}\r\n\r\n7、exist query（2.x中的查询，现在已经不提供了）\r\n',8,0,0,1514609677,0,0,0),(69,1,'bool','','','GET /website/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;must_not&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;author_id&quot;: 111\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n    &quot;bool&quot;: {\r\n        &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }},\r\n        &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;:   &quot;spam&quot; }},\r\n        &quot;should&quot;: [\r\n            { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }}\r\n        ],\r\n        &quot;filter&quot;: {\r\n          &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }} \r\n        }\r\n    }\r\n}\r\n\r\nbool\r\nmust，must_not，should，filter\r\n\r\n每个子查询都会计算一个document针对它的相关度分数，然后bool综合所有分数，合并为一个分数，当然filter是不会计算分数的\r\n\r\n{\r\n    &quot;bool&quot;: {\r\n        &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }},\r\n        &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;:   &quot;spam&quot; }},\r\n        &quot;should&quot;: [\r\n            { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }}\r\n        ],\r\n        &quot;filter&quot;: {\r\n          &quot;bool&quot;: { \r\n              &quot;must&quot;: [\r\n                  { &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }}},\r\n                  { &quot;range&quot;: { &quot;price&quot;: { &quot;lte&quot;: 29.99 }}}\r\n              ],\r\n              &quot;must_not&quot;: [\r\n                  { &quot;term&quot;: { &quot;category&quot;: &quot;ebooks&quot; }}\r\n              ]\r\n          }\r\n        }\r\n    }\r\n}\r\n\r\nGET /company/employee/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;gte&quot;: 30\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514609865,0,0,0),(70,1,'explain','','','GET /test_index/test_type/_validate/query?explain\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;math&quot;: {\r\n      &quot;test_field&quot;: &quot;test&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;valid&quot;: false,\r\n  &quot;error&quot;: &quot;org.elasticsearch.common.ParsingException: no [query] registered for [math]&quot;\r\n}\r\n\r\nGET /test_index/test_type/_validate/query?explain\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;test_field&quot;: &quot;test&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;valid&quot;: true,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;explanations&quot;: [\r\n    {\r\n      &quot;index&quot;: &quot;test_index&quot;,\r\n      &quot;valid&quot;: true,\r\n      &quot;explanation&quot;: &quot;+test_field:test #(#_type:test_type)&quot;\r\n    }\r\n  ]\r\n}\r\n\r\n一般用在那种特别复杂庞大的搜索下，比如你一下子写了上百行的搜索，这个时候可以先用validate api去验证一下，搜索是否合法\r\n',8,0,0,1514609964,0,0,0),(71,1,'排序规则','','','1、默认排序规则\r\n\r\n默认情况下，是按照_score降序排序的\r\n\r\n然而，某些情况下，可能没有有用的_score，比如说filter\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;bool&quot; : {\r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : {\r\n                    &quot;author_id&quot; : 1\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n当然，也可以是constant_score\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : {\r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : {\r\n                    &quot;author_id&quot; : 1\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n2、定制排序规则\r\n\r\nGET /company/employee/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;gte&quot;: 30\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;sort&quot;: [\r\n    {\r\n      &quot;join_date&quot;: {\r\n        &quot;order&quot;: &quot;asc&quot;\r\n      }\r\n    }\r\n  ]\r\n}',8,0,0,1514610176,0,0,0),(72,1,'string field','','','如果对一个string field进行排序，结果往往不准确，因为分词后是多个单词，再排序就不是我们想要的结果了\r\n\r\n通常解决方案是，将一个string field建立两次索引，一个分词，用来进行搜索；一个不分词，用来进行排序\r\n\r\nPUT /website \r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;article&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;fields&quot;: {\r\n            &quot;raw&quot;: {\r\n              &quot;type&quot;: &quot;string&quot;,\r\n              &quot;index&quot;: &quot;not_analyzed&quot;\r\n            }\r\n          },\r\n          &quot;fielddata&quot;: true\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        },\r\n        &quot;post_date&quot;: {\r\n          &quot;type&quot;: &quot;date&quot;\r\n        },\r\n        &quot;author_id&quot;: {\r\n          &quot;type&quot;: &quot;long&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /website/article/1\r\n{\r\n  &quot;title&quot;: &quot;first article&quot;,\r\n  &quot;content&quot;: &quot;this is my second article&quot;,\r\n  &quot;post_date&quot;: &quot;2017-01-01&quot;,\r\n  &quot;author_id&quot;: 110\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;first article&quot;,\r\n          &quot;content&quot;: &quot;this is my first article&quot;,\r\n          &quot;post_date&quot;: &quot;2017-02-01&quot;,\r\n          &quot;author_id&quot;: 110\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;second article&quot;,\r\n          &quot;content&quot;: &quot;this is my second article&quot;,\r\n          &quot;post_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;author_id&quot;: 110\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;third article&quot;,\r\n          &quot;content&quot;: &quot;this is my third article&quot;,\r\n          &quot;post_date&quot;: &quot;2017-03-01&quot;,\r\n          &quot;author_id&quot;: 110\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /website/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_all&quot;: {}\r\n  },\r\n  &quot;sort&quot;: [\r\n    {\r\n      &quot;title.raw&quot;: {\r\n        &quot;order&quot;: &quot;desc&quot;\r\n      }\r\n    }\r\n  ]\r\n}',8,0,0,1514610229,0,0,0),(73,1,'算法','','','1、算法介绍\r\n\r\nrelevance score算法，简单来说，就是计算出，一个索引中的文本，与搜索文本，他们之间的关联匹配程度\r\n\r\nElasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法\r\n\r\nTerm frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关\r\n\r\n搜索请求：hello world\r\n\r\ndoc1：hello you, and world is very good\r\ndoc2：hello, how are you\r\n\r\nInverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关\r\n\r\n搜索请求：hello world\r\n\r\ndoc1：hello, today is very good\r\ndoc2：hi world, how are you\r\n\r\n比如说，在index中有1万条document，hello这个单词在所有的document中，一共出现了1000次；world这个单词在所有的document中，一共出现了100次\r\n\r\ndoc2更相关\r\n\r\nField-length norm：field长度，field越长，相关度越弱\r\n\r\n搜索请求：hello world\r\n\r\ndoc1：{ &quot;title&quot;: &quot;hello article&quot;, &quot;content&quot;: &quot;babaaba 1万个单词&quot; }\r\ndoc2：{ &quot;title&quot;: &quot;my article&quot;, &quot;content&quot;: &quot;blablabala 1万个单词，hi world&quot; }\r\n\r\nhello world在整个index中出现的次数是一样多的\r\n\r\ndoc1更相关，title field更短\r\n\r\n2、_score是如何被计算出来的\r\n\r\nGET /test_index/test_type/_search?explain\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;test_field&quot;: &quot;test hello&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 6,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 4,\r\n    &quot;max_score&quot;: 1.595089,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_shard&quot;: &quot;[test_index][2]&quot;,\r\n        &quot;_node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;20&quot;,\r\n        &quot;_score&quot;: 1.595089,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test hello&quot;\r\n        },\r\n        &quot;_explanation&quot;: {\r\n          &quot;value&quot;: 1.595089,\r\n          &quot;description&quot;: &quot;sum of:&quot;,\r\n          &quot;details&quot;: [\r\n            {\r\n              &quot;value&quot;: 1.595089,\r\n              &quot;description&quot;: &quot;sum of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0.58279467,\r\n                  &quot;description&quot;: &quot;weight(test_field:test in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 0.58279467,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 0.6931472,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 2,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.840795,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.75,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2.56,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1.0122943,\r\n                  &quot;description&quot;: &quot;weight(test_field:hello in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1.0122943,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 1.2039728,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.840795,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.75,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2.56,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            {\r\n              &quot;value&quot;: 0,\r\n              &quot;description&quot;: &quot;match on required clause, product of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0,\r\n                  &quot;description&quot;: &quot;# clause&quot;,\r\n                  &quot;details&quot;: []\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1,\r\n                  &quot;description&quot;: &quot;*:*, product of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;boost&quot;,\r\n                      &quot;details&quot;: []\r\n                    },\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;queryNorm&quot;,\r\n                      &quot;details&quot;: []\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_shard&quot;: &quot;[test_index][2]&quot;,\r\n        &quot;_node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;6&quot;,\r\n        &quot;_score&quot;: 0.58279467,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;tes test&quot;\r\n        },\r\n        &quot;_explanation&quot;: {\r\n          &quot;value&quot;: 0.58279467,\r\n          &quot;description&quot;: &quot;sum of:&quot;,\r\n          &quot;details&quot;: [\r\n            {\r\n              &quot;value&quot;: 0.58279467,\r\n              &quot;description&quot;: &quot;sum of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0.58279467,\r\n                  &quot;description&quot;: &quot;weight(test_field:test in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 0.58279467,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 0.6931472,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 2,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.840795,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.75,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2.56,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            {\r\n              &quot;value&quot;: 0,\r\n              &quot;description&quot;: &quot;match on required clause, product of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0,\r\n                  &quot;description&quot;: &quot;# clause&quot;,\r\n                  &quot;details&quot;: []\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1,\r\n                  &quot;description&quot;: &quot;*:*, product of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;boost&quot;,\r\n                      &quot;details&quot;: []\r\n                    },\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;queryNorm&quot;,\r\n                      &quot;details&quot;: []\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_shard&quot;: &quot;[test_index][3]&quot;,\r\n        &quot;_node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;7&quot;,\r\n        &quot;_score&quot;: 0.5565415,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test client 2&quot;\r\n        },\r\n        &quot;_explanation&quot;: {\r\n          &quot;value&quot;: 0.5565415,\r\n          &quot;description&quot;: &quot;sum of:&quot;,\r\n          &quot;details&quot;: [\r\n            {\r\n              &quot;value&quot;: 0.5565415,\r\n              &quot;description&quot;: &quot;sum of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0.5565415,\r\n                  &quot;description&quot;: &quot;weight(test_field:test in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 0.5565415,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 0.6931472,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.8029196,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 2.5,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            {\r\n              &quot;value&quot;: 0,\r\n              &quot;description&quot;: &quot;match on required clause, product of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0,\r\n                  &quot;description&quot;: &quot;# clause&quot;,\r\n                  &quot;details&quot;: []\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1,\r\n                  &quot;description&quot;: &quot;_type:test_type, product of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;boost&quot;,\r\n                      &quot;details&quot;: []\r\n                    },\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;queryNorm&quot;,\r\n                      &quot;details&quot;: []\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_shard&quot;: &quot;[test_index][1]&quot;,\r\n        &quot;_node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;8&quot;,\r\n        &quot;_score&quot;: 0.25316024,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test client 2&quot;\r\n        },\r\n        &quot;_explanation&quot;: {\r\n          &quot;value&quot;: 0.25316024,\r\n          &quot;description&quot;: &quot;sum of:&quot;,\r\n          &quot;details&quot;: [\r\n            {\r\n              &quot;value&quot;: 0.25316024,\r\n              &quot;description&quot;: &quot;sum of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0.25316024,\r\n                  &quot;description&quot;: &quot;weight(test_field:test in 0) [PerFieldSimilarity], result of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 0.25316024,\r\n                      &quot;description&quot;: &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;,\r\n                      &quot;details&quot;: [\r\n                        {\r\n                          &quot;value&quot;: 0.2876821,\r\n                          &quot;description&quot;: &quot;idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;docFreq&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;docCount&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        },\r\n                        {\r\n                          &quot;value&quot;: 0.88,\r\n                          &quot;description&quot;: &quot;tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:&quot;,\r\n                          &quot;details&quot;: [\r\n                            {\r\n                              &quot;value&quot;: 1,\r\n                              &quot;description&quot;: &quot;termFreq=1.0&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 1.2,\r\n                              &quot;description&quot;: &quot;parameter k1&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 0.75,\r\n                              &quot;description&quot;: &quot;parameter b&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 3,\r\n                              &quot;description&quot;: &quot;avgFieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            },\r\n                            {\r\n                              &quot;value&quot;: 4,\r\n                              &quot;description&quot;: &quot;fieldLength&quot;,\r\n                              &quot;details&quot;: []\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            {\r\n              &quot;value&quot;: 0,\r\n              &quot;description&quot;: &quot;match on required clause, product of:&quot;,\r\n              &quot;details&quot;: [\r\n                {\r\n                  &quot;value&quot;: 0,\r\n                  &quot;description&quot;: &quot;# clause&quot;,\r\n                  &quot;details&quot;: []\r\n                },\r\n                {\r\n                  &quot;value&quot;: 1,\r\n                  &quot;description&quot;: &quot;*:*, product of:&quot;,\r\n                  &quot;details&quot;: [\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;boost&quot;,\r\n                      &quot;details&quot;: []\r\n                    },\r\n                    {\r\n                      &quot;value&quot;: 1,\r\n                      &quot;description&quot;: &quot;queryNorm&quot;,\r\n                      &quot;details&quot;: []\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n3、分析一个document是如何被匹配上的\r\n\r\nGET /test_index/test_type/6/_explain\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;test_field&quot;: &quot;test hello&quot;\r\n    }\r\n  }\r\n}',8,0,0,1514610289,0,0,0),(74,1,'排序','','','搜索的时候，要依靠倒排索引；排序的时候，需要依靠正排索引，看到每个document的每个field，然后进行排序，所谓的正排索引，其实就是doc values\r\n\r\n在建立索引的时候，一方面会建立倒排索引，以供搜索用；一方面会建立正排索引，也就是doc values，以供排序，聚合，过滤等操作使用\r\n\r\ndoc values是被保存在磁盘上的，此时如果内存足够，os会自动将其缓存在内存中，性能还是会很高；如果内存不足够，os会将其写入磁盘上\r\n\r\n\r\ndoc1: hello world you and me\r\ndoc2: hi, world, how are you\r\n\r\nword		doc1		doc2\r\n\r\nhello		*\r\nworld		*		*\r\nyou		*		*\r\nand 		*\r\nme		*\r\nhi				*\r\nhow				*\r\nare				*\r\n\r\nhello you --&gt; hello, you\r\n\r\nhello --&gt; doc1\r\nyou --&gt; doc1,doc2\r\n\r\ndoc1: hello world you and me\r\ndoc2: hi, world, how are you\r\n\r\nsort by age\r\n\r\n\r\ndoc1: { &quot;name&quot;: &quot;jack&quot;, &quot;age&quot;: 27 }\r\ndoc2: { &quot;name&quot;: &quot;tom&quot;, &quot;age&quot;: 30 }\r\n\r\ndocument	name		age\r\n\r\ndoc1		jack		27\r\ndoc2		tom		30	',8,0,0,1514610564,0,0,0),(75,1,'query phase','','','1、query phase\r\n\r\n（1）搜索请求发送到某一个coordinate node，构构建一个priority queue，长度以paging操作from和size为准，默认为10\r\n（2）coordinate node将请求转发到所有shard，每个shard本地搜索，并构建一个本地的priority queue\r\n（3）各个shard将自己的priority queue返回给coordinate node，并构建一个全局的priority queue\r\n\r\n2、replica shard如何提升搜索吞吐量\r\n\r\n一次请求要打到所有shard的一个replica/primary上去，如果每个shard都有多个replica，那么同时并发过来的搜索请求可以同时打到其他的replica上去\r\n',8,0,0,1514610599,0,0,0),(76,1,'fetch phbase','','','1、fetch phbase工作流程\r\n\r\n（1）coordinate node构建完priority queue之后，就发送mget请求去所有shard上获取对应的document\r\n（2）各个shard将document返回给coordinate node\r\n（3）coordinate node将合并后的document结果返回给client客户端\r\n\r\n2、一般搜索，如果不加from和size，就默认搜索前10条，按照_score排序\r\n\r\n',8,0,0,1514610631,0,0,0),(77,1,'preference','','','1、preference\r\n\r\n决定了哪些shard会被用来执行搜索操作\r\n\r\n_primary, _primary_first, _local, _only_node:xyz, _prefer_node:xyz, _shards:2,3\r\n\r\nbouncing results问题，两个document排序，field值相同；不同的shard上，可能排序不同；每次请求轮询打到不同的replica shard上；每次页面上看到的搜索结果的排序都不一样。这就是bouncing result，也就是跳跃的结果。\r\n\r\n搜索的时候，是轮询将搜索请求发送到每一个replica shard（primary shard），但是在不同的shard上，可能document的排序不同\r\n\r\n解决方案就是将preference设置为一个字符串，比如说user_id，让每个user每次搜索的时候，都使用同一个replica shard去执行，就不会看到bouncing results了\r\n\r\n2、timeout，已经讲解过原理了，主要就是限定在一定时间内，将部分获取到的数据直接返回，避免查询耗时过长\r\n\r\n3、routing，document文档路由，_id路由，routing=user_id，这样的话可以让同一个user对应的数据到一个shard上去\r\n\r\n4、search_type\r\n\r\ndefault：query_then_fetch\r\ndfs_query_then_fetch，可以提升revelance sort精准度\r\n',8,0,0,1514610669,0,0,0),(78,1,'scroll','','','如果一次性要查出来比如10万条数据，那么性能会很差，此时一般会采取用scroll滚动查询，一批一批的查，直到所有数据都查询完处理完\r\n\r\n使用scroll滚动搜索，可以先搜索一批数据，然后下次再搜索一批数据，以此类推，直到搜索出全部的数据来\r\nscroll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的\r\n采用基于_doc进行排序的方式，性能较高\r\n每次发送scroll请求，我们还需要指定一个scoll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了\r\n\r\nGET /test_index/test_type/_search?scroll=1m\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_all&quot;: {}\r\n  },\r\n  &quot;sort&quot;: [ &quot;_doc&quot; ],\r\n  &quot;size&quot;: 3\r\n}\r\n\r\n{\r\n  &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACxeFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAALF8WNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACxhFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYhY0b25zVFlWWlRqR3ZJajlfc3BXejJ3&quot;,\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 10,\r\n    &quot;max_score&quot;: null,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;8&quot;,\r\n        &quot;_score&quot;: null,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;test client 2&quot;\r\n        },\r\n        &quot;sort&quot;: [\r\n          0\r\n        ]\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;6&quot;,\r\n        &quot;_score&quot;: null,\r\n        &quot;_source&quot;: {\r\n          &quot;test_field&quot;: &quot;tes test&quot;\r\n        },\r\n        &quot;sort&quot;: [\r\n          0\r\n        ]\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;test_index&quot;,\r\n        &quot;_type&quot;: &quot;test_type&quot;,\r\n        &quot;_id&quot;: &quot;AVp4RN0bhjxldOOnBxaE&quot;,\r\n        &quot;_score&quot;: null,\r\n        &quot;_source&quot;: {\r\n          &quot;test_content&quot;: &quot;my test&quot;\r\n        },\r\n        &quot;sort&quot;: [\r\n          0\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n获得的结果会有一个scoll_id，下一次再发送scoll请求的时候，必须带上这个scoll_id\r\n\r\nGET /_search/scroll\r\n{\r\n    &quot;scroll&quot;: &quot;1m&quot;, \r\n    &quot;scroll_id&quot; : &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACxeFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAALF8WNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACxhFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYhY0b25zVFlWWlRqR3ZJajlfc3BXejJ3&quot;\r\n}\r\n\r\n11,4,7\r\n3,2,1\r\n20\r\n\r\nscoll，看起来挺像分页的，但是其实使用场景不一样。分页主要是用来一页一页搜索，给用户看的；scoll主要是用来一批一批检索数据，让系统进行处理的\r\n',8,0,0,1514610727,0,0,0),(79,1,'创建索引','','','1、为什么我们要手动创建索引？\r\n\r\n2、创建索引\r\n\r\n创建索引的语法\r\n\r\nPUT /my_index\r\n{\r\n    &quot;settings&quot;: { ... any settings ... },\r\n    &quot;mappings&quot;: {\r\n        &quot;type_one&quot;: { ... any mappings ... },\r\n        &quot;type_two&quot;: { ... any mappings ... },\r\n        ...\r\n    }\r\n}\r\n\r\n创建索引的示例\r\n\r\nPUT /my_index\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;number_of_shards&quot;: 1,\r\n    &quot;number_of_replicas&quot;: 0\r\n  },\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;my_field&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n3、修改索引\r\n\r\nPUT /my_index/_settings\r\n{\r\n    &quot;number_of_replicas&quot;: 1\r\n}\r\n\r\n4、删除索引\r\n\r\nDELETE /my_index\r\nDELETE /index_one,index_two\r\nDELETE /index_*\r\nDELETE /_all\r\n\r\nelasticsearch.yml\r\naction.destructive_requires_name: true\r\n\r\n',8,0,0,1514610832,0,0,0),(80,1,'分词器二','','','1、默认的分词器\r\n\r\nstandard\r\n\r\nstandard tokenizer：以单词边界进行切分\r\nstandard token filter：什么都不做\r\nlowercase token filter：将所有字母转换为小写\r\nstop token filer（默认被禁用）：移除停用词，比如a the it等等\r\n\r\n2、修改分词器的设置\r\n\r\n启用english停用词token filter\r\n\r\nPUT /my_index\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;analysis&quot;: {\r\n      &quot;analyzer&quot;: {\r\n        &quot;es_std&quot;: {\r\n          &quot;type&quot;: &quot;standard&quot;,\r\n          &quot;stopwords&quot;: &quot;_english_&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;analyzer&quot;: &quot;standard&quot;, \r\n  &quot;text&quot;: &quot;a dog is in the house&quot;\r\n}\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;analyzer&quot;: &quot;es_std&quot;,\r\n  &quot;text&quot;:&quot;a dog is in the house&quot;\r\n}\r\n\r\n3、定制化自己的分词器\r\n\r\nPUT /my_index\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;analysis&quot;: {\r\n      &quot;char_filter&quot;: {\r\n        &quot;&amp;_to_and&quot;: {\r\n          &quot;type&quot;: &quot;mapping&quot;,\r\n          &quot;mappings&quot;: [&quot;&amp;=&gt; and&quot;]\r\n        }\r\n      },\r\n      &quot;filter&quot;: {\r\n        &quot;my_stopwords&quot;: {\r\n          &quot;type&quot;: &quot;stop&quot;,\r\n          &quot;stopwords&quot;: [&quot;the&quot;, &quot;a&quot;]\r\n        }\r\n      },\r\n      &quot;analyzer&quot;: {\r\n        &quot;my_analyzer&quot;: {\r\n          &quot;type&quot;: &quot;custom&quot;,\r\n          &quot;char_filter&quot;: [&quot;html_strip&quot;, &quot;&amp;_to_and&quot;],\r\n          &quot;tokenizer&quot;: &quot;standard&quot;,\r\n          &quot;filter&quot;: [&quot;lowercase&quot;, &quot;my_stopwords&quot;]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;text&quot;: &quot;tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!&quot;,\r\n  &quot;analyzer&quot;: &quot;my_analyzer&quot;\r\n}\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;content&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;analyzer&quot;: &quot;my_analyzer&quot;\r\n    }\r\n  }\r\n}',8,0,0,1514610880,0,0,0),(81,1,'type','','','type，是一个index中用来区分类似的数据的，类似的数据，但是可能有不同的fields，而且有不同的属性来控制索引建立、分词器\r\nfield的value，在底层的lucene中建立索引的时候，全部是opaque bytes类型，不区分类型的\r\nlucene是没有type的概念的，在document中，实际上将type作为一个document的field来存储，即_type，es通过_type来进行type的过滤和筛选\r\n一个index中的多个type，实际上是放在一起存储的，因此一个index下，不能有多个type重名，而类型或者其他设置不同的，因为那样是无法处理的\r\n\r\n{\r\n   &quot;ecommerce&quot;: {\r\n      &quot;mappings&quot;: {\r\n         &quot;elactronic_goods&quot;: {\r\n            &quot;properties&quot;: {\r\n               &quot;name&quot;: {\r\n                  &quot;type&quot;: &quot;string&quot;,\r\n               },\r\n               &quot;price&quot;: {\r\n                  &quot;type&quot;: &quot;double&quot;\r\n               },\r\n	       &quot;service_period&quot;: {\r\n		  &quot;type&quot;: &quot;string&quot;\r\n	       }			\r\n            }\r\n         },\r\n         &quot;fresh_goods&quot;: {\r\n            &quot;properties&quot;: {\r\n               &quot;name&quot;: {\r\n                  &quot;type&quot;: &quot;string&quot;,\r\n               },\r\n               &quot;price&quot;: {\r\n                  &quot;type&quot;: &quot;double&quot;\r\n               },\r\n	       &quot;eat_period&quot;: {\r\n		  &quot;type&quot;: &quot;string&quot;\r\n	       }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n{\r\n  &quot;name&quot;: &quot;geli kongtiao&quot;,\r\n  &quot;price&quot;: 1999.0,\r\n  &quot;service_period&quot;: &quot;one year&quot;\r\n}\r\n\r\n{\r\n  &quot;name&quot;: &quot;aozhou dalongxia&quot;,\r\n  &quot;price&quot;: 199.0,\r\n  &quot;eat_period&quot;: &quot;one week&quot;\r\n}\r\n\r\n在底层的存储是这样子的。。。。\r\n\r\n{\r\n   &quot;ecommerce&quot;: {\r\n      &quot;mappings&quot;: {\r\n        &quot;_type&quot;: {\r\n          &quot;type&quot;: &quot;string&quot;,\r\n          &quot;index&quot;: &quot;not_analyzed&quot;\r\n        },\r\n        &quot;name&quot;: {\r\n          &quot;type&quot;: &quot;string&quot;\r\n        }\r\n        &quot;price&quot;: {\r\n          &quot;type&quot;: &quot;double&quot;\r\n        }\r\n        &quot;service_period&quot;: {\r\n          &quot;type&quot;: &quot;string&quot;\r\n        }\r\n        &quot;eat_period&quot;: {\r\n          &quot;type&quot;: &quot;string&quot;\r\n        }\r\n      }\r\n   }\r\n}\r\n\r\n{\r\n  &quot;_type&quot;: &quot;elactronic_goods&quot;,\r\n  &quot;name&quot;: &quot;geli kongtiao&quot;,\r\n  &quot;price&quot;: 1999.0,\r\n  &quot;service_period&quot;: &quot;one year&quot;,\r\n  &quot;eat_period&quot;: &quot;&quot;\r\n}\r\n\r\n{\r\n  &quot;_type&quot;: &quot;fresh_goods&quot;,\r\n  &quot;name&quot;: &quot;aozhou dalongxia&quot;,\r\n  &quot;price&quot;: 199.0,\r\n  &quot;service_period&quot;: &quot;&quot;,\r\n  &quot;eat_period&quot;: &quot;one week&quot;\r\n}\r\n\r\n\r\n最佳实践，将类似结构的type放在一个index下，这些type应该有多个field是相同的\r\n假如说，你将两个type的field完全不同，放在一个index下，那么就每条数据都至少有一半的field在底层的lucene中是空值，会有严重的性能问题\r\n\r\n',8,0,0,1514610919,0,0,0),(82,1,'root object','','','1、root object\r\n\r\n就是某个type对应的mapping json，包括了properties，metadata（_id，_source，_type），settings（analyzer），其他settings（比如include_in_all）\r\n\r\nPUT /my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {}\r\n    }\r\n  }\r\n}\r\n\r\n2、properties\r\n\r\ntype，index，analyzer\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;title&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;\r\n    }\r\n  }\r\n}\r\n\r\n3、_source\r\n\r\n好处\r\n\r\n（1）查询的时候，直接可以拿到完整的document，不需要先拿document id，再发送一次请求拿document\r\n（2）partial update基于_source实现\r\n（3）reindex时，直接基于_source实现，不需要从数据库（或者其他外部存储）查询数据再修改\r\n（4）可以基于_source定制返回field\r\n（5）debug query更容易，因为可以直接看到_source\r\n\r\n如果不需要上述好处，可以禁用_source\r\n\r\nPUT /my_index/_mapping/my_type2\r\n{\r\n  &quot;_source&quot;: {&quot;enabled&quot;: false}\r\n}\r\n\r\n4、_all\r\n\r\n将所有field打包在一起，作为一个_all field，建立索引。没指定任何field进行搜索时，就是使用_all field在搜索。\r\n\r\nPUT /my_index/_mapping/my_type3\r\n{\r\n  &quot;_all&quot;: {&quot;enabled&quot;: false}\r\n}\r\n\r\n也可以在field级别设置include_in_all field，设置是否要将field的值包含在_all field中\r\n\r\nPUT /my_index/_mapping/my_type4\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;my_field&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;include_in_all&quot;: false\r\n    }\r\n  }\r\n}\r\n\r\n5、标识性metadata\r\n\r\n_index，_type，_id\r\n',8,0,0,1514610967,0,0,0),(83,1,'dynamic策略','','','1、定制dynamic策略\r\n\r\ntrue：遇到陌生字段，就进行dynamic mapping\r\nfalse：遇到陌生字段，就忽略\r\nstrict：遇到陌生字段，就报错\r\n\r\nPUT /my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;dynamic&quot;: &quot;strict&quot;,\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        },\r\n        &quot;address&quot;: {\r\n          &quot;type&quot;: &quot;object&quot;,\r\n          &quot;dynamic&quot;: &quot;true&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;title&quot;: &quot;my article&quot;,\r\n  &quot;content&quot;: &quot;this is my article&quot;,\r\n  &quot;address&quot;: {\r\n    &quot;province&quot;: &quot;guangdong&quot;,\r\n    &quot;city&quot;: &quot;guangzhou&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;,\r\n        &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;,\r\n    &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed&quot;\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;title&quot;: &quot;my article&quot;,\r\n  &quot;address&quot;: {\r\n    &quot;province&quot;: &quot;guangdong&quot;,\r\n    &quot;city&quot;: &quot;guangzhou&quot;\r\n  }\r\n}\r\n\r\nGET /my_index/_mapping/my_type\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;dynamic&quot;: &quot;strict&quot;,\r\n        &quot;properties&quot;: {\r\n          &quot;address&quot;: {\r\n            &quot;dynamic&quot;: &quot;true&quot;,\r\n            &quot;properties&quot;: {\r\n              &quot;city&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              },\r\n              &quot;province&quot;: {\r\n                &quot;type&quot;: &quot;text&quot;,\r\n                &quot;fields&quot;: {\r\n                  &quot;keyword&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;,\r\n                    &quot;ignore_above&quot;: 256\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          },\r\n          &quot;title&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2、定制dynamic mapping策略\r\n\r\n（1）date_detection\r\n\r\n默认会按照一定格式识别date，比如yyyy-MM-dd。但是如果某个field先过来一个2017-01-01的值，就会被自动dynamic mapping成date，后面如果再来一个&quot;hello world&quot;之类的值，就会报错。可以手动关闭某个type的date_detection，如果有需要，自己手动指定某个field为date类型。\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n    &quot;date_detection&quot;: false\r\n}\r\n\r\n（2）定制自己的dynamic mapping template（type level）\r\n\r\nPUT /my_index\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;my_type&quot;: {\r\n            &quot;dynamic_templates&quot;: [\r\n                { &quot;en&quot;: {\r\n                      &quot;match&quot;:              &quot;*_en&quot;, \r\n                      &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n                      &quot;mapping&quot;: {\r\n                          &quot;type&quot;:           &quot;string&quot;,\r\n                          &quot;analyzer&quot;:       &quot;english&quot;\r\n                      }\r\n                }}\r\n            ]\r\n}}}\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;title&quot;: &quot;this is my first article&quot;\r\n}\r\n\r\nPUT /my_index/my_type/2\r\n{\r\n  &quot;title_en&quot;: &quot;this is my first article&quot;\r\n}\r\n\r\ntitle没有匹配到任何的dynamic模板，默认就是standard分词器，不会过滤停用词，is会进入倒排索引，用is来搜索是可以搜索到的\r\ntitle_en匹配到了dynamic模板，就是english分词器，会过滤停用词，is这种停用词就会被过滤掉，用is来搜索就搜索不到了\r\n\r\n（3）定制自己的default mapping template（index level）\r\n\r\nPUT /my_index\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;_default_&quot;: {\r\n            &quot;_all&quot;: { &quot;enabled&quot;:  false }\r\n        },\r\n        &quot;blog&quot;: {\r\n            &quot;_all&quot;: { &quot;enabled&quot;:  true  }\r\n        }\r\n    }\r\n}',8,0,0,1514611015,0,0,0),(84,1,'重建索引','','','1、重建索引\r\n\r\n一个field的设置是不能被修改的，如果要修改一个Field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入index中\r\n\r\n批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scoll就查询指定日期的一段数据，交给一个线程即可\r\n\r\n（1）一开始，依靠dynamic mapping，插入数据，但是不小心有些数据是2017-01-01这种日期格式的，所以title这种field被自动映射为了date类型，实际上它应该是string类型的\r\n\r\nPUT /my_index/my_type/3\r\n{\r\n  &quot;title&quot;: &quot;2017-01-03&quot;\r\n}\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;title&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n（2）当后期向索引中加入string类型的title值的时候，就会报错\r\n\r\nPUT /my_index/my_type/4\r\n{\r\n  &quot;title&quot;: &quot;my first article&quot;\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;mapper_parsing_exception&quot;,\r\n        &quot;reason&quot;: &quot;failed to parse [title]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;mapper_parsing_exception&quot;,\r\n    &quot;reason&quot;: &quot;failed to parse [title]&quot;,\r\n    &quot;caused_by&quot;: {\r\n      &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n      &quot;reason&quot;: &quot;Invalid format: \\&quot;my first article\\&quot;&quot;\r\n    }\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\n（3）如果此时想修改title的类型，是不可能的\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;title&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n        &quot;reason&quot;: &quot;mapper [title] of different type, current_type [date], merged_type [text]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n    &quot;reason&quot;: &quot;mapper [title] of different type, current_type [date], merged_type [text]&quot;\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\n（4）此时，唯一的办法，就是进行reindex，也就是说，重新建立一个索引，将旧索引的数据查询出来，再导入新索引\r\n\r\n（5）如果说旧索引的名字，是old_index，新索引的名字是new_index，终端java应用，已经在使用old_index在操作了，难道还要去停止java应用，修改使用的index为new_index，才重新启动java应用吗？这个过程中，就会导致java应用停机，可用性降低\r\n\r\n（6）所以说，给java应用一个别名，这个别名是指向旧索引的，java应用先用着，java应用先用goods_index alias来操作，此时实际指向的是旧的my_index\r\n\r\nPUT /my_index/_alias/goods_index\r\n\r\n（7）新建一个index，调整其title的类型为string\r\n\r\nPUT /my_index_new\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n（8）使用scroll api将数据批量查询出来\r\n\r\nGET /my_index/_search?scroll=1m\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_all&quot;: {}\r\n    },\r\n    &quot;sort&quot;: [&quot;_doc&quot;],\r\n    &quot;size&quot;:  1\r\n}\r\n\r\n{\r\n  &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAADpAFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAA6QRY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAAOkIWNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAADpDFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAA6RBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3&quot;,\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: null,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;my_index&quot;,\r\n        &quot;_type&quot;: &quot;my_type&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: null,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;2017-01-02&quot;\r\n        },\r\n        &quot;sort&quot;: [\r\n          0\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n（9）采用bulk api将scoll查出来的一批数据，批量写入新索引\r\n\r\nPOST /_bulk\r\n{ &quot;index&quot;:  { &quot;_index&quot;: &quot;my_index_new&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;title&quot;:    &quot;2017-01-02&quot; }\r\n\r\n（10）反复循环8~9，查询一批又一批的数据出来，采取bulk api将每一批数据批量写入新索引\r\n\r\n（11）将goods_index alias切换到my_index_new上去，java应用会直接通过index别名使用新的索引中的数据，java应用程序不需要停机，零提交，高可用\r\n\r\nPOST /_aliases\r\n{\r\n    &quot;actions&quot;: [\r\n        { &quot;remove&quot;: { &quot;index&quot;: &quot;my_index&quot;, &quot;alias&quot;: &quot;goods_index&quot; }},\r\n        { &quot;add&quot;:    { &quot;index&quot;: &quot;my_index_new&quot;, &quot;alias&quot;: &quot;goods_index&quot; }}\r\n    ]\r\n}\r\n\r\n（12）直接通过goods_index别名来查询，是否ok\r\n\r\nGET /goods_index/my_type/_search\r\n\r\n2、基于alias对client透明切换index\r\n\r\nPUT /my_index_v1/_alias/my_index\r\n\r\nclient对my_index进行操作\r\n\r\nreindex操作，完成之后，切换v1到v2\r\n\r\nPOST /_aliases\r\n{\r\n    &quot;actions&quot;: [\r\n        { &quot;remove&quot;: { &quot;index&quot;: &quot;my_index_v1&quot;, &quot;alias&quot;: &quot;my_index&quot; }},\r\n        { &quot;add&quot;:    { &quot;index&quot;: &quot;my_index_v2&quot;, &quot;alias&quot;: &quot;my_index&quot; }}\r\n    ]\r\n}',8,0,0,1514611132,0,0,0),(85,1,'倒排索引','','','倒排索引，是适合用于进行搜索的\r\n\r\n倒排索引的结构\r\n\r\n（1）包含这个关键词的document list\r\n（2）包含这个关键词的所有document的数量：IDF（inverse document frequency）\r\n（3）这个关键词在每个document中出现的次数：TF（term frequency）\r\n（4）这个关键词在这个document中的次序\r\n（5）每个document的长度：length norm\r\n（6）包含这个关键词的所有document的平均长度\r\n\r\nword		doc1		doc2\r\n\r\ndog		*		*\r\nhello		*\r\nyou				*\r\n\r\n倒排索引不可变的好处\r\n\r\n（1）不需要锁，提升并发能力，避免锁的问题\r\n（2）数据不变，一直保存在os cache中，只要cache内存足够\r\n（3）filter cache一直驻留在内存，因为数据不变\r\n（4）可以压缩，节省cpu和io开销\r\n\r\n倒排索引不可变的坏处：每次都要重新构建整个索引',8,0,0,1514611208,0,0,0),(86,1,'commit point','','','（1）数据写入buffer\r\n（2）commit point\r\n（3）buffer中的数据写入新的index segment\r\n（4）等待在os cache中的index segment被fsync强制刷到磁盘上\r\n（5）新的index sgement被打开，供search使用\r\n（6）buffer被清空\r\n\r\n每次commit point时，会有一个.del文件，标记了哪些segment中的哪些document被标记为deleted了\r\n搜索的时候，会依次查询所有的segment，从旧的到新的，比如被修改过的document，在旧的segment中，会标记为deleted，在新的segment中会有其新的数据\r\n',8,0,0,1514611241,0,0,0),(87,1,'写入流程','','','现有流程的问题，每次都必须等待fsync将segment刷入磁盘，才能将segment打开供search使用，这样的话，从一个document写入，到它可以被搜索，可能会超过1分钟！！！这就不是近实时的搜索了！！！主要瓶颈在于fsync实际发生磁盘IO写数据进磁盘，是很耗时的。\r\n\r\n写入流程别改进如下：\r\n\r\n（1）数据写入buffer\r\n（2）每隔一定时间，buffer中的数据被写入segment文件，但是先写入os cache\r\n（3）只要segment写入os cache，那就直接打开供search使用，不立即执行commit\r\n\r\n数据写入os cache，并被打开供搜索的过程，叫做refresh，默认是每隔1秒refresh一次。也就是说，每隔一秒就会将buffer中的数据写入一个新的index segment file，先写入os cache中。所以，es是近实时的，数据写入到可以被搜索，默认是1秒。\r\n\r\nPOST /my_index/_refresh，可以手动refresh，一般不需要手动执行，没必要，让es自己搞就可以了\r\n\r\n比如说，我们现在的时效性要求，比较低，只要求一条数据写入es，一分钟以后才让我们搜索到就可以了，那么就可以调整refresh interval\r\n\r\nPUT /my_index\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;refresh_interval&quot;: &quot;30s&quot; \r\n  }\r\n}\r\n\r\ncommit。。。稍后就会讲。。。',8,0,0,1514611275,0,0,0),(88,1,'优化的写入流程','','','再次优化的写入流程\r\n\r\n（1）数据写入buffer缓冲和translog日志文件\r\n（2）每隔一秒钟，buffer中的数据被写入新的segment file，并进入os cache，此时segment被打开并供search使用\r\n（3）buffer被清空\r\n（4）重复1~3，新的segment不断添加，buffer不断被清空，而translog中的数据不断累加\r\n（5）当translog长度达到一定程度的时候，commit操作发生\r\n  （5-1）buffer中的所有数据写入一个新的segment，并写入os cache，打开供使用\r\n  （5-2）buffer被清空\r\n  （5-3）一个commit ponit被写入磁盘，标明了所有的index segment\r\n  （5-4）filesystem cache中的所有index segment file缓存数据，被fsync强行刷到磁盘上\r\n  （5-5）现有的translog被清空，创建一个新的translog\r\n\r\n基于translog和commit point，如何进行数据恢复\r\n\r\nfsync+清空translog，就是flush，默认每隔30分钟flush一次，或者当translog过大的时候，也会flush\r\n\r\nPOST /my_index/_flush，一般来说别手动flush，让它自动执行就可以了\r\n\r\ntranslog，每隔5秒被fsync一次到磁盘上。在一次增删改操作之后，当fsync在primary shard和replica shard都成功之后，那次增删改操作才会成功\r\n\r\n但是这种在一次增删改时强行fsync translog可能会导致部分操作比较耗时，也可以允许部分数据丢失，设置异步fsync translog\r\n\r\nPUT /my_index/_settings\r\n{\r\n    &quot;index.translog.durability&quot;: &quot;async&quot;,\r\n    &quot;index.translog.sync_interval&quot;: &quot;5s&quot;\r\n}',8,0,0,1514611329,0,0,0),(89,1,'segment file','','','每秒一个segment file，文件过多，而且每次search都要搜索所有的segment，很耗时\r\n\r\n默认会在后台执行segment merge操作，在merge的时候，被标记为deleted的document也会被彻底物理删除\r\n\r\n每次merge操作的执行流程\r\n\r\n（1）选择一些有相似大小的segment，merge成一个大的segment\r\n（2）将新的segment flush到磁盘上去\r\n（3）写一个新的commit point，包括了新的segment，并且排除旧的那些segment\r\n（4）将新的segment打开供搜索\r\n（5）将旧的segment删除\r\n\r\nPOST /my_index/_optimize?max_num_segments=1，尽量不要手动执行，让它自动默认执行就可以了',8,0,0,1514611461,0,0,0),(90,1,'java api 操作document','','','强调一下，我们的es讲课的风格\r\n\r\n1、es这门技术有点特殊，跟比如其他的像纯java的课程，比如分布式课程，或者大数据类的课程，比如hadoop，spark，storm等。不太一样\r\n\r\n2、es非常重要的一个api，是它的restful api，你自己思考一下，掌握这个es的restful api，可以让你执行一些核心的运维管理的操作，比如说创建索引，维护索引，执行各种refresh、flush、optimize操作，查看集群的健康状况，比如还有其他的一些操作，就不在这里枚举了。或者说探查一些数据，可能用java api并不方便。\r\n\r\n3、es的学习，首先，你必须学好restful api，然后才是你自己的熟悉语言的api，java api。\r\n\r\n这个《核心知识篇（上半季）》，其实主要还是打基础，包括核心的原理，还有核心的操作，还有部分高级的技术和操作，大量的实验，大量的画图，最后初步讲解怎么使用java api\r\n\r\n《核心知识篇（下半季）》，包括深度讲解搜索这块技术，还有聚合分析这块技术，包括数据建模，包括java api的复杂使用，有一个项目实战s\r\n\r\n员工信息\r\n\r\n姓名\r\n年龄\r\n职位\r\n国家\r\n入职日期\r\n薪水\r\n\r\n我是默认大家至少有java基础的，如果你java一点都不会，请先自己补一下\r\n\r\n1、maven依赖\r\n\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\r\n    &lt;artifactId&gt;transport&lt;/artifactId&gt;\r\n    &lt;version&gt;5.2.2&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\r\n    &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;\r\n    &lt;version&gt;2.7&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\r\n    &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;\r\n    &lt;version&gt;2.7&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n\r\nlog4j2.properties\r\n\r\nappender.console.type = Console\r\nappender.console.name = console\r\nappender.console.layout.type = PatternLayout\r\n\r\nrootLogger.level = info\r\nrootLogger.appenderRef.console.ref = console\r\n\r\n2、构建client\r\n\r\nSettings settings = Settings.builder()\r\n        .put(&quot;cluster.name&quot;, &quot;myClusterName&quot;).build();\r\nTransportClient client = new PreBuiltTransportClient(settings);\r\n\r\nTransportClient client = new PreBuiltTransportClient(Settings.EMPTY)\r\n        .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;host1&quot;), 9300))\r\n        .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;host2&quot;), 9300));\r\n\r\nclient.close();\r\n\r\n3、创建document\r\n\r\nIndexResponse response = client.prepareIndex(&quot;index&quot;, &quot;type&quot;, &quot;1&quot;)\r\n        .setSource(jsonBuilder()\r\n                    .startObject()\r\n                        .field(&quot;user&quot;, &quot;kimchy&quot;)\r\n                        .field(&quot;postDate&quot;, new Date())\r\n                        .field(&quot;message&quot;, &quot;trying out Elasticsearch&quot;)\r\n                    .endObject()\r\n                  )\r\n        .get();\r\n\r\n4、查询document\r\n\r\nGetResponse response = client.prepareGet(&quot;index&quot;, &quot;type&quot;, &quot;1&quot;).get();\r\n\r\n5、修改document\r\n\r\nclient.prepareUpdate(&quot;index&quot;, &quot;type&quot;, &quot;1&quot;)\r\n        .setDoc(jsonBuilder()               \r\n            .startObject()\r\n                .field(&quot;gender&quot;, &quot;male&quot;)\r\n            .endObject())\r\n        .get();\r\n\r\n6、删除document\r\n\r\nDeleteResponse response = client.prepareDelete(&quot;index&quot;, &quot;type&quot;, &quot;1&quot;).get();\r\n',8,0,0,1514611530,0,0,0),(91,1,'java api 分页','','','SearchResponse response = client.prepareSearch(&quot;index1&quot;, &quot;index2&quot;)\r\n        .setTypes(&quot;type1&quot;, &quot;type2&quot;)\r\n        .setQuery(QueryBuilders.termQuery(&quot;multi&quot;, &quot;test&quot;))                 // Query\r\n        .setPostFilter(QueryBuilders.rangeQuery(&quot;age&quot;).from(12).to(18))     // Filter\r\n        .setFrom(0).setSize(60)\r\n        .get();\r\n\r\n需求：\r\n\r\n（1）搜索职位中包含technique的员工\r\n（2）同时要求age在30到40岁之间\r\n（3）分页查询，查找第一页\r\n\r\nGET /company/employee/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;position&quot;: &quot;technique&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;gte&quot;: 30,\r\n            &quot;lte&quot;: 40\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;from&quot;: 0,\r\n  &quot;size&quot;: 1\r\n}\r\n\r\n告诉大家，为什么刚才一边运行创建document，一边搜索什么都没搜索到？？？？\r\n\r\n近实时！！！\r\n\r\n默认是1秒以后，写入es的数据，才能被搜索到。很明显刚才，写入数据不到一秒，我门就在搜索。\r\n\r\n',8,0,0,1514611581,0,0,0),(92,1,'java api 分组','','','SearchResponse sr = node.client().prepareSearch()\r\n    .addAggregation(\r\n        AggregationBuilders.terms(&quot;by_country&quot;).field(&quot;country&quot;)\r\n        .subAggregation(AggregationBuilders.dateHistogram(&quot;by_year&quot;)\r\n            .field(&quot;dateOfBirth&quot;)\r\n            .dateHistogramInterval(DateHistogramInterval.YEAR)\r\n            .subAggregation(AggregationBuilders.avg(&quot;avg_children&quot;).field(&quot;children&quot;))\r\n        )\r\n    )\r\n    .execute().actionGet();\r\n\r\n我们先给个需求：\r\n\r\n（1）首先按照country国家来进行分组\r\n（2）然后在每个country分组内，再按照入职年限进行分组\r\n（3）最后计算每个分组内的平均薪资\r\n\r\nPUT /company\r\n{\r\n  &quot;mappings&quot;: {\r\n      &quot;employee&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;age&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          },\r\n          &quot;country&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            },\r\n            &quot;fielddata&quot;: true\r\n          },\r\n          &quot;join_date&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          },\r\n          &quot;name&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          },\r\n          &quot;position&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          },\r\n          &quot;salary&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n}\r\n\r\nGET /company/employee/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_country&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;country&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_join_date&quot;: {\r\n          &quot;date_histogram&quot;: {\r\n            &quot;field&quot;: &quot;join_date&quot;,\r\n            &quot;interval&quot;: &quot;year&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;avg_salary&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;salary&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nMap&lt;String, Aggregation&gt; aggrMap = searchResponse.getAggregations().asMap();\r\n		StringTerms groupByCountry = (StringTerms) aggrMap.get(&quot;group_by_country&quot;);\r\n		Iterator&lt;Bucket&gt; groupByCountryBucketIterator = groupByCountry.getBuckets().iterator();\r\n		\r\n		while(groupByCountryBucketIterator.hasNext()) {\r\n			Bucket groupByCountryBucket = groupByCountryBucketIterator.next();\r\n			\r\n			System.out.println(groupByCountryBucket.getKey() + &quot;\\t&quot; + groupByCountryBucket.getDocCount()); \r\n			\r\n			Histogram groupByJoinDate = (Histogram) groupByCountryBucket.getAggregations().asMap().get(&quot;group_by_join_date&quot;); \r\n			Iterator&lt;org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket&gt; groupByJoinDateBucketIterator = groupByJoinDate.getBuckets().iterator();\r\n			 \r\n			while(groupByJoinDateBucketIterator.hasNext()) {\r\n				org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket groupByJoinDateBucket = groupByJoinDateBucketIterator.next();\r\n				\r\n				System.out.println(groupByJoinDateBucket.getKey() + &quot;\\t&quot; + groupByJoinDateBucket.getDocCount()); \r\n				\r\n				Avg avgSalary = (Avg) groupByJoinDateBucket.getAggregations().asMap().get(&quot;avg_salary&quot;);\r\n				System.out.println(avgSalary.getValue()); \r\n			}\r\n		}\r\n		\r\n		client.close();\r\n	}',8,0,0,1514611634,0,0,0),(93,1,'搜索帖子','','','1、根据用户ID、是否隐藏、帖子ID、发帖日期来搜索帖子\r\n\r\n（1）插入一些测试帖子数据\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 1 }}\r\n{ &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 2 }}\r\n{ &quot;articleID&quot; : &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 3 }}\r\n{ &quot;articleID&quot; : &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 4 }}\r\n{ &quot;articleID&quot; : &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot; }\r\n\r\n初步来说，就先搞4个字段，因为整个es是支持json document格式的，所以说扩展性和灵活性非常之好。如果后续随着业务需求的增加，要在document中增加更多的field，那么我们可以很方便的随时添加field。但是如果是在关系型数据库中，比如mysql，我们建立了一个表，现在要给表中新增一些column，那就很坑爹了，必须用复杂的修改表结构的语法去执行。而且可能对系统代码还有一定的影响。\r\n\r\nGET /forum/_mapping/article\r\n\r\n{\r\n  &quot;forum&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;article&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;articleID&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          },\r\n          &quot;hidden&quot;: {\r\n            &quot;type&quot;: &quot;boolean&quot;\r\n          },\r\n          &quot;postDate&quot;: {\r\n            &quot;type&quot;: &quot;date&quot;\r\n          },\r\n          &quot;userID&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n现在es 5.2版本，type=text，默认会设置两个field，一个是field本身，比如articleID，就是分词的；还有一个的话，就是field.keyword，articleID.keyword，默认不分词，会最多保留256个字符\r\n\r\n（2）根据用户ID搜索帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;userID&quot; : 1\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nterm filter/query：对搜索文本不分词，直接拿去倒排索引中匹配，你输入的是什么，就去匹配什么\r\n比如说，如果对搜索文本进行分词的话，“helle world” --&gt; “hello”和“world”，两个词分别去倒排索引中匹配\r\nterm，“hello world” --&gt; “hello world”，直接去倒排索引中匹配“hello world”\r\n\r\n（3）搜索没有隐藏的帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;hidden&quot; : false\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n（4）根据发帖日期搜索帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;postDate&quot; : &quot;2017-01-01&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n（5）根据帖子ID搜索帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 0,\r\n    &quot;max_score&quot;: null,\r\n    &quot;hits&quot;: []\r\n  }\r\n}\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;articleID.keyword&quot; : &quot;XHDK-A-1293-#fJ3&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\narticleID.keyword，是es最新版本内置建立的field，就是不分词的。所以一个articleID过来的时候，会建立两次索引，一次是自己本身，是要分词的，分词后放入倒排索引；另外一次是基于articleID.keyword，不分词，保留256个字符最多，直接一个字符串放入倒排索引中。\r\n\r\n所以term filter，对text过滤，可以考虑使用内置的field.keyword来进行匹配。但是有个问题，默认就保留256个字符。所以尽可能还是自己去手动建立索引，指定not_analyzed吧。在最新版本的es中，不需要指定not_analyzed也可以，将type=keyword即可。\r\n\r\n（6）查看分词\r\n\r\nGET /forum/_analyze\r\n{\r\n  &quot;field&quot;: &quot;articleID&quot;,\r\n  &quot;text&quot;: &quot;XHDK-A-1293-#fJ3&quot;\r\n}\r\n\r\n默认是analyzed的text类型的field，建立倒排索引的时候，就会对所有的articleID分词，分词以后，原本的articleID就没有了，只有分词后的各个word存在于倒排索引中。\r\nterm，是不对搜索文本分词的，XHDK-A-1293-#fJ3 --&gt; XHDK-A-1293-#fJ3；但是articleID建立索引的时候，XHDK-A-1293-#fJ3 --&gt; xhdk，a，1293，fj3\r\n\r\n（7）重建索引\r\n\r\nDELETE /forum\r\n\r\nPUT /forum\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;article&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;articleID&quot;: {\r\n          &quot;type&quot;: &quot;keyword&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 1 }}\r\n{ &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 2 }}\r\n{ &quot;articleID&quot; : &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 3 }}\r\n{ &quot;articleID&quot; : &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 4 }}\r\n{ &quot;articleID&quot; : &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot; }\r\n\r\n（8）重新根据帖子ID和发帖日期进行搜索\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : { \r\n            &quot;filter&quot; : {\r\n                &quot;term&quot; : { \r\n                    &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n2、梳理学到的知识点\r\n\r\n（1）term filter：根据exact value进行搜索，数字、boolean、date天然支持\r\n（2）text需要建索引时指定为not_analyzed，才能用term query\r\n（3）相当于SQL中的单个where条件\r\n\r\nselect *\r\nfrom forum.article\r\nwhere articleID=&#039;XHDK-A-1293-#fJ3&#039;',8,0,0,1514611855,0,0,0),(94,1,'倒排索引中查找搜索串，获取document list','','','（1）在倒排索引中查找搜索串，获取document list\r\n\r\ndate来举例\r\n\r\nword		doc1		doc2		doc3\r\n\r\n2017-01-01	*		*\r\n2017-02-02			*		*\r\n2017-03-03	*		*		*\r\n\r\nfilter：2017-02-02\r\n\r\n到倒排索引中一找，发现2017-02-02对应的document list是doc2,doc3\r\n\r\n（2）为每个在倒排索引中搜索到的结果，构建一个bitset，[0, 0, 0, 1, 0, 1]\r\n\r\n非常重要\r\n\r\n使用找到的doc list，构建一个bitset，就是一个二进制的数组，数组每个元素都是0或1，用来标识一个doc对一个filter条件是否匹配，如果匹配就是1，不匹配就是0\r\n\r\n[0, 1, 1]\r\n\r\ndoc1：不匹配这个filter的\r\ndoc2和do3：是匹配这个filter的\r\n\r\n尽可能用简单的数据结构去实现复杂的功能，可以节省内存空间，提升性能\r\n\r\n（3）遍历每个过滤条件对应的bitset，优先从最稀疏的开始搜索，查找满足所有条件的document\r\n\r\n后面会讲解，一次性其实可以在一个search请求中，发出多个filter条件，每个filter条件都会对应一个bitset\r\n遍历每个filter条件对应的bitset，先从最稀疏的开始遍历\r\n\r\n[0, 0, 0, 1, 0, 0]：比较稀疏\r\n[0, 1, 0, 1, 0, 1]\r\n\r\n先遍历比较稀疏的bitset，就可以先过滤掉尽可能多的数据\r\n\r\n遍历所有的bitset，找到匹配所有filter条件的doc\r\n\r\n请求：filter，postDate=2017-01-01，userID=1\r\n\r\npostDate: [0, 0, 1, 1, 0, 0]\r\nuserID:   [0, 1, 0, 1, 0, 1]\r\n\r\n遍历完两个bitset之后，找到的匹配所有条件的doc，就是doc4\r\n\r\n就可以将document作为结果返回给client了\r\n\r\n（4）caching bitset，跟踪query，在最近256个query中超过一定次数的过滤条件，缓存其bitset。对于小segment（&lt;1000，或&lt;3%），不缓存bitset。\r\n\r\n比如postDate=2017-01-01，[0, 0, 1, 1, 0, 0]，可以缓存在内存中，这样下次如果再有这个条件过来的时候，就不用重新扫描倒排索引，反复生成bitset，可以大幅度提升性能。\r\n\r\n在最近的256个filter中，有某个filter超过了一定的次数，次数不固定，就会自动缓存这个filter对应的bitset\r\n\r\nsegment（上半季），filter针对小segment获取到的结果，可以不缓存，segment记录数&lt;1000，或者segment大小&lt;index总大小的3%\r\n\r\nsegment数据量很小，此时哪怕是扫描也很快；segment会在后台自动合并，小segment很快就会跟其他小segment合并成大segment，此时就缓存也没有什么意义，segment很快就消失了\r\n\r\n针对一个小segment的bitset，[0, 0, 1, 0]\r\n\r\nfilter比query的好处就在于会caching，但是之前不知道caching的是什么东西，实际上并不是一个filter返回的完整的doc list数据结果。而是filter bitset缓存起来。下次不用扫描倒排索引了。\r\n\r\n（5）filter大部分情况下来说，在query之前执行，先尽量过滤掉尽可能多的数据\r\n\r\nquery：是会计算doc对搜索条件的relevance score，还会根据这个score去排序\r\nfilter：只是简单过滤出想要的数据，不计算relevance score，也不排序\r\n\r\n（6）如果document有新增或修改，那么cached bitset会被自动更新\r\n\r\npostDate=2017-01-01，[0, 0, 1, 0]\r\ndocument，id=5，postDate=2017-01-01，会自动更新到postDate=2017-01-01这个filter的bitset中，全自动，缓存会自动更新。postDate=2017-01-01的bitset，[0, 0, 1, 0, 1]\r\ndocument，id=1，postDate=2016-12-30，修改为postDate-2017-01-01，此时也会自动更新bitset，[1, 0, 1, 0, 1]\r\n\r\n（7）以后只要是有相同的filter条件的，会直接来使用这个过滤条件对应的cached bitset\r\n',8,0,0,1514611981,0,0,0),(95,1,'搜索发帖','','','1、搜索发帖日期为2017-01-01，或者帖子ID为XHDK-A-1293-#fJ3的帖子，同时要求帖子的发帖日期绝对不为2017-01-02\r\n\r\nselect *\r\nfrom forum.article\r\nwhere (post_date=&#039;2017-01-01&#039; or article_id=&#039;XHDK-A-1293-#fJ3&#039;)\r\nand post_date!=&#039;2017-01-02&#039;\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;bool&quot;: {\r\n          &quot;should&quot;: [\r\n            {&quot;term&quot;: { &quot;postDate&quot;: &quot;2017-01-01&quot; }},\r\n            {&quot;term&quot;: {&quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;}}\r\n          ],\r\n          &quot;must_not&quot;: {\r\n            &quot;term&quot;: {\r\n              &quot;postDate&quot;: &quot;2017-01-02&quot;\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nmust，should，must_not，filter：必须匹配，可以匹配其中任意一个即可，必须不匹配\r\n\r\n2、搜索帖子ID为XHDK-A-1293-#fJ3，或者是帖子ID为JODL-X-1937-#pV7而且发帖日期为2017-01-01的帖子\r\n\r\nselect *\r\nfrom forum.article\r\nwhere article_id=&#039;XHDK-A-1293-#fJ3&#039;\r\nor (article_id=&#039;JODL-X-1937-#pV7&#039; and post_date=&#039;2017-01-01&#039;)\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;bool&quot;: {\r\n          &quot;should&quot;: [\r\n            {\r\n              &quot;term&quot;: {\r\n                &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;\r\n              }\r\n            },\r\n            {\r\n              &quot;bool&quot;: {\r\n                &quot;must&quot;: [\r\n                  {\r\n                    &quot;term&quot;:{\r\n                      &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;\r\n                    }\r\n                  },\r\n                  {\r\n                    &quot;term&quot;: {\r\n                      &quot;postDate&quot;: &quot;2017-01-01&quot;\r\n                    }\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n3、梳理学到的知识点\r\n\r\n（1）bool：must，must_not，should，组合多个过滤条件\r\n（2）bool可以嵌套\r\n（3）相当于SQL中的多个and条件：当你把搜索语法学好了以后，基本可以实现部分常用的sql语法对应的功能\r\n\r\n',8,0,0,1514612021,0,0,0),(96,1,'sql in','','','term: {&quot;field&quot;: &quot;value&quot;}\r\nterms: {&quot;field&quot;: [&quot;value1&quot;, &quot;value2&quot;]}\r\n\r\nsql中的in\r\n\r\nselect * from tbl where col in (&quot;value1&quot;, &quot;value2&quot;)\r\n\r\n1、为帖子数据增加tag字段\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag&quot; : [&quot;java&quot;, &quot;hadoop&quot;]} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag&quot; : [&quot;java&quot;]} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag&quot; : [&quot;hadoop&quot;]} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag&quot; : [&quot;java&quot;, &quot;elasticsearch&quot;]} }\r\n\r\n2、搜索articleID为KDKE-B-9947-#kL5或QQPX-R-3956-#aD8的帖子，搜索tag中包含java的帖子\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;terms&quot;: {\r\n          &quot;articleID&quot;: [\r\n            &quot;KDKE-B-9947-#kL5&quot;,\r\n            &quot;QQPX-R-3956-#aD8&quot;\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;constant_score&quot; : {\r\n            &quot;filter&quot; : {\r\n                &quot;terms&quot; : { \r\n                    &quot;tag&quot; : [&quot;java&quot;]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;,\r\n          &quot;userID&quot;: 2,\r\n          &quot;hidden&quot;: true,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;elasticsearch&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n3、优化搜索结果，仅仅搜索tag只包含java的帖子\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag_cnt&quot; : 2} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag_cnt&quot; : 1} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag_cnt&quot; : 1} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;tag_cnt&quot; : 2} }\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;bool&quot;: {\r\n          &quot;must&quot;: [\r\n            {\r\n              &quot;term&quot;: {\r\n                &quot;tag_cnt&quot;: 1\r\n              }\r\n            },\r\n            {\r\n              &quot;terms&quot;: {\r\n                &quot;tag&quot;: [&quot;java&quot;]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n[&quot;java&quot;, &quot;hadoop&quot;, &quot;elasticsearch&quot;]\r\n\r\n4、学到的知识点梳理\r\n\r\n（1）terms多值搜索\r\n（2）优化terms多值搜索的结果\r\n（3）相当于SQL中的in语句',8,0,0,1514612093,0,0,0),(97,1,'range','','','1、为帖子数据增加浏览量的字段\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;view_cnt&quot; : 30} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;view_cnt&quot; : 50} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;view_cnt&quot; : 100} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;view_cnt&quot; : 80} }\r\n\r\n2、搜索浏览量在30~60之间的帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;view_cnt&quot;: {\r\n            &quot;gt&quot;: 30,\r\n            &quot;lt&quot;: 60\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\ngte\r\nlte\r\n\r\n3、搜索发帖日期在最近1个月的帖子\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 5 }}\r\n{ &quot;articleID&quot; : &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot; : 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-03-01&quot;, &quot;tag&quot;: [&quot;elasticsearch&quot;], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10 }\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;postDate&quot;: {\r\n            &quot;gt&quot;: &quot;2017-03-10||-30d&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;postDate&quot;: {\r\n            &quot;gt&quot;: &quot;now-30d&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n4、梳理一下学到的知识点\r\n\r\n（1）range，sql中的between，或者是&gt;=1，&lt;=1\r\n（2）range做范围过滤',8,0,0,1514612128,0,0,0),(98,1,'搜索帖子标题','','','1、为帖子数据增加标题字段\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is java and elasticsearch blog&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is java blog&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is elasticsearch blog&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is java, elasticsearch, hadoop blog&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;this is spark blog&quot;} }\r\n\r\n2、搜索标题中包含java或elasticsearch的blog\r\n\r\n这个，就跟之前的那个term query，不一样了。不是搜索exact value，是进行full text全文检索。\r\nmatch query，是负责进行全文检索的。当然，如果要检索的field，是not_analyzed类型的，那么match query也相当于term query。\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;java elasticsearch&quot;\r\n        }\r\n    }\r\n}\r\n\r\n3、搜索标题中包含java和elasticsearch的blog\r\n\r\n搜索结果精准控制的第一步：灵活使用and关键字，如果你是希望所有的搜索关键字都要匹配的，那么就用and，可以实现单纯match query无法实现的效果\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n		&quot;query&quot;: &quot;java elasticsearch&quot;,\r\n		&quot;operator&quot;: &quot;and&quot;\r\n   	    }\r\n        }\r\n    }\r\n}\r\n\r\n4、搜索包含java，elasticsearch，spark，hadoop，4个关键字中，至少3个的blog\r\n\r\n控制搜索结果的精准度的第二步：指定一些关键字中，必须至少匹配其中的多少个关键字，才能作为结果返回\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;title&quot;: {\r\n        &quot;query&quot;: &quot;java elasticsearch spark hadoop&quot;,\r\n        &quot;minimum_should_match&quot;: &quot;75%&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n5、用bool组合多个搜索条件，来搜索title\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n      &quot;must_not&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;spark&quot;  }},\r\n      &quot;should&quot;: [\r\n                  { &quot;match&quot;: { &quot;title&quot;: &quot;hadoop&quot; }},\r\n                  { &quot;match&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }}\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n6、bool组合多个搜索条件，如何计算relevance score\r\n\r\nmust和should搜索对应的分数，加起来，除以must和should的总数\r\n\r\n排名第一：java，同时包含should中所有的关键字，hadoop，elasticsearch\r\n排名第二：java，同时包含should中的elasticsearch\r\n排名第三：java，不包含should中的任何关键字\r\n\r\nshould是可以影响相关度分数的\r\n\r\nmust是确保说，谁必须有这个关键字，同时会根据这个must的条件去计算出document对这个搜索条件的relevance score\r\n在满足must的基础之上，should中的条件，不匹配也可以，但是如果匹配的更多，那么document的relevance score就会更高\r\n\r\n{\r\n  &quot;took&quot;: 6,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1.3375794,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 1.3375794,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;,\r\n          &quot;userID&quot;: 2,\r\n          &quot;hidden&quot;: true,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 80,\r\n          &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.53484553,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.19856805,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n7、搜索java，hadoop，spark，elasticsearch，至少包含其中3个关键字\r\n\r\n默认情况下，should是可以不匹配任何一个的，比如上面的搜索中，this is java blog，就不匹配任何一个should条件\r\n但是有个例外的情况，如果没有must的话，那么should中必须至少匹配一个才可以\r\n比如下面的搜索，should中有4个条件，默认情况下，只要满足其中一个条件，就可以匹配作为结果返回\r\n\r\n但是可以精准控制，should的4个条件中，至少匹配几个才能作为结果返回\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n        { &quot;match&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }},\r\n        { &quot;match&quot;: { &quot;title&quot;: &quot;hadoop&quot;   }},\r\n	{ &quot;match&quot;: { &quot;title&quot;: &quot;spark&quot;   }}\r\n      ],\r\n      &quot;minimum_should_match&quot;: 3 \r\n    }\r\n  }\r\n}\r\n\r\n梳理一下学到的知识点\r\n\r\n1、全文检索的时候，进行多个值的检索，有两种做法，match query；should\r\n2、控制搜索结果精准度：and operator，minimum_should_match\r\n\r\n',8,0,0,1514612202,0,0,0),(99,1,'bool should','','','1、普通match如何转换为term+should\r\n\r\n{\r\n    &quot;match&quot;: { &quot;title&quot;: &quot;java elasticsearch&quot;}\r\n}\r\n\r\n使用诸如上面的match query进行多值搜索的时候，es会在底层自动将这个match query转换为bool的语法\r\nbool should，指定多个搜索词，同时使用term query\r\n\r\n{\r\n  &quot;bool&quot;: {\r\n    &quot;should&quot;: [\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }}\r\n    ]\r\n  }\r\n}\r\n\r\n2、and match如何转换为term+must\r\n\r\n{\r\n    &quot;match&quot;: {\r\n        &quot;title&quot;: {\r\n            &quot;query&quot;:    &quot;java elasticsearch&quot;,\r\n            &quot;operator&quot;: &quot;and&quot;\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;bool&quot;: {\r\n    &quot;must&quot;: [\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }}\r\n    ]\r\n  }\r\n}\r\n\r\n3、minimum_should_match如何转换\r\n\r\n{\r\n    &quot;match&quot;: {\r\n        &quot;title&quot;: {\r\n            &quot;query&quot;:                &quot;java elasticsearch hadoop spark&quot;,\r\n            &quot;minimum_should_match&quot;: &quot;75%&quot;\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;bool&quot;: {\r\n    &quot;should&quot;: [\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;java&quot; }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;elasticsearch&quot;   }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;hadoop&quot; }},\r\n      { &quot;term&quot;: { &quot;title&quot;: &quot;spark&quot; }}\r\n    ],\r\n    &quot;minimum_should_match&quot;: 3 \r\n  }\r\n}\r\n\r\n上一讲，为啥要讲解两种实现multi-value搜索的方式呢？实际上，就是给这一讲进行铺垫的。match query --&gt; bool + term。\r\n\r\n',8,0,0,1514612290,0,0,0),(100,1,'搜索标题中包含java的帖子','','','需求：搜索标题中包含java的帖子，同时呢，如果标题中包含hadoop或elasticsearch就优先搜索出来，同时呢，如果一个帖子包含java hadoop，一个帖子包含java elasticsearch，包含hadoop的帖子要比elasticsearch优先搜索出来\r\n\r\n知识点，搜索条件的权重，boost，可以将某个搜索条件的权重加大，此时当匹配这个搜索条件和匹配另一个搜索条件的document，计算relevance score时，匹配权重更大的搜索条件的document，relevance score会更高，当然也就会优先被返回回来\r\n\r\n默认情况下，搜索条件的权重都是一样的，都是1\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;blog&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;java&quot;\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;hadoop&quot;\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;elasticsearch&quot;\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;spark&quot;,\r\n              &quot;boost&quot;: 5\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514612368,0,0,0),(101,1,'多shard场景','','','1、多shard场景下relevance score不准确问题大揭秘\r\n\r\n如果你的一个index有多个shard的话，可能搜索结果会不准确\r\n\r\n图解\r\n\r\n2、如何解决该问题？\r\n\r\n（1）生产环境下，数据量大，尽可能实现均匀分配\r\n\r\n数据量很大的话，其实一般情况下，在概率学的背景下，es都是在多个shard中均匀路由数据的，路由的时候根据_id，负载均衡\r\n比如说有10个document，title都包含java，一共有5个shard，那么在概率学的背景下，如果负载均衡的话，其实每个shard都应该有2个doc，title包含java\r\n如果说数据分布均匀的话，其实就没有刚才说的那个问题了\r\n\r\n（2）测试环境下，将索引的primary shard设置为1个，number_of_shards=1，index settings\r\n\r\n如果说只有一个shard，那么当然，所有的document都在这个shard里面，就没有这个问题了\r\n\r\n（3）测试环境下，搜索附带search_type=dfs_query_then_fetch参数，会将local IDF取出来计算global IDF\r\n\r\n计算一个doc的相关度分数的时候，就会将所有shard对的local IDF计算一下，获取出来，在本地进行global IDF分数的计算，会将所有shard的doc作为上下文来进行计算，也能确保准确性。但是production生产环境下，不推荐这个参数，因为性能很差。\r\n',8,0,0,1514612451,0,0,0),(102,1,'为帖子数据增加content字段','','','1、为帖子数据增加content字段\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;i like to write best elasticsearch article&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;i think java is the best programming language&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;i am only an elasticsearch beginner&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;elasticsearch and hadoop are all very good solution, i am a beginner&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;content&quot; : &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;} }\r\n\r\n2、搜索title或content中包含java或solution的帖子\r\n\r\n下面这个就是multi-field搜索，多字段搜索\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;bool&quot;: {\r\n            &quot;should&quot;: [\r\n                { &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }},\r\n                { &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}\r\n            ]\r\n        }\r\n    }\r\n}\r\n\r\n3、结果分析\r\n\r\n期望的是doc5，结果是doc2,doc4排在了前面\r\n\r\n计算每个document的relevance score：每个query的分数，乘以matched query数量，除以总query数量\r\n\r\n算一下doc4的分数\r\n\r\n{ &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }}，针对doc4，是有一个分数的\r\n{ &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}，针对doc4，也是有一个分数的\r\n\r\n所以是两个分数加起来，比如说，1.1 + 1.2 = 2.3\r\nmatched query数量 = 2\r\n总query数量 = 2\r\n\r\n2.3 * 2 / 2 = 2.3\r\n\r\n算一下doc5的分数\r\n\r\n{ &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }}，针对doc5，是没有分数的\r\n{ &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}，针对doc5，是有一个分数的\r\n\r\n所以说，只有一个query是有分数的，比如2.3\r\nmatched query数量 = 1\r\n总query数量 = 2\r\n\r\n2.3 * 1 / 2 = 1.15\r\n\r\ndoc5的分数 = 1.15 &lt; doc4的分数 = 2.3\r\n\r\n4、best fields策略，dis_max\r\n\r\nbest fields策略，就是说，搜索到的结果，应该是某一个field中匹配到了尽可能多的关键词，被排在前面；而不是尽可能多的field匹配到了少数的关键词，排在了前面\r\n\r\ndis_max语法，直接取多个query中，分数最高的那一个query的分数即可\r\n\r\n{ &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }}，针对doc4，是有一个分数的，1.1\r\n{ &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}，针对doc4，也是有一个分数的，1.2\r\n取最大分数，1.2\r\n\r\n{ &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }}，针对doc5，是没有分数的\r\n{ &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}，针对doc5，是有一个分数的，2.3\r\n取最大分数，2.3\r\n\r\n然后doc4的分数 = 1.2 &lt; doc5的分数 = 2.3，所以doc5就可以排在更前面的地方，符合我们的需要\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;dis_max&quot;: {\r\n            &quot;queries&quot;: [\r\n                { &quot;match&quot;: { &quot;title&quot;: &quot;java solution&quot; }},\r\n                { &quot;match&quot;: { &quot;content&quot;:  &quot;java solution&quot; }}\r\n            ]\r\n        }\r\n    }\r\n}',8,0,0,1514612556,0,0,0),(103,1,'搜索title或content中包含java beginne','','','1、搜索title或content中包含java beginner的帖子\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;dis_max&quot;: {\r\n            &quot;queries&quot;: [\r\n                { &quot;match&quot;: { &quot;title&quot;: &quot;java beginner&quot; }},\r\n                { &quot;match&quot;: { &quot;body&quot;:  &quot;java beginner&quot; }}\r\n            ]\r\n        }\r\n    }\r\n}\r\n\r\n有些场景不是太好复现的，因为是这样，你需要尝试去构造不同的文本，然后去构造一些搜索出来，去达到你要的一个效果\r\n\r\n可能在实际场景中出现的一个情况是这样的：\r\n\r\n（1）某个帖子，doc1，title中包含java，content不包含java beginner任何一个关键词\r\n（2）某个帖子，doc2，content中包含beginner，title中不包含任何一个关键词\r\n（3）某个帖子，doc3，title中包含java，content中包含beginner\r\n（4）最终搜索，可能出来的结果是，doc1和doc2排在doc3的前面，而不是我们期望的doc3排在最前面\r\n\r\ndis_max，只是取分数最高的那个query的分数而已。\r\n\r\n2、dis_max只取某一个query最大的分数，完全不考虑其他query的分数\r\n\r\n3、使用tie_breaker将其他query的分数也考虑进去\r\n\r\ntie_breaker参数的意义，在于说，将其他query的分数，乘以tie_breaker，然后综合与最高分数的那个query的分数，综合在一起进行计算\r\n除了取最高分以外，还会考虑其他的query的分数\r\ntie_breaker的值，在0~1之间，是个小数，就ok\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;dis_max&quot;: {\r\n            &quot;queries&quot;: [\r\n                { &quot;match&quot;: { &quot;title&quot;: &quot;java beginner&quot; }},\r\n                { &quot;match&quot;: { &quot;body&quot;:  &quot;java beginner&quot; }}\r\n            ],\r\n            &quot;tie_breaker&quot;: 0.3\r\n        }\r\n    }\r\n}',8,0,0,1514612600,0,0,0),(104,1,'minimum_should_match','','','GET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;multi_match&quot;: {\r\n        &quot;query&quot;:                &quot;java solution&quot;,\r\n        &quot;type&quot;:                 &quot;best_fields&quot;, \r\n        &quot;fields&quot;:               [ &quot;title^2&quot;, &quot;content&quot; ],\r\n        &quot;tie_breaker&quot;:          0.3,\r\n        &quot;minimum_should_match&quot;: &quot;50%&quot; \r\n    }\r\n  } \r\n}\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;dis_max&quot;: {\r\n      &quot;queries&quot;:  [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;java beginner&quot;,\r\n              &quot;minimum_should_match&quot;: &quot;50%&quot;,\r\n	      &quot;boost&quot;: 2\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;body&quot;: {\r\n              &quot;query&quot;: &quot;java beginner&quot;,\r\n              &quot;minimum_should_match&quot;: &quot;30%&quot;\r\n            }\r\n          }\r\n        }\r\n      ],\r\n      &quot;tie_breaker&quot;: 0.3\r\n    }\r\n  } \r\n}\r\n\r\nminimum_should_match，主要是用来干嘛的？\r\n去长尾，long tail\r\n长尾，比如你搜索5个关键词，但是很多结果是只匹配1个关键词的，其实跟你想要的结果相差甚远，这些结果就是长尾\r\nminimum_should_match，控制搜索结果的精准度，只有匹配一定数量的关键词的数据，才能返回\r\n',8,0,0,1514612668,0,0,0),(105,1,'从best-fields换成most-fields策略','','','从best-fields换成most-fields策略\r\nbest-fields策略，主要是说将某一个field匹配尽可能多的关键词的doc优先返回回来\r\nmost-fields策略，主要是说尽可能返回更多field匹配到某个关键词的doc，优先返回回来\r\n\r\nPOST /forum/_mapping/article\r\n{\r\n  &quot;properties&quot;: {\r\n      &quot;sub_title&quot;: { \r\n          &quot;type&quot;:     &quot;string&quot;,\r\n          &quot;analyzer&quot;: &quot;english&quot;,\r\n          &quot;fields&quot;: {\r\n              &quot;std&quot;:   { \r\n                  &quot;type&quot;:     &quot;string&quot;,\r\n                  &quot;analyzer&quot;: &quot;standard&quot;\r\n              }\r\n          }\r\n      }\r\n  }\r\n}\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;learning more courses&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;learned a lot of course&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;we have a lot of fun&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;both of them are good&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;haha, hello world&quot;} }\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;sub_title&quot;: &quot;learning courses&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 3,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1.219939,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1.219939,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.5063205,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;,\r\n          &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;,\r\n          &quot;sub_title&quot;: &quot;learning more courses&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nsub_title用的是enligsh analyzer，所以还原了单词\r\n\r\n为什么，因为如果我们用的是类似于english analyzer这种分词器的话，就会将单词还原为其最基本的形态，stemmer\r\nlearning --&gt; learn\r\nlearned --&gt; learn\r\ncourses --&gt; course\r\n\r\nsub_titile: learning coureses --&gt; learn course\r\n\r\n{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;learned a lot of course&quot;} }，就排在了{ &quot;doc&quot; : {&quot;sub_title&quot; : &quot;learning more courses&quot;} }的前面\r\n\r\nGET /forum/article/_search\r\n{\r\n   &quot;query&quot;: {\r\n        &quot;match&quot;: {\r\n            &quot;sub_title&quot;: &quot;learning courses&quot;\r\n        }\r\n    }\r\n}\r\n\r\n\r\n很绕。。。。我自己都觉得很绕\r\n\r\n很多东西，你看文字就觉得很绕，然后用语言去表述，也很绕，但是我觉得，用语言去说，相对来说会好一点点\r\n\r\nGET /forum/article/_search\r\n{\r\n   &quot;query&quot;: {\r\n        &quot;multi_match&quot;: {\r\n            &quot;query&quot;:  &quot;learning courses&quot;,\r\n            &quot;type&quot;:   &quot;most_fields&quot;, \r\n            &quot;fields&quot;: [ &quot;sub_title&quot;, &quot;sub_title.std&quot; ]\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1.219939,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1.219939,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1.012641,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;,\r\n          &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;,\r\n          &quot;sub_title&quot;: &quot;learning more courses&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n你问我，具体的分数怎么算出来的，很难说，因为这个东西很复杂， 还不只是TF/IDF算法。因为不同的query，不同的语法，都有不同的计算score的细节。\r\n\r\n与best_fields的区别\r\n\r\n（1）best_fields，是对多个field进行搜索，挑选某个field匹配度最高的那个分数，同时在多个query最高分相同的情况下，在一定程度上考虑其他query的分数。简单来说，你对多个field进行搜索，就想搜索到某一个field尽可能包含更多关键字的数据\r\n\r\n优点：通过best_fields策略，以及综合考虑其他field，还有minimum_should_match支持，可以尽可能精准地将匹配的结果推送到最前面\r\n缺点：除了那些精准匹配的结果，其他差不多大的结果，排序结果不是太均匀，没有什么区分度了\r\n\r\n实际的例子：百度之类的搜索引擎，最匹配的到最前面，但是其他的就没什么区分度了\r\n\r\n（2）most_fields，综合多个field一起进行搜索，尽可能多地让所有field的query参与到总分数的计算中来，此时就会是个大杂烩，出现类似best_fields案例最开始的那个结果，结果不一定精准，某一个document的一个field包含更多的关键字，但是因为其他document有更多field匹配到了，所以排在了前面；所以需要建立类似sub_title.std这样的field，尽可能让某一个field精准匹配query string，贡献更高的分数，将更精准匹配的数据排到前面\r\n\r\n优点：将尽可能匹配更多field的结果推送到最前面，整个排序结果是比较均匀的\r\n缺点：可能那些精准匹配的结果，无法推送到最前面\r\n\r\n实际的例子：wiki，明显的most_fields策略，搜索结果比较均匀，但是的确要翻好几页才能找到最匹配的结果\r\n',8,0,0,1514612809,0,0,0),(106,1,'cross-fields搜索','','','cross-fields搜索，一个唯一标识，跨了多个field。比如一个人，标识，是姓名；一个建筑，它的标识是地址。姓名可以散落在多个field中，比如first_name和last_name中，地址可以散落在country，province，city中。\r\n\r\n跨多个field搜索一个标识，比如搜索一个人名，或者一个地址，就是cross-fields搜索\r\n\r\n初步来说，如果要实现，可能用most_fields比较合适。因为best_fields是优先搜索单个field最匹配的结果，cross-fields本身就不是一个field的问题了。\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Peter&quot;, &quot;author_last_name&quot; : &quot;Smith&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Smith&quot;, &quot;author_last_name&quot; : &quot;Williams&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Jack&quot;, &quot;author_last_name&quot; : &quot;Ma&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Robbin&quot;, &quot;author_last_name&quot; : &quot;Li&quot;} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;author_first_name&quot; : &quot;Tonny&quot;, &quot;author_last_name&quot; : &quot;Peter Smith&quot;} }\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;multi_match&quot;: {\r\n      &quot;query&quot;:       &quot;Peter Smith&quot;,\r\n      &quot;type&quot;:        &quot;most_fields&quot;,\r\n      &quot;fields&quot;:      [ &quot;author_first_name&quot;, &quot;author_last_name&quot; ]\r\n    }\r\n  }\r\n}\r\n\r\nPeter Smith，匹配author_first_name，匹配到了Smith，这时候它的分数很高，为什么啊？？？\r\n因为IDF分数高，IDF分数要高，那么这个匹配到的term（Smith），在所有doc中的出现频率要低，author_first_name field中，Smith就出现过1次\r\nPeter Smith这个人，doc 1，Smith在author_last_name中，但是author_last_name出现了两次Smith，所以导致doc 1的IDF分数较低\r\n\r\n不要有过多的疑问，一定是这样吗？\r\n\r\n\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 0.6931472,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.6931472,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.5753642,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;,\r\n          &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;,\r\n          &quot;sub_title&quot;: &quot;learning more courses&quot;,\r\n          &quot;author_first_name&quot;: &quot;Peter&quot;,\r\n          &quot;author_last_name&quot;: &quot;Smith&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 0.51623213,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n问题1：只是找到尽可能多的field匹配的doc，而不是某个field完全匹配的doc\r\n\r\n问题2：most_fields，没办法用minimum_should_match去掉长尾数据，就是匹配的特别少的结果\r\n\r\n问题3：TF/IDF算法，比如Peter Smith和Smith Williams，搜索Peter Smith的时候，由于first_name中很少有Smith的，所以query在所有document中的频率很低，得到的分数很高，可能Smith Williams反而会排在Peter Smith前面\r\n',8,0,0,1514612846,0,0,0),(107,1,'多个field组合成一个field','','','上一讲，我们其实说了，用most_fields策略，去实现cross-fields搜索，有3大弊端，而且搜索结果也显示出了这3大弊端\r\n\r\n第一个办法：用copy_to，将多个field组合成一个field\r\n\r\n问题其实就出在有多个field，有多个field以后，就很尴尬，我们只要想办法将一个标识跨在多个field的情况，合并成一个field即可。比如说，一个人名，本来是first_name，last_name，现在合并成一个full_name，不就ok了吗。。。。。\r\n\r\nPUT /forum/_mapping/article\r\n{\r\n  &quot;properties&quot;: {\r\n      &quot;new_author_first_name&quot;: {\r\n          &quot;type&quot;:     &quot;string&quot;,\r\n          &quot;copy_to&quot;:  &quot;new_author_full_name&quot; \r\n      },\r\n      &quot;new_author_last_name&quot;: {\r\n          &quot;type&quot;:     &quot;string&quot;,\r\n          &quot;copy_to&quot;:  &quot;new_author_full_name&quot; \r\n      },\r\n      &quot;new_author_full_name&quot;: {\r\n          &quot;type&quot;:     &quot;string&quot;\r\n      }\r\n  }\r\n}\r\n\r\n用了这个copy_to语法之后，就可以将多个字段的值拷贝到一个字段中，并建立倒排索引\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Peter&quot;, &quot;new_author_last_name&quot; : &quot;Smith&quot;} }		--&gt; Peter Smith\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }	\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Smith&quot;, &quot;new_author_last_name&quot; : &quot;Williams&quot;} }		--&gt; Smith Williams\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Jack&quot;, &quot;new_author_last_name&quot; : &quot;Ma&quot;} }			--&gt; Jack Ma\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Robbin&quot;, &quot;new_author_last_name&quot; : &quot;Li&quot;} }			--&gt; Robbin Li\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;new_author_first_name&quot; : &quot;Tonny&quot;, &quot;new_author_last_name&quot; : &quot;Peter Smith&quot;} }		--&gt; Tonny Peter Smith\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;new_author_full_name&quot;:       &quot;Peter Smith&quot;\r\n    }\r\n  }\r\n}\r\n\r\n很无奈，很多时候，我们很难复现。比如官网也会给一些例子，说用什么什么文本，怎么怎么搜索，是怎么怎么样的效果。es版本在不断迭代，这个打分的算法也在不断的迭代。所以我们其实很难说，对类似这几讲讲解的best_fields，most_fields，cross_fields，完全复现出来应有的场景和效果。\r\n\r\n更多的把原理和知识点给大家讲解清楚，带着大家演练一遍怎么操作的，做一下实验\r\n\r\n期望的是说，比如大家自己在开发搜索应用的时候，碰到需要best_fields的场景，知道怎么做，知道best_fields的原理，可以达到什么效果；碰到most_fields的场景，知道怎么做，以及原理；碰到搜搜cross_fields标识的场景，知道怎么做，知道原理是什么，效果是什么。。。。\r\n\r\n\r\n\r\n问题1：只是找到尽可能多的field匹配的doc，而不是某个field完全匹配的doc --&gt; 解决，最匹配的document被最先返回\r\n\r\n问题2：most_fields，没办法用minimum_should_match去掉长尾数据，就是匹配的特别少的结果 --&gt; 解决，可以使用minimum_should_match去掉长尾数据\r\n\r\n问题3：TF/IDF算法，比如Peter Smith和Smith Williams，搜索Peter Smith的时候，由于first_name中很少有Smith的，所以query在所有document中的频率很低，得到的分数很高，可能Smith Williams反而会排在Peter Smith前面 --&gt; 解决，Smith和Peter在一个field了，所以在所有document中出现的次数是均匀的，不会有极端的偏差\r\n\r\n',8,0,0,1514613241,0,0,0),(108,1,'field匹配的doc','','','GET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;multi_match&quot;: {\r\n      &quot;query&quot;: &quot;Peter Smith&quot;,\r\n      &quot;type&quot;: &quot;cross_fields&quot;, \r\n      &quot;operator&quot;: &quot;and&quot;,\r\n      &quot;fields&quot;: [&quot;author_first_name&quot;, &quot;author_last_name&quot;]\r\n    }\r\n  }\r\n}\r\n\r\n问题1：只是找到尽可能多的field匹配的doc，而不是某个field完全匹配的doc --&gt; 解决，要求每个term都必须在任何一个field中出现\r\n\r\nPeter，Smith\r\n\r\n要求Peter必须在author_first_name或author_last_name中出现\r\n要求Smith必须在author_first_name或author_last_name中出现\r\n\r\nPeter Smith可能是横跨在多个field中的，所以必须要求每个term都在某个field中出现，组合起来才能组成我们想要的标识，完整的人名\r\n\r\n原来most_fiels，可能像Smith Williams也可能会出现，因为most_fields要求只是任何一个field匹配了就可以，匹配的field越多，分数越高\r\n\r\n问题2：most_fields，没办法用minimum_should_match去掉长尾数据，就是匹配的特别少的结果 --&gt; 解决，既然每个term都要求出现，长尾肯定被去除掉了\r\n\r\njava hadoop spark --&gt; 这3个term都必须在任何一个field出现了\r\n\r\n比如有的document，只有一个field中包含一个java，那就被干掉了，作为长尾就没了\r\n\r\n问题3：TF/IDF算法，比如Peter Smith和Smith Williams，搜索Peter Smith的时候，由于first_name中很少有Smith的，所以query在所有document中的频率很低，得到的分数很高，可能Smith Williams反而会排在Peter Smith前面 --&gt; 计算IDF的时候，将每个query在每个field中的IDF都取出来，取最小值，就不会出现极端情况下的极大值了\r\n\r\nPeter Smith\r\n\r\nPeter\r\nSmith\r\n\r\nSmith，在author_first_name这个field中，在所有doc的这个Field中，出现的频率很低，导致IDF分数很高；Smith在所有doc的author_last_name field中的频率算出一个IDF分数，因为一般来说last_name中的Smith频率都较高，所以IDF分数是正常的，不会太高；然后对于Smith来说，会取两个IDF分数中，较小的那个分数。就不会出现IDF分过高的情况。\r\n',8,0,0,1514613285,0,0,0),(109,1,'近似匹配','','','近似匹配\r\n\r\n1、什么是近似匹配\r\n\r\n两个句子\r\n\r\njava is my favourite programming language, and I also think spark is a very good big data system.\r\njava spark are very related, because scala is spark&#039;s programming language and scala is also based on jvm like java.\r\n\r\nmatch query，搜索java spark\r\n\r\n{\r\n	&quot;match&quot;: {\r\n		&quot;content&quot;: &quot;java spark&quot;\r\n	}\r\n}\r\n\r\nmatch query，只能搜索到包含java和spark的document，但是不知道java和spark是不是离的很近\r\n\r\n包含java或包含spark，或包含java和spark的doc，都会被返回回来。我们其实并不知道哪个doc，java和spark距离的比较近。如果我们就是希望搜索java spark，中间不能插入任何其他的字符，那这个时候match去做全文检索，能搞定我们的需求吗？答案是，搞不定。\r\n\r\n如果我们要尽量让java和spark离的很近的document优先返回，要给它一个更高的relevance score，这就涉及到了proximity match，近似匹配\r\n\r\n如果说，要实现两个需求：\r\n\r\n1、java spark，就靠在一起，中间不能插入任何其他字符，就要搜索出来这种doc\r\n2、java spark，但是要求，java和spark两个单词靠的越近，doc的分数越高，排名越靠前\r\n\r\n要实现上述两个需求，用match做全文检索，是搞不定的，必须得用proximity match，近似匹配\r\n\r\nphrase match，proximity match：短语匹配，近似匹配\r\n\r\n这一讲，要学习的是phrase match，就是仅仅搜索出java和spark靠在一起的那些doc，比如有个doc，是java use&#039;d spark，不行。必须是比如java spark are very good friends，是可以搜索出来的。\r\n\r\nphrase match，就是要去将多个term作为一个短语，一起去搜索，只有包含这个短语的doc才会作为结果返回。不像是match，java spark，java的doc也会返回，spark的doc也会返回。\r\n\r\n2、match_phrase\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;java spark&quot;\r\n    }\r\n  }\r\n}\r\n\r\n单单包含java的doc也返回了，不是我们想要的结果\r\n\r\nPOST /forum/article/5/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;\r\n  }\r\n}\r\n\r\n将一个doc的content设置为恰巧包含java spark这个短语\r\n\r\nmatch_phrase语法\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_phrase&quot;: {\r\n            &quot;content&quot;: &quot;java spark&quot;\r\n        }\r\n    }\r\n}\r\n\r\n成功了，只有包含java spark这个短语的doc才返回了，只包含java的doc不会返回\r\n\r\n3、term position\r\n\r\nhello world, java spark		doc1\r\nhi, spark java				doc2\r\n\r\nhello 		doc1(0)		\r\nwolrd		doc1(1)\r\njava		doc1(2) doc2(2)\r\nspark		doc1(3) doc2(1)\r\n\r\n了解什么是分词后的position\r\n\r\nGET _analyze\r\n{\r\n  &quot;text&quot;: &quot;hello world, java spark&quot;,\r\n  &quot;analyzer&quot;: &quot;standard&quot;\r\n}\r\n4、match_phrase的基本原理\r\n\r\n索引中的position，match_phrase\r\n\r\nhello world, java spark		doc1\r\nhi, spark java				doc2\r\n\r\nhello 		doc1(0)		\r\nwolrd		doc1(1)\r\njava		doc1(2) doc2(2)\r\nspark		doc1(3) doc2(1)\r\n\r\njava spark --&gt; match phrase\r\n\r\njava spark --&gt; java和spark\r\n\r\njava --&gt; doc1(2) doc2(2)\r\nspark --&gt; doc1(3) doc2(1)\r\n\r\n要找到每个term都在的一个共有的那些doc，就是要求一个doc，必须包含每个term，才能拿出来继续计算\r\n\r\ndoc1 --&gt; java和spark --&gt; spark position恰巧比java大1 --&gt; java的position是2，spark的position是3，恰好满足条件\r\n\r\ndoc1符合条件\r\n\r\ndoc2 --&gt; java和spark --&gt; java position是2，spark position是1，spark position比java position小1，而不是大1 --&gt; 光是position就不满足，那么doc2不匹配\r\n\r\n必须理解这块原理！！！！\r\n\r\n因为后面的proximity match就是原理跟这个一模一样！！！\r\n',8,0,0,1514613327,0,0,0),(110,1,'slop','','','GET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_phrase&quot;: {\r\n            &quot;title&quot;: {\r\n                &quot;query&quot;: &quot;java spark&quot;,\r\n                &quot;slop&quot;:  1\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nslop的含义是什么？\r\n\r\nquery string，搜索文本，中的几个term，要经过几次移动才能与一个document匹配，这个移动的次数，就是slop\r\n\r\n实际举例，一个query string经过几次移动之后可以匹配到一个document，然后设置slop\r\n\r\nhello world, java is very good, spark is also very good.\r\n\r\njava spark，match phrase，搜不到\r\n\r\n如果我们指定了slop，那么就允许java spark进行移动，来尝试与doc进行匹配\r\n\r\njava		is		very		good		spark		is\r\n\r\njava		spark\r\njava		--&gt;		spark\r\njava				--&gt;			spark\r\njava							--&gt;			spark\r\n\r\n这里的slop，就是3，因为java spark这个短语，spark移动了3次，就可以跟一个doc匹配上了\r\n\r\nslop的含义，不仅仅是说一个query string terms移动几次，跟一个doc匹配上。一个query string terms，最多可以移动几次去尝试跟一个doc匹配上\r\n\r\nslop，设置的是3，那么就ok\r\n\r\nGET /forum/article/_search\r\n{\r\n    &quot;query&quot;: {\r\n        &quot;match_phrase&quot;: {\r\n            &quot;title&quot;: {\r\n                &quot;query&quot;: &quot;java spark&quot;,\r\n                &quot;slop&quot;:  3\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n就可以把刚才那个doc匹配上，那个doc会作为结果返回\r\n\r\n但是如果slop设置的是2，那么java spark，spark最多只能移动2次，此时跟doc是匹配不上的，那个doc是不会作为结果返回的\r\n\r\n做实验，验证slop的含义\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;query&quot;: &quot;spark data&quot;,\r\n        &quot;slop&quot;: 3\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nspark is best big data solution based on scala ,an programming language similar to java spark\r\n\r\nspark data\r\n	  --&gt; data\r\n	      --&gt; data\r\nspark		  --&gt; data\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;query&quot;: &quot;data spark&quot;,\r\n        &quot;slop&quot;: 5\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nspark		is				best		big			data\r\n\r\ndata		spark\r\n--&gt;			data/spark\r\nspark		&lt;--data\r\nspark		--&gt;				data\r\nspark						--&gt;			data\r\nspark									--&gt;			data\r\n\r\nslop搜索下，关键词离的越近，relevance score就会越高，做实验说明。。。\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1.3728157,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1.3728157,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Smith&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 0.5753642,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Tonny&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.28582606,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;,\r\n            &quot;hadoop&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 2,\r\n          &quot;view_cnt&quot;: 30,\r\n          &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;,\r\n          &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;,\r\n          &quot;sub_title&quot;: &quot;learning more courses&quot;,\r\n          &quot;author_first_name&quot;: &quot;Peter&quot;,\r\n          &quot;author_last_name&quot;: &quot;Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Peter&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;query&quot;: &quot;java best&quot;,\r\n        &quot;slop&quot;: 15\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 3,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0.65380025,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.65380025,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Smith&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 0.07111243,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Tonny&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n其实，加了slop的phrase match，就是proximity match，近似匹配\r\n\r\n1、java spark，短语，doc，phrase match\r\n2、java spark，可以有一定的距离，但是靠的越近，越先搜索出来，proximity match\r\n',8,0,0,1514613382,0,0,0),(111,1,'召回率','','','召回率\r\n\r\n比如你搜索一个java spark，总共有100个doc，能返回多少个doc作为结果，就是召回率，recall\r\n\r\n精准度\r\n\r\n比如你搜索一个java spark，能不能尽可能让包含java spark，或者是java和spark离的很近的doc，排在最前面，precision\r\n\r\n直接用match_phrase短语搜索，会导致必须所有term都在doc field中出现，而且距离在slop限定范围内，才能匹配上\r\n\r\nmatch phrase，proximity match，要求doc必须包含所有的term，才能作为结果返回；如果某一个doc可能就是有某个term没有包含，那么就无法作为结果返回\r\n\r\njava spark --&gt; hello world java --&gt; 就不能返回了\r\njava spark --&gt; hello world, java spark --&gt; 才可以返回\r\n\r\n近似匹配的时候，召回率比较低，精准度太高了\r\n\r\n但是有时可能我们希望的是匹配到几个term中的部分，就可以作为结果出来，这样可以提高召回率。同时我们也希望用上match_phrase根据距离提升分数的功能，让几个term距离越近分数就越高，优先返回\r\n\r\n就是优先满足召回率，意思，java spark，包含java的也返回，包含spark的也返回，包含java和spark的也返回；同时兼顾精准度，就是包含java和spark，同时java和spark离的越近的doc排在最前面\r\n\r\n此时可以用bool组合match query和match_phrase query一起，来实现上述效果\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: {\r\n        &quot;match&quot;: { \r\n          &quot;title&quot;: {\r\n            &quot;query&quot;:                &quot;java spark&quot; --&gt; java或spark或java spark，java和spark靠前，但是没法区分java和spark的距离，也许java和spark靠的很近，但是没法排在最前面\r\n          }\r\n        }\r\n      },\r\n      &quot;should&quot;: {\r\n        &quot;match_phrase&quot;: { --&gt; 在slop以内，如果java spark能匹配上一个doc，那么就会对doc贡献自己的relevance score，如果java和spark靠的越近，那么就分数越高\r\n          &quot;title&quot;: {\r\n            &quot;query&quot;: &quot;java spark&quot;,\r\n            &quot;slop&quot;:  50\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java spark&quot;\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0.68640786,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.68640786,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;followers&quot;: [\r\n            &quot;Tom&quot;,\r\n            &quot;Jack&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 0.68324494,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;followers&quot;: [\r\n            &quot;Jack&quot;,\r\n            &quot;Robbin Li&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java spark&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match_phrase&quot;: {\r\n            &quot;content&quot;: {\r\n              &quot;query&quot;: &quot;java spark&quot;,\r\n              &quot;slop&quot;: 50\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1.258609,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;5&quot;,\r\n        &quot;_score&quot;: 1.258609,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;,\r\n          &quot;userID&quot;: 3,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-03-01&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;elasticsearch&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 10,\r\n          &quot;title&quot;: &quot;this is spark blog&quot;,\r\n          &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;,\r\n          &quot;sub_title&quot;: &quot;haha, hello world&quot;,\r\n          &quot;author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Tonny&quot;,\r\n          &quot;followers&quot;: [\r\n            &quot;Jack&quot;,\r\n            &quot;Robbin Li&quot;\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;forum&quot;,\r\n        &quot;_type&quot;: &quot;article&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 0.68640786,\r\n        &quot;_source&quot;: {\r\n          &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;,\r\n          &quot;userID&quot;: 1,\r\n          &quot;hidden&quot;: false,\r\n          &quot;postDate&quot;: &quot;2017-01-02&quot;,\r\n          &quot;tag&quot;: [\r\n            &quot;java&quot;\r\n          ],\r\n          &quot;tag_cnt&quot;: 1,\r\n          &quot;view_cnt&quot;: 50,\r\n          &quot;title&quot;: &quot;this is java blog&quot;,\r\n          &quot;content&quot;: &quot;i think java is the best programming language&quot;,\r\n          &quot;sub_title&quot;: &quot;learned a lot of course&quot;,\r\n          &quot;author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_last_name&quot;: &quot;Williams&quot;,\r\n          &quot;new_author_first_name&quot;: &quot;Smith&quot;,\r\n          &quot;followers&quot;: [\r\n            &quot;Tom&quot;,\r\n            &quot;Jack&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514613536,0,0,0),(112,1,'match和phrase match(proximity m','','','match和phrase match(proximity match)区别\r\n\r\nmatch --&gt; 只要简单的匹配到了一个term，就可以理解将term对应的doc作为结果返回，扫描倒排索引，扫描到了就ok\r\n\r\nphrase match --&gt; 首先扫描到所有term的doc list; 找到包含所有term的doc list; 然后对每个doc都计算每个term的position，是否符合指定的范围; slop，需要进行复杂的运算，来判断能否通过slop移动，匹配一个doc\r\n\r\nmatch query的性能比phrase match和proximity match（有slop）要高很多。因为后两者都要计算position的距离。\r\nmatch query比phrase match的性能要高10倍，比proximity match的性能要高20倍。\r\n\r\n但是别太担心，因为es的性能一般都在毫秒级别，match query一般就在几毫秒，或者几十毫秒，而phrase match和proximity match的性能在几十毫秒到几百毫秒之间，所以也是可以接受的。\r\n\r\n优化proximity match的性能，一般就是减少要进行proximity match搜索的document数量。主要思路就是，用match query先过滤出需要的数据，然后再用proximity match来根据term距离提高doc的分数，同时proximity match只针对每个shard的分数排名前n个doc起作用，来重新调整它们的分数，这个过程称之为rescoring，重计分。因为一般用户会分页查询，只会看到前几页的数据，所以不需要对所有结果进行proximity match操作。\r\n\r\n用我们刚才的说法，match + proximity match同时实现召回率和精准度\r\n\r\n默认情况下，match也许匹配了1000个doc，proximity match全都需要对每个doc进行一遍运算，判断能否slop移动匹配上，然后去贡献自己的分数\r\n但是很多情况下，match出来也许1000个doc，其实用户大部分情况下是分页查询的，所以可能最多只会看前几页，比如一页是10条，最多也许就看5页，就是50条\r\nproximity match只要对前50个doc进行slop移动去匹配，去贡献自己的分数即可，不需要对全部1000个doc都去进行计算和贡献分数\r\n\r\nrescore：重打分\r\n\r\nmatch：1000个doc，其实这时候每个doc都有一个分数了; proximity match，前50个doc，进行rescore，重打分，即可; 让前50个doc，term举例越近的，排在越前面\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;java spark&quot;\r\n    }\r\n  },\r\n  &quot;rescore&quot;: {\r\n    &quot;window_size&quot;: 50,\r\n    &quot;query&quot;: {\r\n      &quot;rescore_query&quot;: {\r\n        &quot;match_phrase&quot;: {\r\n          &quot;content&quot;: {\r\n            &quot;query&quot;: &quot;java spark&quot;,\r\n            &quot;slop&quot;: 50\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514613593,0,0,0),(113,1,'前缀搜索','','','1、前缀搜索\r\n\r\nC3D0-KD345\r\nC3K5-DFG65\r\nC4I8-UI365\r\n\r\nC3 --&gt; 上面这两个都搜索出来 --&gt; 根据字符串的前缀去搜索\r\n\r\n不用帖子的案例背景，因为比较简单，直接用自己手动建的新索引，给大家演示一下就可以了\r\n\r\nPUT my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;keyword&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET my_index/my_type/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;prefix&quot;: {\r\n      &quot;title&quot;: {\r\n        &quot;value&quot;: &quot;C3&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2、前缀搜索的原理\r\n\r\nprefix query不计算relevance score，与prefix filter唯一的区别就是，filter会cache bitset\r\n\r\n扫描整个倒排索引，举例说明\r\n\r\n前缀越短，要处理的doc越多，性能越差，尽可能用长前缀搜索\r\n\r\n前缀搜索，它是怎么执行的？性能为什么差呢？\r\n\r\nmatch\r\n\r\nC3-D0-KD345\r\nC3-K5-DFG65\r\nC4-I8-UI365\r\n\r\n全文检索\r\n\r\n每个字符串都需要被分词\r\n\r\nc3			doc1,doc2\r\nd0\r\nkd345\r\nk5\r\ndfg65\r\nc4\r\ni8\r\nui365\r\n\r\nc3 --&gt; 扫描倒排索引 --&gt; 一旦扫描到c3，就可以停了，因为带c3的就2个doc，已经找到了 --&gt; 没有必要继续去搜索其他的term了\r\n\r\nmatch性能往往是很高的\r\n\r\n不分词\r\n\r\nC3-D0-KD345\r\nC3-K5-DFG65\r\nC4-I8-UI365\r\n\r\nc3 --&gt; 先扫描到了C3-D0-KD345，很棒，找到了一个前缀带c3的字符串 --&gt; 还是要继续搜索的，因为后面还有一个C3-K5-DFG65，也许还有其他很多的前缀带c3的字符串 --&gt; 你扫描到了一个前缀匹配的term，不能停，必须继续搜索 --&gt; 直到扫描完整个的倒排索引，才能结束\r\n\r\n因为实际场景中，可能有些场景是全文检索解决不了的\r\n\r\nC3D0-KD345\r\nC3K5-DFG65\r\nC4I8-UI365\r\n\r\nc3d0\r\nkd345\r\n\r\nc3 --&gt; match --&gt; 扫描整个倒排索引，能找到吗\r\n\r\nc3 --&gt; 只能用prefix\r\n\r\nprefix性能很差\r\n\r\n3、通配符搜索\r\n\r\n跟前缀搜索类似，功能更加强大\r\n\r\nC3D0-KD345\r\nC3K5-DFG65\r\nC4I8-UI365\r\n\r\n5字符-D任意个字符5\r\n\r\n5?-*5：通配符去表达更加复杂的模糊搜索的语义\r\n\r\nGET my_index/my_type/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;wildcard&quot;: {\r\n      &quot;title&quot;: {\r\n        &quot;value&quot;: &quot;C?K*5&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n?：任意字符\r\n*：0个或任意多个字符\r\n\r\n性能一样差，必须扫描整个倒排索引，才ok\r\n\r\n4、正则搜索\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;regexp&quot;: {\r\n      &quot;title&quot;: &quot;C[0-9].+&quot;\r\n    }\r\n  }\r\n}\r\n\r\nC[0-9].+\r\n\r\n[0-9]：指定范围内的数字\r\n[a-z]：指定范围内的字母\r\n.：一个字符\r\n+：前面的正则表达式可以出现一次或多次\r\n\r\nwildcard和regexp，与prefix原理一致，都会扫描整个索引，性能很差\r\n\r\n主要是给大家介绍一些高级的搜索语法。在实际应用中，能不用尽量别用。性能太差了。\r\n',8,0,0,1514613629,0,0,0),(114,1,'搜索推荐，search as you type','','','搜索推荐，search as you type，搜索提示，解释一下什么意思\r\n\r\nhello w --&gt; 搜索\r\n\r\nhello world\r\nhello we\r\nhello win\r\nhello wind\r\nhello dog\r\nhello cat\r\n\r\nhello w --&gt;\r\n\r\nhello world\r\nhello we\r\nhello win\r\nhello wind\r\n\r\n搜索推荐的功能\r\n\r\n百度 --&gt; elas --&gt; elasticsearch --&gt; elasticsearch权威指南\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase_prefix&quot;: {\r\n      &quot;title&quot;: &quot;hello d&quot;\r\n    }\r\n  }\r\n}\r\n\r\n原理跟match_phrase类似，唯一的区别，就是把最后一个term作为前缀去搜索\r\n\r\nhello就是去进行match，搜索对应的doc\r\nw，会作为前缀，去扫描整个倒排索引，找到所有w开头的doc\r\n然后找到所有doc中，即包含hello，又包含w开头的字符的doc\r\n根据你的slop去计算，看在slop范围内，能不能让hello w，正好跟doc中的hello和w开头的单词的position相匹配\r\n\r\n也可以指定slop，但是只有最后一个term会作为前缀\r\n\r\nmax_expansions：指定prefix最多匹配多少个term，超过这个数量就不继续匹配了，限定性能\r\n\r\n默认情况下，前缀要扫描所有的倒排索引中的term，去查找w打头的单词，但是这样性能太差。可以用max_expansions限定，w前缀最多匹配多少个term，就不再继续搜索倒排索引了。\r\n\r\n尽量不要用，因为，最后一个前缀始终要去扫描大量的索引，性能可能会很差\r\n',8,0,0,1514613705,0,0,0),(115,1,'ngram和index-time搜索推荐原理','','','1、ngram和index-time搜索推荐原理\r\n\r\n什么是ngram\r\n\r\nquick，5种长度下的ngram\r\n\r\nngram length=1，q u i c k\r\nngram length=2，qu ui ic ck\r\nngram length=3，qui uic ick\r\nngram length=4，quic uick\r\nngram length=5，quick\r\n\r\n什么是edge ngram\r\n\r\nquick，anchor首字母后进行ngram\r\n\r\nq\r\nqu\r\nqui\r\nquic\r\nquick\r\n\r\n使用edge ngram将每个单词都进行进一步的分词切分，用切分后的ngram来实现前缀搜索推荐功能\r\n\r\nhello world\r\nhello we\r\n\r\nh\r\nhe\r\nhel\r\nhell\r\nhello		doc1,doc2\r\n\r\nw			doc1,doc2\r\nwo\r\nwor\r\nworl\r\nworld\r\ne			doc2\r\n\r\nhelloworld\r\n\r\nmin ngram = 1\r\nmax ngram = 3\r\n\r\nh\r\nhe\r\nhel\r\n\r\nhello w\r\n\r\nhello --&gt; hello，doc1\r\nw --&gt; w，doc1\r\n\r\ndoc1，hello和w，而且position也匹配，所以，ok，doc1返回，hello world\r\n\r\n搜索的时候，不用再根据一个前缀，然后扫描整个倒排索引了; 简单的拿前缀去倒排索引中匹配即可，如果匹配上了，那么就好了; match，全文检索\r\n\r\n2、实验一下ngram\r\n\r\nPUT /my_index\r\n{\r\n    &quot;settings&quot;: {\r\n        &quot;analysis&quot;: {\r\n            &quot;filter&quot;: {\r\n                &quot;autocomplete_filter&quot;: { \r\n                    &quot;type&quot;:     &quot;edge_ngram&quot;,\r\n                    &quot;min_gram&quot;: 1,\r\n                    &quot;max_gram&quot;: 20\r\n                }\r\n            },\r\n            &quot;analyzer&quot;: {\r\n                &quot;autocomplete&quot;: {\r\n                    &quot;type&quot;:      &quot;custom&quot;,\r\n                    &quot;tokenizer&quot;: &quot;standard&quot;,\r\n                    &quot;filter&quot;: [\r\n                        &quot;lowercase&quot;,\r\n                        &quot;autocomplete_filter&quot; \r\n                    ]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;analyzer&quot;: &quot;autocomplete&quot;,\r\n  &quot;text&quot;: &quot;quick brown&quot;\r\n}\r\n\r\nPUT /my_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n      &quot;title&quot;: {\r\n          &quot;type&quot;:     &quot;string&quot;,\r\n          &quot;analyzer&quot;: &quot;autocomplete&quot;,\r\n          &quot;search_analyzer&quot;: &quot;standard&quot;\r\n      }\r\n  }\r\n}\r\n\r\nhello world\r\n\r\nh\r\nhe\r\nhel\r\nhell\r\nhello		\r\n\r\nw			\r\nwo\r\nwor\r\nworl\r\nworld\r\n\r\nhello w\r\n\r\nh\r\nhe\r\nhel\r\nhell\r\nhello	\r\n\r\nw\r\n\r\nhello w --&gt; hello --&gt; w\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match_phrase&quot;: {\r\n      &quot;title&quot;: &quot;hello w&quot;\r\n    }\r\n  }\r\n}\r\n\r\n如果用match，只有hello的也会出来，全文检索，只是分数比较低\r\n推荐使用match_phrase，要求每个term都有，而且position刚好靠着1位，符合我们的期望的\r\n\r\n\r\n',8,0,0,1514613746,0,0,0),(116,1,'boolean model','','','1、boolean model\r\n\r\n类似and这种逻辑操作符，先过滤出包含指定term的doc\r\n\r\nquery &quot;hello world&quot; --&gt; 过滤 --&gt; hello / world / hello &amp; world\r\nbool --&gt; must/must not/should --&gt; 过滤 --&gt; 包含 / 不包含 / 可能包含\r\ndoc --&gt; 不打分数 --&gt; 正或反 true or false --&gt; 为了减少后续要计算的doc的数量，提升性能\r\n\r\n2、TF/IDF\r\n\r\n单个term在doc中的分数\r\n\r\nquery: hello world --&gt; doc.content\r\ndoc1: java is my favourite programming language, hello world !!!\r\ndoc2: hello java, you are very good, oh hello world!!!\r\n\r\nhello对doc1的评分\r\n\r\nTF: term frequency \r\n\r\n找到hello在doc1中出现了几次，1次，会根据出现的次数给个分数\r\n一个term在一个doc中，出现的次数越多，那么最后给的相关度评分就会越高\r\n\r\nIDF：inversed document frequency\r\n\r\n找到hello在所有的doc中出现的次数，3次\r\n一个term在所有的doc中，出现的次数越多，那么最后给的相关度评分就会越低\r\n\r\nlength norm\r\n\r\nhello搜索的那个field的长度，field长度越长，给的相关度评分越低; field长度越短，给的相关度评分越高\r\n\r\n最后，会将hello这个term，对doc1的分数，综合TF，IDF，length norm，计算出来一个综合性的分数\r\n\r\nhello world --&gt; doc1 --&gt; hello对doc1的分数，world对doc1的分数 --&gt; 但是最后hello world query要对doc1有一个总的分数 --&gt; vector space model\r\n\r\n3、vector space model\r\n\r\n多个term对一个doc的总分数\r\n\r\nhello world --&gt; es会根据hello world在所有doc中的评分情况，计算出一个query vector，query向量\r\n\r\nhello这个term，给的基于所有doc的一个评分就是2\r\nworld这个term，给的基于所有doc的一个评分就是5\r\n\r\n[2, 5]\r\n\r\nquery vector\r\n\r\ndoc vector，3个doc，一个包含1个term，一个包含另一个term，一个包含2个term\r\n\r\n3个doc\r\n\r\ndoc1：包含hello --&gt; [2, 0]\r\ndoc2：包含world --&gt; [0, 5]\r\ndoc3：包含hello, world --&gt; [2, 5]\r\n\r\n会给每一个doc，拿每个term计算出一个分数来，hello有一个分数，world有一个分数，再拿所有term的分数组成一个doc vector\r\n\r\n画在一个图中，取每个doc vector对query vector的弧度，给出每个doc对多个term的总分数\r\n\r\n每个doc vector计算出对query vector的弧度，最后基于这个弧度给出一个doc相对于query中多个term的总分数\r\n弧度越大，分数月底; 弧度越小，分数越高\r\n\r\n如果是多个term，那么就是线性代数来计算，无法用图表示\r\n',8,0,0,1514613799,0,0,0),(117,1,'boolean mode','','','我们boolean model、TF/IDF、vector space model\r\n\r\n深入讲解TF/IDF算法，在lucene中，底层，到底进行TF/IDF算法计算的一个完整的公式是什么？\r\n\r\n0、boolean model\r\n\r\nquery: hello world\r\n\r\n&quot;match&quot;: {\r\n	&quot;title&quot;: &quot;hello world&quot;\r\n}\r\n\r\n&quot;bool&quot;: {\r\n	&quot;should&quot;: [\r\n		{\r\n			&quot;match&quot;: {\r\n				&quot;title&quot;: &quot;hello&quot;\r\n			}\r\n		},\r\n		{\r\n			&quot;natch&quot;: {\r\n				&quot;title&quot;: &quot;world&quot;\r\n			}\r\n		}\r\n	]\r\n}\r\n\r\n普通multivalue搜索，转换为bool搜索，boolean model\r\n\r\n1、lucene practical scoring function\r\n\r\npractical scoring function，来计算一个query对一个doc的分数的公式，该函数会使用一个公式来计算\r\n\r\nscore(q,d)  =  \r\n            queryNorm(q)  \r\n          · coord(q,d)    \r\n          · ∑ (           \r\n                tf(t in d)   \r\n              · idf(t)2      \r\n              · t.getBoost() \r\n              · norm(t,d)    \r\n            ) (t in q) \r\n\r\nscore(q,d) score(q,d) is the relevance score of document d for query q.\r\n\r\n这个公式的最终结果，就是说是一个query（叫做q），对一个doc（叫做d）的最终的总评分\r\n\r\nqueryNorm(q) is the query normalization factor (new).\r\n\r\nqueryNorm，是用来让一个doc的分数处于一个合理的区间内，不要太离谱，举个例子，一个doc分数是10000，一个doc分数是0.1，你们说好不好，肯定不好\r\n\r\ncoord(q,d) is the coordination factor (new).\r\n\r\n简单来说，就是对更加匹配的doc，进行一些分数上的成倍的奖励\r\n\r\nThe sum of the weights for each term t in the query q for document d.\r\n\r\n∑：求和的符号\r\n\r\n∑ (t in q)：query中每个term，query = hello world，query中的term就包含了hello和world\r\n\r\nquery中每个term对doc的分数，进行求和，多个term对一个doc的分数，组成一个vector space，然后计算吗，就在这一步\r\n\r\ntf(t in d) is the term frequency for term t in document d.\r\n\r\n计算每一个term对doc的分数的时候，就是TF/IDF算法\r\n\r\nidf(t) is the inverse document frequency for term t.\r\n\r\nt.getBoost() is the boost that has been applied to the query (new).\r\n\r\nnorm(t,d) is the field-length norm, combined with the index-time field-level boost, if any. (new).\r\n\r\n2、query normalization factor\r\n\r\nqueryNorm = 1 / √sumOfSquaredWeights\r\n\r\nsumOfSquaredWeights = 所有term的IDF分数之和，开一个平方根，然后做一个平方根分之1\r\n主要是为了将分数进行规范化 --&gt; 开平方根，首先数据就变小了 --&gt; 然后还用1去除以这个平方根，分数就会很小 --&gt; 1.几 / 零点几\r\n分数就不会出现几万，几十万，那样的离谱的分数\r\n\r\n3、query coodination\r\n\r\n奖励那些匹配更多字符的doc更多的分数\r\n\r\nDocument 1 with hello → score: 1.5\r\nDocument 2 with hello world → score: 3.0\r\nDocument 3 with hello world java → score: 4.5\r\n\r\nDocument 1 with hello → score: 1.5 * 1 / 3 = 0.5\r\nDocument 2 with hello world → score: 3.0 * 2 / 3 = 2.0\r\nDocument 3 with hello world java → score: 4.5 * 3 / 3 = 4.5\r\n\r\n把计算出来的总分数 * 匹配上的term数量 / 总的term数量，让匹配不同term/query数量的doc，分数之间拉开差距\r\n\r\n4、field level boost',8,0,0,1514613877,0,0,0),(118,1,'query-time boost','','','之前两节课，我觉得已经很了解整个es的相关度评分的算法了，算法思想，TF/IDF，vector model，boolean model; 实际的公式，query norm，query coordination，boost\r\n\r\n对相关度评分进行调节和优化的常见的4种方法\r\n\r\n1、query-time boost\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: {\r\n              &quot;query&quot;: &quot;java spark&quot;,\r\n              &quot;boost&quot;: 2\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java spark&quot;\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n2、重构查询结构\r\n\r\n重构查询结果，在es新版本中，影响越来越小了。一般情况下，没什么必要的话，大家不用也行。\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;spark&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;bool&quot;: {\r\n            &quot;should&quot;: [\r\n              {\r\n                &quot;match&quot;: {\r\n                  &quot;content&quot;: &quot;solution&quot;\r\n                }\r\n              },\r\n              {\r\n                &quot;match&quot;: {\r\n                  &quot;content&quot;: &quot;beginner&quot;\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n3、negative boost\r\n\r\n搜索包含java，不包含spark的doc，但是这样子很死板\r\n搜索包含java，尽量不包含spark的doc，如果包含了spark，不会说排除掉这个doc，而是说将这个doc的分数降低\r\n包含了negative term的doc，分数乘以negative boost，分数降低\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;java&quot;\r\n          }\r\n        }\r\n      ],\r\n      &quot;must_not&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;spark&quot;\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;boosting&quot;: {\r\n      &quot;positive&quot;: {\r\n        &quot;match&quot;: {\r\n          &quot;content&quot;: &quot;java&quot;\r\n        }\r\n      },\r\n      &quot;negative&quot;: {\r\n        &quot;match&quot;: {\r\n          &quot;content&quot;: &quot;spark&quot;\r\n        }\r\n      },\r\n      &quot;negative_boost&quot;: 0.2\r\n    }\r\n  }\r\n}\r\n\r\nnegative的doc，会乘以negative_boost，降低分数\r\n\r\n4、constant_score\r\n\r\n如果你压根儿不需要相关度评分，直接走constant_score加filter，所有的doc分数都是1，没有评分的概念了\r\n\r\nGET /forum/article/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;constant_score&quot;: {\r\n            &quot;query&quot;: {\r\n              &quot;match&quot;: {\r\n                &quot;title&quot;: &quot;java&quot;\r\n              }\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;constant_score&quot;: {\r\n            &quot;query&quot;: {\r\n              &quot;match&quot;: {\r\n                &quot;title&quot;: &quot;spark&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514613937,0,0,0),(119,1,'帖子数据增加follower数量','','','我们可以做到自定义一个function_score函数，自己将某个field的值，跟es内置算出来的分数进行运算，然后由自己指定的field来进行分数的增强\r\n\r\n给所有的帖子数据增加follower数量\r\n\r\nPOST /forum/article/_bulk\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 5} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 10} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 25} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 3} }\r\n{ &quot;update&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;doc&quot; : {&quot;follower_num&quot; : 60} }\r\n\r\n将对帖子搜索得到的分数，跟follower_num进行运算，由follower_num在一定程度上增强帖子的分数\r\n看帖子的人越多，那么帖子的分数就越高\r\n\r\nGET /forum/article/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;function_score&quot;: {\r\n      &quot;query&quot;: {\r\n        &quot;multi_match&quot;: {\r\n          &quot;query&quot;: &quot;java spark&quot;,\r\n          &quot;fields&quot;: [&quot;tile&quot;, &quot;content&quot;]\r\n        }\r\n      },\r\n      &quot;field_value_factor&quot;: {\r\n        &quot;field&quot;: &quot;follower_num&quot;,\r\n        &quot;modifier&quot;: &quot;log1p&quot;,\r\n        &quot;factor&quot;: 0.5\r\n      },\r\n      &quot;boost_mode&quot;: &quot;sum&quot;,\r\n      &quot;max_boost&quot;: 2\r\n    }\r\n  }\r\n}\r\n\r\n如果只有field，那么会将每个doc的分数都乘以follower_num，如果有的doc follower是0，那么分数就会变为0，效果很不好。因此一般会加个log1p函数，公式会变为，new_score = old_score * log(1 + number_of_votes)，这样出来的分数会比较合理\r\n再加个factor，可以进一步影响分数，new_score = old_score * log(1 + factor * number_of_votes)\r\nboost_mode，可以决定分数与指定字段的值如何计算，multiply，sum，min，max，replace\r\nmax_boost，限制计算出来的分数不要超过max_boost指定的值\r\n',8,0,0,1514613993,0,0,0),(120,1,' fuzzy搜索技术','','','搜索的时候，可能输入的搜索文本会出现误拼写的情况\r\n\r\ndoc1: hello world\r\ndoc2: hello java\r\n\r\n搜索：hallo world\r\n\r\nfuzzy搜索技术 --&gt; 自动将拼写错误的搜索文本，进行纠正，纠正以后去尝试匹配索引中的数据\r\n\r\nPOST /my_index/my_type/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 1 }}\r\n{ &quot;text&quot;: &quot;Surprise me!&quot;}\r\n{ &quot;index&quot;: { &quot;_id&quot;: 2 }}\r\n{ &quot;text&quot;: &quot;That was surprising.&quot;}\r\n{ &quot;index&quot;: { &quot;_id&quot;: 3 }}\r\n{ &quot;text&quot;: &quot;I wasn&#039;t surprised.&quot;}\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;fuzzy&quot;: {\r\n      &quot;text&quot;: {\r\n        &quot;value&quot;: &quot;surprize&quot;,\r\n        &quot;fuzziness&quot;: 2\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nsurprize --&gt; 拼写错误 --&gt; surprise --&gt; s -&gt; z\r\n\r\nsurprize --&gt; surprise -&gt; z -&gt; s，纠正一个字母，就可以匹配上，所以在fuziness指定的2范围内\r\nsurprize --&gt; surprised -&gt; z -&gt; s，末尾加个d，纠正了2次，也可以匹配上，在fuziness指定的2范围内\r\nsurprize --&gt; surprising -&gt; z -&gt; s，去掉e，ing，3次，总共要5次，才可以匹配上，始终纠正不了\r\n\r\nfuzzy搜索以后，会自动尝试将你的搜索文本进行纠错，然后去跟文本进行匹配\r\nfuzziness，你的搜索文本最多可以纠正几个字母去跟你的数据进行匹配，默认如果不设置，就是2\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;text&quot;: {\r\n        &quot;query&quot;: &quot;SURPIZE ME&quot;,\r\n        &quot;fuzziness&quot;: &quot;AUTO&quot;,\r\n        &quot;operator&quot;: &quot;and&quot;\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514614036,0,0,0),(121,1,'中文分词','','','之前大家会发现，我们全部是用英文在玩儿。。。好玩儿不好玩儿。。。不好玩儿\r\n\r\n中国人，其实我们用来进行搜索的，绝大多数，都是中文应用，很少做英文的\r\nstandard：没有办法对中文进行合理分词的，只是将每个中文字符一个一个的切割开来，比如说中国人 --&gt; 中 国 人\r\n\r\n英语的也要学：所以说，我们利用核心知识篇的相关的知识，来把es这种英文原生的搜索引擎，先学一下; 因为有些知识点，可能用英文讲更靠谱，因为比如说analyzed，palyed，students --&gt; stemmer，analyze，play，student。有些知识点，仅仅适用于英文，不太适用于中文\r\n\r\n从这一讲开始，大家就会觉得很爽，因为全部都是我们熟悉的中文了，没有英文了，高阶知识点，搜索，聚合，全部是中文了\r\n\r\n在搜索引擎领域，比较成熟和流行的，就是ik分词器\r\n\r\n中国人很喜欢吃油条\r\n\r\nstandard：中 国 人 很 喜 欢 吃 油 条\r\nik：中国人 很 喜欢 吃 油条\r\n\r\n1、在elasticsearch中安装ik中文分词器\r\n\r\n（1）git clone https://github.com/medcl/elasticsearch-analysis-ik\r\n（2）git checkout tags/v5.2.0\r\n（3）mvn package\r\n（4）将target/releases/elasticsearch-analysis-ik-5.2.0.zip拷贝到es/plugins/ik目录下\r\n（5）在es/plugins/ik下对elasticsearch-analysis-ik-5.2.0.zip进行解压缩\r\n在plugins目录下创建ik目录 --- 然后进行解压缩 --- 会出现以下内容\r\nconfig\r\ncommons-codec-1.9.jar\r\ncommons-logging-1.2.jar\r\nelasticsearch-analysis-ik-5.2.0.jar\r\nhttpclient-4.5.2.jar\r\nhttpcore-4.4.4jar\r\n（6）重启es\r\n\r\n2、ik分词器基础知识\r\n\r\n两种analyzer，你根据自己的需要自己选吧，但是一般是选用ik_max_word\r\n\r\nik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合；\r\n\r\nik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。\r\n\r\n共和国 --&gt; 中华人民共和国和国歌，搜到吗？？？？\r\n\r\n3、ik分词器的使用\r\n\r\nPUT /my_index \r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;text&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPOST /my_index/my_type/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;1&quot;} }\r\n{ &quot;text&quot;: &quot;男子偷上万元发红包求交女友 被抓获时仍然单身&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;2&quot;} }\r\n{ &quot;text&quot;: &quot;16岁少女为结婚“变”22岁 7年后想离婚被法院拒绝&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;3&quot;} }\r\n{ &quot;text&quot;: &quot;深圳女孩骑车逆行撞奔驰 遭索赔被吓哭(图)&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;4&quot;} }\r\n{ &quot;text&quot;: &quot;女人对护肤品比对男票好？网友神怼&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;5&quot;} }\r\n{ &quot;text&quot;: &quot;为什么国内的街道招牌用的都是红黄配？&quot; }\r\n\r\nGET /my_index/_analyze\r\n{\r\n  &quot;text&quot;: &quot;男子偷上万元发红包求交女友 被抓获时仍然单身&quot;,\r\n  &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n}\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;text&quot;: &quot;16岁少女结婚好还是单身好？&quot;\r\n    }\r\n  }\r\n}',8,0,0,1514614361,0,0,0),(122,1,'ik配置文件','','','1、ik配置文件\r\n\r\nik配置文件地址：es/plugins/ik/config目录\r\n\r\nIKAnalyzer.cfg.xml：用来配置自定义词库\r\nmain.dic：ik原生内置的中文词库，总共有27万多条，只要是这些单词，都会被分在一起\r\nquantifier.dic：放了一些单位相关的词\r\nsuffix.dic：放了一些后缀\r\nsurname.dic：中国的姓氏\r\nstopword.dic：英文停用词\r\n\r\nik原生最重要的两个配置文件\r\n\r\nmain.dic：包含了原生的中文词语，会按照这个里面的词语去分词\r\nstopword.dic：包含了英文的停用词\r\n\r\n停用词，stopword\r\n\r\na the and at but\r\n\r\n一般，像停用词，会在分词的时候，直接被干掉，不会建立在倒排索引中\r\n\r\n2、自定义词库\r\n\r\n（1）自己建立词库：每年都会涌现一些特殊的流行词，网红，蓝瘦香菇，喊麦，鬼畜，一般不会在ik的原生词典里\r\n\r\n自己补充自己的最新的词语，到ik的词库里面去\r\n\r\nIKAnalyzer.cfg.xml：ext_dict，custom/mydict.dic\r\n\r\n补充自己的词语，然后需要重启es，才能生效\r\n\r\n（2）自己建立停用词库：比如了，的，啥，么，我们可能并不想去建立索引，让人家搜索\r\n\r\ncustom/ext_stopword.dic，已经有了常用的中文停用词，可以补充自己的停用词，然后重启es\r\n',8,0,0,1514614473,0,0,0),(123,1,'热更新','','','热更新\r\n\r\n每次都是在es的扩展词典中，手动添加新词语，很坑\r\n（1）每次添加完，都要重启es才能生效，非常麻烦\r\n（2）es是分布式的，可能有数百个节点，你不能每次都一个一个节点上面去修改\r\n\r\nes不停机，直接我们在外部某个地方添加新的词语，es中立即热加载到这些新词语\r\n\r\n热更新的方案\r\n\r\n（1）修改ik分词器源码，然后手动支持从mysql中每隔一定时间，自动加载新的词库\r\n（2）基于ik分词器原生支持的热更新方案，部署一个web服务器，提供一个http接口，通过modified和tag两个http响应头，来提供词语的热更新\r\n\r\n用第一种方案，第二种，ik git社区官方都不建议采用，觉得不太稳定\r\n\r\n1、下载源码\r\n\r\nhttps://github.com/medcl/elasticsearch-analysis-ik/tree/v5.2.0\r\n\r\nik分词器，是个标准的java maven工程，直接导入eclipse就可以看到源码\r\n\r\n2、修改源码\r\n\r\nDictionary类，169行：Dictionary单例类的初始化方法，在这里需要创建一个我们自定义的线程，并且启动它\r\nHotDictReloadThread类：就是死循环，不断调用Dictionary.getSingleton().reLoadMainDict()，去重新加载词典\r\nDictionary类，389行：this.loadMySQLExtDict();\r\nDictionary类，683行：this.loadMySQLStopwordDict();\r\n\r\n3、mvn package打包代码\r\n\r\ntarget\\releases\\elasticsearch-analysis-ik-5.2.0.zip\r\n\r\n4、解压缩ik压缩包\r\n\r\n将mysql驱动jar，放入ik的目录下\r\n\r\n5、修改jdbc相关配置\r\ncreate table hot_words(\r\n    id int unsigned not null auto_increment primary key,\r\n	word varchar(50) not null default &#039;&#039; comment &#039;热词&#039;\r\n)engine=innodb default charset=utf8 comment=&#039;热词表&#039;;\r\n\r\ncreate table hot_stopwords(\r\n    id int unsigned not null auto_increment primary key,\r\n	stopword varchar(50) not null default &#039;&#039; comment &#039;停用词&#039;\r\n)engine=innodb default charset=utf8 comment=&#039;停用词表&#039;;\r\n\r\n\r\n6、重启es\r\n\r\n观察日志，日志中就会显示我们打印的那些东西，比如加载了什么配置，加载了什么词语，什么停用词\r\n\r\n7、在mysql中添加词库与停用词\r\n\r\n8、分词实验，验证热更新生效',8,0,0,1514614518,0,0,0),(124,1,'文本编辑器介绍','','','1、文本编辑器介绍\r\n\r\n（1）windows操作系统，原生的txt文本编辑器，一些json格式，不太方便去调整\r\n（2）notepad++，功能不是太丰富\r\n（3）sublime，整个功能也比较丰富，比较好，自己可以上网去下载，官网，免费的\r\n\r\n2、两个核心概念：bucket和metric\r\n\r\nbucket：一个数据分组\r\n\r\ncity name\r\n\r\n北京 小李\r\n北京 小王\r\n上海 小张\r\n上海 小丽\r\n上海 小陈\r\n\r\n基于city划分buckets\r\n\r\n划分出来两个bucket，一个是北京bucket，一个是上海bucket\r\n\r\n北京bucket：包含了2个人，小李，小王\r\n上海bucket：包含了3个人，小张，小丽，小陈\r\n\r\n按照某个字段进行bucket划分，那个字段的值相同的那些数据，就会被划分到一个bucket中\r\n\r\n有一些mysql的sql知识的话，聚合，首先第一步就是分组，对每个组内的数据进行聚合分析，分组，就是我们的bucket\r\n\r\nmetric：对一个数据分组执行的统计\r\n\r\n当我们有了一堆bucket之后，就可以对每个bucket中的数据进行聚合分词了，比如说计算一个bucket内所有数据的数量，或者计算一个bucket内所有数据的平均值，最大值，最小值\r\n\r\nmetric，就是对一个bucket执行的某种聚合分析的操作，比如说求平均值，求最大值，求最小值\r\n\r\nselect count(*)\r\nfrom access_log\r\ngroup by user_id\r\n\r\nbucket：group by user_id --&gt; 那些user_id相同的数据，就会被划分到一个bucket中\r\nmetric：count(*)，对每个user_id bucket中所有的数据，计算一个数量\r\n',8,0,0,1514614564,0,0,0),(125,1,'家电卖场案例背景','','','1、家电卖场案例背景\r\n\r\n以一个家电卖场中的电视销售数据为背景，来对各种品牌，各种颜色的电视的销量和销售额，进行各种各样角度的分析\r\n\r\nPUT /tvs\r\n{\r\n	&quot;mappings&quot;: {\r\n		&quot;sales&quot;: {\r\n			&quot;properties&quot;: {\r\n				&quot;price&quot;: {\r\n					&quot;type&quot;: &quot;long&quot;\r\n				},\r\n				&quot;color&quot;: {\r\n					&quot;type&quot;: &quot;keyword&quot;\r\n				},\r\n				&quot;brand&quot;: {\r\n					&quot;type&quot;: &quot;keyword&quot;\r\n				},\r\n				&quot;sold_date&quot;: {\r\n					&quot;type&quot;: &quot;date&quot;\r\n				}\r\n			}\r\n		}\r\n	}\r\n}\r\n\r\nPOST /tvs/sales/_bulk\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 1000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 3000, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2016-05-18&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 1500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-07-02&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 1200, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-08-19&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 8000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;三星&quot;, &quot;sold_date&quot; : &quot;2017-01-01&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;price&quot; : 2500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2017-02-12&quot; }\r\n\r\n2、统计哪种颜色的电视销量最高\r\n\r\nGET /tvs/sales/_search\r\n{\r\n    &quot;size&quot; : 0,\r\n    &quot;aggs&quot; : { \r\n        &quot;popular_colors&quot; : { \r\n            &quot;terms&quot; : { \r\n              &quot;field&quot; : &quot;color&quot;\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nsize：只获取聚合结果，而不要执行聚合的原始数据\r\naggs：固定语法，要对一份数据执行分组聚合操作\r\npopular_colors：就是对每个aggs，都要起一个名字，这个名字是随机的，你随便取什么都ok\r\nterms：根据字段的值进行分组\r\nfield：根据指定的字段的值进行分组\r\n\r\n{\r\n  &quot;took&quot;: 61,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;popular_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nhits.hits：我们指定了size是0，所以hits.hits就是空的，否则会把执行聚合的那些原始数据给你返回回来\r\naggregations：聚合结果\r\npopular_color：我们指定的某个聚合的名称\r\nbuckets：根据我们指定的field划分出的buckets\r\nkey：每个bucket对应的那个值\r\ndoc_count：这个bucket分组内，有多少个数据\r\n数量，其实就是这种颜色的销量\r\n\r\n每种颜色对应的bucket中的数据的\r\n默认的排序规则：按照doc_count降序排序',8,0,0,1514614642,0,0,0),(126,1,'按照color去分bucket','','','GET /tvs/sales/_search\r\n{\r\n   &quot;size&quot; : 0,\r\n   &quot;aggs&quot;: {\r\n      &quot;colors&quot;: {\r\n         &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;color&quot;\r\n         },\r\n         &quot;aggs&quot;: { \r\n            &quot;avg_price&quot;: { \r\n               &quot;avg&quot;: {\r\n                  &quot;field&quot;: &quot;price&quot; \r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n按照color去分bucket，可以拿到每个color bucket中的数量，这个仅仅只是一个bucket操作，doc_count其实只是es的bucket操作默认执行的一个内置metric\r\n\r\n这一讲，就是除了bucket操作，分组，还要对每个bucket执行一个metric聚合统计操作\r\n\r\n在一个aggs执行的bucket操作（terms），平级的json结构下，再加一个aggs，这个第二个aggs内部，同样取个名字，执行一个metric操作，avg，对之前的每个bucket中的数据的指定的field，price field，求一个平均值\r\n\r\n&quot;aggs&quot;: { \r\n   &quot;avg_price&quot;: { \r\n      &quot;avg&quot;: {\r\n         &quot;field&quot;: &quot;price&quot; \r\n      }\r\n   }\r\n}\r\n\r\n就是一个metric，就是一个对一个bucket分组操作之后，对每个bucket都要执行的一个metric\r\n\r\n第一个metric，avg，求指定字段的平均值\r\n\r\n{\r\n  &quot;took&quot;: 28,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 3250\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2100\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2000\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nbuckets，除了key和doc_count\r\navg_price：我们自己取的metric aggs的名字\r\nvalue：我们的metric计算的结果，每个bucket中的数据的price字段求平均值后的结果\r\n\r\nselect avg(price)\r\nfrom tvs.sales\r\ngroup by color',8,0,0,1514614743,0,0,0),(127,1,'从颜色到品牌进行下钻分析','','','从颜色到品牌进行下钻分析，每种颜色的平均价格，以及找到每种颜色每个品牌的平均价格\r\n\r\n我们可以进行多层次的下钻\r\n\r\n比如说，现在红色的电视有4台，同时这4台电视中，有3台是属于长虹的，1台是属于小米的\r\n\r\n红色电视中的3台长虹的平均价格是多少？\r\n红色电视中的1台小米的平均价格是多少？\r\n\r\n下钻的意思是，已经分了一个组了，比如说颜色的分组，然后还要继续对这个分组内的数据，再分组，比如一个颜色内，还可以分成多个不同的品牌的组，最后对每个最小粒度的分组执行聚合分析操作，这就叫做下钻分析\r\n\r\nes，下钻分析，就要对bucket进行多层嵌套，多次分组\r\n\r\n按照多个维度（颜色+品牌）多层下钻分析，而且学会了每个下钻维度（颜色，颜色+品牌），都可以对每个维度分别执行一次metric聚合操作\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;color_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        },\r\n        &quot;group_by_brand&quot;: {\r\n          &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;brand&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;brand_avg_price&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;price&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 8,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;color_avg_price&quot;: {\r\n            &quot;value&quot;: 3250\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;长虹&quot;,\r\n                &quot;doc_count&quot;: 3,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 1666.6666666666667\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;三星&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 8000\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;color_avg_price&quot;: {\r\n            &quot;value&quot;: 2100\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 1200\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 3000\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;color_avg_price&quot;: {\r\n            &quot;value&quot;: 2000\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 1500\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;brand_avg_price&quot;: {\r\n                  &quot;value&quot;: 2500\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514614857,0,0,0),(128,1,'要学更多的metric','','','要学更多的metric\r\n\r\ncount，avg\r\n\r\ncount：bucket，terms，自动就会有一个doc_count，就相当于是count\r\navg：avg aggs，求平均值\r\nmax：求一个bucket内，指定field值最大的那个数据\r\nmin：求一个bucket内，指定field值最小的那个数据\r\nsum：求一个bucket内，指定field值的总和\r\n\r\n一般来说，90%的常见的数据分析的操作，metric，无非就是count，avg，max，min，sum\r\nm\r\nGET /tvs/sales/_search\r\n{\r\n   &quot;size&quot; : 0,\r\n   &quot;aggs&quot;: {\r\n      &quot;colors&quot;: {\r\n         &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;color&quot;\r\n         },\r\n         &quot;aggs&quot;: {\r\n            &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } },\r\n            &quot;min_price&quot; : { &quot;min&quot;: { &quot;field&quot;: &quot;price&quot;} }, \r\n            &quot;max_price&quot; : { &quot;max&quot;: { &quot;field&quot;: &quot;price&quot;} },\r\n            &quot;sum_price&quot; : { &quot;sum&quot;: { &quot;field&quot;: &quot;price&quot; } } \r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n求总和，就可以拿到一个颜色下的所有电视的销售总额\r\n\r\n{\r\n  &quot;took&quot;: 16,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;max_price&quot;: {\r\n            &quot;value&quot;: 8000\r\n          },\r\n          &quot;min_price&quot;: {\r\n            &quot;value&quot;: 1000\r\n          },\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 3250\r\n          },\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 13000\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;max_price&quot;: {\r\n            &quot;value&quot;: 3000\r\n          },\r\n          &quot;min_price&quot;: {\r\n            &quot;value&quot;:\r\n          }, 1200\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2100\r\n          },\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 4200\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;max_price&quot;: {\r\n            &quot;value&quot;: 2500\r\n          },\r\n          &quot;min_price&quot;: {\r\n            &quot;value&quot;: 1500\r\n          },\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2000\r\n          },\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 4000\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514614927,0,0,0),(129,1,'histogram','','','histogram：类似于terms，也是进行bucket分组操作，接收一个field，按照这个field的值的各个范围区间，进行bucket分组操作\r\n\r\n&quot;histogram&quot;:{ \r\n  &quot;field&quot;: &quot;price&quot;,\r\n  &quot;interval&quot;: 2000\r\n},\r\n\r\ninterval：2000，划分范围，0~2000，2000~4000，4000~6000，6000~8000，8000~10000，buckets\r\n\r\n去根据price的值，比如2500，看落在哪个区间内，比如2000~4000，此时就会将这条数据放入2000~4000对应的那个bucket中\r\n\r\nbucket划分的方法，terms，将field值相同的数据划分到一个bucket中\r\n\r\nbucket有了之后，一样的，去对每个bucket执行avg，count，sum，max，min，等各种metric操作，聚合分析\r\n\r\nGET /tvs/sales/_search\r\n{\r\n   &quot;size&quot; : 0,\r\n   &quot;aggs&quot;:{\r\n      &quot;price&quot;:{\r\n         &quot;histogram&quot;:{ \r\n            &quot;field&quot;: &quot;price&quot;,\r\n            &quot;interval&quot;: 2000\r\n         },\r\n         &quot;aggs&quot;:{\r\n            &quot;revenue&quot;: {\r\n               &quot;sum&quot;: { \r\n                 &quot;field&quot; : &quot;price&quot;\r\n               }\r\n             }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 13,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_price&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: 0,\r\n          &quot;doc_count&quot;: 3,\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 3700\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: 2000,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 9500\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: 4000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: 6000,\r\n          &quot;doc_count: {\r\n            &quot;value&quot;:&quot;: 0,\r\n          &quot;sum_price&quot; 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: 8000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;sum_price&quot;: {\r\n            &quot;value&quot;: 8000\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514614998,0,0,0),(130,1,'bucket，分组操作','','','bucket，分组操作，histogram，按照某个值指定的interval，划分一个一个的bucket\r\n\r\ndate histogram，按照我们指定的某个date类型的日期field，以及日期interval，按照一定的日期间隔，去划分bucket\r\n\r\ndate interval = 1m，\r\n\r\n2017-01-01~2017-01-31，就是一个bucket\r\n2017-02-01~2017-02-28，就是一个bucket\r\n\r\n然后会去扫描每个数据的date field，判断date落在哪个bucket中，就将其放入那个bucket\r\n\r\n2017-01-05，就将其放入2017-01-01~2017-01-31，就是一个bucket\r\n\r\nmin_doc_count：即使某个日期interval，2017-01-01~2017-01-31中，一条数据都没有，那么这个区间也是要返回的，不然默认是会过滤掉这个区间的\r\nextended_bounds，min，max：划分bucket的时候，会限定在这个起始日期，和截止日期内\r\n\r\nGET /tvs/sales/_search\r\n{\r\n   &quot;size&quot; : 0,\r\n   &quot;aggs&quot;: {\r\n      &quot;sales&quot;: {\r\n         &quot;date_histogram&quot;: {\r\n            &quot;field&quot;: &quot;sold_date&quot;,\r\n            &quot;interval&quot;: &quot;month&quot;, \r\n            &quot;format&quot;: &quot;yyyy-MM-dd&quot;,\r\n            &quot;min_doc_count&quot; : 0, \r\n            &quot;extended_bounds&quot; : { \r\n                &quot;min&quot; : &quot;2016-01-01&quot;,\r\n                &quot;max&quot; : &quot;2017-12-31&quot;\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 16,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_sold_date&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-01-01&quot;,\r\n          &quot;key&quot;: 1451606400000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-02-01&quot;,\r\n          &quot;key&quot;: 1454284800000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-03-01&quot;,\r\n          &quot;key&quot;: 1456790400000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-04-01&quot;,\r\n          &quot;key&quot;: 1459468800000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-05-01&quot;,\r\n          &quot;key&quot;: 1462060800000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-06-01&quot;,\r\n          &quot;key&quot;: 1464739200000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-07-01&quot;,\r\n          &quot;key&quot;: 1467331200000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key_as_strin\r\n          &quot;key_as_string&quot;: &quot;2016-09-01&quot;,\r\n          &quot;key&quot;: 1472688000000,\r\n          &quot;doc_count&quot;: 0\r\n        },g&quot;: &quot;2016-08-01&quot;,\r\n          &quot;key&quot;: 1470009600000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-10-01&quot;,\r\n          &quot;key&quot;: 1475280000000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-11-01&quot;,\r\n          &quot;key&quot;: 1477958400000,\r\n          &quot;doc_count&quot;: 2\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-12-01&quot;,\r\n          &quot;key&quot;: 1480550400000,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-01-01&quot;,\r\n          &quot;key&quot;: 1483228800000,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-02-01&quot;,\r\n          &quot;key&quot;: 1485907200000,\r\n          &quot;doc_count&quot;: 1\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615043,0,0,0),(131,1,'aggs','','','GET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_sold_date&quot;: {\r\n      &quot;date_histogram&quot;: {\r\n        &quot;field&quot;: &quot;sold_date&quot;,\r\n        &quot;interval&quot;: &quot;quarter&quot;,\r\n        &quot;format&quot;: &quot;yyyy-MM-dd&quot;,\r\n        &quot;min_doc_count&quot;: 0,\r\n        &quot;extended_bounds&quot;: {\r\n          &quot;min&quot;: &quot;2016-01-01&quot;,\r\n          &quot;max&quot;: &quot;2017-12-31&quot;\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_brand&quot;: {\r\n          &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;brand&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;sum_price&quot;: {\r\n              &quot;sum&quot;: {\r\n                &quot;field&quot;: &quot;price&quot;\r\n              }\r\n            }\r\n          }\r\n        },\r\n        &quot;total_sum_price&quot;: {\r\n          &quot;sum&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 10,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_sold_date&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-01-01&quot;,\r\n          &quot;key&quot;: 1451606400000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: []\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-04-01&quot;,\r\n          &quot;key&quot;: 1459468800000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 3000\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 3000\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-07-01&quot;,\r\n          &quot;key&quot;: 1467331200000,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 2700\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 2,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 2700\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-10-01&quot;,\r\n          &quot;key&quot;: 1475280000000,\r\n          &quot;doc_count&quot;: 3,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 5000\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;长虹&quot;,\r\n                &quot;doc_count&quot;: 3,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 5000\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-01-01&quot;,\r\n          &quot;key&quot;: 1483228800000,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 10500\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;三星&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 8000\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;sum_price&quot;: {\r\n                  &quot;value&quot;: 2500\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-04-01&quot;,\r\n          &quot;key&quot;: 1491004800000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: []\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-07-01&quot;,\r\n          &quot;key&quot;: 1498867200000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: []\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-10-01&quot;,\r\n          &quot;key&quot;: 1506816000000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;total_sum_price&quot;: {\r\n            &quot;value&quot;: 0\r\n          },\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: []\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615215,0,0,0),(132,1,'es aggregation，scope','','','实际上来说，我们之前学习的搜索相关的知识，完全可以和聚合组合起来使用\r\n\r\nselect count(*)\r\nfrom tvs.sales\r\nwhere brand like &quot;%长%&quot;\r\ngroup by price\r\n\r\nes aggregation，scope，任何的聚合，都必须在搜索出来的结果数据中之行，搜索结果，就是聚合分析操作的scope\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;brand&quot;: {\r\n        &quot;value&quot;: &quot;小米&quot;\r\n      }\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 1\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615302,0,0,0),(133,1,'基于query搜索结果','','','aggregation，scope，一个聚合操作，必须在query的搜索结果范围内执行\r\n\r\n出来两个结果，一个结果，是基于query搜索结果来聚合的; 一个结果，是对所有数据执行聚合的\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0, \r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;brand&quot;: {\r\n        &quot;value&quot;: &quot;长虹&quot;\r\n      }\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;single_brand_avg_price&quot;: {\r\n      &quot;avg&quot;: {\r\n        &quot;field&quot;: &quot;price&quot;\r\n      }\r\n    },\r\n    &quot;all&quot;: {\r\n      &quot;global&quot;: {},\r\n      &quot;aggs&quot;: {\r\n        &quot;all_brand_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nglobal：就是global bucket，就是将所有数据纳入聚合的scope，而不管之前的query\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;all&quot;: {\r\n      &quot;doc_count&quot;: 8,\r\n      &quot;all_brand_avg_price&quot;: {\r\n        &quot;value&quot;: 2650\r\n      }\r\n    },\r\n    &quot;single_brand_avg_price&quot;: {\r\n      &quot;value&quot;: 1666.6666666666667\r\n    }\r\n  }\r\n}\r\n\r\nsingle_brand_avg_price：就是针对query搜索结果，执行的，拿到的，就是长虹品牌的平均价格\r\nall.all_brand_avg_price：拿到所有品牌的平均价格\r\n',8,0,0,1514615361,0,0,0),(134,1,'搜索+聚合 过滤+聚合','','','搜索+聚合\r\n过滤+聚合\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;price&quot;: {\r\n            &quot;gte&quot;: 1200\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;avg_price&quot;: {\r\n      &quot;avg&quot;: {\r\n        &quot;field&quot;: &quot;price&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 41,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 7,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;avg_price&quot;: {\r\n      &quot;value&quot;: 2885.714285714286\r\n    }\r\n  }\r\n}',8,0,0,1514615400,0,0,0),(135,1,'aggs.filter','','','GET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;brand&quot;: {\r\n        &quot;value&quot;: &quot;长虹&quot;\r\n      }\r\n    }\r\n  },\r\n  &quot;aggs&quot;: {\r\n    &quot;recent_150d&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;sold_date&quot;: {\r\n            &quot;gte&quot;: &quot;now-150d&quot;\r\n          }\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;recent_150d_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    },\r\n    &quot;recent_140d&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;sold_date&quot;: {\r\n            &quot;gte&quot;: &quot;now-140d&quot;\r\n          }\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;recent_140d_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    },\r\n    &quot;recent_130d&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;sold_date&quot;: {\r\n            &quot;gte&quot;: &quot;now-130d&quot;\r\n          }\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;recent_130d_avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\naggs.filter，针对的是聚合去做的\r\n\r\n如果放query里面的filter，是全局的，会对所有的数据都有影响\r\n\r\n但是，如果，比如说，你要统计，长虹电视，最近1个月的平均值; 最近3个月的平均值; 最近6个月的平均值\r\n\r\nbucket filter：对不同的bucket下的aggs，进行filter\r\n',8,0,0,1514615485,0,0,0),(136,1,'bucket的doc_count降序','','','之前的话，排序，是按照每个bucket的doc_count降序来排的\r\n\r\n但是假如说，我们现在统计出来每个颜色的电视的销售额，需要按照销售额降序排序？？？？\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 3250\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2100\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;avg_price&quot;: {\r\n            &quot;value&quot;: 2000\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nGET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;,\r\n        &quot;order&quot;: {\r\n          &quot;avg_price&quot;: &quot;asc&quot;\r\n        }\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;avg_price&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;price&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514615589,0,0,0),(137,1,'聚合计算二','','','GET /tvs/sales/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;color&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_brand&quot;: {\r\n          &quot;terms&quot;: {\r\n            &quot;field&quot;: &quot;brand&quot;,\r\n            &quot;order&quot;: {\r\n              &quot;avg_price&quot;: &quot;desc&quot;\r\n            }\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;avg_price&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;price&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_color&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;红色&quot;,\r\n          &quot;doc_count&quot;: 4,\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;三星&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 8000\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;长虹&quot;,\r\n                &quot;doc_count&quot;: 3,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 1666.6666666666667\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;绿色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 3000\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 1200\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;蓝色&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;group_by_brand&quot;: {\r\n            &quot;doc_count_error_upper_bound&quot;: 0,\r\n            &quot;sum_other_doc_count&quot;: 0,\r\n            &quot;buckets&quot;: [\r\n              {\r\n                &quot;key&quot;: &quot;小米&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 2500\r\n                }\r\n              },\r\n              {\r\n                &quot;key&quot;: &quot;TCL&quot;,\r\n                &quot;doc_count&quot;: 1,\r\n                &quot;avg_price&quot;: {\r\n                  &quot;value&quot;: 1500\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615673,0,0,0),(138,1,'聚合算法：max','','','1、画图讲解易并行聚合算法：max\r\n\r\n有些聚合分析的算法，是很容易就可以并行的，比如说max\r\n\r\n有些聚合分析的算法，是不好并行的，比如说，count(distinct)，并不是说，在每个node上，直接就出一些distinct value，就可以的，因为数据可能会很多\r\n\r\nes会采取近似聚合的方式，就是采用在每个node上进行近估计的方式，得到最终的结论，cuont(distcint)，100万，1050万/95万 --&gt; 5%左右的错误率\r\n近似估计后的结果，不完全准确，但是速度会很快，一般会达到完全精准的算法的性能的数十倍\r\n\r\n2、三角选择原则\r\n\r\n精准+实时+大数据 --&gt; 选择2个\r\n\r\n（1）精准+实时: 没有大数据，数据量很小，那么一般就是单击跑，随便你则么玩儿就可以\r\n（2）精准+大数据：hadoop，批处理，非实时，可以处理海量数据，保证精准，可能会跑几个小时\r\n（3）大数据+实时：es，不精准，近似估计，可能会有百分之几的错误率\r\n\r\n3、近似聚合算法\r\n\r\n如果采取近似估计的算法：延时在100ms左右，0.5%错误\r\n如果采取100%精准的算法：延时一般在5s~几十s，甚至几十分钟，几小时， 0%错误\r\n',8,0,0,1514615714,0,0,0),(139,1,'去重','','','es，去重，cartinality metric，对每个bucket中的指定的field进行去重，取去重后的count，类似于count(distcint)\r\n\r\nGET /tvs/sales/_search\r\n{\r\n  &quot;size&quot; : 0,\r\n  &quot;aggs&quot; : {\r\n      &quot;months&quot; : {\r\n        &quot;date_histogram&quot;: {\r\n          &quot;field&quot;: &quot;sold_date&quot;,\r\n          &quot;interval&quot;: &quot;month&quot;\r\n        },\r\n        &quot;aggs&quot;: {\r\n          &quot;distinct_colors&quot; : {\r\n              &quot;cardinality&quot; : {\r\n                &quot;field&quot; : &quot;brand&quot;\r\n              }\r\n          }\r\n        }\r\n      }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 70,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 8,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_sold_date&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-05-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1462060800000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-06-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1464739200000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-07-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1467331200000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-08-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1470009600000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-09-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1472688000000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-10-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1475280000000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-11-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1477958400000,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2016-12-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1480550400000,\r\n          &quot;doc_count&quot;: 0,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 0\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-01-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1483228800000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        },\r\n        {\r\n          &quot;key_as_string&quot;: &quot;2017-02-01T00:00:00.000Z&quot;,\r\n          &quot;key&quot;: 1485907200000,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;distinct_brand_cnt&quot;: {\r\n            &quot;value&quot;: 1\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514615764,0,0,0),(140,1,'cardinality，count(distinct)','','','cardinality，count(distinct)，5%的错误率，性能在100ms左右\r\n\r\n1、precision_threshold优化准确率和内存开销\r\n\r\nGET /tvs/sales/_search\r\n{\r\n    &quot;size&quot; : 0,\r\n    &quot;aggs&quot; : {\r\n        &quot;distinct_brand&quot; : {\r\n            &quot;cardinality&quot; : {\r\n              &quot;field&quot; : &quot;brand&quot;,\r\n              &quot;precision_threshold&quot; : 100 \r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nbrand去重，如果brand的unique value，在100个以内，小米，长虹，三星，TCL，HTL。。。\r\n\r\n在多少个unique value以内，cardinality，几乎保证100%准确\r\ncardinality算法，会占用precision_threshold * 8 byte 内存消耗，100 * 8 = 800个字节\r\n占用内存很小。。。而且unique value如果的确在值以内，那么可以确保100%准确\r\n100，数百万的unique value，错误率在5%以内\r\n\r\nprecision_threshold，值设置的越大，占用内存越大，1000 * 8 = 8000 / 1000 = 8KB，可以确保更多unique value的场景下，100%的准确\r\n\r\nfield，去重，count，这时候，unique value，10000，precision_threshold=10000，10000 * 8 = 80000个byte，80KB\r\n\r\n2、HyperLogLog++ (HLL)算法性能优化\r\n\r\ncardinality底层算法：HLL算法，HLL算法的性能\r\n\r\n会对所有的uqniue value取hash值，通过hash值近似去求distcint count，误差\r\n\r\n默认情况下，发送一个cardinality请求的时候，会动态地对所有的field value，取hash值; 将取hash值的操作，前移到建立索引的时候\r\n\r\nPUT /tvs/\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;sales&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;brand&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;fields&quot;: {\r\n            &quot;hash&quot;: {\r\n              &quot;type&quot;: &quot;murmur3&quot; \r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /tvs/sales/_search\r\n{\r\n    &quot;size&quot; : 0,\r\n    &quot;aggs&quot; : {\r\n        &quot;distinct_brand&quot; : {\r\n            &quot;cardinality&quot; : {\r\n              &quot;field&quot; : &quot;brand.hash&quot;,\r\n              &quot;precision_threshold&quot; : 100 \r\n            }\r\n        }\r\n    }\r\n}',8,0,0,1514615837,0,0,0),(141,1,'统计tp50','','','需求：比如有一个网站，记录下了每次请求的访问的耗时，需要统计tp50，tp90，tp99\r\n\r\ntp50：50%的请求的耗时最长在多长时间\r\ntp90：90%的请求的耗时最长在多长时间\r\ntp99：99%的请求的耗时最长在多长时间\r\n\r\nPUT /website\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;logs&quot;: {\r\n            &quot;properties&quot;: {\r\n                &quot;latency&quot;: {\r\n                    &quot;type&quot;: &quot;long&quot;\r\n                },\r\n                &quot;province&quot;: {\r\n                    &quot;type&quot;: &quot;keyword&quot;\r\n                },\r\n                &quot;timestamp&quot;: {\r\n                    &quot;type&quot;: &quot;date&quot;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nPOST /website/logs/_bulk\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 105, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 83, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 92, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 112, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 68, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 76, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 101, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 275, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 166, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 654, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 389, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; }\r\n{ &quot;index&quot;: {}}\r\n{ &quot;latency&quot; : 302, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; }\r\n\r\npencentiles\r\n\r\nGET /website/logs/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;latency_percentiles&quot;: {\r\n      &quot;percentiles&quot;: {\r\n        &quot;field&quot;: &quot;latency&quot;,\r\n        &quot;percents&quot;: [\r\n          50,\r\n          95,\r\n          99\r\n        ]\r\n      }\r\n    },\r\n    &quot;latency_avg&quot;: {\r\n      &quot;avg&quot;: {\r\n        &quot;field&quot;: &quot;latency&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 31,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 12,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;latency_avg&quot;: {\r\n      &quot;value&quot;: 201.91666666666666\r\n    },\r\n    &quot;latency_percentiles&quot;: {\r\n      &quot;values&quot;: {\r\n        &quot;50.0&quot;: 108.5,\r\n        &quot;95.0&quot;: 508.24999999999983,\r\n        &quot;99.0&quot;: 624.8500000000001\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n50%的请求，数值的最大的值是多少，不是完全准确的\r\n\r\nGET /website/logs/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_province&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;province&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;latency_percentiles&quot;: {\r\n          &quot;percentiles&quot;: {\r\n            &quot;field&quot;: &quot;latency&quot;,\r\n            &quot;percents&quot;: [\r\n              50,\r\n              95,\r\n              99\r\n            ]\r\n          }\r\n        },\r\n        &quot;latency_avg&quot;: {\r\n          &quot;avg&quot;: {\r\n            &quot;field&quot;: &quot;latency&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 33,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 12,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_province&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;新疆&quot;,\r\n          &quot;doc_count&quot;: 6,\r\n          &quot;latency_avg&quot;: {\r\n            &quot;value&quot;: 314.5\r\n          },\r\n          &quot;latency_percentiles&quot;: {\r\n            &quot;values&quot;: {\r\n              &quot;50.0&quot;: 288.5,\r\n              &quot;95.0&quot;: 587.75,\r\n              &quot;99.0&quot;: 640.75\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;江苏&quot;,\r\n          &quot;doc_count&quot;: 6,\r\n          &quot;latency_avg&quot;: {\r\n            &quot;value&quot;: 89.33333333333333\r\n          },\r\n          &quot;latency_percentiles&quot;: {\r\n            &quot;values&quot;: {\r\n              &quot;50.0&quot;: 87.5,\r\n              &quot;95.0&quot;: 110.25,\r\n              &quot;99.0&quot;: 111.65\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514616114,0,0,0),(142,1,'SLA','','','SLA：就是你提供的服务的标准\r\n\r\n我们的网站的提供的访问延时的SLA，确保所有的请求100%，都必须在200ms以内，大公司内，一般都是要求100%在200ms以内\r\n\r\n如果超过1s，则需要升级到A级故障，代表网站的访问性能和用户体验急剧下降\r\n\r\n需求：在200ms以内的，有百分之多少，在1000毫秒以内的有百分之多少，percentile ranks metric\r\n\r\n这个percentile ranks，其实比pencentile还要常用\r\n\r\n按照品牌分组，计算，电视机，售价在1000占比，2000占比，3000占比\r\n\r\nGET /website/logs/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_province&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;province&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;latency_percentile_ranks&quot;: {\r\n          &quot;percentile_ranks&quot;: {\r\n            &quot;field&quot;: &quot;latency&quot;,\r\n            &quot;values&quot;: [\r\n              200,\r\n              1000\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 38,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 12,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_province&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;新疆&quot;,\r\n          &quot;doc_count&quot;: 6,\r\n          &quot;latency_percentile_ranks&quot;: {\r\n            &quot;values&quot;: {\r\n              &quot;200.0&quot;: 29.40613026819923,\r\n              &quot;1000.0&quot;: 100\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;江苏&quot;,\r\n          &quot;doc_count&quot;: 6,\r\n          &quot;latency_percentile_ranks&quot;: {\r\n            &quot;values&quot;: {\r\n              &quot;200.0&quot;: 100,\r\n              &quot;1000.0&quot;: 100\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\npercentile的优化\r\n\r\nTDigest算法，用很多节点来执行百分比的计算，近似估计，有误差，节点越多，越精准\r\n\r\ncompression\r\n\r\n限制节点数量最多 compression * 20 = 2000个node去计算\r\n\r\n默认100\r\n\r\n越大，占用内存越多，越精准，性能越差\r\n\r\n一个节点占用32字节，100 * 20 * 32 = 64KB\r\n\r\n如果你想要percentile算法越精准，compression可以设置的越大\r\n',8,0,0,1514616727,0,0,0),(143,1,'聚合分析','','','聚合分析的内部原理是什么？？？？aggs，term，metric avg max，执行一个聚合操作的时候，内部原理是怎样的呢？用了什么样的数据结构去执行聚合？是不是用的倒排索引？\r\n\r\n搜索+聚合，写个示例\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n	&quot;query&quot;: {\r\n		&quot;match&quot;: {\r\n			&quot;search_field&quot;: &quot;test&quot;\r\n		}\r\n	},\r\n	&quot;aggs&quot;: {\r\n		&quot;group_by_agg_field&quot;: {\r\n			&quot;terms&quot;: {\r\n				&quot;field&quot;: &quot;agg_field&quot;\r\n			}\r\n		}\r\n	}\r\n}\r\n\r\n纯用倒排索引来实现的弊端\r\n\r\nes肯定不是纯用倒排索引来实现聚合+搜索的\r\n\r\nsearch_field\r\n\r\ndoc1: hello world test1, test2\r\ndoc2: hello test\r\ndoc3: world	test\r\n\r\nhello	doc1,doc2\r\nworld	doc1,doc3\r\ntest1	doc1\r\ntest2	doc1\r\ntest 	doc2,doc3\r\n\r\n&quot;query&quot;: {\r\n	&quot;match&quot;: {\r\n		&quot;search_field&quot;: &quot;test&quot;\r\n	}\r\n}\r\n\r\ntest --&gt; doc2,doc3 --&gt; search result, doc2,doc3\r\n\r\nagg_field\r\n\r\ndoc2: agg1\r\ndoc3: agg2\r\n\r\n\r\n100万个值\r\n...\r\n...\r\n...\r\n...\r\nagg1	doc2\r\nagg2	doc3\r\n\r\ndoc2, doc3, search result --&gt; 实际上，要搜索到doc2的agg_field的值是多少，doc3的agg_field的值是多少\r\n\r\ndoc2和doc3的agg_field的值之后，就可以根据值进行分组，实现terms bucket操作\r\n\r\ndoc2的agg_field的值是多少，这个时候，如果你手上只有一个倒排索引，你该怎么办？？？你要扫描整个倒排索引，去一个一个的搜，拿到每个值，比如说agg1，看一下，它是不是doc2的值，拿到agg2,看一下，是不是doc2的值，直到找到doc2的agg_field的值，在倒排索引中\r\n\r\n如果用纯倒排索引去实现聚合，现实不现实啊？？？性能是很低下的。。。搜索，search，搜倒排索引，搜那个term，就结束了。。。聚合，搜索出了1万个doc，每个doc都要在倒排索引中搜索出它的那个聚合field的值\r\n\r\n倒排索引+正排索引（doc value）的原理和优势\r\n\r\ndoc value：正排索引\r\n\r\nsearch_field\r\n\r\ndoc1: hello world test1, test2\r\ndoc2: hello test\r\ndoc3: world	test\r\n\r\nhello	doc1,doc2\r\nworld	doc1,doc3\r\ntest1	doc1\r\ntest2	doc1\r\ntest 	doc2,doc3\r\n\r\n&quot;query&quot;: {\r\n	&quot;match&quot;: {\r\n		&quot;search_field&quot;: &quot;test&quot;\r\n	}\r\n}\r\n\r\ntest --&gt; doc2,doc3 --&gt; search result, doc2,doc3\r\n\r\ndoc value数据结构，正排索引\r\n\r\n\r\n\r\n...\r\n...\r\n...\r\n100万个\r\ndoc2: agg1\r\ndoc3: agg2\r\n\r\n倒排索引的话，必须遍历完整个倒排索引才可以。。。。\r\n\r\n因为可能你要聚合的那个field的值，是分词的，比如说hello world my name --&gt; 一个doc的聚合field的值可能在倒排索引中对应多个value\r\n\r\n所以说，当你在倒排索引中找到一个值，发现它是属于某个doc的时候，还不能停，必须遍历完整个倒排索引，才能说确保找到了每个doc对应的所有terms，然后进行分组聚合\r\n\r\n...\r\n...\r\n...\r\n100万个\r\ndoc2: agg1 hello world\r\ndoc3: agg2 test hello\r\n\r\n我们有没有必要搜索完整个正排索引啊？？1万个doc --&gt; 搜 -&gt; 可能跟搜索到15000次，就搜索完了，就找到了1万个doc的聚合field的所有值了，然后就可以执行分组聚合操作了\r\n',8,0,0,1514616775,0,0,0),(144,1,'doc value原理','','','1、doc value原理\r\n\r\n（1）index-time生成\r\n\r\nPUT/POST的时候，就会生成doc value数据，也就是正排索引\r\n\r\n（2）核心原理与倒排索引类似\r\n\r\n正排索引，也会写入磁盘文件中，然后呢，os cache先进行缓存，以提升访问doc value正排索引的性能\r\n如果os cache内存大小不足够放得下整个正排索引，doc value，就会将doc value的数据写入磁盘文件中\r\n\r\n（3）性能问题：给jvm更少内存，64g服务器，给jvm最多16g\r\n\r\nes官方是建议，es大量是基于os cache来进行缓存和提升性能的，不建议用jvm内存来进行缓存，那样会导致一定的gc开销和oom问题\r\n给jvm更少的内存，给os cache更大的内存\r\n64g服务器，给jvm最多16g，几十个g的内存给os cache\r\nos cache可以提升doc value和倒排索引的缓存和查询效率\r\n\r\n2、column压缩\r\n\r\ndoc1: 550\r\ndoc2: 550\r\ndoc3: 500\r\n\r\n合并相同值，550，doc1和doc2都保留一个550的标识即可\r\n\r\n（1）所有值相同，直接保留单值\r\n（2）少于256个值，使用table encoding模式：一种压缩方式\r\n（3）大于256个值，看有没有最大公约数，有就除以最大公约数，然后保留这个最大公约数\r\n\r\ndoc1: 36\r\ndoc2: 24\r\n\r\n6 --&gt; doc1: 6, doc2: 4 --&gt; 保留一个最大公约数6的标识，6也保存起来\r\n\r\n（4）如果没有最大公约数，采取offset结合压缩的方式：\r\n\r\n3、disable doc value\r\n\r\n如果的确不需要doc value，比如聚合等操作，那么可以禁用，减少磁盘空间占用\r\n\r\nPUT my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;my_field&quot;: {\r\n          &quot;type&quot;:       &quot;keyword&quot;\r\n          &quot;doc_values&quot;: false \r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514616821,0,0,0),(145,1,'分词的field执行aggregation','','','1、对于分词的field执行aggregation，发现报错。。。\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;test_field&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n        &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;search_phase_execution_exception&quot;,\r\n    &quot;reason&quot;: &quot;all shards failed&quot;,\r\n    &quot;phase&quot;: &quot;query&quot;,\r\n    &quot;grouped&quot;: true,\r\n    &quot;failed_shards&quot;: [\r\n      {\r\n        &quot;shard&quot;: 0,\r\n        &quot;index&quot;: &quot;test_index&quot;,\r\n        &quot;node&quot;: &quot;4onsTYVZTjGvIj9_spWz2w&quot;,\r\n        &quot;reason&quot;: {\r\n          &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n          &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot;\r\n        }\r\n      }\r\n    ],\r\n    &quot;caused_by&quot;: {\r\n      &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n      &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot;\r\n    }\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\n对分词的field，直接执行聚合操作，会报错，大概意思是说，你必须要打开fielddata，然后将正排索引数据加载到内存中，才可以对分词的field执行聚合操作，而且会消耗很大的内存\r\n\r\n2、给分词的field，设置fielddata=true，发现可以执行，但是结果却。。。\r\n\r\nPOST /test_index/_mapping/test_type \r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;test_field&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;fielddata&quot;: true\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;test_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;test_type&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;test_field&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            },\r\n            &quot;fielddata&quot;: true\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;size&quot;: 0, \r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;test_field&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 23,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;test&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n如果要对分词的field执行聚合操作，必须将fielddata设置为true\r\n\r\n3、使用内置field不分词，对string field进行聚合\r\n\r\nGET /test_index/test_type/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;test_field.keyword&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 3,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_test_field&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;test&quot;,\r\n          &quot;doc_count&quot;: 2\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n如果对不分词的field执行聚合操作，直接就可以执行，不需要设置fieldata=true\r\n\r\n4、分词field+fielddata的工作原理\r\n\r\ndoc value --&gt; 不分词的所有field，可以执行聚合操作 --&gt; 如果你的某个field不分词，那么在index-time，就会自动生成doc value --&gt; 针对这些不分词的field执行聚合操作的时候，自动就会用doc value来执行\r\n\r\n分词field，是没有doc value的。。。在index-time，如果某个field是分词的，那么是不会给它建立doc value正排索引的，因为分词后，占用的空间过于大，所以默认是不支持分词field进行聚合的\r\n\r\n分词field默认没有doc value，所以直接对分词field执行聚合操作，是会报错的\r\n\r\n对于分词field，必须打开和使用fielddata，完全存在于纯内存中。。。结构和doc value类似。。。如果是ngram或者是大量term，那么必将占用大量的内存。。。\r\n\r\n如果一定要对分词的field执行聚合，那么必须将fielddata=true，然后es就会在执行聚合操作的时候，现场将field对应的数据，建立一份fielddata正排索引，fielddata正排索引的结构跟doc value是类似的，但是只会讲fielddata正排索引加载到内存中来，然后基于内存中的fielddata正排索引执行分词field的聚合操作\r\n\r\n如果直接对分词field执行聚合，报错，才会让我们开启fielddata=true，告诉我们，会将fielddata uninverted index，正排索引，加载到内存，会耗费内存空间\r\n\r\n为什么fielddata必须在内存？因为大家自己思考一下，分词的字符串，需要按照term进行聚合，需要执行更加复杂的算法和操作，如果基于磁盘和os cache，那么性能会很差\r\n',8,0,0,1514616864,0,0,0),(146,1,'fielddata','','','1、fielddata核心原理\r\n\r\nfielddata加载到内存的过程是lazy加载的，对一个analzyed field执行聚合时，才会加载，而且是field-level加载的\r\n一个index的一个field，所有doc都会被加载，而不是少数doc\r\n不是index-time创建，是query-time创建\r\n\r\n2、fielddata内存限制\r\n\r\nindices.fielddata.cache.size: 20%，超出限制，清除内存已有fielddata数据\r\nfielddata占用的内存超出了这个比例的限制，那么就清除掉内存中已有的fielddata数据\r\n默认无限制，限制内存使用，但是会导致频繁evict和reload，大量IO性能损耗，以及内存碎片和gc\r\n\r\n3、监控fielddata内存使用\r\n\r\nGET /_stats/fielddata?fields=*\r\nGET /_nodes/stats/indices/fielddata?fields=*\r\nGET /_nodes/stats/indices/fielddata?level=indices&amp;fields=*\r\n\r\n4、circuit breaker\r\n\r\n如果一次query load的feilddata超过总内存，就会oom --&gt; 内存溢出\r\n\r\ncircuit breaker会估算query要加载的fielddata大小，如果超出总内存，就短路，query直接失败\r\n\r\nindices.breaker.fielddata.limit：fielddata的内存限制，默认60%\r\nindices.breaker.request.limit：执行聚合的内存限制，默认40%\r\nindices.breaker.total.limit：综合上面两个，限制在70%以内\r\n\r\n',8,0,0,1514616950,0,0,0),(147,1,'fielddata实例','','','POST /test_index/_mapping/my_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;my_field&quot;: {\r\n      &quot;type&quot;: &quot;text&quot;,\r\n      &quot;fielddata&quot;: { \r\n        &quot;filter&quot;: {\r\n          &quot;frequency&quot;: { \r\n            &quot;min&quot;:              0.01, \r\n            &quot;min_segment_size&quot;: 500  \r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nmin：仅仅加载至少在1%的doc中出现过的term对应的fielddata\r\n\r\n比如说某个值，hello，总共有1000个doc，hello必须在10个doc中出现，那么这个hello对应的fielddata才会加载到内存中来\r\n\r\nmin_segment_size：少于500 doc的segment不加载fielddata\r\n\r\n加载fielddata的时候，也是按照segment去进行加载的，某个segment里面的doc数量少于500个，那么这个segment的fielddata就不加载\r\n\r\n这个，就我的经验来看，有点底层了，一般不会去设置它，大家知道就好\r\n',8,0,0,1514616998,0,0,0),(148,1,'query-time现场','','','如果真的要对分词的field执行聚合，那么每次都在query-time现场生产fielddata并加载到内存中来，速度可能会比较慢\r\n\r\n我们是不是可以预先生成加载fielddata到内存中来？？？\r\n\r\n1、fielddata预加载\r\n\r\nPOST /test_index/_mapping/test_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;test_field&quot;: {\r\n      &quot;type&quot;: &quot;string&quot;,\r\n      &quot;fielddata&quot;: {\r\n        &quot;loading&quot; : &quot;eager&quot; \r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nquery-time的fielddata生成和加载到内存，变为index-time，建立倒排索引的时候，会同步生成fielddata并且加载到内存中来，这样的话，对分词field的聚合性能当然会大幅度增强\r\n\r\n2、序号标记预加载\r\n\r\nglobal ordinal原理解释\r\n\r\ndoc1: status1\r\ndoc2: status2\r\ndoc3: status2\r\ndoc4: status1\r\n\r\n有很多重复值的情况，会进行global ordinal标记\r\n\r\nstatus1 --&gt; 0\r\nstatus2 --&gt; 1\r\n\r\ndoc1: 0\r\ndoc2: 1\r\ndoc3: 1\r\ndoc4: 0\r\n\r\n建立的fielddata也会是这个样子的，这样的好处就是减少重复字符串的出现的次数，减少内存的消耗\r\n\r\nPOST /test_index/_mapping/test_type\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;test_field&quot;: {\r\n      &quot;type&quot;: &quot;string&quot;,\r\n      &quot;fielddata&quot;: {\r\n        &quot;loading&quot; : &quot;eager_global_ordinals&quot; \r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514617047,0,0,0),(149,1,'深度优先和广度优先','','','当buckets数量特别多的时候，深度优先和广度优先的原理，图解\r\n\r\n我们的数据，是每个演员的每个电影的评论\r\n\r\n每个演员的评论的数量 --&gt; 每个演员的每个电影的评论的数量\r\n\r\n评论数量排名前10个的演员 --&gt; 每个演员的电影取到评论数量排名前5的电影\r\n\r\n{\r\n  &quot;aggs&quot; : {\r\n    &quot;actors&quot; : {\r\n      &quot;terms&quot; : {\r\n         &quot;field&quot; :        &quot;actors&quot;,\r\n         &quot;size&quot; :         10,\r\n         &quot;collect_mode&quot; : &quot;breadth_first&quot; \r\n      },\r\n      &quot;aggs&quot; : {\r\n        &quot;costars&quot; : {\r\n          &quot;terms&quot; : {\r\n            &quot;field&quot; : &quot;films&quot;,\r\n            &quot;size&quot; :  5\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n深度优先的方式去执行聚合操作的\r\n\r\n    actor1            actor2            .... actor\r\nfilm1 film2 film3   film1 film2 film3   ...film\r\n\r\n比如说，我们有10万个actor，最后其实是主要10个actor就可以了\r\n\r\n但是我们已经深度优先的方式，构建了一整颗完整的树出来了，10万个actor，每个actor平均有10部电影，10万 + 100万 --&gt; 110万的数据量的一颗树\r\n\r\n裁剪掉10万个actor中的99990 actor，99990 * 10 = film，剩下10个actor，每个actor的10个film裁剪掉5个，110万 --&gt; 10 * 5 = 50个\r\n\r\n构建了大量的数据，然后裁剪掉了99.99%的数据，浪费了\r\n\r\n广度优先的方式去执行聚合\r\n\r\nactor1    actor2    actor3    ..... n个actor\r\n\r\n10万个actor，不去构建它下面的film数据，10万 --&gt; 99990，10个actor，构建出film，裁剪出其中的5个film即可，10万 -&gt; 50个\r\n\r\n10倍',8,0,0,1514617089,0,0,0),(150,1,'es的document数据模型','','','关系型数据库的数据模型\r\n\r\nes的document数据模型\r\n\r\npublic class Department {\r\n	\r\n	private Integer deptId;\r\n	private String name;\r\n	private String desc;\r\n	private List&lt;Employee&gt; employees;\r\n\r\n}\r\n\r\npublic class Employee {\r\n	\r\n	private Integer empId;\r\n	private String name;\r\n	private Integer age;\r\n	private String gender;\r\n	private Department dept;\r\n\r\n}\r\n\r\n关系型数据库中\r\n\r\ndepartment表\r\n\r\ndept_id\r\nname\r\ndesc\r\n\r\nemployee表\r\n\r\nemp_id\r\nname\r\nage\r\ngender\r\ndept_id\r\n\r\n三范式 --&gt; 将每个数据实体拆分为一个独立的数据表，同时使用主外键关联关系将多个数据表关联起来 --&gt; 确保没有任何冗余的数据\r\n\r\n一份数据，只会放在一个数据表中 --&gt; dept name，部门名称，就只会放在department表中，不会在employee表中也放一个dept name，如果说你要查看某个员工的部门名称，那么必须通过员工表中的外键，dept_id，找到在部门表中对应的记录，然后找到部门名称\r\n\r\nes文档数据模型\r\n\r\n{\r\n	&quot;deptId&quot;: &quot;1&quot;,\r\n	&quot;name&quot;: &quot;研发部门&quot;,\r\n	&quot;desc&quot;: &quot;负责公司的所有研发项目&quot;,\r\n	&quot;employees&quot;: [\r\n		{\r\n			&quot;empId&quot;: &quot;1&quot;,\r\n			&quot;name&quot;: &quot;张三&quot;,\r\n			&quot;age&quot;: 28,\r\n			&quot;gender&quot;: &quot;男&quot;\r\n		},\r\n		{\r\n			&quot;empId&quot;: &quot;2&quot;,\r\n			&quot;name&quot;: &quot;王兰&quot;,\r\n			&quot;age&quot;: 25,\r\n			&quot;gender&quot;: &quot;女&quot;\r\n		},\r\n		{\r\n			&quot;empId&quot;: &quot;3&quot;,\r\n			&quot;name&quot;: &quot;李四&quot;,\r\n			&quot;age&quot;: 34,\r\n			&quot;gender&quot;: &quot;男&quot;\r\n		}\r\n	]\r\n}\r\n\r\nes，更加类似于面向对象的数据模型，将所有由关联关系的数据，放在一个doc json类型数据中，整个数据的关系，还有完整的数据，都放在了一起\r\n\r\n',8,0,0,1514617123,0,0,0),(151,1,'构造用户与博客数据','','','DELETE /website #删除website索引\r\n\r\n1、构造用户与博客数据\r\n\r\n在构造数据模型的时候，还是将有关联关系的数据，然后分割为不同的实体，类似于关系型数据库中的模型\r\n\r\n案例背景：博客网站， 我们会模拟各种用户发表各种博客，然后针对用户和博客之间的关系进行数据建模，同时针对建模好的数据执行各种搜索/聚合的操作\r\n\r\nPUT /website/users/1 \r\n{\r\n  &quot;name&quot;:     &quot;小鱼儿&quot;,\r\n  &quot;email&quot;:    &quot;xiaoyuer@sina.com&quot;,\r\n  &quot;birthday&quot;:      &quot;1980-01-01&quot;\r\n}\r\n\r\nPUT /website/blogs/1\r\n{\r\n  &quot;title&quot;:    &quot;我的第一篇博客&quot;,\r\n  &quot;content&quot;:     &quot;这是我的第一篇博客，开通啦！！！&quot;\r\n  &quot;userId&quot;:     1 \r\n}\r\n\r\n一个用户对应多个博客，一对多的关系，做了建模\r\n\r\n建模方式，分割实体，类似三范式的方式，用主外键关联关系，将多个实体关联起来\r\n\r\n2、搜索小鱼儿发表的所有博客\r\n\r\nGET /website/users/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;name.keyword&quot;: {\r\n        &quot;value&quot;: &quot;小鱼儿&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 91,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 0.2876821,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;users&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.2876821,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;小鱼儿&quot;,\r\n          &quot;email&quot;: &quot;xiaoyuer@sina.com&quot;,\r\n          &quot;birthday&quot;: &quot;1980-01-01&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n比如这里搜索的是，1万个用户的博客，可能第一次搜索，会得到1万个userId\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;constant_score&quot;: {\r\n      &quot;filter&quot;: {\r\n        &quot;terms&quot;: {\r\n          &quot;userId&quot;: [\r\n            1\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n第二次搜索的时候，要放入terms中1万个userId，才能进行搜索，这个时候性能比较差了\r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;blogs&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot;,\r\n          &quot;content&quot;: &quot;大家好，我是小鱼儿，这是我写的第一篇博客！&quot;,\r\n          &quot;userId&quot;: 1\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n上面的操作，就属于应用层的join，在应用层先查出一份数据，然后再查出一份数据，进行关联\r\n\r\n3、优点和缺点\r\n\r\n优点：数据不冗余，维护方便\r\n缺点：应用层join，如果关联数据过多，导致查询过大，性能很差\r\n\r\n',8,0,0,1514617249,0,0,0),(152,1,'构造冗余的用户和博客数据','','','1、构造冗余的用户和博客数据\r\n\r\n第二种建模方式：用冗余数据，采用文档数据模型，进行数据建模，实现用户和博客的关联\r\n\r\nPUT /website/users/1\r\n{\r\n  &quot;name&quot;:     &quot;小鱼儿&quot;,\r\n  &quot;email&quot;:    &quot;xiaoyuer@sina.com&quot;,\r\n  &quot;birthday&quot;:      &quot;1980-01-01&quot;\r\n}\r\n\r\nPUT /website/blogs/1\r\n{\r\n  &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot;,\r\n  &quot;content&quot;: &quot;大家好，我是小鱼儿。。。&quot;,\r\n  &quot;userInfo&quot;: {\r\n    &quot;userId&quot;: 1,\r\n    &quot;username&quot;: &quot;小鱼儿&quot;\r\n  }\r\n}\r\n\r\n冗余数据，就是说，将可能会进行搜索的条件和要搜索的数据，放在一个doc中\r\n\r\n2、基于冗余用户数据搜索博客\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;userInfo.username.keyword&quot;: {\r\n        &quot;value&quot;: &quot;小鱼儿&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n就不需要走应用层的join，先搜一个数据，找到id，再去搜另一份数据\r\n\r\n直接走一个有冗余数据的type即可，指定要的搜索条件，即可搜索出自己想要的数据来\r\n\r\n3、优点和缺点\r\n\r\n优点：性能高，不需要执行两次搜索\r\n缺点：数据冗余，维护成本高 --&gt; 每次如果你的username变化了，同时要更新user type和blog type\r\n\r\n一般来说，对于es这种NoSQL类型的数据存储来讲，都是冗余模式....\r\n\r\n当然，你要去维护数据的关联关系，也是很有必要的，所以一旦出现冗余数据的修改，必须记得将所有关联的数据全部更新\r\n',8,0,0,1514618911,0,0,0),(153,1,'构造更多测试数据','','','1、构造更多测试数据\r\n\r\nPUT /website/users/3\r\n{\r\n  &quot;name&quot;: &quot;黄药师&quot;,\r\n  &quot;email&quot;: &quot;huangyaoshi@sina.com&quot;,\r\n  &quot;birthday&quot;: &quot;1970-10-24&quot;\r\n}\r\n\r\nPUT /website/blogs/3\r\n{\r\n  &quot;title&quot;: &quot;我是黄药师&quot;,\r\n  &quot;content&quot;: &quot;我是黄药师啊，各位同学们！！！&quot;,\r\n  &quot;userInfo&quot;: {\r\n    &quot;userId&quot;: 1,\r\n    &quot;userName&quot;: &quot;黄药师&quot;\r\n  }\r\n}\r\n\r\nPUT /website/users/2\r\n{\r\n  &quot;name&quot;: &quot;花无缺&quot;,\r\n  &quot;email&quot;: &quot;huawuque@sina.com&quot;,\r\n  &quot;birthday&quot;: &quot;1980-02-02&quot;\r\n}\r\n\r\nPUT /website/blogs/4\r\n{\r\n  &quot;title&quot;: &quot;花无缺的身世揭秘&quot;,\r\n  &quot;content&quot;: &quot;大家好，我是花无缺，所以我的身世是。。。&quot;,\r\n  &quot;userInfo&quot;: {\r\n    &quot;userId&quot;: 2,\r\n    &quot;userName&quot;: &quot;花无缺&quot;\r\n  }\r\n}\r\n\r\n2、对每个用户发表的博客进行分组\r\n\r\n比如说，小鱼儿发表的那些博客，花无缺发表了哪些博客，黄药师发表了哪些博客\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;size&quot;: 0, \r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_username&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;userInfo.username.keyword&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;top_blogs&quot;: {\r\n          &quot;top_hits&quot;: {\r\n            &quot;_source&quot;: {\r\n              &quot;include&quot;: &quot;title&quot;\r\n            }, \r\n            &quot;size&quot;: 5\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514618962,0,0,0),(154,1,'文件系统数据建模','','','数据建模，对类似文件系统这种的有多层级关系的数据进行建模\r\n\r\n1、文件系统数据构造\r\n\r\nPUT /fs\r\n{\r\n  &quot;settings&quot;: {\r\n    &quot;analysis&quot;: {\r\n      &quot;analyzer&quot;: {\r\n        &quot;paths&quot;: { \r\n          &quot;tokenizer&quot;: &quot;path_hierarchy&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\npath_hierarchy tokenizer讲解\r\n\r\n/a/b/c/d --&gt; path_hierarchy -&gt; /a/b/c/d, /a/b/c, /a/b, /a\r\n\r\nfs: filesystem\r\n\r\nPUT /fs/_mapping/file\r\n{\r\n  &quot;properties&quot;: {\r\n    &quot;name&quot;: { \r\n      &quot;type&quot;:  &quot;keyword&quot;\r\n    },\r\n    &quot;path&quot;: { \r\n      &quot;type&quot;:  &quot;keyword&quot;,\r\n      &quot;fields&quot;: {\r\n        &quot;tree&quot;: { \r\n          &quot;type&quot;:     &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;paths&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /fs/file/1\r\n{\r\n  &quot;name&quot;:     &quot;README.txt&quot;, \r\n  &quot;path&quot;:     &quot;/workspace/projects/helloworld&quot;, \r\n  &quot;contents&quot;: &quot;这是我的第一个elasticsearch程序&quot;\r\n}\r\n\r\n2、对文件系统执行搜索\r\n\r\n文件搜索需求：查找一份，内容包括elasticsearch，在/workspace/projects/hellworld这个目录下的文件\r\n\r\nGET /fs/file/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;contents&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;constant_score&quot;: {\r\n            &quot;filter&quot;: {\r\n              &quot;term&quot;: {\r\n                &quot;path&quot;: &quot;/workspace/projects/helloworld&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1.284885,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;fs&quot;,\r\n        &quot;_type&quot;: &quot;file&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1.284885,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;README.txt&quot;,\r\n          &quot;path&quot;: &quot;/workspace/projects/helloworld&quot;,\r\n          &quot;contents&quot;: &quot;这是我的第一个elasticsearch程序&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n搜索需求2：搜索/workspace目录下，内容包含elasticsearch的所有的文件\r\n\r\n/workspace/projects/helloworld    doc1\r\n/workspace/projects               doc1\r\n/workspace                        doc1\r\n\r\nGET /fs/file/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;contents&quot;: &quot;elasticsearch&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;constant_score&quot;: {\r\n            &quot;filter&quot;: {\r\n              &quot;term&quot;: {\r\n                &quot;path.tree&quot;: &quot;/workspace&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514619049,0,0,0),(155,1,'悲观锁的简要说明','','','1、悲观锁的简要说明\r\n\r\n基于version的乐观锁并发控制\r\n\r\n在数据建模，结合文件系统建模的这个案例，把悲观锁的并发控制，3种锁粒度，都给大家仔细讲解一下\r\n\r\n最粗的一个粒度，全局锁\r\n\r\n/workspace/projects/helloworld\r\n\r\n如果多个线程，都过来，要并发地给/workspace/projects/helloworld下的README.txt修改文件名\r\n\r\n实际上要进行并发的控制，避免出现多线程的并发安全问题，比如多个线程修改，纯并发，先执行的修改操作被后执行的修改操作给覆盖了\r\n\r\nget current version\r\n\r\n带着这个current version去执行修改，如果一旦发现数据已经被别人给修改了，version号跟之前自己获取的已经不一样了; 那么必须重新获取新的version号再次尝试修改\r\n\r\n上来就尝试给这条数据加个锁，然后呢，此时就只有你能执行各种各样的操作了，其他人不能执行操作\r\n\r\n第一种锁：全局锁，直接锁掉整个fs index\r\n\r\n2、全局锁的上锁实验\r\n\r\nPUT /fs/lock/global/_create\r\n{}\r\n\r\nfs: 你要上锁的那个index\r\nlock: 就是你指定的一个对这个index上全局锁的一个type\r\nglobal: 就是你上的全局锁对应的这个doc的id\r\n_create：强制必须是创建，如果/fs/lock/global这个doc已经存在，那么创建失败，报错\r\n\r\n利用了doc来进行上锁\r\n\r\n/fs/lock/global /index/type/id --&gt; doc\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;global&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n另外一个线程同时尝试上锁\r\n\r\nPUT /fs/lock/global/_create\r\n{}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[lock][global]: version conflict, document already exists (current version [1])&quot;,\r\n        &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n        &quot;shard&quot;: &quot;2&quot;,\r\n        &quot;index&quot;: &quot;fs&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[lock][global]: version conflict, document already exists (current version [1])&quot;,\r\n    &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n    &quot;shard&quot;: &quot;2&quot;,\r\n    &quot;index&quot;: &quot;fs&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n如果失败，就再次重复尝试上锁\r\n\r\n执行各种操作。。。\r\n\r\nPOST /fs/file/1/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;name&quot;: &quot;README1.txt&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;file&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nDELETE /fs/lock/global\r\n\r\n{\r\n  &quot;found&quot;: true,\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;global&quot;,\r\n  &quot;_version&quot;: 2,\r\n  &quot;result&quot;: &quot;deleted&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\n另外一个线程，因为之前发现上锁失败，反复尝试重新上锁，终于上锁成功了，因为之前获取到全局锁的那个线程已经delete /fs/lock/global全局锁了\r\n\r\nPUT /fs/lock/global/_create\r\n{}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;global&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nPOST /fs/file/1/_update \r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;name&quot;: &quot;README.txt&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;file&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nDELETE /fs/lock/global\r\n\r\n3、全局锁的优点和缺点\r\n\r\n优点：操作非常简单，非常容易使用，成本低\r\n缺点：你直接就把整个index给上锁了，这个时候对index中所有的doc的操作，都会被block住，导致整个系统的并发能力很低\r\n\r\n上锁解锁的操作不是频繁，然后每次上锁之后，执行的操作的耗时不会太长，用这种方式，方便\r\n\r\n',8,0,0,1514619107,0,0,0),(156,1,'document level锁','','','1、对document level锁，详细的讲解\r\n\r\n全局锁，一次性就锁整个index，对这个index的所有增删改操作都会被block住，如果上锁不频繁，还可以，比较简单\r\n\r\n细粒度的一个锁，document锁，顾名思义，每次就锁你要操作的，你要执行增删改的那些doc，doc锁了，其他线程就不能对这些doc执行增删改操作了\r\n但是你只是锁了部分doc，其他线程对其他的doc还是可以上锁和执行增删改操作的\r\n\r\ndocument锁，是用脚本进行上锁\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: &quot;if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;&quot;\r\n  &quot;params&quot;: {\r\n    &quot;process_id&quot;: 123\r\n  }\r\n}\r\n\r\n/fs/lock，是固定的，就是说fs下的lock type，专门用于进行上锁\r\n/fs/lock/id，比如1，id其实就是你要上锁的那个doc的id，代表了某个doc数据对应的lock（也是一个doc）\r\n_update + upsert：执行upsert操作\r\n\r\nparams，里面有个process_id，process_id，是你的要执行增删改操作的进程的唯一id，比如说可以在java系统，启动的时候，给你的每个线程都用UUID自动生成一个thread id，你的系统进程启动的时候给整个进程也分配一个UUID。process_id + thread_id就代表了某一个进程下的某个线程的唯一标识。可以自己用UUID生成一个唯一ID\r\n\r\nprocess_id很重要，会在lock中，设置对对应的doc加锁的进程的id，这样其他进程过来的时候，才知道，这条数据已经被别人给锁了\r\n\r\nassert false，不是当前进程加锁的话，则抛出异常\r\nctx.op=&#039;noop&#039;，不做任何修改\r\n\r\n如果该document之前没有被锁，/fs/lock/1之前不存在，也就是doc id=1没有被别人上过锁; upsert的语法，那么执行index操作，创建一个/fs/lock/id这条数据，而且用params中的数据作为这个lock的数据。process_id被设置为123，script不执行。这个时候象征着process_id=123的进程已经锁了一个doc了。\r\n\r\n如果document被锁了，就是说/fs/lock/1已经存在了，代表doc id=1已经被某个进程给锁了。那么执行update操作，script，此时会比对process_id，如果相同，就是说，某个进程，之前锁了这个doc，然后这次又过来，就可以直接对这个doc执行操作，说明是该进程之前锁的doc，则不报错，不执行任何操作，返回success; 如果process_id比对不上，说明doc被其他doc给锁了，此时报错\r\n\r\n/fs/lock/1\r\n{\r\n  &quot;process_id&quot;: 123\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: &quot;if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;&quot;\r\n  &quot;params&quot;: {\r\n    &quot;process_id&quot;: 123\r\n  }\r\n}\r\n\r\n\r\nscript：ctx._source.process_id，123\r\nprocess_id：加锁的upsert请求中带过来额proess_id\r\n\r\n如果两个process_id相同，说明是一个进程先加锁，然后又过来尝试加锁，可能是要执行另外一个操作，此时就不会block，对同一个process_id是不会block，ctx.op= &#039;noop&#039;，什么都不做，返回一个success\r\n\r\n如果说已经有一个进程加了锁了\r\n\r\n/fs/lock/1\r\n{\r\n  &quot;process_id&quot;: 123\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: &quot;if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;&quot;\r\n  &quot;params&quot;: {\r\n    &quot;process_id&quot;: 234\r\n  }\r\n}\r\n\r\n&quot;script&quot;: &quot;if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;&quot;\r\n\r\nctx._source.process_id：123\r\nprocess_id: 234\r\n\r\nprocess_id不相等，说明这个doc之前已经被别人上锁了，process_id=123上锁了; process_id=234过来再次尝试上锁，失败，assert false，就会报错\r\n\r\n此时遇到报错的process，就应该尝试重新上锁，直到上锁成功\r\n\r\n有报错的话，如果有些doc被锁了，那么需要重试\r\n\r\n直到所有锁定都成功，执行自己的操作。。。\r\n\r\n释放所有的锁\r\n\r\n2、上document锁的完整实验过程\r\n\r\nscripts/judge-lock.groovy: if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = &#039;noop&#039;;\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;judge-lock&quot;, \r\n    &quot;params&quot;: {\r\n      &quot;process_id&quot;: 123\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nGET /fs/lock/1\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;process_id&quot;: 123\r\n  }\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 234 },\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;judge-lock&quot;, \r\n    &quot;params&quot;: {\r\n      &quot;process_id&quot;: 234\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;remote_transport_exception&quot;,\r\n        &quot;reason&quot;: &quot;[4onsTYV][127.0.0.1:9300][indices:data/write/update[s]]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n    &quot;reason&quot;: &quot;failed to execute script&quot;,\r\n    &quot;caused_by&quot;: {\r\n      &quot;type&quot;: &quot;script_exception&quot;,\r\n      &quot;reason&quot;: &quot;error evaluating judge-lock&quot;,\r\n      &quot;caused_by&quot;: {\r\n        &quot;type&quot;: &quot;power_assertion_error&quot;,\r\n        &quot;reason&quot;: &quot;assert false\\n&quot;\r\n      },\r\n      &quot;script_stack&quot;: [],\r\n      &quot;script&quot;: &quot;&quot;,\r\n      &quot;lang&quot;: &quot;groovy&quot;\r\n    }\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 123 },\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;judge-lock&quot;, \r\n    &quot;params&quot;: {\r\n      &quot;process_id&quot;: 123\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;noop&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 0,\r\n    &quot;successful&quot;: 0,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nPOST /fs/file/1/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;name&quot;: &quot;README1.txt&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;file&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 4,\r\n  &quot;result&quot;: &quot;updated&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  }\r\n}\r\n\r\nPOST /fs/_refresh \r\n\r\nGET /fs/lock/_search?scroll=1m\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;term&quot;: {\r\n      &quot;process_id&quot;: 123\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACPkFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAj5RY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAAI-YWNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACPnFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAj6BY0b25zVFlWWlRqR3ZJajlfc3BXejJ3&quot;,\r\n  &quot;took&quot;: 51,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;fs&quot;,\r\n        &quot;_type&quot;: &quot;lock&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;process_id&quot;: 123\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nPUT /fs/lock/_bulk\r\n{ &quot;delete&quot;: { &quot;_id&quot;: 1}}\r\n\r\n{\r\n  &quot;took&quot;: 20,\r\n  &quot;errors&quot;: false,\r\n  &quot;items&quot;: [\r\n    {\r\n      &quot;delete&quot;: {\r\n        &quot;found&quot;: true,\r\n        &quot;_index&quot;: &quot;fs&quot;,\r\n        &quot;_type&quot;: &quot;lock&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_version&quot;: 2,\r\n        &quot;result&quot;: &quot;deleted&quot;,\r\n        &quot;_shards&quot;: {\r\n          &quot;total&quot;: 2,\r\n          &quot;successful&quot;: 1,\r\n          &quot;failed&quot;: 0\r\n        },\r\n        &quot;status&quot;: 200\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;upsert&quot;: { &quot;process_id&quot;: 234 },\r\n  &quot;script&quot;: {\r\n    &quot;lang&quot;: &quot;groovy&quot;,\r\n    &quot;file&quot;: &quot;judge-lock&quot;, \r\n    &quot;params&quot;: {\r\n      &quot;process_id&quot;: 234\r\n    }\r\n  }\r\n}\r\n\r\nprocess_id=234上锁就成功了\r\n',8,0,0,1514619187,0,0,0),(157,1,'共享锁和排他锁的说明','','','1、共享锁和排他锁的说明\r\n\r\n共享锁：这份数据是共享的，然后多个线程过来，都可以获取同一个数据的共享锁，然后对这个数据执行读操作\r\n排他锁：是排他的操作，只能一个线程获取排他锁，然后执行增删改操作\r\n\r\n读写锁的分离\r\n\r\n如果只是要读取数据的话，那么任意个线程都可以同时进来然后读取数据，每个线程都可以上一个共享锁\r\n但是这个时候，如果有线程要过来修改数据，那么会尝试上排他锁，排他锁会跟共享锁互斥，也就是说，如果有人已经上了共享锁了，那么排他锁就不能上，就得等\r\n\r\n如果有人在读数据，就不允许别人来修改数据\r\n\r\n反之，也是一样的\r\n\r\n如果有人在修改数据，就是加了排他锁\r\n那么其他线程过来要修改数据，也会尝试加排他锁，此时会失败，锁冲突，必须等待，同时只能有一个线程修改数据\r\n如果有人过来同时要读取数据，那么会尝试加共享锁，此时会失败，因为共享锁和排他锁是冲突的\r\n\r\n如果有在修改数据，就不允许别人来修改数据，也不允许别人来读取数据\r\n\r\n2、共享锁和排他锁的实验\r\n\r\n第一步：有人在读数据，其他人也能过来读数据\r\n\r\njudge-lock-2.groovy: if (ctx._source.lock_type == &#039;exclusive&#039;) { assert false }; ctx._source.lock_count++\r\n\r\nPOST /fs/lock/1/_update \r\n{\r\n  &quot;upsert&quot;: { \r\n    &quot;lock_type&quot;:  &quot;shared&quot;,\r\n    &quot;lock_count&quot;: 1\r\n  },\r\n  &quot;script&quot;: {\r\n  	&quot;lang&quot;: &quot;groovy&quot;,\r\n  	&quot;file&quot;: &quot;judge-lock-2&quot;\r\n  }\r\n}\r\n\r\nPOST /fs/lock/1/_update \r\n{\r\n  &quot;upsert&quot;: { \r\n    &quot;lock_type&quot;:  &quot;shared&quot;,\r\n    &quot;lock_count&quot;: 1\r\n  },\r\n  &quot;script&quot;: {\r\n  	&quot;lang&quot;: &quot;groovy&quot;,\r\n  	&quot;file&quot;: &quot;judge-lock-2&quot;\r\n  }\r\n}\r\n\r\nGET /fs/lock/1\r\n\r\n{\r\n  &quot;_index&quot;: &quot;fs&quot;,\r\n  &quot;_type&quot;: &quot;lock&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 3,\r\n  &quot;found&quot;: true,\r\n  &quot;_source&quot;: {\r\n    &quot;lock_type&quot;: &quot;shared&quot;,\r\n    &quot;lock_count&quot;: 3\r\n  }\r\n}\r\n\r\n就给大家模拟了，有人上了共享锁，你还是要上共享锁，直接上就行了，没问题，只是lock_count加1\r\n\r\n2、已经有人上了共享锁，然后有人要上排他锁\r\n\r\nPUT /fs/lock/1/_create\r\n{ &quot;lock_type&quot;: &quot;exclusive&quot; }\r\n\r\n排他锁用的不是upsert语法，create语法，要求lock必须不能存在，直接自己是第一个上锁的人，上的是排他锁\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [3])&quot;,\r\n        &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n        &quot;shard&quot;: &quot;3&quot;,\r\n        &quot;index&quot;: &quot;fs&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [3])&quot;,\r\n    &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n    &quot;shard&quot;: &quot;3&quot;,\r\n    &quot;index&quot;: &quot;fs&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n如果已经有人上了共享锁，明显/fs/lock/1是存在的，create语法去上排他锁，肯定会报错\r\n\r\n3、对共享锁进行解锁\r\n\r\nPOST /fs/lock/1/_update\r\n{\r\n  &quot;script&quot;: {\r\n  	&quot;lang&quot;: &quot;groovy&quot;,\r\n  	&quot;file&quot;: &quot;unlock-shared&quot;\r\n  }\r\n}\r\n\r\n连续解锁3次，此时共享锁就彻底没了\r\n\r\n每次解锁一个共享锁，就对lock_count先减1，如果减了1之后，是0，那么说明所有的共享锁都解锁完了，此时就就将/fs/lock/1删除，就彻底解锁所有的共享锁\r\n\r\n3、上排他锁，再上排他锁\r\n\r\nPUT /fs/lock/1/_create\r\n{ &quot;lock_type&quot;: &quot;exclusive&quot; }\r\n\r\n其他线程\r\n\r\nPUT /fs/lock/1/_create\r\n{ &quot;lock_type&quot;: &quot;exclusive&quot; }\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n        &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [7])&quot;,\r\n        &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n        &quot;shard&quot;: &quot;3&quot;,\r\n        &quot;index&quot;: &quot;fs&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,\r\n    &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [7])&quot;,\r\n    &quot;index_uuid&quot;: &quot;IYbj0OLGQHmMUpLfbhD4Hw&quot;,\r\n    &quot;shard&quot;: &quot;3&quot;,\r\n    &quot;index&quot;: &quot;fs&quot;\r\n  },\r\n  &quot;status&quot;: 409\r\n}\r\n\r\n4、上排他锁，上共享锁\r\n\r\nPOST /fs/lock/1/_update \r\n{\r\n  &quot;upsert&quot;: { \r\n    &quot;lock_type&quot;:  &quot;shared&quot;,\r\n    &quot;lock_count&quot;: 1\r\n  },\r\n  &quot;script&quot;: {\r\n  	&quot;lang&quot;: &quot;groovy&quot;,\r\n  	&quot;file&quot;: &quot;judge-lock-2&quot;\r\n  }\r\n}\r\n\r\n{\r\n  &quot;error&quot;: {\r\n    &quot;root_cause&quot;: [\r\n      {\r\n        &quot;type&quot;: &quot;remote_transport_exception&quot;,\r\n        &quot;reason&quot;: &quot;[4onsTYV][127.0.0.1:9300][indices:data/write/update[s]]&quot;\r\n      }\r\n    ],\r\n    &quot;type&quot;: &quot;illegal_argument_exception&quot;,\r\n    &quot;reason&quot;: &quot;failed to execute script&quot;,\r\n    &quot;caused_by&quot;: {\r\n      &quot;type&quot;: &quot;script_exception&quot;,\r\n      &quot;reason&quot;: &quot;error evaluating judge-lock-2&quot;,\r\n      &quot;caused_by&quot;: {\r\n        &quot;type&quot;: &quot;power_assertion_error&quot;,\r\n        &quot;reason&quot;: &quot;assert false\\n&quot;\r\n      },\r\n      &quot;script_stack&quot;: [],\r\n      &quot;script&quot;: &quot;&quot;,\r\n      &quot;lang&quot;: &quot;groovy&quot;\r\n    }\r\n  },\r\n  &quot;status&quot;: 400\r\n}\r\n\r\n5、解锁排他锁\r\n\r\nDELETE /fs/lock/1',8,0,0,1514619254,0,0,0),(158,1,'nested object','','','1、做一个实验，引出来为什么需要nested object\r\n\r\n冗余数据方式的来建模，其实用的就是object类型，我们这里又要引入一种新的object类型，nested object类型\r\n\r\n博客，评论，做的这种数据模型\r\n\r\nPUT /website/blogs/6\r\n{\r\n  &quot;title&quot;: &quot;花无缺发表的一篇帖子&quot;,\r\n  &quot;content&quot;:  &quot;我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。&quot;,\r\n  &quot;tags&quot;:  [ &quot;投资&quot;, &quot;理财&quot; ],\r\n  &quot;comments&quot;: [ \r\n    {\r\n      &quot;name&quot;:    &quot;小鱼儿&quot;,\r\n      &quot;comment&quot;: &quot;什么股票啊？推荐一下呗&quot;,\r\n      &quot;age&quot;:     28,\r\n      &quot;stars&quot;:   4,\r\n      &quot;date&quot;:    &quot;2016-09-01&quot;\r\n    },\r\n    {\r\n      &quot;name&quot;:    &quot;黄药师&quot;,\r\n      &quot;comment&quot;: &quot;我喜欢投资房产，风，险大收益也大&quot;,\r\n      &quot;age&quot;:     31,\r\n      &quot;stars&quot;:   5,\r\n      &quot;date&quot;:    &quot;2016-10-22&quot;\r\n    }\r\n  ]\r\n}\r\n\r\n被年龄是28岁的黄药师评论过的博客，搜索\r\n\r\nGET /website/blogs/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        { &quot;match&quot;: { &quot;comments.name&quot;: &quot;黄药师&quot; }},\r\n        { &quot;match&quot;: { &quot;comments.age&quot;:  28      }} \r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 102,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1.8022683,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;website&quot;,\r\n        &quot;_type&quot;: &quot;blogs&quot;,\r\n        &quot;_id&quot;: &quot;6&quot;,\r\n        &quot;_score&quot;: 1.8022683,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;花无缺发表的一篇帖子&quot;,\r\n          &quot;content&quot;: &quot;我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。&quot;,\r\n          &quot;tags&quot;: [\r\n            &quot;投资&quot;,\r\n            &quot;理财&quot;\r\n          ],\r\n          &quot;comments&quot;: [\r\n            {\r\n              &quot;name&quot;: &quot;小鱼儿&quot;,\r\n              &quot;comment&quot;: &quot;什么股票啊？推荐一下呗&quot;,\r\n              &quot;age&quot;: 28,\r\n              &quot;stars&quot;: 4,\r\n              &quot;date&quot;: &quot;2016-09-01&quot;\r\n            },\r\n            {\r\n              &quot;name&quot;: &quot;黄药师&quot;,\r\n              &quot;comment&quot;: &quot;我喜欢投资房产，风，险大收益也大&quot;,\r\n              &quot;age&quot;: 31,\r\n              &quot;stars&quot;: 5,\r\n              &quot;date&quot;: &quot;2016-10-22&quot;\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n结果是。。。好像不太对啊？？？\r\n\r\nobject类型数据结构的底层存储。。。\r\n\r\n{\r\n  &quot;title&quot;:            [ &quot;花无缺&quot;, &quot;发表&quot;, &quot;一篇&quot;, &quot;帖子&quot; ],\r\n  &quot;content&quot;:             [ &quot;我&quot;, &quot;是&quot;, &quot;花无缺&quot;, &quot;大家&quot;, &quot;要不要&quot;, &quot;考虑&quot;, &quot;一下&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;买&quot;, &quot;股票&quot;, &quot;事情&quot; ],\r\n  &quot;tags&quot;:             [ &quot;投资&quot;, &quot;理财&quot; ],\r\n  &quot;comments.name&quot;:    [ &quot;小鱼儿&quot;, &quot;黄药师&quot; ],\r\n  &quot;comments.comment&quot;: [ &quot;什么&quot;, &quot;股票&quot;, &quot;推荐&quot;, &quot;我&quot;, &quot;喜欢&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;风险&quot;, &quot;收益&quot;, &quot;大&quot; ],\r\n  &quot;comments.age&quot;:     [ 28, 31 ],\r\n  &quot;comments.stars&quot;:   [ 4, 5 ],\r\n  &quot;comments.date&quot;:    [ 2016-09-01, 2016-10-22 ]\r\n}\r\n\r\nobject类型底层数据结构，会将一个json数组中的数据，进行扁平化\r\n\r\n所以，直接命中了这个document，name=黄药师，age=28，正好符合\r\n\r\n2、引入nested object类型，来解决object类型底层数据结构导致的问题\r\n\r\n修改mapping，将comments的类型从object设置为nested\r\n\r\nPUT /website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;blogs&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;comments&quot;: {\r\n          &quot;type&quot;: &quot;nested&quot;, \r\n          &quot;properties&quot;: {\r\n            &quot;name&quot;:    { &quot;type&quot;: &quot;string&quot;  },\r\n            &quot;comment&quot;: { &quot;type&quot;: &quot;string&quot;  },\r\n            &quot;age&quot;:     { &quot;type&quot;: &quot;short&quot;   },\r\n            &quot;stars&quot;:   { &quot;type&quot;: &quot;short&quot;   },\r\n            &quot;date&quot;:    { &quot;type&quot;: &quot;date&quot;    }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{ \r\n  &quot;comments.name&quot;:    [ &quot;小鱼儿&quot; ],\r\n  &quot;comments.comment&quot;: [ &quot;什么&quot;, &quot;股票&quot;, &quot;推荐&quot; ],\r\n  &quot;comments.age&quot;:     [ 28 ],\r\n  &quot;comments.stars&quot;:   [ 4 ],\r\n  &quot;comments.date&quot;:    [ 2014-09-01 ]\r\n}\r\n{ \r\n  &quot;comments.name&quot;:    [ &quot;黄药师&quot; ],\r\n  &quot;comments.comment&quot;: [ &quot;我&quot;, &quot;喜欢&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;风险&quot;, &quot;收益&quot;, &quot;大&quot; ],\r\n  &quot;comments.age&quot;:     [ 31 ],\r\n  &quot;comments.stars&quot;:   [ 5 ],\r\n  &quot;comments.date&quot;:    [ 2014-10-22 ]\r\n}\r\n{ \r\n  &quot;title&quot;:            [ &quot;花无缺&quot;, &quot;发表&quot;, &quot;一篇&quot;, &quot;帖子&quot; ],\r\n  &quot;body&quot;:             [ &quot;我&quot;, &quot;是&quot;, &quot;花无缺&quot;, &quot;大家&quot;, &quot;要不要&quot;, &quot;考虑&quot;, &quot;一下&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;买&quot;, &quot;股票&quot;, &quot;事情&quot; ],\r\n  &quot;tags&quot;:             [ &quot;投资&quot;, &quot;理财&quot; ]\r\n}\r\n\r\n再次搜索，成功了。。。\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;花无缺&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;nested&quot;: {\r\n            &quot;path&quot;: &quot;comments&quot;,\r\n            &quot;query&quot;: {\r\n              &quot;bool&quot;: {\r\n                &quot;must&quot;: [\r\n                  {\r\n                    &quot;match&quot;: {\r\n                      &quot;comments.name&quot;: &quot;黄药师&quot;\r\n                    }\r\n                  },\r\n                  {\r\n                    &quot;match&quot;: {\r\n                      &quot;comments.age&quot;: 28\r\n                    }\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nscore_mode：max，min，avg，none，默认是avg\r\n\r\n如果搜索命中了多个nested document，如何讲个多个nested document的分数合并为一个分数\r\n',8,0,0,1514619301,0,0,0),(159,1,'基于nested object中的数据进行聚合分析','','','我们讲解一下基于nested object中的数据进行聚合分析\r\n\r\n聚合数据分析的需求1：按照评论日期进行bucket划分，然后拿到每个月的评论的评分的平均值\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;size&quot;: 0, \r\n  &quot;aggs&quot;: {\r\n    &quot;comments_path&quot;: {\r\n      &quot;nested&quot;: {\r\n        &quot;path&quot;: &quot;comments&quot;\r\n      }, \r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_comments_date&quot;: {\r\n          &quot;date_histogram&quot;: {\r\n            &quot;field&quot;: &quot;comments.date&quot;,\r\n            &quot;interval&quot;: &quot;month&quot;,\r\n            &quot;format&quot;: &quot;yyyy-MM&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;avg_stars&quot;: {\r\n              &quot;avg&quot;: {\r\n                &quot;field&quot;: &quot;comments.stars&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 52,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;comments_path&quot;: {\r\n      &quot;doc_count&quot;: 4,\r\n      &quot;group_by_comments_date&quot;: {\r\n        &quot;buckets&quot;: [\r\n          {\r\n            &quot;key_as_string&quot;: &quot;2016-08&quot;,\r\n            &quot;key&quot;: 1470009600000,\r\n            &quot;doc_count&quot;: 1,\r\n            &quot;avg_stars&quot;: {\r\n              &quot;value&quot;: 3\r\n            }\r\n          },\r\n          {\r\n            &quot;key_as_string&quot;: &quot;2016-09&quot;,\r\n            &quot;key&quot;: 1472688000000,\r\n            &quot;doc_count&quot;: 2,\r\n            &quot;avg_stars&quot;: {\r\n              &quot;value&quot;: 4.5\r\n            }\r\n          },\r\n          {\r\n            &quot;key_as_string&quot;: &quot;2016-10&quot;,\r\n            &quot;key&quot;: 1475280000000,\r\n            &quot;doc_count&quot;: 1,\r\n            &quot;avg_stars&quot;: {\r\n              &quot;value&quot;: 5\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\n\r\nGET /website/blogs/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;comments_path&quot;: {\r\n      &quot;nested&quot;: {\r\n        &quot;path&quot;: &quot;comments&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_comments_age&quot;: {\r\n          &quot;histogram&quot;: {\r\n            &quot;field&quot;: &quot;comments.age&quot;,\r\n            &quot;interval&quot;: 10\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;reverse_path&quot;: {\r\n              &quot;reverse_nested&quot;: {}, \r\n              &quot;aggs&quot;: {\r\n                &quot;group_by_tags&quot;: {\r\n                  &quot;terms&quot;: {\r\n                    &quot;field&quot;: &quot;tags.keyword&quot;\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;comments_path&quot;: {\r\n      &quot;doc_count&quot;: 4,\r\n      &quot;group_by_comments_age&quot;: {\r\n        &quot;buckets&quot;: [\r\n          {\r\n            &quot;key&quot;: 20,\r\n            &quot;doc_count&quot;: 1,\r\n            &quot;reverse_path&quot;: {\r\n              &quot;doc_count&quot;: 1,\r\n              &quot;group_by_tags&quot;: {\r\n                &quot;doc_count_error_upper_bound&quot;: 0,\r\n                &quot;sum_other_doc_count&quot;: 0,\r\n                &quot;buckets&quot;: [\r\n                  {\r\n                    &quot;key&quot;: &quot;投资&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  },\r\n                  {\r\n                    &quot;key&quot;: &quot;理财&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          },\r\n          {\r\n            &quot;key&quot;: 30,\r\n            &quot;doc_count&quot;: 3,\r\n            &quot;reverse_path&quot;: {\r\n              &quot;doc_count&quot;: 2,\r\n              &quot;group_by_tags&quot;: {\r\n                &quot;doc_count_error_upper_bound&quot;: 0,\r\n                &quot;sum_other_doc_count&quot;: 0,\r\n                &quot;buckets&quot;: [\r\n                  {\r\n                    &quot;key&quot;: &quot;大侠&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  },\r\n                  {\r\n                    &quot;key&quot;: &quot;投资&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  },\r\n                  {\r\n                    &quot;key&quot;: &quot;理财&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  },\r\n                  {\r\n                    &quot;key&quot;: &quot;练功&quot;,\r\n                    &quot;doc_count&quot;: 1\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514619351,0,0,0),(160,1,'nested object的建模','','','nested object的建模，有个不好的地方，就是采取的是类似冗余数据的方式，将多个数据都放在一起了，维护成本就比较高\r\n\r\nparent child建模方式，采取的是类似于关系型数据库的三范式类的建模，多个实体都分割开来，每个实体之间都通过一些关联方式，进行了父子关系的关联，各种数据不需要都放在一起，父doc和子doc分别在进行更新的时候，都不会影响对方\r\n\r\n一对多关系的建模，维护起来比较方便，而且我们之前说过，类似关系型数据库的建模方式，应用层join的方式，会导致性能比较差，因为做多次搜索。父子关系的数据模型，不会，性能很好。因为虽然数据实体之间分割开来，但是我们在搜索的时候，由es自动为我们处理底层的关联关系，并且通过一些手段保证搜索性能。\r\n\r\n父子关系数据模型，相对于nested数据模型来说，优点是父doc和子doc互相之间不会影响\r\n\r\n要点：父子关系元数据映射，用于确保查询时候的高性能，但是有一个限制，就是父子数据必须存在于一个shard中\r\n\r\n父子关系数据存在一个shard中，而且还有映射其关联关系的元数据，那么搜索父子关系数据的时候，不用跨分片，一个分片本地自己就搞定了，性能当然高咯\r\n\r\n案例背景：研发中心员工管理案例，一个IT公司有多个研发中心，每个研发中心有多个员工\r\n\r\nPUT /company\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;rd_center&quot;: {},\r\n    &quot;employee&quot;: {\r\n      &quot;_parent&quot;: {\r\n        &quot;type&quot;: &quot;rd_center&quot; \r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n父子关系建模的核心，多个type之间有父子关系，用_parent指定父type\r\n\r\nPOST /company/rd_center/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;name&quot;: &quot;上海研发中心&quot;, &quot;city&quot;: &quot;上海&quot;, &quot;country&quot;: &quot;中国&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;3&quot; }}\r\n{ &quot;name&quot;: &quot;硅谷人工智能实验室&quot;, &quot;city&quot;: &quot;硅谷&quot;, &quot;country&quot;: &quot;美国&quot; }\r\n\r\nshard路由的时候，id=1的rd_center doc，默认会根据id进行路由，到某一个shard\r\n\r\nPUT /company/employee/1?parent=1 \r\n{\r\n  &quot;name&quot;:  &quot;张三&quot;,\r\n  &quot;birthday&quot;:   &quot;1970-10-24&quot;,\r\n  &quot;hobby&quot;: &quot;爬山&quot;\r\n}\r\n\r\n维护父子关系的核心，parent=1，指定了这个数据的父doc的id\r\n\r\n此时，parent-child关系，就确保了说，父doc和子doc都是保存在一个shard上的。内部原理还是doc routing，employee和rd_center的数据，都会用parent id作为routing，这样就会到一个shard\r\n\r\n就不会根据id=1的employee doc的id进行路由了，而是根据parent=1进行路由，会根据父doc的id进行路由，那么就可以通过底层的路由机制，保证父子数据存在于一个shard中\r\n\r\nPOST /company/employee/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: 2, &quot;parent&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;李四&quot;, &quot;birthday&quot;: &quot;1982-05-16&quot;, &quot;hobby&quot;: &quot;游泳&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 3, &quot;parent&quot;: &quot;2&quot; }}\r\n{ &quot;name&quot;: &quot;王二&quot;, &quot;birthday&quot;: &quot;1979-04-01&quot;, &quot;hobby&quot;: &quot;爬山&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: 4, &quot;parent&quot;: &quot;3&quot; }}\r\n{ &quot;name&quot;: &quot;赵五&quot;, &quot;birthday&quot;: &quot;1987-05-11&quot;, &quot;hobby&quot;: &quot;骑马&quot; }\r\n\r\n',8,0,0,1514619392,0,0,0),(161,1,'父子关系的数据模型','','','我们已经建立了父子关系的数据模型之后，就要基于这个模型进行各种搜索和聚合了\r\n\r\n1、搜索有1980年以后出生的员工的研发中心\r\n\r\nGET /company/rd_center/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_child&quot;: {\r\n      &quot;type&quot;: &quot;employee&quot;,\r\n      &quot;query&quot;: {\r\n        &quot;range&quot;: {\r\n          &quot;birthday&quot;: {\r\n            &quot;gte&quot;: &quot;1980-01-01&quot;\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 33,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;rd_center&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;北京研发总部&quot;,\r\n          &quot;city&quot;: &quot;北京&quot;,\r\n          &quot;country&quot;: &quot;中国&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;rd_center&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;硅谷人工智能实验室&quot;,\r\n          &quot;city&quot;: &quot;硅谷&quot;,\r\n          &quot;country&quot;: &quot;美国&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n2、搜索有名叫张三的员工的研发中心\r\n\r\nGET /company/rd_center/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_child&quot;: {\r\n      &quot;type&quot;:       &quot;employee&quot;,\r\n      &quot;query&quot;: {\r\n        &quot;match&quot;: {\r\n          &quot;name&quot;: &quot;张三&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;rd_center&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;北京研发总部&quot;,\r\n          &quot;city&quot;: &quot;北京&quot;,\r\n          &quot;country&quot;: &quot;中国&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n3、搜索有至少2个以上员工的研发中心\r\n\r\nGET /company/rd_center/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_child&quot;: {\r\n      &quot;type&quot;:         &quot;employee&quot;,\r\n      &quot;min_children&quot;: 2, \r\n      &quot;query&quot;: {\r\n        &quot;match_all&quot;: {}\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;rd_center&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;北京研发总部&quot;,\r\n          &quot;city&quot;: &quot;北京&quot;,\r\n          &quot;country&quot;: &quot;中国&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n4、搜索在中国的研发中心的员工\r\n\r\nGET /company/employee/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_parent&quot;: {\r\n      &quot;parent_type&quot;: &quot;rd_center&quot;,\r\n      &quot;query&quot;: {\r\n        &quot;term&quot;: {\r\n          &quot;country.keyword&quot;: &quot;中国&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;employee&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_routing&quot;: &quot;2&quot;,\r\n        &quot;_parent&quot;: &quot;2&quot;,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;王二&quot;,\r\n          &quot;birthday&quot;: &quot;1979-04-01&quot;,\r\n          &quot;hobby&quot;: &quot;爬山&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;employee&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_routing&quot;: &quot;1&quot;,\r\n        &quot;_parent&quot;: &quot;1&quot;,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;张三&quot;,\r\n          &quot;birthday&quot;: &quot;1970-10-24&quot;,\r\n          &quot;hobby&quot;: &quot;爬山&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;employee&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_routing&quot;: &quot;1&quot;,\r\n        &quot;_parent&quot;: &quot;1&quot;,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;李四&quot;,\r\n          &quot;birthday&quot;: &quot;1982-05-16&quot;,\r\n          &quot;hobby&quot;: &quot;游泳&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514620472,0,0,0),(162,1,'统计国家员工爱好','','','统计每个国家的喜欢每种爱好的员工有多少个\r\n\r\nGET /company/rd_center/_search \r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;group_by_country&quot;: {\r\n      &quot;terms&quot;: {\r\n        &quot;field&quot;: &quot;country.keyword&quot;\r\n      },\r\n      &quot;aggs&quot;: {\r\n        &quot;group_by_child_employee&quot;: {\r\n          &quot;children&quot;: {\r\n            &quot;type&quot;: &quot;employee&quot;\r\n          },\r\n          &quot;aggs&quot;: {\r\n            &quot;group_by_hobby&quot;: {\r\n              &quot;terms&quot;: {\r\n                &quot;field&quot;: &quot;hobby.keyword&quot;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 15,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;group_by_country&quot;: {\r\n      &quot;doc_count_error_upper_bound&quot;: 0,\r\n      &quot;sum_other_doc_count&quot;: 0,\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;中国&quot;,\r\n          &quot;doc_count&quot;: 2,\r\n          &quot;group_by_child_employee&quot;: {\r\n            &quot;doc_count&quot;: 3,\r\n            &quot;group_by_hobby&quot;: {\r\n              &quot;doc_count_error_upper_bound&quot;: 0,\r\n              &quot;sum_other_doc_count&quot;: 0,\r\n              &quot;buckets&quot;: [\r\n                {\r\n                  &quot;key&quot;: &quot;爬山&quot;,\r\n                  &quot;doc_count&quot;: 2\r\n                },\r\n                {\r\n                  &quot;key&quot;: &quot;游泳&quot;,\r\n                  &quot;doc_count&quot;: 1\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;美国&quot;,\r\n          &quot;doc_count&quot;: 1,\r\n          &quot;group_by_child_employee&quot;: {\r\n            &quot;doc_count&quot;: 1,\r\n            &quot;group_by_hobby&quot;: {\r\n              &quot;doc_count_error_upper_bound&quot;: 0,\r\n              &quot;sum_other_doc_count&quot;: 0,\r\n              &quot;buckets&quot;: [\r\n                {\r\n                  &quot;key&quot;: &quot;骑马&quot;,\r\n                  &quot;doc_count&quot;: 1\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}',8,0,0,1514620525,0,0,0),(163,1,'父子关系，祖孙三层关系','','','父子关系，祖孙三层关系的数据建模，搜索\r\n\r\nPUT /company\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;country&quot;: {},\r\n    &quot;rd_center&quot;: {\r\n      &quot;_parent&quot;: {\r\n        &quot;type&quot;: &quot;country&quot; \r\n      }\r\n    },\r\n    &quot;employee&quot;: {\r\n      &quot;_parent&quot;: {\r\n        &quot;type&quot;: &quot;rd_center&quot; \r\n      }\r\n    }\r\n  }\r\n}\r\n\r\ncountry -&gt; rd_center -&gt; employee，祖孙三层数据模型\r\n\r\nPOST /company/country/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;中国&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;2&quot; }}\r\n{ &quot;name&quot;: &quot;美国&quot; }\r\n\r\nPOST /company/rd_center/_bulk\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;1&quot;, &quot;parent&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;北京研发总部&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;2&quot;, &quot;parent&quot;: &quot;1&quot; }}\r\n{ &quot;name&quot;: &quot;上海研发中心&quot; }\r\n{ &quot;index&quot;: { &quot;_id&quot;: &quot;3&quot;, &quot;parent&quot;: &quot;2&quot; }}\r\n{ &quot;name&quot;: &quot;硅谷人工智能实验室&quot; }\r\n\r\nPUT /company/employee/1?parent=1&amp;routing=1\r\n{\r\n  &quot;name&quot;:  &quot;张三&quot;,\r\n  &quot;dob&quot;:   &quot;1970-10-24&quot;,\r\n  &quot;hobby&quot;: &quot;爬山&quot;\r\n}\r\n\r\nrouting参数的讲解，必须跟grandparent相同，否则有问题\r\n\r\ncountry，用的是自己的id去路由; rd_center，parent，用的是country的id去路由; employee，如果也是仅仅指定一个parent，那么用的是rd_center的id去路由，这就导致祖孙三层数据不会在一个shard上\r\n\r\n孙子辈儿，要手动指定routing，指定为爷爷辈儿的数据的id\r\n\r\n搜索有爬山爱好的员工所在的国家\r\n\r\nGET /company/country/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;has_child&quot;: {\r\n      &quot;type&quot;: &quot;rd_center&quot;,\r\n      &quot;query&quot;: {\r\n        &quot;has_child&quot;: {\r\n          &quot;type&quot;: &quot;employee&quot;,\r\n          &quot;query&quot;: {\r\n            &quot;match&quot;: {\r\n              &quot;hobby&quot;: &quot;爬山&quot;\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 10,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;company&quot;,\r\n        &quot;_type&quot;: &quot;country&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;name&quot;: &quot;中国&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514620568,0,0,0),(164,1,'term vector介绍','','','1、term vector介绍\r\n\r\n获取document中的某个field内的各个term的统计信息\r\n\r\nterm information: term frequency in the field, term positions, start and end offsets, term payloads\r\n\r\nterm statistics: 设置term_statistics=true; total term frequency, 一个term在所有document中出现的频率; document frequency，有多少document包含这个term\r\n\r\nfield statistics: document count，有多少document包含这个field; sum of document frequency，一个field中所有term的df之和; sum of total term frequency，一个field中的所有term的tf之和\r\n\r\nGET /twitter/tweet/1/_termvectors\r\nGET /twitter/tweet/1/_termvectors?fields=text\r\n\r\nterm statistics和field statistics并不精准，不会被考虑有的doc可能被删除了\r\n\r\n我告诉大家，其实很少用，用的时候，一般来说，就是你需要对一些数据做探查的时候。比如说，你想要看到某个term，某个词条，大话西游，这个词条，在多少个document中出现了。或者说某个field，film_desc，电影的说明信息，有多少个doc包含了这个说明信息。\r\n\r\n2、index-iime term vector实验\r\n\r\nterm vector，涉及了很多的term和field相关的统计信息，有两种方式可以采集到这个统计信息\r\n\r\n（1）index-time，你在mapping里配置一下，然后建立索引的时候，就直接给你生成这些term和field的统计信息了\r\n（2）query-time，你之前没有生成过任何的Term vector信息，然后在查看term vector的时候，直接就可以看到了，会on the fly，现场计算出各种统计信息，然后返回给你\r\n\r\n这一讲，不会手敲任何命令，直接copy我做好的命令，因为这一讲的重点，不是掌握什么搜索或者聚合的语法，而是说，掌握，如何采集term vector信息，然后如何看懂term vector信息，你能掌握利用term vector进行数据探查\r\n\r\nPUT /my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;text&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;term_vector&quot;: &quot;with_positions_offsets_payloads&quot;,\r\n            &quot;store&quot; : true,\r\n            &quot;analyzer&quot; : &quot;fulltext_analyzer&quot;\r\n         },\r\n         &quot;fullname&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;analyzer&quot; : &quot;fulltext_analyzer&quot;\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;settings&quot; : {\r\n    &quot;index&quot; : {\r\n      &quot;number_of_shards&quot; : 1,\r\n      &quot;number_of_replicas&quot; : 0\r\n    },\r\n    &quot;analysis&quot;: {\r\n      &quot;analyzer&quot;: {\r\n        &quot;fulltext_analyzer&quot;: {\r\n          &quot;type&quot;: &quot;custom&quot;,\r\n          &quot;tokenizer&quot;: &quot;whitespace&quot;,\r\n          &quot;filter&quot;: [\r\n            &quot;lowercase&quot;,\r\n            &quot;type_as_payload&quot;\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n  &quot;text&quot; : &quot;hello test test test &quot;\r\n}\r\n\r\nPUT /my_index/my_type/2\r\n{\r\n  &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n  &quot;text&quot; : &quot;other hello test ...&quot;\r\n}\r\n\r\nGET /my_index/my_type/1/_termvectors\r\n{\r\n  &quot;fields&quot; : [&quot;text&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;payloads&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;my_index&quot;,\r\n  &quot;_type&quot;: &quot;my_type&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;found&quot;: true,\r\n  &quot;took&quot;: 10,\r\n  &quot;term_vectors&quot;: {\r\n    &quot;text&quot;: {\r\n      &quot;field_statistics&quot;: {\r\n        &quot;sum_doc_freq&quot;: 6,\r\n        &quot;doc_count&quot;: 2,\r\n        &quot;sum_ttf&quot;: 8\r\n      },\r\n      &quot;terms&quot;: {\r\n        &quot;hello&quot;: {\r\n          &quot;doc_freq&quot;: 2,\r\n          &quot;ttf&quot;: 2,\r\n          &quot;term_freq&quot;: 1,\r\n          &quot;tokens&quot;: [\r\n            {\r\n              &quot;position&quot;: 0,\r\n              &quot;start_offset&quot;: 0,\r\n              &quot;end_offset&quot;: 5,\r\n              &quot;payload&quot;: &quot;d29yZA==&quot;\r\n            }\r\n          ]\r\n        },\r\n        &quot;test&quot;: {\r\n          &quot;doc_freq&quot;: 2,\r\n          &quot;ttf&quot;: 4,\r\n          &quot;term_freq&quot;: 3,\r\n          &quot;tokens&quot;: [\r\n            {\r\n              &quot;position&quot;: 1,\r\n              &quot;start_offset&quot;: 6,\r\n              &quot;end_offset&quot;: 10,\r\n              &quot;payload&quot;: &quot;d29yZA==&quot;\r\n            },\r\n            {\r\n              &quot;position&quot;: 2,\r\n              &quot;start_offset&quot;: 11,\r\n              &quot;end_offset&quot;: 15,\r\n              &quot;payload&quot;: &quot;d29yZA==&quot;\r\n            },\r\n            {\r\n              &quot;position&quot;: 3,\r\n              &quot;start_offset&quot;: 16,\r\n              &quot;end_offset&quot;: 20,\r\n              &quot;payload&quot;: &quot;d29yZA==&quot;\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n3、query-time term vector实验\r\n\r\nGET /my_index/my_type/1/_termvectors\r\n{\r\n  &quot;fields&quot; : [&quot;fullname&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true\r\n}\r\n\r\n一般来说，如果条件允许，你就用query time的term vector就可以了，你要探查什么数据，现场去探查一下就好了\r\n\r\n4、手动指定doc的term vector\r\n\r\nGET /my_index/my_type/_termvectors\r\n{\r\n  &quot;doc&quot; : {\r\n    &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n    &quot;text&quot; : &quot;hello test test test&quot;\r\n  },\r\n  &quot;fields&quot; : [&quot;text&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;payloads&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true\r\n}\r\n\r\n手动指定一个doc，实际上不是要指定doc，而是要指定你想要安插的词条，hello test，那么就可以放在一个field中\r\n\r\n将这些term分词，然后对每个term，都去计算它在现有的所有doc中的一些统计信息\r\n\r\n这个挺有用的，可以让你手动指定要探查的term的数据情况，你就可以指定探查“大话西游”这个词条的统计信息\r\n\r\n5、手动指定analyzer来生成term vector\r\n\r\nGET /my_index/my_type/_termvectors\r\n{\r\n  &quot;doc&quot; : {\r\n    &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n    &quot;text&quot; : &quot;hello test test test&quot;\r\n  },\r\n  &quot;fields&quot; : [&quot;text&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;payloads&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true,\r\n  &quot;per_field_analyzer&quot; : {\r\n    &quot;text&quot;: &quot;standard&quot;\r\n  }\r\n}\r\n\r\n6、terms filter\r\n\r\nGET /my_index/my_type/_termvectors\r\n{\r\n  &quot;doc&quot; : {\r\n    &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n    &quot;text&quot; : &quot;hello test test test&quot;\r\n  },\r\n  &quot;fields&quot; : [&quot;text&quot;],\r\n  &quot;offsets&quot; : true,\r\n  &quot;payloads&quot; : true,\r\n  &quot;positions&quot; : true,\r\n  &quot;term_statistics&quot; : true,\r\n  &quot;field_statistics&quot; : true,\r\n  &quot;filter&quot; : {\r\n      &quot;max_num_terms&quot; : 3,\r\n      &quot;min_term_freq&quot; : 1,\r\n      &quot;min_doc_freq&quot; : 1\r\n    }\r\n}\r\n\r\n这个就是说，根据term统计信息，过滤出你想要看到的term vector统计结果\r\n也挺有用的，比如你探查数据把，可以过滤掉一些出现频率过低的term，就不考虑了\r\n\r\n7、multi term vector\r\n\r\nGET _mtermvectors\r\n{\r\n   &quot;docs&quot;: [\r\n      {\r\n         &quot;_index&quot;: &quot;my_index&quot;,\r\n         &quot;_type&quot;: &quot;my_type&quot;,\r\n         &quot;_id&quot;: &quot;2&quot;,\r\n         &quot;term_statistics&quot;: true\r\n      },\r\n      {\r\n         &quot;_index&quot;: &quot;my_index&quot;,\r\n         &quot;_type&quot;: &quot;my_type&quot;,\r\n         &quot;_id&quot;: &quot;1&quot;,\r\n         &quot;fields&quot;: [\r\n            &quot;text&quot;\r\n         ]\r\n      }\r\n   ]\r\n}\r\n\r\nGET /my_index/_mtermvectors\r\n{\r\n   &quot;docs&quot;: [\r\n      {\r\n         &quot;_type&quot;: &quot;test&quot;,\r\n         &quot;_id&quot;: &quot;2&quot;,\r\n         &quot;fields&quot;: [\r\n            &quot;text&quot;\r\n         ],\r\n         &quot;term_statistics&quot;: true\r\n      },\r\n      {\r\n         &quot;_type&quot;: &quot;test&quot;,\r\n         &quot;_id&quot;: &quot;1&quot;\r\n      }\r\n   ]\r\n}\r\n\r\nGET /my_index/my_type/_mtermvectors\r\n{\r\n   &quot;docs&quot;: [\r\n      {\r\n         &quot;_id&quot;: &quot;2&quot;,\r\n         &quot;fields&quot;: [\r\n            &quot;text&quot;\r\n         ],\r\n         &quot;term_statistics&quot;: true\r\n      },\r\n      {\r\n         &quot;_id&quot;: &quot;1&quot;\r\n      }\r\n   ]\r\n}\r\n\r\nGET /_mtermvectors\r\n{\r\n   &quot;docs&quot;: [\r\n      {\r\n         &quot;_index&quot;: &quot;my_index&quot;,\r\n         &quot;_type&quot;: &quot;my_type&quot;,\r\n         &quot;doc&quot; : {\r\n            &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n            &quot;text&quot; : &quot;hello test test test&quot;\r\n         }\r\n      },\r\n      {\r\n         &quot;_index&quot;: &quot;my_index&quot;,\r\n         &quot;_type&quot;: &quot;my_type&quot;,\r\n         &quot;doc&quot; : {\r\n           &quot;fullname&quot; : &quot;Leo Li&quot;,\r\n           &quot;text&quot; : &quot;other hello test ...&quot;\r\n         }\r\n      }\r\n   ]\r\n}',8,0,0,1514620612,0,0,0),(165,1,'最基本的高亮例子','','','1、一个最基本的高亮例子\r\n\r\nPUT /blog_website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;blogs&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /blog_website/blogs/1\r\n{\r\n  &quot;title&quot;: &quot;我的第一篇博客&quot;,\r\n  &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot;\r\n}\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;title&quot;: &quot;博客&quot;\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;fields&quot;: {\r\n      &quot;title&quot;: {}\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 103,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 0.28582606,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;blog_website&quot;,\r\n        &quot;_type&quot;: &quot;blogs&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 0.28582606,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;我的第一篇博客&quot;,\r\n          &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot;\r\n        },\r\n        &quot;highlight&quot;: {\r\n          &quot;title&quot;: [\r\n            &quot;我的第一篇&lt;em&gt;博客&lt;/em&gt;&quot;\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n&lt;em&gt;&lt;/em&gt;表现，会变成红色，所以说你的指定的field中，如果包含了那个搜索词的话，就会在那个field的文本中，对搜索词进行红色的高亮显示\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;should&quot;: [\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;title&quot;: &quot;博客&quot;\r\n          }\r\n        },\r\n        {\r\n          &quot;match&quot;: {\r\n            &quot;content&quot;: &quot;博客&quot;\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;fields&quot;: {\r\n      &quot;title&quot;: {},\r\n      &quot;content&quot;: {}\r\n    }\r\n  }\r\n}\r\n\r\nhighlight中的field，必须跟query中的field一一对齐的\r\n\r\n2、三种highlight介绍\r\n\r\nplain highlight，lucene highlight，默认\r\n\r\nposting highlight，index_options=offsets\r\n\r\n（1）性能比plain highlight要高，因为不需要重新对高亮文本进行分词\r\n（2）对磁盘的消耗更少\r\n（3）将文本切割为句子，并且对句子进行高亮，效果更好\r\n\r\nPUT /blog_website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;blogs&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n          &quot;index_options&quot;: &quot;offsets&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /blog_website/blogs/1\r\n{\r\n  &quot;title&quot;: &quot;我的第一篇博客&quot;,\r\n  &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot;\r\n}\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;博客&quot;\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;fields&quot;: {\r\n      &quot;content&quot;: {}\r\n    }\r\n  }\r\n}\r\n\r\nfast vector highlight\r\n\r\nindex-time term vector设置在mapping中，就会用fast verctor highlight\r\n\r\n（1）对大field而言（大于1mb），性能更高\r\n\r\nPUT /blog_website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;blogs&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;title&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n          &quot;term_vector&quot; : &quot;with_positions_offsets&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n强制使用某种highlighter，比如对于开启了term vector的field而言，可以强制使用plain highlight\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;博客&quot;\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;fields&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;type&quot;: &quot;plain&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n总结一下，其实可以根据你的实际情况去考虑，一般情况下，用plain highlight也就足够了，不需要做其他额外的设置\r\n如果对高亮的性能要求很高，可以尝试启用posting highlight\r\n如果field的值特别大，超过了1M，那么可以用fast vector highlight\r\n\r\n3、设置高亮html标签，默认是&lt;em&gt;标签\r\n\r\nGET /blog_website/blogs/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;博客&quot;\r\n    }\r\n  },\r\n  &quot;highlight&quot;: {\r\n    &quot;pre_tags&quot;: [&quot;&lt;tag1&gt;&quot;],\r\n    &quot;post_tags&quot;: [&quot;&lt;/tag2&gt;&quot;], \r\n    &quot;fields&quot;: {\r\n      &quot;content&quot;: {\r\n        &quot;type&quot;: &quot;plain&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n4、高亮片段fragment的设置\r\n\r\nGET /_search\r\n{\r\n    &quot;query&quot; : {\r\n        &quot;match&quot;: { &quot;user&quot;: &quot;kimchy&quot; }\r\n    },\r\n    &quot;highlight&quot; : {\r\n        &quot;fields&quot; : {\r\n            &quot;content&quot; : {&quot;fragment_size&quot; : 150, &quot;number_of_fragments&quot; : 3, &quot;no_match_size&quot;: 150 }\r\n        }\r\n    }\r\n}\r\n\r\nfragment_size: 你一个Field的值，比如有长度是1万，但是你不可能在页面上显示这么长啊。。。设置要显示出来的fragment文本判断的长度，默认是100\r\nnumber_of_fragments：你可能你的高亮的fragment文本片段有多个片段，你可以指定就显示几个片段\r\n',8,0,0,1514620732,0,0,0),(166,1,'搜索模板','','','搜索模板，search template，高级功能，就可以将我们的一些搜索进行模板化，然后的话，每次执行这个搜索，就直接调用模板，给传入一些参数就可以了\r\n\r\n越高级的功能，越少使用，可能只有在你真的遇到特别合适的场景的时候，才会去使用某个高级功能。但是，这些高级功能你是否掌握，其实就是普通的es开发人员，和es高手之间的一个区别。高手，一般来说，会把一个技术掌握的特别好，特别全面，特别深入，也许他平时用不到这个技术，但是当真的遇到一定的场景的时候，高手可以基于自己的深厚的技术储备，立即反应过来，找到一个合适的解决方案。\r\n\r\n如果是一个普通的技术人员，一般只会学习跟自己当前工作相关的一些知识和技术，只要求自己掌握的技术可以解决工作目前遇到的问题就可以了，就满足了，就会止步不前了，然后就不会更加深入的去学习一个技术。但是，当你的项目真正遇到问题的时候，遇到了一些难题，你之前的那一点技术储备已经没法去应付这些更加困难的问题了，此时，普通的技术人员就会扎耳挠腮，没有任何办法。\r\n\r\n高手，对技术是很有追求，能够精通很多自己遇到过的技术，但是也许自己学的很多东西，自己都没用过，但是不要紧，这是你的一种技术储备。\r\n\r\n1、search template入门\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot; : {\r\n    &quot;query&quot;: { \r\n      &quot;match&quot; : { \r\n        &quot;{{field}}&quot; : &quot;{{value}}&quot; \r\n      } \r\n    }\r\n  },\r\n  &quot;params&quot; : {\r\n      &quot;field&quot; : &quot;title&quot;,\r\n      &quot;value&quot; : &quot;博客&quot;\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: { \r\n    &quot;match&quot; : { \r\n      &quot;title&quot; : &quot;博客&quot; \r\n    } \r\n  }\r\n}\r\n\r\nsearch template：&quot;{{field}}&quot; : &quot;{{value}}&quot; \r\n\r\n2、toJson\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot;: &quot;{\\&quot;query\\&quot;: {\\&quot;match\\&quot;: {{#toJson}}matchCondition{{/toJson}}}}&quot;,\r\n  &quot;params&quot;: {\r\n    &quot;matchCondition&quot;: {\r\n      &quot;title&quot;: &quot;博客&quot;\r\n    }\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: { \r\n    &quot;match&quot; : { \r\n      &quot;title&quot; : &quot;博客&quot; \r\n    } \r\n  }\r\n}\r\n\r\n3、join\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot;: {\r\n    &quot;query&quot;: {\r\n      &quot;match&quot;: {\r\n        &quot;title&quot;: &quot;{{#join delimiter=&#039; &#039;}}titles{{/join delimiter=&#039; &#039;}}&quot;\r\n      }\r\n    }\r\n  },\r\n  &quot;params&quot;: {\r\n    &quot;titles&quot;: [&quot;博客&quot;, &quot;网站&quot;]\r\n  }\r\n}\r\n\r\n博客,网站\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: { \r\n    &quot;match&quot; : { \r\n      &quot;title&quot; : &quot;博客 网站&quot; \r\n    } \r\n  }\r\n}\r\n\r\n4、default value\r\n\r\nPOST /blog_website/blogs/1/_update\r\n{\r\n  &quot;doc&quot;: {\r\n    &quot;views&quot;: 5\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot;: {\r\n    &quot;query&quot;: {\r\n      &quot;range&quot;: {\r\n        &quot;views&quot;: {\r\n          &quot;gte&quot;: &quot;{{start}}&quot;,\r\n          &quot;lte&quot;: &quot;{{end}}{{^end}}20{{/end}}&quot;\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;params&quot;: {\r\n    &quot;start&quot;: 1,\r\n    &quot;end&quot;: 10\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;range&quot;: {\r\n      &quot;views&quot;: {\r\n        &quot;gte&quot;: 1,\r\n        &quot;lte&quot;: 10\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search/template\r\n{\r\n  &quot;inline&quot;: {\r\n    &quot;query&quot;: {\r\n      &quot;range&quot;: {\r\n        &quot;views&quot;: {\r\n          &quot;gte&quot;: &quot;{{start}}&quot;,\r\n          &quot;lte&quot;: &quot;{{end}}{{^end}}20{{/end}}&quot;\r\n        }\r\n      }\r\n    }\r\n  },\r\n  &quot;params&quot;: {\r\n    &quot;start&quot;: 1\r\n  }\r\n}\r\n\r\nGET /blog_website/blogs/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;range&quot;: {\r\n      &quot;views&quot;: {\r\n        &quot;gte&quot;: 1,\r\n        &quot;lte&quot;: 20\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\n5、conditional\r\n\r\nes的config/scripts目录下，预先保存这个复杂的模板，后缀名是.mustache，文件名是conditonal\r\n\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: {\r\n        &quot;match&quot;: {\r\n          &quot;line&quot;: &quot;{{text}}&quot; \r\n        }\r\n      },\r\n      &quot;filter&quot;: {\r\n        {{#line_no}} \r\n          &quot;range&quot;: {\r\n            &quot;line_no&quot;: {\r\n              {{#start}} \r\n                &quot;gte&quot;: &quot;{{start}}&quot; \r\n                {{#end}},{{/end}} \r\n              {{/start}} \r\n              {{#end}} \r\n                &quot;lte&quot;: &quot;{{end}}&quot; \r\n              {{/end}} \r\n            }\r\n          }\r\n        {{/line_no}} \r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /my_index/my_type/_search \r\n\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;my_index&quot;,\r\n        &quot;_type&quot;: &quot;my_type&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;line&quot;: &quot;我的博客&quot;,\r\n          &quot;line_no&quot;: 5\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /my_index/my_type/_search/template\r\n{\r\n  &quot;file&quot;: &quot;conditional&quot;,\r\n  &quot;params&quot;: {\r\n    &quot;text&quot;: &quot;博客&quot;,\r\n    &quot;line_no&quot;: true,\r\n    &quot;start&quot;: 1,\r\n    &quot;end&quot;: 10\r\n  }\r\n}\r\n\r\n6、保存search template\r\n\r\nconfig/scripts，.mustache \r\n\r\n提供一个思路\r\n\r\n比如说，一般在大型的团队中，可能不同的人，都会想要执行一些类似的搜索操作\r\n这个时候，有一些负责底层运维的一些同学，就可以基于search template，封装一些模板出来，然后是放在各个es进程的scripts目录下的\r\n其他的团队，其实就不用各个团队自己反复手写复杂的通用的查询语句了，直接调用某个搜索模板，传入一些参数就好了\r\n\r\n',8,0,0,1514620781,0,0,0),(167,1,'completion suggest','','','suggest，completion suggest，自动完成，搜索推荐，搜索提示 --&gt; 自动完成，auto completion\r\n\r\nauto completion\r\n\r\n比如说我们在百度，搜索，你现在搜索“大话西游” --&gt; \r\n百度，自动给你提示，“大话西游电影”，“大话西游小说”， “大话西游手游”\r\n\r\n不用你把所有你想要输入的文本都输入完，搜索引擎会自动提示你可能是你想要搜索的那个文本\r\n\r\nPUT /news_website\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;news&quot; : {\r\n      &quot;properties&quot; : {\r\n        &quot;title&quot; : {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n          &quot;fields&quot;: {\r\n            &quot;suggest&quot; : {\r\n              &quot;type&quot; : &quot;completion&quot;,\r\n              &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n            }\r\n          }\r\n        },\r\n        &quot;content&quot;: {\r\n          &quot;type&quot;: &quot;text&quot;,\r\n          &quot;analyzer&quot;: &quot;ik_max_word&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\ncompletion，es实现的时候，是非常高性能的，会构建不是倒排索引，也不是正拍索引，就是纯的用于进行前缀搜索的一种特殊的数据结构，而且会全部放在内存中，所以auto completion进行的前缀搜索提示，性能是非常高的\r\n\r\n大话西游\r\n\r\nPUT /news_website/news/1\r\n{\r\n  &quot;title&quot;: &quot;大话西游电影&quot;,\r\n  &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot;\r\n}\r\nPUT /news_website/news/2\r\n{\r\n  &quot;title&quot;: &quot;大话西游小说&quot;,\r\n  &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;\r\n}\r\nPUT /news_website/news/3\r\n{\r\n  &quot;title&quot;: &quot;大话西游手游&quot;,\r\n  &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot;\r\n}\r\n\r\nGET /news_website/news/_search\r\n{\r\n  &quot;suggest&quot;: {\r\n    &quot;my-suggest&quot; : {\r\n      &quot;prefix&quot; : &quot;大话西游&quot;,\r\n      &quot;completion&quot; : {\r\n        &quot;field&quot; : &quot;title.suggest&quot;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 6,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 0,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;suggest&quot;: {\r\n    &quot;my-suggest&quot;: [\r\n      {\r\n        &quot;text&quot;: &quot;大话西游&quot;,\r\n        &quot;offset&quot;: 0,\r\n        &quot;length&quot;: 4,\r\n        &quot;options&quot;: [\r\n          {\r\n            &quot;text&quot;: &quot;大话西游小说&quot;,\r\n            &quot;_index&quot;: &quot;news_website&quot;,\r\n            &quot;_type&quot;: &quot;news&quot;,\r\n            &quot;_id&quot;: &quot;2&quot;,\r\n            &quot;_score&quot;: 1,\r\n            &quot;_source&quot;: {\r\n              &quot;title&quot;: &quot;大话西游小说&quot;,\r\n              &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;\r\n            }\r\n          },\r\n          {\r\n            &quot;text&quot;: &quot;大话西游手游&quot;,\r\n            &quot;_index&quot;: &quot;news_website&quot;,\r\n            &quot;_type&quot;: &quot;news&quot;,\r\n            &quot;_id&quot;: &quot;3&quot;,\r\n            &quot;_score&quot;: 1,\r\n            &quot;_source&quot;: {\r\n              &quot;title&quot;: &quot;大话西游手游&quot;,\r\n              &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot;\r\n            }\r\n          },\r\n          {\r\n            &quot;text&quot;: &quot;大话西游电影&quot;,\r\n            &quot;_index&quot;: &quot;news_website&quot;,\r\n            &quot;_type&quot;: &quot;news&quot;,\r\n            &quot;_id&quot;: &quot;1&quot;,\r\n            &quot;_score&quot;: 1,\r\n            &quot;_source&quot;: {\r\n              &quot;title&quot;: &quot;大话西游电影&quot;,\r\n              &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot;\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\nGET /news_website/news/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;content&quot;: &quot;大话西游电影&quot;\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 9,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1.3495269,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;news_website&quot;,\r\n        &quot;_type&quot;: &quot;news&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1.3495269,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;大话西游电影&quot;,\r\n          &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;news_website&quot;,\r\n        &quot;_type&quot;: &quot;news&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1.217097,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;大话西游手游&quot;,\r\n          &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;news_website&quot;,\r\n        &quot;_type&quot;: &quot;news&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1.1299736,\r\n        &quot;_source&quot;: {\r\n          &quot;title&quot;: &quot;大话西游小说&quot;,\r\n          &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514620827,0,0,0),(168,1,'高级的用法','','','高级的用法\r\n\r\n比如说，我们本来没有某个type，或者没有某个field，但是希望在插入数据的时候，es自动为我们做一个识别，动态映射出这个type的mapping，包括每个field的数据类型，一般用的动态映射，dynamic mapping\r\n\r\n这里有个问题，如果说，我们其实对dynamic mapping有一些自己独特的需求，比如说，es默认来说，如经过识别到一个数字，field: 10，默认是搞成这个field的数据类型是long，再比如说，如果我们弄了一个field : &quot;10&quot;，默认就是text，还会带一个keyword的内置field。我们没法改变。\r\n\r\n但是我们现在就是希望动态映射的时候，根据我们的需求去映射，而不是让es自己按照默认的规则去玩儿\r\n\r\ndyanmic mapping template，动态映射模板\r\n\r\n我们自己预先定义一个模板，然后插入数据的时候，相关的field，如果能够根据我们预先定义的规则，匹配上某个我们预定义的模板，那么就会根据我们的模板来进行mapping，决定这个Field的数据类型\r\n\r\n0、默认的动态映射的效果咋样\r\n\r\nDELETE /my_index\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;test_string&quot;: &quot;hello world&quot;,\r\n  &quot;test_number&quot;: 10\r\n}\r\n\r\nes的自动的默认的，动态映射是咋样的。。。\r\n\r\nGET /my_index/_mapping/my_type\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;properties&quot;: {\r\n          &quot;test_number&quot;: {\r\n            &quot;type&quot;: &quot;long&quot;\r\n          },\r\n          &quot;test_string&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n这个就是es的默认的动态映射规则，可能就不是我们想要的。。。\r\n\r\n我们比如说，现在想要的效果是啥。。。\r\n\r\ntest_number，如果是个数字，我们希望默认就是integer类型的\r\ntest_string，如果是字符串，我们希望默认是个text，这个没问题，但是内置的field名字，叫做raw，不叫座keyword，类型还是keyword，保留500个字符\r\n\r\n1、根据类型匹配映射模板\r\n\r\n动态映射模板，有两种方式，第一种，是根据新加入的field的默认的数据类型，来进行匹配，匹配上某个预定义的模板；第二种，是根据新加入的field的名字，去匹配预定义的名字，或者去匹配一个预定义的通配符，然后匹配上某个预定义的模板\r\n\r\nPUT my_index\r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;dynamic_templates&quot;: [\r\n        {\r\n          &quot;integers&quot;: {\r\n            &quot;match_mapping_type&quot;: &quot;long&quot;,\r\n            &quot;mapping&quot;: {\r\n              &quot;type&quot;: &quot;integer&quot;\r\n            }\r\n          }\r\n        },\r\n        {\r\n          &quot;strings&quot;: {\r\n            &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n            &quot;mapping&quot;: {\r\n              &quot;type&quot;: &quot;text&quot;,\r\n              &quot;fields&quot;: {\r\n                &quot;raw&quot;: {\r\n                  &quot;type&quot;: &quot;keyword&quot;,\r\n                  &quot;ignore_above&quot;: 500\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nPUT /my_index/my_type/1\r\n{\r\n  &quot;test_long&quot;: 1,\r\n  &quot;test_string&quot;: &quot;hello world&quot;\r\n}\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;dynamic_templates&quot;: [\r\n          {\r\n            &quot;integers&quot;: {\r\n              &quot;match_mapping_type&quot;: &quot;long&quot;,\r\n              &quot;mapping&quot;: {\r\n                &quot;type&quot;: &quot;integer&quot;\r\n              }\r\n            }\r\n          },\r\n          {\r\n            &quot;strings&quot;: {\r\n              &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n              &quot;mapping&quot;: {\r\n                &quot;fields&quot;: {\r\n                  &quot;raw&quot;: {\r\n                    &quot;ignore_above&quot;: 500,\r\n                    &quot;type&quot;: &quot;keyword&quot;\r\n                  }\r\n                },\r\n                &quot;type&quot;: &quot;text&quot;\r\n              }\r\n            }\r\n          }\r\n        ],\r\n        &quot;properties&quot;: {\r\n          &quot;test_number&quot;: {\r\n            &quot;type&quot;: &quot;integer&quot;\r\n          },\r\n          &quot;test_string&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;raw&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 500\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n\r\n2、根据字段名配映射模板\r\n\r\nPUT /my_index \r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;dynamic_templates&quot;: [\r\n        {\r\n          &quot;string_as_integer&quot;: {\r\n            &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n            &quot;match&quot;: &quot;long_*&quot;,\r\n            &quot;unmatch&quot;: &quot;*_text&quot;,\r\n            &quot;mapping&quot;: {\r\n              &quot;type&quot;: &quot;integer&quot;\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n举个例子，field : &quot;10&quot;，把类似这种field，弄成long型\r\n\r\n{\r\n  &quot;my_index&quot;: {\r\n    &quot;mappings&quot;: {\r\n      &quot;my_type&quot;: {\r\n        &quot;dynamic_templates&quot;: [\r\n          {\r\n            &quot;string_as_integer&quot;: {\r\n              &quot;match&quot;: &quot;long_*&quot;,\r\n              &quot;unmatch&quot;: &quot;*_text&quot;,\r\n              &quot;match_mapping_type&quot;: &quot;string&quot;,\r\n              &quot;mapping&quot;: {\r\n                &quot;type&quot;: &quot;integer&quot;\r\n              }\r\n            }\r\n          }\r\n        ],\r\n        &quot;properties&quot;: {\r\n          &quot;long_field&quot;: {\r\n            &quot;type&quot;: &quot;integer&quot;\r\n          },\r\n          &quot;long_field_text&quot;: {\r\n            &quot;type&quot;: &quot;text&quot;,\r\n            &quot;fields&quot;: {\r\n              &quot;keyword&quot;: {\r\n                &quot;type&quot;: &quot;keyword&quot;,\r\n                &quot;ignore_above&quot;: 256\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n场景，有些时候，dynamic mapping + template，每天有一堆日志，每天有一堆数据\r\n\r\n这些数据，每天的数据都放一个新的type中，每天的数据都会哗哗的往新的tye中写入，此时你就可以定义一个模板，搞一个脚本，每天都预先生成一个新type的模板，里面讲你的各个Field都匹配到一个你预定义的模板中去，就好了\r\n',8,0,0,1514620874,0,0,0),(169,1,'基于地理位置的搜索，和聚合分析','','','这一讲开始，后面会跟着几讲内容，将地理位置相关的知识给大家讲解一下\r\n\r\n主要是es支持基于地理位置的搜索，和聚合分析的\r\n\r\n举个例子，比如说，我们后面就会给大家演示一下，你现在如果说做了一个酒店o2o app，让你的用户在任何地方，都可以根据当前所在的位置，找到自己身边的符合条件的一些酒店，那么此时就完全可以使用es来实现，非常合适\r\n\r\n我现在在上海某个大厦附近，我要搜索到距离我2公里以内的5星级的带游泳池的一个酒店s，用es就完全可以实现类似这样的基于地理位置的搜索引擎\r\n\r\n1、建立geo_point类型的mapping\r\n\r\n第一个地理位置的数据类型，就是geo_point，geo_point，说白了，就是一个地理位置坐标点，包含了一个经度，一个维度，经纬度，就可以唯一定位一个地球上的坐标\r\n\r\nPUT /my_index \r\n{\r\n  &quot;mappings&quot;: {\r\n    &quot;my_type&quot;: {\r\n      &quot;properties&quot;: {\r\n        &quot;location&quot;: {\r\n          &quot;type&quot;: &quot;geo_point&quot;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2、写入geo_point的3种方法\r\n\r\nPUT my_index/my_type/1\r\n{\r\n  &quot;text&quot;: &quot;Geo-point as an object&quot;,\r\n  &quot;location&quot;: { \r\n    &quot;lat&quot;: 41.12,\r\n    &quot;lon&quot;: -71.34\r\n  }\r\n}\r\n\r\nlatitude：维度\r\nlongitude：经度\r\n\r\n我们这里就不用去关心，这些坐标到底代表什么地方，其实都是我自己随便写的，只要能够作为课程，给大家演示清楚就可以了，自己去找一些提供地理位置的一些公司，供应商，api，百度地图，也是提供各个地方的经纬度的\r\n\r\n不建议用下面两种语法\r\n\r\nPUT my_index/my_type/2\r\n{\r\n  &quot;text&quot;: &quot;Geo-point as a string&quot;,\r\n  &quot;location&quot;: &quot;41.12,-71.34&quot; \r\n}\r\n\r\nPUT my_index/my_type/4\r\n{\r\n  &quot;text&quot;: &quot;Geo-point as an array&quot;,\r\n  &quot;location&quot;: [ -71.34, 41.12 ] \r\n}\r\n\r\n3、根据地理位置进行查询\r\n\r\n最最简单的，根据地理位置查询一些点，比如说，下面geo_bounding_box查询，查询某个矩形的地理位置范围内的坐标点\r\n\r\nGET /my_index/my_type/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;geo_bounding_box&quot;: {\r\n      &quot;location&quot;: {\r\n        &quot;top_left&quot;: {\r\n          &quot;lat&quot;: 42,\r\n          &quot;lon&quot;: -72\r\n        },\r\n        &quot;bottom_right&quot;: {\r\n          &quot;lat&quot;: 40,\r\n          &quot;lon&quot;: -74\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 81,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;my_index&quot;,\r\n        &quot;_type&quot;: &quot;my_type&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;location&quot;: {\r\n            &quot;lat&quot;: 41.12,\r\n            &quot;lon&quot;: -71.34\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n比如41.12,-71.34就是一个酒店，然后我们现在搜索的是从42,-72（代表了大厦A）和40,-74（代表了马路B）作为矩形的范围，在这个范围内的酒店，是什么\r\n',8,0,0,1514620910,0,0,0),(170,1,'搜索指定区域范围内的酒店','','','稍微真实点的案例，酒店o2o app作为一个背景，用各种各样的方式，去搜索你当前所在的地理位置附近的酒店\r\n\r\n搜索指定区域范围内的酒店，比如说，我们可以在搜索的时候，指定两个地点，就要在东方明珠大厦和上海路组成的矩阵的范围内，搜索我想要的酒店\r\n\r\nPUT /hotel_app\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;hotels&quot;: {\r\n            &quot;properties&quot;: {\r\n                &quot;pin&quot;: {\r\n                    &quot;properties&quot;: {\r\n                        &quot;location&quot;: {\r\n                            &quot;type&quot;: &quot;geo_point&quot;\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nPUT /hotel_app/hotels/1\r\n{\r\n    &quot;name&quot;: &quot;喜来登大酒店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}\r\n\r\nGET /hotel_app/hotels/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match_all&quot;: {}\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;geo_bounding_box&quot;: {\r\n          &quot;pin.location&quot;: {\r\n            &quot;top_left&quot; : {\r\n                &quot;lat&quot; : 40.73,\r\n                &quot;lon&quot; : -74.1\r\n            },\r\n            &quot;bottom_right&quot; : {\r\n                &quot;lat&quot; : 40.01,\r\n                &quot;lon&quot; : -71.12\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /hotel_app/hotels/_search \r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match_all&quot;: {}\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;geo_polygon&quot;: {\r\n          &quot;pin.location&quot;: {\r\n            &quot;points&quot;: [\r\n              {&quot;lat&quot; : 40.73, &quot;lon&quot; : -74.1},\r\n              {&quot;lat&quot; : 40.01, &quot;lon&quot; : -71.12},\r\n              {&quot;lat&quot; : 50.56, &quot;lon&quot; : -90.58}\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n我们现在要指定东方明珠大厦，上海路，上海博物馆，这三个地区组成的多边形的范围内，我要搜索这里面的酒店\r\n',8,0,0,1514620963,0,0,0),(171,1,'酒店o2o app','','','酒店o2o app，作为案例背景\r\n\r\n比如说，现在用户，所在的位置，是个地理位置的坐标，我是知道我的坐标的，app是知道的，android，地理位置api，都可以拿到当前手机app的经纬度\r\n\r\n我说，我现在就要搜索出，举例我200m，或者1公里内的酒店\r\n\r\n重要！！！！\r\n\r\n我们之前出去玩儿，都会用一些酒店o2o app，典型的代表，很多旅游app，一般来说，我们怎么搜索，到了一个地方，就会搜索说，我这里举例几百米，2公里内的酒店，搜索一下\r\n\r\n上节课讲解的，其实也很重要，一般来说，发生在我们在去旅游之前，会现在旅游app上搜索一个区域内的酒店，比如说，指定了西安火车站、西安博物馆，拿指定的几个地方的地理位置，组成一个多边形区域范围，去搜索这个区域内的酒店\r\n\r\n承认，一些案例，当然不可能说达到讲解真实的复杂的大型的项目的效果来的那么好，光是学知识，学技术而言，有一些案例就非常不错了\r\n\r\n后面，会讲解真正的企业级的大型的搜索引擎，真实复杂业务的数据分析系统的项目\r\n\r\nGET /hotel_app/hotels/_search\r\n{\r\n  &quot;query&quot;: {\r\n    &quot;bool&quot;: {\r\n      &quot;must&quot;: [\r\n        {\r\n          &quot;match_all&quot;: {}\r\n        }\r\n      ],\r\n      &quot;filter&quot;: {\r\n        &quot;geo_distance&quot;: {\r\n          &quot;distance&quot;: &quot;200km&quot;,\r\n          &quot;pin.location&quot;: {\r\n            &quot;lat&quot;: 40,\r\n            &quot;lon&quot;: -70\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}',8,0,0,1514621614,0,0,0),(172,1,'基于地理位置进行聚合','','','最后一个知识点，基于地理位置进行聚合分析\r\n\r\n我的需求就是，统计一下，举例我当前坐标的几个范围内的酒店的数量，比如说举例我0~100m有几个酒店，100m~300m有几个酒店，300m以上有几个酒店\r\n\r\n一般来说，做酒店app，一般来说，我们是不是会有一个地图，用户可以在地图上直接查看和搜索酒店，此时就可以显示出来举例你当前的位置，几个举例范围内，有多少家酒店，让用户知道，心里清楚，用户体验就比较好\r\n\r\nGET /hotel_app/hotels/_search\r\n{\r\n  &quot;size&quot;: 0,\r\n  &quot;aggs&quot;: {\r\n    &quot;agg_by_distance_range&quot;: {\r\n      &quot;geo_distance&quot;: {\r\n        &quot;field&quot;: &quot;pin.location&quot;,\r\n        &quot;origin&quot;: {\r\n          &quot;lat&quot;: 40,\r\n          &quot;lon&quot;: -70\r\n        },\r\n        &quot;unit&quot;: &quot;mi&quot;, \r\n        &quot;ranges&quot;: [\r\n          {\r\n            &quot;to&quot;: 100\r\n          },\r\n          {\r\n            &quot;from&quot;: 100,\r\n            &quot;to&quot;: 300\r\n          },\r\n          {\r\n            &quot;from&quot;: 300\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  &quot;took&quot;: 5,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 0,\r\n    &quot;hits&quot;: []\r\n  },\r\n  &quot;aggregations&quot;: {\r\n    &quot;agg_by_distance_range&quot;: {\r\n      &quot;buckets&quot;: [\r\n        {\r\n          &quot;key&quot;: &quot;*-100.0&quot;,\r\n          &quot;from&quot;: 0,\r\n          &quot;to&quot;: 100,\r\n          &quot;doc_count&quot;: 1\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;100.0-300.0&quot;,\r\n          &quot;from&quot;: 100,\r\n          &quot;to&quot;: 300,\r\n          &quot;doc_count&quot;: 0\r\n        },\r\n        {\r\n          &quot;key&quot;: &quot;300.0-*&quot;,\r\n          &quot;from&quot;: 300,\r\n          &quot;doc_count&quot;: 0\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nm (metres) but it can also accept: m (miles), km (kilometers)\r\n\r\nsloppy_arc (the default), arc (most accurate) and plane (fastest)\r\n',8,0,0,1514621664,0,0,0),(173,1,'基本的java api','','','快速入门篇，讲解过了一些基本的java api，包括了document增删改查，基本的搜索，基本的聚合\r\n\r\n高手进阶篇，必须将java api这块深入讲解一下，介绍一些最常用的，最核心的一些api的使用，用一个模拟现实的案例背景，让大家在学习的时候更加贴近业务\r\n\r\n话说在前面，我们是不可能将所有的java api用视频全部录制一遍的，因为api太多了。。。。\r\n\r\n我们之前讲解各种功能，各种知识点，花了那么多的时间，哪儿些才是最最关键的，知识，原理，功能，es restful api，最次最次，哪怕是搞php，搞python的人也可以来学习\r\n\r\n如果说，现在要将所有所有的api全部用java api实现一遍和讲解，太耗费时间了，几乎不可能接受\r\n\r\n采取的粗略，将核心的java api语法，还有最最常用的那些api都给大家上课演示了\r\n\r\n然后最后一讲，会告诉大家，在掌握了之前那些课程讲解的各种知识点之后，如果要用java api去实现和开发，应该怎么自己去探索和掌握\r\n\r\njava api，api的学习，实际上是最最简单的，纯用，没什么难度，技术难度，你掌握了课上讲解的这些api之后，自己应该就可以举一反三，后面自己去探索和尝试出自己要用的各种功能对应的java api是什么。。。\r\n\r\n1、client集群自动探查\r\n\r\n默认情况下，是根据我们手动指定的所有节点，依次轮询这些节点，来发送各种请求的，如下面的代码，我们可以手动为client指定多个节点\r\n\r\nTransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost1&quot;), 9300))\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost2&quot;), 9300))\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost3&quot;), 9300));\r\n\r\n但是问题是，如果我们有成百上千个节点呢？难道也要这样手动添加吗？\r\n\r\nes client提供了一种集群节点自动探查的功能，打开这个自动探查机制以后，es client会根据我们手动指定的几个节点连接过去，然后通过集群状态自动获取当前集群中的所有data node，然后用这份完整的列表更新自己内部要发送请求的node list。默认每隔5秒钟，就会更新一次node list。\r\n\r\n但是注意，es cilent是不会将Master node纳入node list的，因为要避免给master node发送搜索等请求。\r\n\r\n这样的话，我们其实直接就指定几个master node，或者1个node就好了，client会自动去探查集群的所有节点，而且每隔5秒还会自动刷新。非常棒。\r\n\r\nSettings settings = Settings.builder()\r\n        .put(&quot;client.transport.sniff&quot;, true).build();\r\nTransportClient client = new PreBuiltTransportClient(settings);\r\n\r\n使用上述的settings配置，将client.transport.sniff设置为true即可打开集群节点自动探查功能\r\n\r\n在实际的生产环境中，都是这么玩儿的。。。\r\n\r\n2、汽车零售案例背景\r\n\r\n简单来说，会涉及到三个数据，汽车信息，汽车销售记录，汽车4S店信息\r\n',8,0,0,1514621706,0,0,0),(174,1,'汽车零售数据的mapping','','','做一个汽车零售数据的mapping，我们要做的第一份数据，其实汽车信息\r\n\r\nPUT /car_shop\r\n{\r\n    &quot;mappings&quot;: {\r\n        &quot;cars&quot;: {\r\n            &quot;properties&quot;: {\r\n                &quot;brand&quot;: {\r\n                    &quot;type&quot;: &quot;text&quot;,\r\n                    &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n                    &quot;fields&quot;: {\r\n                        &quot;raw&quot;: {\r\n                            &quot;type&quot;: &quot;keyword&quot;\r\n                        }\r\n                    }\r\n                },\r\n                &quot;name&quot;: {\r\n                    &quot;type&quot;: &quot;text&quot;,\r\n                    &quot;analyzer&quot;: &quot;ik_max_word&quot;,\r\n                    &quot;fields&quot;: {\r\n                        &quot;raw&quot;: {\r\n                            &quot;type&quot;: &quot;keyword&quot;\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n返回值 (注意 No living connections 时需要重启kibana)\r\n{\r\n  &quot;acknowledged&quot;: true,\r\n  &quot;shards_acknowledged&quot;: true\r\n}\r\n\r\n首先的话呢，第一次调整宝马320这个汽车的售价，我们希望将售价设置为32万，用一个upsert语法，如果这个汽车的信息之前不存在，那么就insert，如果存在，那么就update\r\n\r\nIndexRequest indexRequest = new IndexRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n        .source(jsonBuilder()\r\n            .startObject()\r\n                .field(&quot;brand&quot;, &quot;宝马&quot;)\r\n                .field(&quot;name&quot;, &quot;宝马320&quot;)\r\n                .field(&quot;price&quot;, 320000)\r\n                .field(&quot;produce_date&quot;, &quot;2017-01-01&quot;)\r\n            .endObject());\r\n\r\nUpdateRequest updateRequest = new UpdateRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n        .doc(jsonBuilder()\r\n            .startObject()\r\n                .field(&quot;price&quot;, 320000)\r\n            .endObject())\r\n        .upsert(indexRequest);       \r\n               \r\nclient.update(updateRequest).get();\r\n\r\nIndexRequest indexRequest = new IndexRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n        .source(jsonBuilder()\r\n            .startObject()\r\n                .field(&quot;brand&quot;, &quot;宝马&quot;)\r\n                .field(&quot;name&quot;, &quot;宝马320&quot;)\r\n                .field(&quot;price&quot;, 310000)\r\n                .field(&quot;produce_date&quot;, &quot;2017-01-01&quot;)\r\n            .endObject());\r\nUpdateRequest updateRequest = new UpdateRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n        .doc(jsonBuilder()\r\n            .startObject()\r\n                .field(&quot;price&quot;, 310000)\r\n            .endObject())\r\n        .upsert(indexRequest);              \r\nclient.update(updateRequest).get();\r\n\r\nNew --- Maven Project --- Next --- Group Id:comm.peng.es Artifact Id:es-senior Package:com.peng.es.senior\r\n删除无关文件 \r\n修改pom.xml\r\n\r\n编辑代码 UpsertCarInfoApp.java\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.index.IndexRequest;\r\nimport org.elasticsearch.action.update.UpdateRequest;\r\nimport org.elasticsearch.action.update.UpdateResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.common.xcontent.XContentFactory;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class UpsertCarInfoApp {\r\n	\r\n	@SuppressWarnings({ &quot;unchecked&quot;, &quot;resource&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.put(&quot;client.transport.sniff&quot;, true)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300));\r\n	\r\n		IndexRequest indexRequest = new IndexRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n				.source(XContentFactory.jsonBuilder()\r\n							.startObject()\r\n								.field(&quot;brand&quot;, &quot;宝马&quot;)\r\n								.field(&quot;name&quot;, &quot;宝马320&quot;)\r\n								.field(&quot;price&quot;, 310000)\r\n								.field(&quot;produce_date&quot;, &quot;2017-01-01&quot;)\r\n							.endObject());\r\n		\r\n		UpdateRequest updateRequest = new UpdateRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n				.doc(XContentFactory.jsonBuilder()\r\n						.startObject()\r\n							.field(&quot;price&quot;, 300000)\r\n						.endObject())\r\n				.upsert(indexRequest); \r\n		\r\n		UpdateResponse updateResponse = client.update(updateRequest).get();\r\n	\r\n		System.out.println(updateResponse.getVersion()); \r\n		//System.out.println(updateResponse.getResult().getOp());\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n运行代码 Run As\r\n\r\n查询 GET /car_shop/cars/_search\r\n{\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;cars&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 310000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n运行java代码 返回版本号:2\r\n查询 GET /car_shop/cars/_search\r\n{\r\n  &quot;took&quot;: 1,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 1,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;cars&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 300000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514621746,0,0,0),(175,1,'混合销售','','','场景，一般来说，我们都可以在一些汽车网站上，或者在混合销售多个品牌的汽车4S店的内部，都可以在系统里调出来多个汽车的信息，放在网页上，进行对比\r\n\r\nmget，一次性将多个document的数据查询出来，放在一起显示，多个汽车的型号，一次性拿出了多辆汽车的信息\r\n\r\n手动添加\r\nPUT /car_shop/cars/2\r\n{\r\n	&quot;brand&quot;: &quot;奔驰&quot;,\r\n	&quot;name&quot;: &quot;奔驰C200&quot;,\r\n	&quot;price&quot;: 350000,\r\n	&quot;produce_date&quot;: &quot;2017-01-05&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;cars&quot;,\r\n  &quot;_id&quot;: &quot;2&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nMultiGetResponse multiGetItemResponses = client.prepareMultiGet()\r\n    .add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)           \r\n    .add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;2&quot;)        \r\n    .get();\r\n\r\nfor (MultiGetItemResponse itemResponse : multiGetItemResponses) { \r\n    GetResponse response = itemResponse.getResponse();\r\n    if (response.isExists()) {                      \r\n        String json = response.getSourceAsString(); \r\n    }\r\n}\r\n\r\n编写代码\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.get.GetResponse;\r\nimport org.elasticsearch.action.get.MultiGetItemResponse;\r\nimport org.elasticsearch.action.get.MultiGetResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class MGetMultiCarInfoApp {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300)); \r\n	\r\n		MultiGetResponse multiGetResponse = client.prepareMultiGet()\r\n				.add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;)\r\n				.add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;2&quot;) \r\n				.get();\r\n		\r\n		for(MultiGetItemResponse multiGetItemResponse : multiGetResponse) {\r\n			GetResponse getResponse = multiGetItemResponse.getResponse();\r\n			if(getResponse.isExists()) {\r\n				System.out.println(getResponse.getSourceAsString());  \r\n			}\r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n运行\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}\r\n{\r\n	&quot;brand&quot;: &quot;奔驰&quot;,\r\n	&quot;name&quot;: &quot;奔驰C200&quot;,\r\n	&quot;price&quot;: 350000,\r\n	&quot;produce_date&quot;: &quot;2017-01-05&quot;\r\n}',8,0,0,1514621794,0,0,0),(176,1,'批量上传到es中去','','','业务场景：有一个汽车销售公司，拥有很多家4S店，这些4S店的数据，都会在一段时间内陆续传递过来，汽车的销售数据，现在希望能够在内存中缓存比如1000条销售数据，然后一次性批量上传到es中去\r\n\r\nPUT /car_shop/sales/1\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 300000,\r\n    &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;sales&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nPUT /car_shop/sales/2\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 300000,\r\n    &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;sales&quot;,\r\n  &quot;_id&quot;: &quot;2&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nBulkRequestBuilder bulkRequest = client.prepareBulk();\r\n\r\nbulkRequest.add(client.prepareIndex(&quot;car_shop&quot;, &quot;sales&quot;, &quot;3&quot;)\r\n        .setSource(jsonBuilder()\r\n                    .startObject()\r\n                        .field(&quot;brand&quot;, &quot;奔驰&quot;)\r\n                        .field(&quot;name&quot;, &quot;奔驰C200&quot;)\r\n                        .field(&quot;price&quot;, 350000)\r\n                        .field(&quot;produce_date&quot;, &quot;2017-01-05&quot;)\r\n                        .field(&quot;sale_price&quot;, 340000)\r\n                        .field(&quot;sale_date&quot;, &quot;2017-02-03&quot;)\r\n                    .endObject()\r\n                  )\r\n        );\r\n\r\nbulkRequest.add(client.prepareUpdate(&quot;car_shop&quot;, &quot;sales&quot;, &quot;1&quot;)\r\n        .setDoc(jsonBuilder()               \r\n		            .startObject()\r\n		                .field(&quot;sale_price&quot;, &quot;290000&quot;)\r\n		            .endObject()\r\n		        )\r\n        );\r\n\r\nbulkRequest.add(client.prepareDelete(&quot;car_shop&quot;, &quot;sales&quot;, &quot;2&quot;));\r\n\r\nBulkResponse bulkResponse = bulkRequest.get();\r\n\r\nif (bulkResponse.hasFailures()) {}\r\n\r\nGET /car_shop/sales/_search\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;2&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 300000,\r\n          &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 300000,\r\n          &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n编写代码\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.bulk.BulkItemResponse;\r\nimport org.elasticsearch.action.bulk.BulkRequestBuilder;\r\nimport org.elasticsearch.action.bulk.BulkResponse;\r\nimport org.elasticsearch.action.delete.DeleteRequestBuilder;\r\nimport org.elasticsearch.action.index.IndexRequestBuilder;\r\nimport org.elasticsearch.action.update.UpdateRequestBuilder;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.common.xcontent.XContentFactory;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class BulkUploadSalesDataApp {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300)); \r\n	\r\n		BulkRequestBuilder bulkRequestBuilder = client.prepareBulk();\r\n		\r\n		IndexRequestBuilder indexRequestBuilder = client.prepareIndex(&quot;car_shop&quot;, &quot;sales&quot;, &quot;3&quot;) \r\n				.setSource(XContentFactory.jsonBuilder()\r\n							.startObject()\r\n								.field(&quot;brand&quot;, &quot;奔驰&quot;)\r\n								.field(&quot;name&quot;, &quot;奔驰C200&quot;)\r\n								.field(&quot;price&quot;, 350000)\r\n								.field(&quot;produce_date&quot;, &quot;2017-01-20&quot;)\r\n								.field(&quot;sale_price&quot;, 320000)\r\n								.field(&quot;sale_date&quot;, &quot;2017-01-25&quot;)\r\n							.endObject());\r\n		bulkRequestBuilder.add(indexRequestBuilder);\r\n		\r\n		UpdateRequestBuilder updateRequestBuilder = client.prepareUpdate(&quot;car_shop&quot;, &quot;sales&quot;, &quot;1&quot;)\r\n				.setDoc(XContentFactory.jsonBuilder()\r\n						.startObject()\r\n							.field(&quot;sale_price&quot;, 290000)\r\n						.endObject());\r\n		bulkRequestBuilder.add(updateRequestBuilder);\r\n		\r\n		DeleteRequestBuilder deleteReqeustBuilder = client.prepareDelete(&quot;car_shop&quot;, &quot;sales&quot;, &quot;2&quot;); \r\n		bulkRequestBuilder.add(deleteReqeustBuilder);\r\n		\r\n		BulkResponse bulkResponse = bulkRequestBuilder.get();\r\n		\r\n		for(BulkItemResponse bulkItemResponse : bulkResponse.getItems()) {\r\n			System.out.println(&quot;version: &quot; + bulkItemResponse.getVersion()); \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n运行\r\nversion: 1\r\nversion: 2\r\nversion: 2\r\n\r\n运行后查询 GET /car_shop/sales/_search\r\n{\r\n  &quot;took&quot;: 2,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 290000,\r\n          &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;奔驰&quot;,\r\n          &quot;name&quot;: &quot;奔驰C200&quot;,\r\n          &quot;price&quot;: 350000,\r\n          &quot;produce_date&quot;: &quot;2017-01-20&quot;,\r\n          &quot;sale_price&quot;: 320000,\r\n          &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}',8,0,0,1514621918,0,0,0),(177,1,'销售记录','','','比如说，现在要下载大批量的数据，从es，放到excel中，我们说，月度，或者年度，销售记录，很多，比如几千条，几万条，几十万条\r\n\r\n其实就要用到我们之前讲解的es scroll api，对大量数据批量的获取和处理\r\n\r\nPUT /car_shop/sales/4\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 280000,\r\n    &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;sales&quot;,\r\n  &quot;_id&quot;: &quot;4&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nGET /car_shop/sales/_search\r\n{\r\n  &quot;took&quot;: 4,\r\n  &quot;timed_out&quot;: false,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 5,\r\n    &quot;successful&quot;: 5,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;hits&quot;: {\r\n    &quot;total&quot;: 3,\r\n    &quot;max_score&quot;: 1,\r\n    &quot;hits&quot;: [\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;4&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 280000,\r\n          &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;1&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;宝马&quot;,\r\n          &quot;name&quot;: &quot;宝马320&quot;,\r\n          &quot;price&quot;: 320000,\r\n          &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n          &quot;sale_price&quot;: 290000,\r\n          &quot;sale_date&quot;: &quot;2017-01-21&quot;\r\n        }\r\n      },\r\n      {\r\n        &quot;_index&quot;: &quot;car_shop&quot;,\r\n        &quot;_type&quot;: &quot;sales&quot;,\r\n        &quot;_id&quot;: &quot;3&quot;,\r\n        &quot;_score&quot;: 1,\r\n        &quot;_source&quot;: {\r\n          &quot;brand&quot;: &quot;奔驰&quot;,\r\n          &quot;name&quot;: &quot;奔驰C200&quot;,\r\n          &quot;price&quot;: 350000,\r\n          &quot;produce_date&quot;: &quot;2017-01-20&quot;,\r\n          &quot;sale_price&quot;: 320000,\r\n          &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n就是要看宝马的销售记录\r\n\r\n2条数据，做一个演示，每个批次下载一条宝马的销售记录，分2个批次给它下载完\r\n\r\nSearchResponse scrollResp = client.prepareSearch(&quot;car_shop&quot;)\r\n		.addTypes(&quot;sales&quot;)\r\n        .setScroll(new TimeValue(60000))\r\n        .setQuery(termQuery(&quot;brand.raw&quot;, &quot;宝马&quot;))\r\n        .setSize(1)\r\n        .get(); \r\n\r\ndo {\r\n    for (SearchHit hit : scrollResp.getHits().getHits()) {\r\n    	\r\n    }\r\n    \r\n    scrollResp = client.prepareSearchScroll(scrollResp.getScrollId())\r\n            .setScroll(new TimeValue(60000))\r\n            .execute()\r\n            .actionGet();\r\n} while(scrollResp.getHits().getHits().length != 0);\r\n\r\n编写代码\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.common.unit.TimeValue;\r\nimport org.elasticsearch.index.query.QueryBuilders;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class ScollDownloadSalesDataApp {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300)); \r\n	\r\n		SearchResponse searchResponse = client.prepareSearch(&quot;car_shop&quot;) \r\n				.setTypes(&quot;sales&quot;)\r\n				.setQuery(QueryBuilders.termQuery(&quot;brand.keyword&quot;, &quot;宝马&quot;))\r\n				.setScroll(new TimeValue(60000))\r\n				.setSize(1)\r\n				.get();\r\n		\r\n		int batchCount = 0;\r\n		\r\n		do {\r\n			for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n				System.out.println(&quot;batch: &quot; + ++batchCount); \r\n				System.out.println(searchHit.getSourceAsString());  \r\n				\r\n				// 每次查询一批数据，比如1000行，然后写入本地的一个excel文件中\r\n				\r\n				// 如果说你一下子查询几十万条数据，不现实，jvm内存可能都会爆掉\r\n			}\r\n			\r\n			searchResponse = client.prepareSearchScroll(searchResponse.getScrollId())\r\n					.setScroll(new TimeValue(60000))\r\n					.execute()\r\n					.actionGet();\r\n		} while(searchResponse.getHits().getHits().length != 0);\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\nbatch: 1\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 280000,\r\n    &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n}\r\n\r\nbatch: 2\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:320000,&quot;produce_date&quot;:&quot;2017-01-01&quot;,&quot;sale_price&quot;:290000,&quot;sale_date&quot;:&quot;2017-01-21&quot;}\r\n',8,0,0,1514621979,0,0,0),(178,1,'调用一个搜索模板','','','搜索模板的功能，java api怎么去调用一个搜索模板\r\n\r\npage_query_by_brand.mustache /es安装目录/config/script #注意编码(全使用UTF-8)\r\n\r\n{\r\n  &quot;from&quot;: {{from}},\r\n  &quot;size&quot;: {{size}},\r\n  &quot;query&quot;: {\r\n    &quot;match&quot;: {\r\n      &quot;brand.keyword&quot;: &quot;{{brand}}&quot; \r\n    }\r\n  }\r\n}\r\n\r\nSearchResponse sr = new SearchTemplateRequestBuilder(client)\r\n    .setScript(&quot;page_query_by_brand&quot;)                 \r\n    .setScriptType(ScriptService.ScriptType.FILE) \r\n    .setScriptParams(template_params)             \r\n    .setRequest(new SearchRequest())              \r\n    .get()                                        \r\n    .getResponse(); \r\n\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\nimport java.util.HashMap;\r\nimport java.util.Map;\r\n\r\nimport org.elasticsearch.action.search.SearchRequest;\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.script.ScriptType;\r\nimport org.elasticsearch.script.mustache.SearchTemplateRequestBuilder;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class SearchTemplatePageQuery {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300)); \r\n	\r\n		Map&lt;String, Object&gt; scriptParams = new HashMap&lt;String, Object&gt;();\r\n		scriptParams.put(&quot;from&quot;, 0);\r\n		scriptParams.put(&quot;size&quot;, 1);\r\n		scriptParams.put(&quot;brand&quot;, &quot;宝马&quot;);\r\n		\r\n		SearchResponse searchResponse = new SearchTemplateRequestBuilder(client)\r\n				.setScript(&quot;page_query_by_brand&quot;)\r\n				.setScriptType(ScriptType.FILE)\r\n				.setScriptParams(scriptParams)\r\n				.setRequest(new SearchRequest(&quot;car_shop&quot;).types(&quot;sales&quot;))\r\n				.get()\r\n				.getResponse();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n\r\n{\r\n    &quot;brand&quot;: &quot;宝马&quot;,\r\n    &quot;name&quot;: &quot;宝马320&quot;,\r\n    &quot;price&quot;: 320000,\r\n    &quot;produce_date&quot;: &quot;2017-01-01&quot;,\r\n    &quot;sale_price&quot;: 280000,\r\n    &quot;sale_date&quot;: &quot;2017-01-25&quot;\r\n}',8,0,0,1514622025,0,0,0),(179,1,'精简查询','','','PUT /car_shop/cars/5\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;cars&quot;,\r\n  &quot;_id&quot;: &quot;5&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;宝马&quot;))                \r\n        .get();\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(QueryBuilders.multiMatchQuery(&quot;宝马&quot;, &quot;brand&quot;, &quot;name&quot;))                \r\n        .get();\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(QueryBuilders.commonTermsQuery(&quot;name&quot;, &quot;宝马320&quot;))                \r\n        .get();\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(QueryBuilders.prefixQuery(&quot;name&quot;, &quot;宝&quot;))                \r\n        .get();\r\n\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.index.query.QueryBuilders;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class FullTextSearchByBrand {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300));  \r\n	\r\n		SearchResponse searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;宝马&quot;))\r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(QueryBuilders.multiMatchQuery(&quot;宝马&quot;, &quot;brand&quot;, &quot;name&quot;))  \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(QueryBuilders.termQuery(&quot;name.raw&quot;, &quot;宝马318&quot;))    \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(QueryBuilders.prefixQuery(&quot;name&quot;, &quot;宝&quot;))      \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}\r\n====================================================\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}\r\n====================================================\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n====================================================\r\n{\r\n        &quot;brand&quot;: &quot;华晨宝马&quot;,\r\n        &quot;name&quot;: &quot;宝马318&quot;,\r\n        &quot;price&quot;: 270000,\r\n        &quot;produce_date&quot;: &quot;2017-01-20&quot;\r\n}\r\n\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}',8,0,0,1514622136,0,0,0),(180,1,'多条件组合搜索','','','\r\nQueryBuilder qb = boolQuery()\r\n    .must(matchQuery(&quot;brand&quot;, &quot;宝马&quot;))    \r\n    .mustNot(termQuery(&quot;name.raw&quot;, &quot;宝马318&quot;)) \r\n    .should(termQuery(&quot;produce_date&quot;, &quot;2017-01-02&quot;))  \r\n    .filter(rangeQuery(&quot;price&quot;).gte(&quot;280000&quot;).lt(&quot;350000&quot;));\r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;cars&quot;)\r\n        .setQuery(qb)                \r\n        .get();\r\n\r\n代码\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\n\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.index.query.QueryBuilder;\r\nimport org.elasticsearch.index.query.QueryBuilders;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class BoolQuerySearchBrand {\r\n	\r\n	@SuppressWarnings({ &quot;resource&quot;, &quot;unchecked&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300));  \r\n	\r\n		QueryBuilder queryBuilder = QueryBuilders.boolQuery()\r\n				.must(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;宝马&quot;))\r\n				.mustNot(QueryBuilders.termQuery(&quot;name.raw&quot;, &quot;宝马318&quot;))\r\n				.should(QueryBuilders.rangeQuery(&quot;produce_date&quot;).gte(&quot;2017-01-01&quot;).lte(&quot;2017-01-31&quot;))\r\n				.filter(QueryBuilders.rangeQuery(&quot;price&quot;).gte(280000).lte(350000));    \r\n		\r\n		SearchResponse searchResponse = client.prepareSearch(&quot;car_shop&quot;)  \r\n				.setTypes(&quot;cars&quot;)\r\n				.setQuery(queryBuilder)\r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n\r\n{&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:300000,&quot;produce_date&quot;:&quot;2017-01-01&quot;}',8,0,0,1514622733,0,0,0),(181,1,'基于地理位置的搜索','','','&lt;dependency&gt;\r\n    &lt;groupId&gt;org.locationtech.spatial4j&lt;/groupId&gt;\r\n    &lt;artifactId&gt;spatial4j&lt;/artifactId&gt;\r\n    &lt;version&gt;0.6&lt;/version&gt;                        \r\n&lt;/dependency&gt;\r\n\r\n&lt;dependency&gt;\r\n    &lt;groupId&gt;com.vividsolutions&lt;/groupId&gt;\r\n    &lt;artifactId&gt;jts&lt;/artifactId&gt;\r\n    &lt;version&gt;1.13&lt;/version&gt;                         \r\n    &lt;exclusions&gt;\r\n        &lt;exclusion&gt;\r\n            &lt;groupId&gt;xerces&lt;/groupId&gt;\r\n            &lt;artifactId&gt;xercesImpl&lt;/artifactId&gt;\r\n        &lt;/exclusion&gt;\r\n    &lt;/exclusions&gt;\r\n&lt;/dependency&gt;\r\n\r\n比如我们有很多的4s店，然后呢给了用户一个app，在某个地方的时候，可以根据当前的地理位置搜索一下，自己附近的4s店\r\n\r\nPOST /car_shop/_mapping/shops\r\n{\r\n  &quot;properties&quot;: {\r\n      &quot;pin&quot;: {\r\n          &quot;properties&quot;: {\r\n              &quot;location&quot;: {\r\n                  &quot;type&quot;: &quot;geo_point&quot;\r\n              }\r\n          }\r\n      }\r\n  }\r\n}\r\n{\r\n  &quot;acknowledged&quot;: true\r\n}\r\n\r\nPUT /car_shop/shops/1\r\n{\r\n    &quot;name&quot;: &quot;上海至全宝马4S店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n  &quot;_index&quot;: &quot;car_shop&quot;,\r\n  &quot;_type&quot;: &quot;shops&quot;,\r\n  &quot;_id&quot;: &quot;1&quot;,\r\n  &quot;_version&quot;: 1,\r\n  &quot;result&quot;: &quot;created&quot;,\r\n  &quot;_shards&quot;: {\r\n    &quot;total&quot;: 2,\r\n    &quot;successful&quot;: 1,\r\n    &quot;failed&quot;: 0\r\n  },\r\n  &quot;created&quot;: true\r\n}\r\n\r\n第一个需求：搜索两个坐标点组成的一个区域\r\n\r\nQueryBuilder qb = geoBoundingBoxQuery(&quot;pin.location&quot;).setCorners(40.73, -74.1, 40.01, -71.12); \r\n\r\n第二个需求：指定一个区域，由三个坐标点，组成，比如上海大厦，东方明珠塔，上海火车站\r\n\r\nList&lt;GeoPoint&gt; points = new ArrayList&lt;&gt;();             \r\npoints.add(new GeoPoint(40.73, -74.1));\r\npoints.add(new GeoPoint(40.01, -71.12));\r\npoints.add(new GeoPoint(50.56, -90.58));\r\n\r\nQueryBuilder qb = geoPolygonQuery(&quot;pin.location&quot;, points); \r\n\r\n第三个需求：搜索距离当前位置在200公里内的4s店\r\n\r\nQueryBuilder qb = geoDistanceQuery(&quot;pin.location&quot;).point(40, -70).distance(200, DistanceUnit.KILOMETERS);   \r\n\r\nSearchResponse response = client.prepareSearch(&quot;car_shop&quot;)\r\n        .setTypes(&quot;shops&quot;)\r\n        .setQuery(qb)                \r\n        .get();\r\n\r\npackage com.peng.es.senior;\r\n\r\nimport java.net.InetAddress;\r\nimport java.util.ArrayList;\r\nimport java.util.List;\r\n\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.geo.GeoPoint;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\nimport org.elasticsearch.common.unit.DistanceUnit;\r\nimport org.elasticsearch.index.query.QueryBuilders;\r\nimport org.elasticsearch.search.SearchHit;\r\nimport org.elasticsearch.transport.client.PreBuiltTransportClient;\r\n\r\npublic class GeoLocationShopSearchApp {\r\n\r\n	@SuppressWarnings({ &quot;unchecked&quot;, &quot;resource&quot; })\r\n	public static void main(String[] args) throws Exception {\r\n		Settings settings = Settings.builder()\r\n				.put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)\r\n				.build();\r\n		\r\n		TransportClient client = new PreBuiltTransportClient(settings)\r\n				.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;localhost&quot;), 9300));\r\n		\r\n		SearchResponse searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;shops&quot;)\r\n				.setQuery(QueryBuilders.geoBoundingBoxQuery(&quot;pin.location&quot;)\r\n								.setCorners(40.73, -74.1, 40.01, -71.12))\r\n				.get();\r\n	\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		List&lt;GeoPoint&gt; points = new ArrayList&lt;GeoPoint&gt;();             \r\n		points.add(new GeoPoint(40.73, -74.1));\r\n		points.add(new GeoPoint(40.01, -71.12));\r\n		points.add(new GeoPoint(50.56, -90.58));\r\n\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;shops&quot;)\r\n				.setQuery(QueryBuilders.geoPolygonQuery(&quot;pin.location&quot;, points))  \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		System.out.println(&quot;====================================================&quot;);\r\n		\r\n		searchResponse = client.prepareSearch(&quot;car_shop&quot;)\r\n				.setTypes(&quot;shops&quot;)\r\n				.setQuery(QueryBuilders.geoDistanceQuery(&quot;pin.location&quot;)\r\n						.point(40, -70)\r\n						.distance(200, DistanceUnit.KILOMETERS))  \r\n				.get();\r\n		\r\n		for(SearchHit searchHit : searchResponse.getHits().getHits()) {\r\n			System.out.println(searchHit.getSourceAsString());  \r\n		}\r\n		\r\n		client.close();\r\n	}\r\n	\r\n}\r\n		\r\n{\r\n    &quot;name&quot;: &quot;上海至全宝马4S店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}\r\n\r\n====================================================\r\n{\r\n    &quot;name&quot;: &quot;上海至全宝马4S店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}\r\n\r\n====================================================\r\n{\r\n    &quot;name&quot;: &quot;上海至全宝马4S店&quot;,\r\n    &quot;pin&quot; : {\r\n        &quot;location&quot; : {\r\n            &quot;lat&quot; : 40.12,\r\n            &quot;lon&quot; : -71.34\r\n        }\r\n    }\r\n}',8,0,0,1514624054,0,0,0),(182,1,'logstash 安装','','','yum 方式安装\r\n------------------------------------------\r\njava -version 检测环境是否安装java\r\nrpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch\r\nvi /etc/yum.repos.d/logstash.repo\r\n[logstash-5.x]\r\nname=Elastic repository for 5.x packages\r\nbaseurl=https://artifacts.elastic.co/packages/5.x/yum\r\ngpgcheck=1\r\ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch\r\nenabled=1\r\nautorefresh=1\r\ntype=rpm-md\r\n\r\nsudo yum install logstash\r\n\r\n手动安装\r\n------------------------------------------\r\nhttps://www.elastic.co/downloads/logstash 下载 tar.gz包\r\n\r\nmkdir /opt/logstash\r\n将tar.gz包上传到/opt/logstash目录下\r\ncd /opt/logstash\r\ntar -zxvf logstash-5.1.1.tar.gz\r\ncd logstash-5.1.1\r\n\r\n第一个事件\r\n------------------------------------------\r\nbin/logstash -e &#039;input { stdin { } } output { stdout {} }&#039;\r\nthe stdin plugin is now waiting for input: #控制台提示输入\r\nwelcome to logstash world\r\n2016-12-05T08:04:32.142Z 0.0.0.0 hello welcome to logstash world\r\n\r\nbin/logstash -e &#039;input{stdin{}}output{stdout{codec=&gt;rubydebug}}&#039;\r\nwelcome to logstash world\r\n{\r\n    &quot;@timestamp&quot; =&gt; 2016-12-05T11:51:12.840Z,\r\n      &quot;@version&quot; =&gt; &quot;1&quot;,\r\n          &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,\r\n       &quot;message&quot; =&gt; &quot;welcome to logstash world&quot;,\r\n          &quot;tags&quot; =&gt; []\r\n}\r\n\r\n@timestamp 标记事件的发生时间\r\nmessage 客户端输入信息\r\nhost 标记事件发生在哪里\r\ntags 标记事件的某方面属性\r\n\r\n使用配置文件\r\n------------------------------------------\r\nvi logstash.conf #注意换行\r\n\r\ninput {stdin {}}\r\noutput {stdout {codec =&gt; rubydebug {}}}\r\nbin/logstash -f logstash.conf\r\nwelcome to logstash world\r\n{\r\n    &quot;@timestamp&quot; =&gt; 2016-12-05T11:51:12.840Z,\r\n      &quot;@version&quot; =&gt; &quot;1&quot;,\r\n          &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,\r\n       &quot;message&quot; =&gt; &quot;welcome to logstash world&quot;,\r\n          &quot;tags&quot; =&gt; []\r\n}',8,0,0,1514631304,0,0,0),(183,1,'stdout 标准输出','','','stdout 标准输出\r\n\r\nvi file.conf\r\ninput { stdin { } }\r\nfilter { }\r\noutput { stdout { } }\r\n\r\nbin/logstash -f file.conf\r\nwelcome # 输入\r\n2016-12-06T05:01:27.056Z 0.0.0.0 welcome # 输出\r\n\r\nvi logstash.conf\r\ninput { stdin { } }\r\nfilter { }\r\noutput { stdout { codec =&gt; rubydebug { } } }\r\n\r\nbin/logstash -f logstash.conf\r\nwelcome # 输入\r\n{\r\n    &quot;@timestamp&quot; =&gt; 2016-12-06T05:07:33.662Z,\r\n      &quot;@version&quot; =&gt; &quot;1&quot;,\r\n          &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,\r\n       &quot;message&quot; =&gt; &quot;welcome&quot;,\r\n          &quot;tags&quot; =&gt; []\r\n}',8,0,0,1514631391,0,0,0),(184,1,'软件包管理','','','    软件包管理简介\r\n    rpm 命令管理\r\n    yum 在线管理\r\n    源码包管理\r\n    脚本安装包\r\n\r\n    学习 linux 中最基本的软件安装方法\r\n\r\n    软件包分类\r\n    源码包 脚本安装包\r\n    二进制包 (RPM 包, 系统默认包) 编译\r\n\r\n    源码包\r\n    源码包的优点是\r\n    开源 如果有足够的能力 可以修改源代码\r\n    可以自由选择所需的功能\r\n    软件是编译安装 所以更加适合自己的系统 更加稳定也效率更高\r\n    卸载方便\r\n\r\n    源码包的缺点\r\n    安装过程步骤较多 尤其安装较大的软件集合时 (如LAMP 环境搭建)　容易出现拼写错误\r\n    编译过程时间较长 安装比二进制较长\r\n    因为编译安装 安装过程中一旦报错新手很难解决\r\n\r\n    如果只有源码包可用怎么办呢\r\n    初学者很苦恼\r\n    源码包不适合初学者\r\n\r\n    脚本安装包\r\n    所谓的脚本安装包 就是把复杂的软件包安装过程写成了程序脚本 初学者可以执行程序脚本实现一键安装　但实际安装的还是源码包和二进制包\r\n    优点 安装简单 快捷\r\n    缺点 完全丧失了自定义性\r\n\r\n    RPM 包\r\n    二进制包的优点\r\n    包管理系统简单 只通过几个命令就可以实现包的安装　升级　查询和卸载\r\n    安装速度比源码包安装快的多\r\n\r\n    二进制包缺点\r\n    经过编译　不再可以看到源代码\r\n    功能选择不如源码包灵活\r\n    依赖性\r\n\r\n\r\nRPM 命令管理\r\n    RPM 包命令规则\r\n    安装命令\r\n    升级与卸载\r\n    RPM 包查询\r\n    RPM 包校验\r\n\r\n    RPM 包的来源\r\n    RPM 包在系统光盘中\r\n\r\n    RPM 包命名原则\r\n    httpd-2.2.15-15.el6.centos.1.i686.rpm\r\n    httpd 软件包名\r\n    2.2.15 软件版本\r\n    15 软件发布的次数\r\n    el6.centos 适合的 linux 平台\r\n    i686 适合的硬件平台\r\n    rpm rpm 包扩展名\r\n\r\n    RPM 包依赖性\r\n    树形依赖 a-&gt;b-&gt;c\r\n    环形依赖 a-&gt;b-&gt;c-&gt;a\r\n    模块依赖 模块依赖 查询网站 www.rpmfind.net\r\n\r\n    包全名与包名\r\n    包全名 操作的包是没有安装的软件包时, 使用包全名, 而且要注意路径\r\n    包名   操作已经安装的软件包时 使用包名 是搜索 /var/lib/rpm/ 中的数据库\r\n\r\n    RPM 安装 \r\n    rpm -ivh 包全名\r\n    -i (install) 安装\r\n    -v (verbose) 显示详细信息\r\n    -h (hash) 显示进度\r\n    --nodeps 不检测依赖性\r\n\r\n    RPM 包升级\r\n    -U (upgrade) 升级\r\n\r\n    查询是否安装\r\n    rpm -q 包名 #查询包是否安装 -q 查询 (query)\r\n    rpm -qa #查询所有已经安装的 RPM 包 -a 所有 (all)\r\n\r\n    查询软件包详细信息\r\n    rpm -qi 包名 \r\n    -i 查询软件信息 (information)\r\n    -p 查询未安装包信息 (package)\r\n\r\n    查询包中文件安装位置\r\n    rpm -ql 包名\r\n    -l 列表 (list)\r\n    -p 查询未安装包信息 (package)\r\n\r\n    查询系统文件属于哪个 RPM 包\r\n    rpm -qf 系统文件名\r\n    -f 查询系统文件属于哪个软件包 (file)\r\n    rpm -qf yum.conf\r\n\r\n    查询软件包的依赖性\r\n    rpm -qR 包名\r\n    -R 查询软件包的依赖性 (requires)\r\n    -p 查询未安装包信息 (package)\r\n\r\n    RPM 包校验\r\n    rpm -V 已安装的包名\r\n    -V 校验指定 RPM 包中的文件 (verify)\r\n\r\n    验证内容中的 8 个信息的具体内容如下\r\n    S 文件大小是否改变\r\n    M 文件的类型或文件的权限 (rwx) 是否被改变\r\n    5 文件MD5 校验和是否改变 (可以看成文件内容是否改变)\r\n    D 设备的主从代码是否改变\r\n    L 文件路径是否改变\r\n    U 文件的属主 (所有者) 是否改变\r\n    G 文件的属组是否改变\r\n    T 文件的修改时间是否改变\r\n\r\n    RPM 包中文件提取\r\n    rpm2cpio 包全名 | cpio -idv .文件绝对路径\r\n    -rpm2cpio #将rpm 包转换为 cpio 格式的命令\r\n    -cpio #是一个标准工具 它用于创建软件档案文件和从档案文件中提取文件\r\n\r\n    rpm -qf /bin/ls #查询 ls 命令属于哪个软件包\r\n    mv /bin/ls /tmp/ #造成 ls 命令误删除假象\r\n    rpm2cpio /mnt/cdrom/Packages/coreutils-8.4-19.el6.i686.rpm | cpio -idv ./bin/ls #提取 RPM 包中 ls 命令到当前目录的 /bin/ls 下\r\n    cp /root/bin/ls /bin/ #把 ls 命令复制到 /bin/ 目录 修复文件丢失\r\n\r\n	    脚本安装包\r\n\r\n    强大的 nginx 服务器\r\n    nginx 是一款轻量级的 web 服务器 / 反向代理服务器及电子邮件 (IMAP/POP3) 代理服务器 2004年发布\r\n\r\n    web服务器    nginx  apache lighttpd\r\n    反回代理     非常好 好     一般\r\n    rewrite 规则 非常好 好     一般\r\n    fastcgi      好     差     非常好\r\n    热部署       支持   不支付 不支持\r\n    系统压力比较 很小   小     很大\r\n    稳定性       非常好 好     一般\r\n    安全性       一般   好     一般\r\n    技术资料     很少   非常多 一般\r\n    静态文件处理 非常好 一般   好\r\n    虚拟主机     支持   支持   支持\r\n    内存消耗     非常小 很大   非常小\r\n\r\n    nginx 在反向代理 rewrite 规则 稳定性 静态文件处理 内存消耗等方面 表现出了很强的优势 选用 nginx 取代传统的 apache 服务器 将会获得多方面的性能提升\r\n\r\n    准备工作\r\n    关闭 RPM 包安装的 httpd 和 mysql\r\n    保证 yum 源正常使用\r\n    关闭 selinux 和防火墙 (vi /etc/selinux/config SELINUX=enforcing =&gt; SELINUX=disabled)\r\n\r\n    centos.sh 脚本分析\r\n    所谓的一键安装包 实际上还是安装的源码包与 RPM 包 只是把安装过程写成了脚本 便于初学者安装\r\n    优点 简单 快速 方便\r\n    缺点\r\n    不能定义安装软件的版本\r\n    不能定义所需要的软件功能\r\n    源码包的优势丧失\r\n\r\n    ps aux 查看进程\r\n    php-fpm 在虚拟机中常常卡出导致出错\r\n    pkill -9 php-fpm #杀死 php-fpm 进程\r\n    /etc/rc.d/init/init.d/php-fpm start #重启\r\n	\r\n	    源码包与 RPM 的区别\r\n\r\n    安装之前的区别 概念上的区别\r\n    安装之后的区别 安装位置不同\r\n\r\n    RPM 包安装位置\r\n    是安装在默认位置中\r\n    RPM 包默认安装路径\r\n    /etc/ 配置文件安装目录\r\n    /usr/bin/ 可执行的命令安装目录\r\n    /usr/lib/ 程序所使用的函数库保存位置\r\n    /usr/share/doc/ 基本的软件使用手册保存位置\r\n    /usr/share/man/ 帮助文件保存位置\r\n\r\n    源码包安装过程\r\n    rpm --help | grep prefix\r\n    rpm 安装可以指定安装位置 (--prefix=&lt;div&gt;)\r\n\r\n    安装位置不同带来的影响\r\n    RPM 包安装的服务可以使用系统服务管理命令 (service) 来管理 例如 RPM 包安装的 apache 的启动方法是\r\n    /etc/rc.d/init.d/httpd start\r\n    service httpd start\r\n\r\n    源码包安装位置\r\n    安装在指定位置当中 一般是 /usr/local/软件名/ #源码包没有卸载命令\r\n\r\n    而源码包安装的服务则不能被服务管理命令管理 因为没有安装到默认路径中 所以只能用绝对路径进行服务的管理\r\n    /usr/local/apache2/bin/apachectl start\r\n    (只要把安装目录删除掉, 就卸载了对应的软件)\r\n\r\n    源码包安装\r\n\r\n    安装准备\r\n    安装 C 语言编译器 (rpm -qa | grep gcc 查看是否安装 gcc)\r\n    下载源码包\r\n    http://mirror.bit.edu.cn/apache/httpd/\r\n    npm 包和源码包 选择哪一个呢\r\n\r\n    安装注意事项\r\n    源代码保存位置 /usr/local/src/\r\n    软件安装位置 /usr/local/\r\n    如何确定安装过程报错 安装过程停止 并出现 error warning 或 no 的提示\r\n\r\n    源码包安装过程\r\n    下载源码包\r\n    解压缩下载的源码包\r\n    进入解压缩目录 \r\n\r\n    ./configure 软件配置与检查\r\n    定义需要的功能选项\r\n    检测系统环境是否符合安装要求\r\n    把定义好的功能选项和检测系统环境的信息都写入 makefile 文件 用于后续的编辑\r\n    make 编译 make clean 清除所有编译的文件\r\n    make install 编译安装\r\n    vi INSTALL #打开编译安装文件 查看安装要求\r\n\r\n    源码包的卸载\r\n    不需要卸载命令 直接删除安装目录即可 不会遗留任何垃圾文件\r\n	\r\n	    yum 在线安装\r\n    好处 将所有软件包放到官方服务器上 当进行 yum 在线安装时 可以自动解决依赖性问题\r\n    redhat 的 yum 在线安装是需要付费的\r\n\r\n    yum 源文件\r\n    cd /etc/yum.repos.d/\r\n    ls\r\n    vi /etc/yum.repos.d/CentOS-Base.repo\r\n    [base] 容器名称 一定要放在 [] 中\r\n    name 容器说明 可以自己随便写\r\n    mirrorlist 镜像站点 这个可以注释掉\r\n    baseurl 我们的 yum 源服务器的地址 默认是CentOs 官方的 yum 源服务器 是可以使用的 如果你觉得慢可以改成你喜欢的 yum 源地址\r\n    enabled 此容器是否生效果 如果不写或写成 enable=1 都是生效 写成 enable=0 就是不生效\r\n    gpgcheck 如果是 1 是指 RPM 的数字证书生效 如果是 0 则不生效\r\n    gpgkey 数字证书的公钥文件保存位置 不用修改\r\n\r\n    如果没有网络 如何使用 yum 源\r\n    挂载光盘\r\n    mkdir /mnt/cdrom #建立挂载点\r\n    mount /dev/cdrom /mnt/cdrom/ #挂载光盘\r\n\r\n    使网络yum 源失效\r\n    cd /etc/yum.repos.d/ #进入 yum 源目录\r\n    mv CentOS-Base.repo CentOS-Base.repo.bak #修改 yum 源文件后缀名 使其失效\r\n    vim CentOS-Media.repo\r\n    name=CentOS-$releasever - Media\r\n    baseurl=file:///mnt/cdrom #地址为你自己的光盘挂载地址\r\n            file:///media/cdrom\r\n            file:///media/cdrecorder/ #注释这两个不存在的地址\r\n    gpgcheck=1\r\n    enabled=1 #把enabled=0 改为enabled=1 让这个 yum 源配置文件生效\r\n    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\r\n\r\n    常用 yum 命令\r\n    查询 \r\n    yum list #查询所有可用软件包列表\r\n    yum search 关键字 #搜索服务器上所有和关键字相关的包\r\n\r\n    安装\r\n    yum -y install 包名\r\n    install 安装\r\n    -y 自动回答 yes\r\n    yum -y install gcc\r\n\r\n    升级\r\n    yum -y update 包名\r\n    update 升级\r\n    -y 自动回答 yes\r\n    yum -y update 升级所有的内容 (不建议使用)\r\n    yum -y update httpd 升级 httpd\r\n\r\n    卸载\r\n    yum -y remove 包名\r\n    remove 卸载\r\n    -y 自动回答 yes\r\n    服务器使用最小化安装 用什么软件安装什么 尽量不卸载\r\n\r\n    YUM 软件组管理命令\r\n    yum grouplist #列出所有可用的软件组列表\r\n    yum groupinstall 软件组名 #安装指定软件组 组名可以由 grouplist 查询出来\r\n    yum groupremove 软件组名 #卸载指定软件组',9,0,0,1514631646,0,0,0),(185,1,'vmware 简介','','','vmware 是一个虚拟 PC 的软件\r\n可以在现有的操作系统上虚拟出一个新的硬件环境\r\n相当于模拟出一台新的 PC\r\n以此来实现在一台机器上真正同时运行两个独立的操作系统\r\n官网 http://www.vmware.com\r\n\r\nvmware 主要特点\r\n不需要分区或重新开机就能在同一台 PC 上使用两种以上的操作系统\r\n本机系统可以与虚拟机系统网络通信\r\n可以设定并且随时修改虚拟机操作系统的硬件环境\r\n\r\n需要配置\r\nCPU 建议主频为 1GHz 以上\r\n内存 建议 1GB 以上\r\n硬盘 建议分区空闲空间 8GB 以上\r\n\r\n安装虚拟机\r\n启动 vm\r\n创建新的虚拟机\r\n自定义\r\n稍后安装操作系统 (安装完虚拟机再安装操作系统)\r\n下一步\r\n选择 (安装 linux 系统 centos)\r\n选择安装路径 (d:/files/vm)\r\n选择处理器 (以下几步可以在虚拟机网络设置中调)\r\n选择内存\r\n选择网络连接 (桥接 方便 但占用真实机 nat 使用 vm8虚拟网卡与真实机通信 host-only 使用vm1 虚拟网卡与真实机通信并且不能与外部电脑通信)\r\nI / O 控制类型 (推荐)\r\n虚拟磁盘类型 (推荐)\r\n创建新的虚拟器\r\n磁盘大小 20 G 选择将虚拟磁盘存储为单个文件\r\n完成\r\n\r\n虚拟机安装 linux \r\nCD/DVD 自动检测\r\n浏览 (选择第一个镜像文件)\r\n\r\n开启此虚拟机 (不再显示此提示 进入全屏模式)\r\ninstall or upgrade on existing system 安装或升级现在的系统\r\ninstall system with basic video driver 安装过程采用基本的显卡驱动\r\nrescue installed system 进入系统修复模式\r\nboot from local drive 退出安装从硬盘启动\r\nmemory test 存储介质检测\r\n选择 skip (是否检测你的盘里内容 tab 键 方向键 -&gt; 都可以) 下一步\r\n中文简体\r\n基本存储设备\r\n主机名\r\n时区\r\n密码 (学习时使用最简单的密码 无论如何都使用)\r\n  密码原则\r\n  复杂性\r\n	  八位字符以上 大小写字母 数字 符号\r\n	  不能是英文单词\r\n	  不能是和用户相关的内容\r\n  易记忆性\r\n  时效性\r\n创建自定义布局\r\n选择创建 ---&gt; 标准分区 ---&gt; /boot(挂载点) 200M\r\n选择创建 ---&gt; 标准分区 ---&gt; swap(文件系统类型) 200M (一般内存的1.5倍至二倍)\r\n选择创建 ---&gt; 标准分区 ---&gt; / 使用全部可用空间\r\n在 /dev/sda 中安装引导装载程序\r\n软件包程序选择 basic server\r\n  Desktop 桌面\r\n  Minimal Desktop 最小化桌面\r\n  Minimal 最小化\r\n  Basic Server 基本服务器\r\n  Database Server 数据库服务器\r\n  Web Server 网页服务器\r\n  Virtual Host 虚拟主机\r\n  software development workstation 软件开发工作站\r\n安装引导程序\r\n  \r\nlogin: root # 用户名\r\npassword: \r\n[root@localhost]# \r\n\r\n[root@localhost]# ls\r\ninstall.log install.log.syslog anaconda-ks.cfg\r\n/root/install.log 存储了安装在系统中的软件包及其版本信息\r\n/root/install.log.syslog 存储了安装过程中留下的事件记录\r\n/root/anaconda-ks.log 以Kickstart 配置文件的格式记录安装过程中设置的选项信息\r\n\r\nls /etc/sysconfig/network-scripts/ifcfg-eth0\r\nvim /etc/sysconfig/network-scripts/ifcfg-eth0\r\n修改 ONBOOT = &#039;yes&#039; (按i将no 改为 yes vim 中强制退出 q! Tab 键补全)\r\n按 esc 键\r\n输入 :wq (esc 键进入编辑模式 : 进入命令模式)\r\n重启 service network restart\r\nifconfig (查看ip 地址)\r\ninet addr:192. \r\n开启 vm (可以通过修改网络适配器 的连接方式 来修改 ip)\r\n启动 putty.exe (第三方登录 linux 系统)\r\n主机名称为 ip 地址\r\n转换 ---&gt; utf8 \r\n会话 ---&gt; 默认设置 ---&gt; 保存\r\n打开\r\n\r\nlinux (三大核心技术)\r\n防火墙配置 内核截切 邮箱服务器\r\n\r\n关闭防火墙\r\nsetup\r\n\r\n 配置 IP\r\n\r\n \r\n虚拟机(vmware)安装\r\n------------------------------------------\r\n双击虚拟机安装文件 --- 下一步 --- 选择典型 --- 选择安装目录 --- \r\n软件更新页(去除勾选项) --- 用户体验改进计划页(去除勾选项) \r\n--- 快捷方式页(选择桌面 开始菜单) --- 输入许可证密钥 --- 安装完成\r\n\r\nvm虚拟机\r\n\r\n禁用443端口\r\n编辑 --- 自选项 --- 共享虚拟机 --- 禁用443端口\r\n\r\n删除无用虚拟机\r\n左边框 --- 要删除的计算名 --- 右击 --- 管理 --- 从磁盘删除\r\n\r\n不用时选择挂起 用时选择在后台运行\r\n\r\n进入centos 目录 --- 启动 vmware 10 安装程序 --- 选择典型 --- 下一步 ...\r\n输入密钥 5F0YA-FFK9P-VZCK1-1T07K-2CYNX\r\n\r\nlinux 系统安装\r\n启动 vmware 10 --- 创建新的虚拟机 --- 典型 --- 稍后安装系统 --- linux --- 选择 CentOS 64 位 --- 输入虚拟机名 apeng1 --- 将虚拟磁盘存储为单个文件 --- 完成\r\n\r\nvmware 10 界面 --- 点击 CD/DVD --- 使用 ISO 镜像文件 --- 选择第一个镜像文件 --- 启动虚拟机 --- ...\r\n',9,0,0,1514642238,0,0,0),(186,1,'linux的命令操作','','','1、日常操作命令  \r\n\r\n**查看当前所在的工作目录\r\npwd\r\n\r\n**查看当前系统的时间 \r\ndate\r\n\r\n**查看有谁在线（哪些人登陆到了服务器）\r\nwho  查看当前在线\r\nlast 查看最近的登陆历史记录\r\n\r\n\r\n2、文件系统操作\r\n**\r\nls /    查看根目录下的子节点（文件夹和文件）信息\r\nls -al  -a是显示隐藏文件   -l是以更详细的列表形式显示\r\n\r\n**切换目录\r\ncd  /home\r\n\r\n**创建文件夹\r\nmkdir aaa     这是相对路径的写法 \r\nmkdir -p aaa/bbb/ccc\r\nmkdir  /data    这是绝对路径的写法 \r\n\r\n**删除文件夹\r\nrmdir   可以删除空目录\r\nrm -r aaa   可以把aaa整个文件夹及其中的所有子节点全部删除\r\nrm -rf aaa   强制删除aaa\r\n\r\n**修改文件夹名称\r\nmv aaa angelababy\r\n\r\n**创建文件\r\ntouch  somefile.1   创建一个空文件\r\necho &quot;i miss you,my baby&quot; &gt; somefile.2  利用重定向“&gt;”的功能，将一条指令的输出结果写入到一个文件中，会覆盖原文件内容\r\necho &quot;huangxiaoming ,gun dan&quot; &gt;&gt; somefile.2     将一条指令的输出结果追加到一个文件中，不会覆盖原文件内容\r\n\r\n用vi文本编辑器来编辑生成文件\r\n******最基本用法\r\nvi  somefile.4\r\n1、首先会进入“一般模式”，此模式只接受各种快捷键，不能编辑文件内容\r\n2、按i键，就会从一般模式进入编辑模式，此模式下，敲入的都是文件内容\r\n3、编辑完成之后，按Esc键退出编辑模式，回到一般模式；\r\n4、再按：，进入“底行命令模式”，输入wq命令，回车即可\r\n\r\n******一些常用快捷键\r\n一些有用的快捷键（在一般模式下使用）：\r\na  在光标后一位开始插入\r\nA   在该行的最后插入\r\nI   在该行的最前面插入\r\ngg   直接跳到文件的首行\r\nG    直接跳到文件的末行\r\ndd   删除行，如果  5dd   ，则一次性删除光标后的5行\r\nyy  复制当前行,  复制多行，则  3yy，则复制当前行附近的3行\r\np   粘贴\r\nv  进入字符选择模式，选择完成后，按y复制，按p粘贴\r\nctrl+v  进入块选择模式，选择完成后，按y复制，按p粘贴\r\nshift+v  进入行选择模式，选择完成后，按y复制，按p粘贴\r\n\r\n查找并替换（在底行命令模式中输入）\r\n%s/sad/88888888888888     效果：查找文件中所有sad，替换为88888888888888\r\n/you       效果：查找文件中出现的you，并定位到第一个找到的地方，按n可以定位到下一个匹配位置（按N定位到上一个）\r\n\r\n\r\n3、文件权限的操作\r\n\r\n****linux文件权限的描述格式解读\r\ndrwxr-xr-x      （也可以用二进制表示  111 101 101  --&gt;  755）\r\n\r\nd：标识节点类型（d：文件夹   -：文件  l:链接）\r\nr：可读   w：可写    x：可执行 \r\n第一组rwx：  表示这个文件的拥有者对它的权限：可读可写可执行\r\n第二组r-x：  表示这个文件的所属组对它的权限：可读，不可写，可执行\r\n第三组r-x：  表示这个文件的其他用户（相对于上面两类用户）对它的权限：可读，不可写，可执行\r\n\r\n\r\n****修改文件权限\r\nchmod g-rw haha.dat    表示将haha.dat对所属组的rw权限取消\r\nchmod o-rw haha.dat 	表示将haha.dat对其他人的rw权限取消\r\nchmod u+x haha.dat      表示将haha.dat对所属用户的权限增加x\r\n\r\n也可以用数字的方式来修改权限\r\nchmod 664 haha.dat   \r\n就会修改成   rw-rw-r--\r\n\r\n如果要将一个文件夹的所有内容权限统一修改，则可以-R参数\r\nchmod -R 770 aaa/\r\nchown angela:angela aaa/    &lt;只有root能执行&gt;\r\n\r\n目录没有执行权限的时候普通用户不能进入\r\n文件只有读写权限的时候普通用户是可以删除的(删除文件不是修改它,是操作父及目录),只要父级目录有执行和修改的权限\r\n\r\n4、基本的用户管理\r\n\r\n*****添加用户\r\nuseradd  angela\r\n要修改密码才能登陆 \r\npasswd angela  按提示输入密码即可\r\n\r\n\r\n**为用户配置sudo权限\r\n用root编辑 vi /etc/sudoers\r\n在文件的如下位置，为hadoop添加一行即可\r\nroot    ALL=(ALL)       ALL     \r\nhadoop  ALL=(ALL)       ALL\r\n\r\n然后，hadoop用户就可以用sudo来执行系统级别的指令\r\n[hadoop@shizhan ~]$ sudo useradd huangxiaoming\r\n\r\n\r\n5、系统管理操作\r\n*****查看主机名\r\nhostname\r\n****修改主机名(重启后无效)\r\nhostname hadoop\r\n\r\n*****修改主机名(重启后永久生效)\r\nvi /ect/sysconfig/network\r\n****修改IP(重启后无效)\r\nifconfig eth0 192.168.12.22\r\n\r\n****修改IP(重启后永久生效)\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\n\r\n\r\nmount ****  挂载外部存储设备到文件系统中\r\nmkdir   /mnt/cdrom      创建一个目录，用来挂载\r\nmount -t iso9660 -o ro /dev/cdrom /mnt/cdrom/     将设备/dev/cdrom挂载到 挂载点 ：  /mnt/cdrom中\r\n\r\n*****umount\r\numount /mnt/cdrom\r\n\r\n\r\n*****统计文件或文件夹的大小\r\ndu -sh  /mnt/cdrom/Packages\r\ndf -h    查看磁盘的空间\r\n****关机\r\nhalt\r\n****重启\r\nreboot\r\n\r\n\r\n******配置主机之间的免密ssh登陆\r\n假如 A  要登陆  B\r\n在A上操作：\r\n%%首先生成密钥对\r\nssh-keygen   (提示时，直接回车即可)\r\n%%再将A自己的公钥拷贝并追加到B的授权列表文件authorized_keys中\r\nssh-copy-id   B',9,0,0,1514642350,0,0,0),(187,1,'   修改Linux的基本配置','','','    1. 修改主机名\r\nvi /etc/sysconfig/network\r\nNETWORKING=yes\r\nHOSTNAME=server1.itcast.cn\r\n    2. 修改ip地址\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\nDEVICE=eth0\r\nTYPE=Ethernet\r\nONBOOT=yes\r\nBOOTPROTO=static\r\nIPADDR=192.168.0.101\r\nNETMASK=255.255.255.0\r\nservice network restart\r\n    3. 修改ip地址和主机名的映射关系\r\nvi /etc/hosts\r\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\r\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\r\n192.168.0.101 server1.itcast.cn\r\n    4. 关闭iptables并设置其开机启动/不启动\r\nservice iptables stop\r\nchkconfig iptables on\r\nchkconfig iptables off',9,0,0,1514642478,0,0,0),(188,1,'安装JDK','','','    1. 上传jdk-7u45-linux-x64.tar.gz到Linux上\r\n    2. 解压jdk到/usr/local目录\r\ntar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local/\r\n    3. 设置环境变量，在/etc/profile文件最后追加相关内容\r\nvi /etc/profile\r\nexport JAVA_HOME=/usr/local/jdk1.7.0_45\r\nexport CLASSPATH=$JAVA_HOME/lib\r\nexport PATH=$PATH:$JAVA_HOME/bin\r\n    4. 刷新环境变量\r\nsource /etc/profile\r\n    5. 测试java命令是否可用\r\njava -version\r\n\r\n#!/bin/bash\r\n\r\nBASE_SERVER=172.16.203.100\r\nyum install -y wget\r\nwget $BASE_SERVER/soft/jdk-7u45-linux-x64.tar.gz\r\ntar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local\r\ncat &gt;&gt; /etc/profile &lt;&lt;',9,0,0,1514642531,0,0,0),(189,1,'  安装Tomcat','','','    1. 上传apache-tomcat-7.0.68.tar.gz到Linux上\r\n    2. 解压tomcat\r\ntar -zxvf apache-tomcat-7.0.68.tar.gz -C /usr/local/\r\n    3. 启动tomcat\r\n/usr/local/apache-tomcat-7.0.68/bin/startup.sh\r\n    4. 查看tomcat进程是否启动\r\njps\r\n\r\n    5. 查看tomcat进程端口\r\nnetstat -anpt | grep 2465\r\n    6. 通过浏览器访问tomcat\r\nhttp://192.168.0.101:8080/',9,0,0,1514642574,0,0,0),(190,1,'安装MySQL','','','    1. 上传MySQL-server-5.5.48-1.linux2.6.x86_64.rpm、MySQL-client-5.5.48-1.linux2.6.x86_64.rpm到Linux上\r\n    2. 使用rpm命令安装MySQL-server-5.5.48-1.linux2.6.x86_64.rpm，缺少perl依赖\r\nrpm -ivh MySQL-server-5.5.48-1.linux2.6.x86_64.rpm \r\n\r\n    3. 安装perl依赖，上传6个perl相关的rpm包\r\n\r\nrpm -ivh perl-*\r\n    4. 再安装MySQL-server，rpm包冲突\r\nrpm -ivh MySQL-server-5.5.48-1.linux2.6.x86_64.rpm\r\n\r\n    5. 卸载冲突的rpm包\r\nrpm -e mysql-libs-5.1.73-5.el6_6.x86_64 --nodeps\r\n    6. 再安装MySQL-client和MySQL-server\r\nrpm -ivh MySQL-client-5.5.48-1.linux2.6.x86_64.rpm\r\nrpm -ivh MySQL-server-5.5.48-1.linux2.6.x86_64.rpm\r\n    7. 启动MySQL服务，然后初始化MySQL\r\nservice mysql start\r\n/usr/bin/mysql_secure_installation\r\n    8. 测试MySQL\r\nmysql -u root -p',9,0,0,1514642618,0,0,0),(191,1,'YUM相关概念','','','        1.1. 什么是YUM\r\nYUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。\r\n        1.2. YUM的作用\r\n在Linux上使用源码的方式安装软件非常满分，使用yum可以简化安装的过程\r\n    2. YUM的常用命令\r\n安装httpd并确认安装\r\nyum instll -y httpd\r\n\r\n列出所有可用的package和package组\r\nyum list\r\n\r\n清除所有缓冲数据\r\nyum clean all\r\n\r\n列出一个包所有依赖的包\r\nyum deplist httpd\r\n\r\n删除httpd\r\nyum remove httpd',9,0,0,1514642731,0,0,0),(192,1,'制作本地YUM源','','','        3.1. 为什么要制作本地YUM源\r\nYUM源虽然可以简化我们在Linux上安装软件的过程，但是生成环境通常无法上网，不能连接外网的YUM源，说以接就无法使用yum命令安装软件了。为了在内网中也可以使用yum安装相关的软件，就要配置yum源。\r\n        3.2. YUM源的原理\r\nYUM源其实就是一个保存了多个RPM包的服务器，可以通过http的方式来检索、下载并安装相关的RPM包\r\n\r\n        3.3. 制作本地YUM源\r\n    1. 准备一台Linux服务器，用最简单的版本CentOS-6.7-x86_64-minimal.iso\r\n    2. 配置好这台服务器的IP地址\r\n    3. 上传CentOS-6.7-x86_64-bin-DVD1.iso到服务器\r\n    4. 将CentOS-6.7-x86_64-bin-DVD1.iso镜像挂载到某个目录\r\nmkdir /var/iso\r\nmount -o loop CentOS-6.7-x86_64-bin-DVD1.iso /var/iso\r\n    5. 修改本机上的YUM源配置文件，将源指向自己\r\n备份原有的YUM源的配置文件\r\ncd /etc/yum.repos.d/\r\nrename .repo .repo.bak *\r\nvi CentOS-Local.repo\r\n[base]\r\nname=CentOS-Local\r\nbaseurl=file:///var/iso\r\ngpgcheck=1\r\nenabled=1   #很重要，1才启用\r\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\r\n添加上面内容保存退出\r\n    6. 清除YUM缓冲\r\nyum clean all\r\n    7. 列出可用的YUM源\r\nyum repolist\r\n    8. 安装相应的软件\r\nyum install -y httpd\r\n9.开启httpd使用浏览器访问http://192.168.0.100:80（如果访问不通，检查防火墙是否开启了80端口或关闭防火墙）\r\nservice httpd start\r\n10.将YUM源配置到httpd（Apache Server）中，其他的服务器即可通过网络访问这个内网中的YUM源了\r\ncp -r /var/iso/ /var/www/html/CentOS-6.7\r\n11.取消先前挂载的镜像\r\numount /var/iso\r\n12.在浏览器中访问http://192.168.0.100/CentOS-6.7/\r\n\r\n    13. 让其他需要安装RPM包的服务器指向这个YUM源，准备一台新的服务器，备份或删除原有的YUM源配置文件\r\ncd /etc/yum.repos.d/\r\nrename .repo .repo.bak *\r\nvi CentOS-Local.repo\r\n[base]\r\nname=CentOS-Local\r\nbaseurl=http://192.168.0.100/CentOS-6.7\r\ngpgcheck=1\r\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\r\n添加上面内容保存退出\r\n    14. 在这台新的服务器上执行YUM的命令\r\nyum clean all\r\nyum repolist\r\n    15. 安装相应的软件\r\nyum install -y gcc\r\n\r\n    16、 加入依赖包到私有yum的repository\r\n进入到repo目录\r\n执行命令：  createrepo  .',9,0,0,1514642833,0,0,0),(193,1,'VMware虚拟机三种联网方法及原理','','','一、Brigde——桥接：默认使用VMnet0 \r\n \r\n1、原理： \r\n \r\nBridge  桥&quot;就是一个主机，这个机器拥有两块网卡，分别处于两个局域网中，同时在&quot;桥&quot;上，运行着程序，让局域网A中的所有数据包原封不动的流入B，反之亦然。这样，局域网A和B就无缝的在链路层连接起来了，在桥接时，VMWare网卡和物理网卡应该处于同一IP网段  当然要保证两个局域网没有冲突的IP. \r\n \r\nVMWare 的桥也是同样的道理，只不过，本来作为硬件的一块网卡，现在由VMWare软件虚拟了！当采用桥接时，VMWare会虚拟一块网卡和真正的物理网卡就行桥接，这样，发到物理网卡的所有数据包就到了VMWare虚拟机，而由VMWare发出的数据包也会通过桥从物理网卡的那端发出。 \r\n \r\n所以，如果物理网卡可以上网，那么桥接的软网卡也没有问题了，这就是桥接上网的原理了。  　　    \r\n2、联网方式： \r\n \r\n这一种联网方式最简单，在局域网内，你的主机是怎么联网的，你在虚拟机里就怎么连网。把虚拟机看成局域网内的另一台电脑就行了！ \r\n \r\n提示：主机网卡处在一个可以访问Internet的局域网中，虚拟机才能通过Bridge访问Internet。 \r\n \r\n\r\n````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` \r\n \r\n二、NAT——网络地址转换  ：默认使用VMnet8 \r\n \r\n1、原理： \r\n \r\nNAT 是  Network  address  translate的简称。NAT技术应用在internet网关和路由器上，比如192.168.0.123这个地址要访问internet，它的数据包就要通过一个网关或者路由器，而网关或者路由器拥有一个能访问internet的ip地址，这样的网关和路由器就要在收发数据包时，对数据包的IP协议层数据进行更改（即  NAT），以使私有网段的主机能够顺利访问internet。此技术解决了IP地址稀缺的问题。同样的私有IP可以网关NAT  上网。    \r\n \r\nVMWare的NAT上网也是同样的道理，它在主机和虚拟机之间用软件伪造出一块网卡，这块网卡和虚拟机的ip处于一个地址段。同时，在这块网卡和主机的网络接口之间进行NAT。虚拟机发出的每一块数据包都会经过虚拟网卡，然后NAT，然后由主机的接口发出。 \r\n \r\n虚拟网卡和虚拟机处于一个地址段，虚拟机和主机不同一个地址段，主机相当于虚拟机的网关，所以虚拟机能ping到主机的IP，但是主机ping不到虚拟机的IP。 \r\n　　　　 \r\n2、联网方式： \r\n \r\n方法1、动态IP地址。 \r\n \r\n主机是静态IP或动态IP，都无所谓，将虚拟机设置成使用DHCP方式上网,Windows下选择“自动获取IP“，linux下开启DHCP服务即可。（这种方法最简单，不用过多的设置，但要在VMware中进行“编辑→虚拟网络设置”，将NAT和DHCP都开启了。一般NAT默认开启，DHCP默认关闭） \r\n　　 \r\n方法2、静态IP地址。 \r\n \r\n如果不想使用DHCP，也可以手动设置：　 \r\n \r\nIP设置与vmnet1同网段,网关设置成vmnet8的网关（在“虚拟网络设置”里的Net选项卡里能找到Gateway）通常是xxx.xxx.xxx.2。 \r\n子网掩码设置与VMnet8相同（设置好IP地址后，子网掩码自动生成） \r\nDNS设置与主机相同。 \r\n \r\n例如：主机IP是10.70.54.31,设置虚拟机IP为10.70.54.22。Netmask,Gateway,DNS都与主机相同即可实现  虚拟机  ---主机  虚拟机&lt;----&gt;互联网  通信。    \r\n \r\n提示：使用NAT技术，主机能上网，虚拟机就可以访问Internet，但是主机不能访问虚拟机。 \r\n\r\n\r\n\r\n`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\r\n\r\n\r\n \r\n三、Host-Only——私有网络共享主机：默认使用VMnet1 \r\n \r\n1、原理： \r\n \r\n提供的是主机和虚拟机之间的网络互访。只想让虚拟机和主机之间有数据交换，而不想让虚拟机访问Internet，就要采用这个设置了。 \r\n \r\nHost-only的条件下，VMWare在真正的Windows系统中，建立一块软网卡。这块网卡可以在网络连接中看到，一般是VMNET1，这块网卡的作用就是使Windows看到虚拟机的IP。 \r\n \r\n2、联网方法： \r\n \r\n方法1、动态IP地址。 \r\n像上面那样开启DHCP后，虚拟机直接自动获取IP地址和DNS。就可以和主机相连了。当然，还要进行一些局域网共享的操作，这里不再赘述。 \r\n \r\n方法2、静态IP地址。    \r\n \r\n也可以手动设置，将虚拟机IP设置与VMnet1同网段,网关设置成VMnet1的网关相同,其余设置与VMnet1相同,DNS设置与主机相同。 \r\n \r\n例如：VMnet1  IP:172.16.249.1        Gateway  :172.16.249.2 \r\n　　 \r\n那么虚拟机  IP:172.16.249.100        Gateway:  172.16.249.2 \r\n　　 \r\n这样、      虚拟机&lt;---&gt;主机              可以通信        \r\n但是、        虚拟机&lt;---&gt;互联网      无法通信 \r\n \r\n提示：Host-only技术只用于主机和虚拟机互访，于访问internet无关。\r\n',9,0,0,1514642947,0,0,0),(194,1,'linux相关问题一','','','批量自动删除rpm包：\r\nrpm -qa | grep mysql | while read c; do rpm -e $c --nodeps; done\r\n\r\nminimal最小化安装\r\neth0默认没有自启用\r\n修改配置文件\r\nonboot=true\r\n\r\n修改静态地址后发现无法ping外网\r\n需要设置网关\r\nroute add default gw 192.168.33.1\r\n添加nameserver\r\nvi /etc/resolv.conf\r\nnameserver 192.168.33.1\r\n\r\n挂载光盘\r\nmkdir /mnt/cdrom\r\nmount -t iso9660 -o ro /dev/cdrom/ /mnt/cdrom \r\n\r\n\r\n解决克隆后eth0不见的问题\r\n\r\n直接修改  /etc/sysconfig/network-script/ifcfg-eth0\r\n删掉UUID  HWADDR\r\n配置静态地址\r\n然后：\r\nrm -rf 　/etc/udev/rules.d/70-persistent-net.rules\r\n然后 reboot',9,0,0,1514643054,0,0,0),(195,1,'man_page','','','1.内部命令：echo\r\n查看内部命令帮助：help echo 或者 man echo\r\n\r\n2.外部命令：ls\r\n查看外部命令帮助：ls --help 或者 man ls 或者 info ls\r\n\r\n3.man文档的类型(1~9)\r\nman 7 man\r\nman 5 passwd\r\n\r\n4.快捷键：\r\nctrl + c：停止进程\r\n\r\nctrl + l：清屏\r\n\r\nctrl + r：搜索历史命令\r\n\r\nctrl + q：退出\r\n\r\n5.善于用tab键',9,0,0,1514643149,0,0,0),(196,1,'linux常用命令','','','说明：安装linux时，创建一个itcast用户，然后使用root用户登陆系统\r\n\r\n1.进入到用户根目录\r\ncd ~ 或 cd\r\n\r\n2.查看当前所在目录\r\npwd\r\n\r\n3.进入到itcast用户根目录\r\ncd ~itcast\r\n\r\n4.返回到原来目录\r\ncd -\r\n\r\n5.返回到上一级目录\r\ncd ..\r\n\r\n6.查看itcast用户根目录下的所有文件\r\nls -la\r\n\r\n7.在根目录下创建一个itcast的文件夹\r\nmkdir /itcast\r\n\r\n8.在/itcast目录下创建src和WebRoot两个文件夹\r\n分别创建：mkdir /itcast/src\r\n		  mkdir /itcast/WebRoot\r\n同时创建：mkdir /itcast/{src,WebRoot}\r\n\r\n进入到/itcast目录，在该目录下创建.classpath和README文件\r\n分别创建：touch .classpath\r\n		  touch README\r\n同时创建：touch {.classpath,README}\r\n\r\n查看/itcast目录下面的所有文件\r\nls -la\r\n\r\n在/itcast目录下面创建一个test.txt文件,同时写入内容&quot;this is test&quot;\r\necho &quot;this is test&quot; &gt; test.txt\r\n\r\n查看一下test.txt的内容\r\ncat test.txt\r\nmore test.txt\r\nless test.txt\r\n\r\n向README文件追加写入&quot;please read me first&quot;\r\necho &quot;please read me first&quot; &gt;&gt; README\r\n\r\n将test.txt的内容追加到README文件中\r\ncat test.txt &gt;&gt; README\r\n\r\n拷贝/itcast目录下的所有文件到/itcast-bak\r\ncp -r /itcast /itcast-bak\r\n\r\n进入到/itcast-bak目录，将test.txt移动到src目录下，并修改文件名为Student.java\r\nmv test.txt src/Student.java\r\n\r\n在src目录下创建一个struts.xml\r\n&gt; struts.xml\r\n\r\n删除所有的xml类型的文件\r\nrm -rf *.xml\r\n\r\n删除/itcast-bak目录和下面的所有文件\r\nrm -rf /itcast-bak\r\n\r\n返回到/itcast目录，查看一下README文件有多单词，多少个少行\r\nwc -w README\r\nwc -l README\r\n\r\n返回到根目录，将/itcast目录先打包，再用gzip压缩\r\n分步完成：tar -cvf itcast.tar itcast\r\n		  gzip itcast.tar\r\n一步完成：tar -zcvf itcast.tar.gz itcast\r\n		  \r\n将其解压缩，再取消打包\r\n分步完成：gzip -d itcast.tar.gz 或 gunzip itcast.tar.gz\r\n一步完成：tar -zxvf itcast.tar.gz\r\n\r\n将/itcast目录先打包，同时用bzip2压缩，并保存到/tmp目录下\r\ntar -jcvf /tmp/itcast.tar.bz2 itcast\r\n\r\n将/tmp/itcast.tar.bz2解压到/usr目录下面\r\ntar -jxvf itcast.tar.bz2 -C /usr/',9,0,0,1514643190,0,0,0),(197,1,'linux文件相关命令','','','1.进入到用户根目录\r\ncd ~ 或者 cd\r\ncd ~hadoop\r\n回到原来路径\r\ncd -\r\n\r\n2.查看文件详情\r\nstat a.txt\r\n\r\n3.移动\r\nmv a.txt /ect/\r\n改名\r\nmv b.txt a.txt\r\n移动并改名\r\nmv a.txt ../b.txt\r\n\r\n4拷贝并改名\r\ncp a.txt /etc/b.txt\r\n\r\n5.vi撤销修改\r\nctrl + u (undo)\r\n恢复\r\nctrl + r (redo)\r\n\r\n6.名令设置别名(重启后无效)\r\nalias ll=&quot;ls -l&quot;\r\n取消\r\nunalias ll\r\n\r\n7.如果想让别名重启后仍然有效需要修改\r\nvi ~/.bashrc\r\n\r\n8.添加用户\r\nuseradd hadoop\r\npasswd hadoop\r\n\r\n9创建多个文件\r\ntouch a.txt b.txt\r\ntouch /home/{a.txt,b.txt}\r\n\r\n10.将一个文件的内容复制到里另一个文件中\r\ncat a.txt &gt; b.txt\r\n追加内容\r\ncat a.txt &gt;&gt; b.txt \r\n\r\n\r\n11.将a.txt 与b.txt设为其拥有者和其所属同一个组者可写入，但其他以外的人则不可写入:\r\nchmod ug+w,o-w a.txt b.txt\r\n\r\nchmod a=wx c.txt\r\n\r\n12.将当前目录下的所有文件与子目录皆设为任何人可读取:\r\nchmod -R a+r *\r\n\r\n13.将a.txt的用户拥有者设为users,组的拥有者设为jessie:\r\nchown users:jessie a.txt\r\n\r\n14.将当前目录下的所有文件与子目录的用户的使用者为lamport,组拥有者皆设为users，\r\nchown -R lamport:users *\r\n\r\n15.将所有的java语言程式拷贝至finished子目录中:\r\ncp *.java finished\r\n\r\n16.将目前目录及其子目录下所有扩展名是java的文件列出来。\r\nfind -name &quot;*.java&quot;\r\n查找当前目录下扩展名是java 的文件\r\nfind -name *.java\r\n\r\n17.删除当前目录下扩展名是java的文件\r\nrm -f *.java',9,0,0,1514643223,0,0,0),(198,1,'linux系统命令','','','1.查看主机名\r\nhostname\r\n\r\n2.修改主机名(重启后无效)\r\nhostname hadoop\r\n\r\n3.修改主机名(重启后永久生效)\r\nvi /ect/sysconfig/network\r\n\r\n4.修改IP(重启后无效)\r\nifconfig eth0 192.168.12.22\r\n\r\n5.修改IP(重启后永久生效)\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\n\r\n6.查看系统信息\r\nuname -a\r\nuname -r\r\n\r\n7.查看ID命令\r\nid -u\r\nid -g\r\n\r\n8.日期\r\ndate\r\ndate +%Y-%m-%d\r\ndate +%T\r\ndate +%Y-%m-%d&quot; &quot;%T\r\n\r\n9.日历\r\ncal 2012\r\n\r\n10.查看文件信息\r\nfile filename\r\n\r\n11.挂载硬盘\r\nmount\r\numount\r\n加载windows共享\r\nmount -t cifs //192.168.1.100/tools /mnt\r\n\r\n12.查看文件大小\r\ndu -h\r\ndu -ah\r\n\r\n13.查看分区\r\ndf -h\r\n\r\n14.ssh\r\nssh hadoop@192.168.1.1\r\n\r\n15.关机\r\nshutdown -h now /init 0\r\nshutdown -r now /reboot',9,0,0,1514643256,0,0,0),(199,1,'用户与组','','','添加一个tom用户，设置它属于users组，并添加注释信息\r\n分步完成：useradd tom\r\n          usermod -g users tom\r\n	      usermod -c &quot;hr tom&quot; tom\r\n一步完成：useradd -g users -c &quot;hr tom&quot; tom\r\n\r\n设置tom用户的密码\r\npasswd tom\r\n\r\n修改tom用户的登陆名为tomcat\r\nusermod -l tomcat tom\r\n\r\n将tomcat添加到sys和root组中\r\nusermod -G sys,root tomcat\r\n\r\n查看tomcat的组信息\r\ngroups tomcat\r\n\r\n添加一个jerry用户并设置密码\r\nuseradd jerry\r\npasswd jerry\r\n\r\n添加一个交america的组\r\ngroupadd america\r\n\r\n将jerry添加到america组中\r\nusermod -g america jerry\r\n\r\n将tomcat用户从root组和sys组删除\r\ngpasswd -d tomcat root\r\ngpasswd -d tomcat sys\r\n\r\n将america组名修改为am\r\ngroupmod -n am america',9,0,0,1514643300,0,0,0),(200,1,'linux权限','','','创建a.txt和b.txt文件，将他们设为其拥有者和所在组可写入，但其他以外的人则不可写入:\r\nchmod ug+w,o-w a.txt b.txt\r\n\r\n创建c.txt文件所有人都可以写和执行\r\nchmod a=wx c.txt 或chmod 666 c.txt\r\n\r\n将/itcast目录下的所有文件与子目录皆设为任何人可读取\r\nchmod -R a+r /itcast\r\n\r\n将/itcast目录下的所有文件与子目录的拥有者设为root，用户拥有组为users\r\nchown -R root:users /itcast\r\n\r\n将当前目录下的所有文件与子目录的用户皆设为itcast，组设为users\r\nchown -R itcast:users *',9,0,0,1514643352,0,0,0),(201,1,'文件夹属性','','','1.查看文件夹属性\r\nls -ld test\r\n\r\n2.文件夹的rwx\r\n--x:可以cd进去\r\nr-x:可以cd进去并ls\r\n-wx:可以cd进去并touch，rm自己的文件，并且可以vi其他用户的文件\r\n-wt:可以cd进去并touch，rm自己的文件\r\n\r\nls -ld /tmp\r\ndrwxrwxrwt的权限值是1777(sticky)',9,0,0,1514643380,0,0,0),(202,1,'vim','','','i\r\na/A\r\no/O\r\nr + ?替换\r\n\r\n0:文件当前行的开头\r\n$:文件当前行的末尾\r\nG:文件的最后一行开头\r\n1 + G到第一行 \r\n9 + G到第九行 = :9\r\n\r\ndd:删除一行\r\n3dd：删除3行\r\nyy:复制一行\r\n3yy:复制3行\r\np:粘贴\r\nu:undo\r\nctrl + r:redo\r\n\r\n&quot;a剪切板a\r\n&quot;b剪切板b\r\n\r\n&quot;ap粘贴剪切板a的内容\r\n\r\n每次进入vi就有行号\r\nvi ~/.vimrc\r\nset nu\r\n\r\n:w a.txt另存为\r\n:w &gt;&gt; a.txt内容追加到a.txt\r\n\r\n:e!恢复到最初状态\r\n\r\n:1,$s/hadoop/root/g 将第一行到追后一行的hadoop替换为root\r\n:1,$s/hadoop/root/c 将第一行到追后一行的hadoop替换为root(有提示)\r\n\r\n用vi文本编辑器来编辑生成文件\r\n******最基本用法\r\nvi  somefile.4\r\n1、首先会进入“一般模式”，此模式只接受各种快捷键，不能编辑文件内容\r\n2、按i键，就会从一般模式进入编辑模式，此模式下，敲入的都是文件内容\r\n3、编辑完成之后，按Esc键退出编辑模式，回到一般模式；\r\n4、再按：，进入“底行命令模式”，输入wq命令，回车即可\r\n\r\n******一些常用快捷键\r\n一些有用的快捷键（在一般模式下使用）：\r\na  在光标后一位开始插入\r\nA   在该行的最后插入\r\nI   在该行的最前面插入\r\ngg   直接跳到文件的首行\r\nG    直接跳到文件的末行\r\ndd   删除行，如果  5dd   ，则一次性删除光标后的5行\r\nyy  复制当前行,  复制多行，则  3yy，则复制当前行附近的3行\r\np   粘贴\r\nv  进入字符选择模式，选择完成后，按y复制，按p粘贴\r\nctrl+v  进入块选择模式，选择完成后，按y复制，按p粘贴\r\nshift+v  进入行选择模式，选择完成后，按y复制，按p粘贴\r\n\r\n查找并替换（在底行命令模式中输入）\r\n%s/sad/88888888888888     效果：查找文件中所有sad，替换为88888888888888\r\n/you       效果：查找文件中出现的you，并定位到第一个找到的地方，按n可以定位到下一个匹配位置（按N定位到上一个）\r\n\r\n\r\n\r\n*****拷贝文件\r\ncp  somefile.1   /home/hadoop/\r\n\r\n\r\n*****查看文件内容\r\ncat    somefile    一次性将文件内容全部输出（控制台）\r\nmore   somefile     可以翻页查看, 下翻一页(空格)    上翻一页（b）   退出（q）\r\nless   somefile      可以翻页查看,下翻一页(空格)    上翻一页（b），上翻一行(↑)  下翻一行（↓）  可以搜索关键字（/keyword）\r\n\r\ntail -10  install.log   查看文件尾部的10行\r\ntail -f install.log    小f跟踪文件的唯一inode号，就算文件改名后，还是跟踪原来这个inode表示的文件\r\ntail -F install.log    大F按照文件名来跟踪\r\n\r\nhead -10  install.log   查看文件头部的10行\r\n\r\n\r\n\r\n\r\n',9,0,0,1514643408,0,0,0),(203,1,'查找','','','1.查找可执行的命令：\r\nwhich ls\r\n\r\n2.查找可执行的命令和帮助的位置：\r\nwhereis ls\r\n\r\n3.查找文件(需要更新库:updatedb)\r\nlocate hadoop.txt\r\n\r\n4.从某个文件夹开始查找\r\nfind / -name &quot;hadooop*&quot;\r\nfind / -name &quot;hadooop*&quot; -ls\r\n\r\n5.查找并删除\r\nfind / -name &quot;hadooop*&quot; -ok rm {} \\;\r\nfind / -name &quot;hadooop*&quot; -exec rm {} \\;\r\n\r\n6.查找用户为hadoop的文件\r\nfind /usr -user hadoop -ls\r\n\r\n7.查找用户为hadoop并且(-a)拥有组为root的文件\r\nfind /usr -user hadoop -a -group root -ls\r\n\r\n8.查找用户为hadoop或者(-o)拥有组为root并且是文件夹类型的文件\r\nfind /usr -user hadoop -o -group root -a -type d\r\n\r\n9.查找权限为777的文件\r\nfind / -perm -777 -type d -ls\r\n\r\n10.显示命令历史\r\nhistory\r\n\r\n11.grep\r\ngrep hadoop /etc/password',9,0,0,1514643560,0,0,0),(204,1,'打包与压缩','','','1.gzip压缩\r\ngzip a.txt\r\n\r\n2.解压\r\ngunzip a.txt.gz\r\ngzip -d a.txt.gz\r\n\r\n3.bzip2压缩\r\nbzip2 a\r\n\r\n4.解压\r\nbunzip2 a.bz2\r\nbzip2 -d a.bz2\r\n\r\n5.将当前目录的文件打包\r\ntar -cvf bak.tar .\r\n将/etc/password追加文件到bak.tar中(r)\r\ntar -rvf bak.tar /etc/password\r\n\r\n6.解压\r\ntar -xvf bak.tar\r\n\r\n7.打包并压缩gzip\r\ntar -zcvf a.tar.gz\r\n\r\n8.解压缩\r\ntar -zxvf a.tar.gz\r\n解压到/usr/下\r\ntar -zxvf a.tar.gz -C /usr\r\n\r\n9.查看压缩包内容\r\ntar -ztvf a.tar.gz\r\n\r\nzip/unzip\r\n\r\n10.打包并压缩成bz2\r\ntar -jcvf a.tar.bz2\r\n\r\n11.解压bz2\r\ntar -jxvf a.tar.bz2\r\n\r\n',9,0,0,1514643586,0,0,0),(205,1,'正则表达式','','','1.cut截取以:分割保留第七段\r\ngrep hadoop /etc/passwd | cut -d: -f7\r\n\r\n2.排序\r\ndu | sort -n \r\n\r\n3.查询不包含hadoop的\r\ngrep -v hadoop /etc/passwd\r\n\r\n4.正则表达包含hadoop\r\ngrep &#039;hadoop&#039; /etc/passwd\r\n\r\n5.正则表达(点代表任意一个字符)\r\ngrep &#039;h.*p&#039; /etc/passwd\r\n\r\n6.正则表达以hadoop开头\r\ngrep &#039;^hadoop&#039; /etc/passwd\r\n\r\n7.正则表达以hadoop结尾\r\ngrep &#039;hadoop$&#039; /etc/passwd\r\n\r\n规则：\r\n.  : 任意一个字符\r\na* : 任意多个a(零个或多个a)\r\na? : 零个或一个a\r\na+ : 一个或多个a\r\n.* : 任意多个任意字符\r\n\\. : 转义.\r\n\\&lt;h.*p\\&gt; ：以h开头，p结尾的一个单词\r\no\\{2\\} : o重复两次\r\n\r\ngrep &#039;^i.\\{18\\}n$&#039; /usr/share/dict/words\r\n\r\n查找不是以#开头的行\r\ngrep -v &#039;^#&#039; a.txt | grep -v &#039;^$&#039; \r\n\r\n以h或r开头的\r\ngrep &#039;^[hr]&#039; /etc/passwd\r\n\r\n不是以h和r开头的\r\ngrep &#039;^[^hr]&#039; /etc/passwd\r\n\r\n不是以h到r开头的\r\ngrep &#039;^[^h-r]&#039; /etc/passwd',9,0,0,1514643616,0,0,0),(206,1,'输入输出重定向及管道','','','1.新建一个文件\r\ntouch a.txt\r\n&gt; b.txt\r\n\r\n2.错误重定向:2&gt;\r\nfind /etc -name zhaoxing.txt 2&gt; error.txt\r\n\r\n3.将正确或错误的信息都输入到log.txt中\r\nfind /etc -name passwd &gt; /tmp/log.txt 2&gt;&amp;1 \r\nfind /etc -name passwd &amp;&gt; /tmp/log.txt\r\n\r\n4.追加&gt;&gt;\r\n\r\n5.将小写转为大写（输入重定向）\r\ntr &quot;a-z&quot; &quot;A-Z&quot; &lt; /etc/passwd\r\n\r\n6.自动创建文件\r\ncat &gt; log.txt &lt;&lt; EXIT\r\n&gt; ccc\r\n&gt; ddd\r\n&gt; EXI\r\n\r\n7.查看/etc下的文件有多少个？\r\nls -l /etc/ | grep &#039;^d&#039; | wc -l\r\n\r\n8.查看/etc下的文件有多少个，并将文件详情输入到result.txt中\r\nls -l /etc/ | grep &#039;^d&#039; | tee result.txt | wc -l',9,0,0,1514643646,0,0,0),(207,1,'进程控制','','','1.查看用户最近登录情况\r\nlast\r\nlastlog\r\n\r\n2.查看硬盘使用情况\r\ndf\r\n\r\n3.查看文件大小\r\ndu\r\n\r\n4.查看内存使用情况\r\nfree\r\n\r\n5.查看文件系统\r\n/proc\r\n\r\n6.查看日志\r\nls /var/log/\r\n\r\n7.查看系统报错日志\r\ntail /var/log/messages\r\n\r\n8.查看进程\r\ntop\r\n\r\n9.结束进程\r\nkill 1234\r\nkill -9 4333',9,0,0,1514643667,0,0,0),(208,1,'awk简介','','','awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。\r\n\r\nawk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。\r\n\r\nawk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。\r\n\r\n \r\n\r\n使用方法\r\nawk &#039;{pattern + action}&#039; {filenames}\r\n尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。\r\n\r\nawk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。\r\n\r\n通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。\r\n\r\n \r\n\r\n调用awk\r\n有三种方式调用awk\r\n\r\n \r\n1.命令行方式\r\nawk [-F  field-separator]  &#039;commands&#039;  input-file(s)\r\n其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。\r\n在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。\r\n\r\n2.shell脚本方式\r\n将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。\r\n相当于shell脚本首行的：#!/bin/sh\r\n可以换成：#!/bin/awk\r\n\r\n3.将所有的awk命令插入一个单独文件，然后调用：\r\nawk -f awk-script-file input-file(s)\r\n其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。\r\n \r\n 本章重点介绍命令行方式。\r\n\r\n \r\n\r\n入门实例\r\n假设last -n 5的输出如下\r\n\r\n[root@www ~]# last -n 5 &lt;==仅取出前五行\r\nroot     pts/1   192.168.1.100  Tue Feb 10 11:21   still logged in\r\nroot     pts/1   192.168.1.100  Tue Feb 10 00:46 - 02:28  (01:41)\r\nroot     pts/1   192.168.1.100  Mon Feb  9 11:41 - 18:30  (06:48)\r\ndmtsai   pts/1   192.168.1.100  Mon Feb  9 11:41 - 11:41  (00:00)\r\nroot     tty1                   Fri Sep  5 14:09 - 14:10  (00:01)\r\n如果只是显示最近登录的5个帐号\r\n\r\n#last -n 5 | awk  &#039;{print $1}&#039;\r\nroot\r\nroot\r\nroot\r\ndmtsai\r\nroot\r\nawk工作流程是这样的：读入有&#039;\\n&#039;换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是&quot;空白键&quot; 或 &quot;[tab]键&quot;,所以$1表示登录用户，$3表示登录用户ip,以此类推。\r\n\r\n \r\n\r\n如果只是显示/etc/passwd的账户\r\n\r\n#cat /etc/passwd |awk  -F &#039;:&#039;  &#039;{print $1}&#039;  \r\nroot\r\ndaemon\r\nbin\r\nsys\r\n这种是awk+action的示例，每行都会执行action{print $1}。\r\n\r\n-F指定域分隔符为&#039;:&#039;。\r\n\r\n \r\n\r\n如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以tab键分割\r\n\r\n#cat /etc/passwd |awk  -F &#039;:&#039;  &#039;{print $1&quot;\\t&quot;$7}&#039;\r\nroot    /bin/bash\r\ndaemon  /bin/sh\r\nbin     /bin/sh\r\nsys     /bin/sh\r\n \r\n\r\n如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加&quot;blue,/bin/nosh&quot;。\r\n\r\n \r\ncat /etc/passwd |awk  -F &#039;:&#039;  &#039;BEGIN {print &quot;name,shell&quot;}  {print $1&quot;,&quot;$7} END {print &quot;blue,/bin/nosh&quot;}&#039;\r\nname,shell\r\nroot,/bin/bash\r\ndaemon,/bin/sh\r\nbin,/bin/sh\r\nsys,/bin/sh\r\n....\r\nblue,/bin/nosh\r\n \r\nawk工作流程是这样的：先执行BEGING，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域,随后开始执行模式所对应的动作action。接着开始读入第二条记录······直到所有的记录都读完，最后执行END操作。\r\n\r\n \r\n\r\n搜索/etc/passwd有root关键字的所有行\r\n\r\n#awk -F: &#039;/root/&#039; /etc/passwd\r\nroot:x:0:0:root:/root:/bin/bash\r\n这种是pattern的使用示例，匹配了pattern(这里是root)的行才会执行action(没有指定action，默认输出每行的内容)。\r\n\r\n搜索支持正则，例如找root开头的: awk -F: &#039;/^root/&#039; /etc/passwd\r\n\r\n \r\n\r\n搜索/etc/passwd有root关键字的所有行，并显示对应的shell\r\n\r\n# awk -F: &#039;/root/{print $7}&#039; /etc/passwd             \r\n/bin/bash\r\n 这里指定了action{print $7}\r\n\r\n \r\n\r\nawk内置变量\r\nawk有许多内置变量用来设置环境信息，这些变量可以被改变，下面给出了最常用的一些变量。\r\n\r\n \r\nARGC               命令行参数个数\r\nARGV               命令行参数排列\r\nENVIRON            支持队列中系统环境变量的使用\r\nFILENAME           awk浏览的文件名\r\nFNR                浏览文件的记录数\r\nFS                 设置输入域分隔符，等价于命令行 -F选项\r\nNF                 浏览记录的域的个数\r\nNR                 已读的记录数\r\nOFS                输出域分隔符\r\nORS                输出记录分隔符\r\nRS                 控制记录分隔符\r\n \r\n 此外,$0变量是指整条记录。$1表示当前行的第一个域,$2表示当前行的第二个域,......以此类推。\r\n\r\n \r\n\r\n统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容:\r\n\r\n#awk  -F &#039;:&#039;  &#039;{print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0}&#039; /etc/passwd\r\nfilename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/bash\r\nfilename:/etc/passwd,linenumber:2,columns:7,linecontent:daemon:x:1:1:daemon:/usr/sbin:/bin/sh\r\nfilename:/etc/passwd,linenumber:3,columns:7,linecontent:bin:x:2:2:bin:/bin:/bin/sh\r\nfilename:/etc/passwd,linenumber:4,columns:7,linecontent:sys:x:3:3:sys:/dev:/bin/sh\r\n \r\n\r\n使用printf替代print,可以让代码更加简洁，易读\r\n\r\n awk  -F &#039;:&#039;  &#039;{printf(&quot;filename:%s,linenumber:%s,columns:%s,linecontent:%s\\n&quot;,FILENAME,NR,NF,$0)}&#039; /etc/passwd\r\n \r\n\r\nprint和printf\r\nawk中同时提供了print和printf两种打印输出的函数。\r\n\r\n其中print函数的参数可以是变量、数值或者字符串。字符串必须用双引号引用，参数用逗号分隔。如果没有逗号，参数就串联在一起而无法区分。这里，逗号的作用与输出文件的分隔符的作用是一样的，只是后者是空格而已。\r\n\r\nprintf函数，其用法和c语言中printf基本相似,可以格式化字符串,输出复杂时，printf更加好用，代码更易懂。\r\n\r\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \r\n\r\n awk编程\r\n 变量和赋值\r\n\r\n除了awk的内置变量，awk还可以自定义变量。\r\n\r\n下面统计/etc/passwd的账户人数\r\n\r\nawk &#039;{count++;print $0;} END{print &quot;user count is &quot;, count}&#039; /etc/passwd\r\nroot:x:0:0:root:/root:/bin/bash\r\n......\r\nuser count is  40\r\ncount是自定义变量。之前的action{}里都是只有一个print,其实print只是一个语句，而action{}可以有多个语句，以;号隔开。\r\n\r\n \r\n\r\n这里没有初始化count，虽然默认是0，但是妥当的做法还是初始化为0:\r\n\r\nawk &#039;BEGIN {count=0;print &quot;[start]user count is &quot;, count} {count=count+1;print $0;} END{print &quot;[end]user count is &quot;, count}&#039; /etc/passwd\r\n[start]user count is  0\r\nroot:x:0:0:root:/root:/bin/bash\r\n...\r\n[end]user count is  40\r\n \r\n\r\n统计某个文件夹下的文件占用的字节数\r\n\r\nls -l |awk &#039;BEGIN {size=0;} {size=size+$5;} END{print &quot;[end]size is &quot;, size}&#039;\r\n[end]size is  8657198\r\n \r\n\r\n如果以M为单位显示:\r\n\r\nls -l |awk &#039;BEGIN {size=0;} {size=size+$5;} END{print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;}&#039; \r\n[end]size is  8.25889 M\r\n注意，统计不包括文件夹的子目录。\r\n\r\n \r\n\r\n条件语句\r\n\r\n awk中的条件语句是从C语言中借鉴来的，见如下声明方式：\r\n\r\n \r\nif (expression) {\r\n    statement;\r\n    statement;\r\n    ... ...\r\n}\r\n\r\nif (expression) {\r\n    statement;\r\n} else {\r\n    statement2;\r\n}\r\n\r\nif (expression) {\r\n    statement1;\r\n} else if (expression1) {\r\n    statement2;\r\n} else {\r\n    statement3;\r\n}\r\n \r\n \r\n\r\n统计某个文件夹下的文件占用的字节数,过滤4096大小的文件(一般都是文件夹):\r\n\r\nls -l |awk &#039;BEGIN {size=0;print &quot;[start]size is &quot;, size} {if($5!=4096){size=size+$5;}} END{print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;}&#039; \r\n[end]size is  8.22339 M\r\n \r\n\r\n循环语句\r\n\r\nawk中的循环语句同样借鉴于C语言，支持while、do/while、for、break、continue，这些关键字的语义和C语言中的语义完全相同。\r\n\r\n \r\n\r\n数组\r\n\r\n  因为awk中数组的下标可以是数字和字母，数组的下标通常被称为关键字(key)。值和关键字都存储在内部的一张针对key/value应用hash的表格里。由于hash不是顺序存储，因此在显示数组内容时会发现，它们并不是按照你预料的顺序显示出来的。数组和变量一样，都是在使用时自动创建的，awk也同样会自动判断其存储的是数字还是字符串。一般而言，awk中的数组用来从记录中收集信息，可以用于计算总和、统计单词以及跟踪模板被匹配的次数等等。\r\n\r\n \r\n\r\n显示/etc/passwd的账户\r\n\r\n \r\nawk -F &#039;:&#039; &#039;BEGIN {count=0;} {name[count] = $1;count++;}; END{for (i = 0; i &lt; NR; i++) print i, name[i]}&#039; /etc/passwd\r\n0 root\r\n1 daemon\r\n2 bin\r\n3 sys\r\n4 sync\r\n5 games\r\n......\r\n \r\n这里使用for循环遍历数组\r\n\r\n \r\n\r\nawk编程的内容极多，这里只罗列简单常用的用法，更多请参考 http://www.gnu.org/software/gawk/manual/gawk.html\r\n',9,0,0,1514648047,0,0,0),(209,1,'crontab的使用','','','基本格式 : \r\n*　　*　　*　　*　　*　　command \r\n\r\n\r\n分　 时　 日　 月　 周　 命令 \r\n第1列表示分钟1～59 每分钟用*或者 */1表示 \r\n第2列表示小时0～23（0表示0点） \r\n第3列表示日期1～31 \r\n第4列表示月份1～12 \r\n第5列标识号星期0～6（0表示星期天） \r\n第6列要运行的命令 \r\ncrontab文件的一些例子： \r\n30 21 * * * /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示每晚的21:30重启apache。 \r\n45 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示每月1、10、22日的4 : 45重启apache。 \r\n10 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示每周六、周日的1 : 10重启apache。 \r\n0,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache。 \r\n0 23 * * 6 /usr/local/etc/rc.d/lighttpd restart \r\n上面的例子表示每星期六的11 : 00 pm重启apache。 \r\n* */1 * * * /usr/local/etc/rc.d/lighttpd restart \r\n每一小时重启apache \r\n* 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart \r\n晚上11点到早上7点之间，每隔一小时重启apache \r\n0 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart \r\n每月的4号与每周一到周三的11点重启apache \r\n0 4 1 jan * /usr/local/etc/rc.d/lighttpd restart \r\n一月一号的4点重启apache \r\n名称 : crontab \r\n使用权限 : 所有使用者 \r\n使用方式 : \r\ncrontab file [-u user]-用指定的文件替代目前的crontab。 \r\ncrontab-[-u user]-用标准输入替代目前的crontab. \r\ncrontab-1[user]-列出用户目前的crontab. \r\ncrontab-e[user]-编辑用户目前的crontab. \r\ncrontab-d[user]-删除用户目前的crontab. \r\ncrontab-c dir- 指定crontab的目录。 \r\ncrontab文件的格式：M H D m d cmd. \r\nM: 分钟（0-59）。 \r\nH：小时（0-23）。 \r\nD：天（1-31）。 \r\nm: 月（1-12）。 \r\nd: 一星期内的天（0~6，0为星期天）。 \r\ncmd要运行的程序，程序被送入sh执行，这个shell只有USER,HOME,SHELL这三个环境变量 \r\n说明 : \r\ncrontab 是用来让使用者在固定时间或固定间隔执行程序之用，换句话说，也就是类似使用者的时程表。-u user 是指设定指定 \r\nuser 的时程表，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设 \r\n定自己的时程表。 \r\n参数 : \r\ncrontab -e : 执行文字编辑器来设定时程表，内定的文字编辑器是 VI，如果你想用别的文字编辑器，则请先设定 VISUAL 环境变数 \r\n来指定使用那个文字编辑器(比如说 setenv VISUAL joe) \r\ncrontab -r : 删除目前的时程表 \r\ncrontab -l : 列出目前的时程表 \r\ncrontab file [-u user]-用指定的文件替代目前的crontab。 \r\n时程表的格式如下 : \r\nf1 f2 f3 f4 f5 program \r\n其中 f1 是表示分钟，f2 表示小时，f3 表示一个月份中的第几日，f4 表示月份，f5 表示一个星期中的第几天。program 表示要执 \r\n行的程序。 \r\n当 f1 为 * 时表示每分钟都要执行 program，f2 为 * 时表示每小时都要执行程序，其馀类推 \r\n当 f1 为 a-b 时表示从第 a 分钟到第 b 分钟这段时间内要执行，f2 为 a-b 时表示从第 a 到第 b 小时都要执行，其馀类推 \r\n当 f1 为 */n 时表示每 n 分钟个时间间隔执行一次，f2 为 */n 表示每 n 小时个时间间隔执行一次，其馀类推 \r\n当 f1 为 a, b, c,... 时表示第 a, b, c,... 分钟要执行，f2 为 a, b, c,... 时表示第 a, b, c...个小时要执行，其馀类推 \r\n使用者也可以将所有的设定先存放在档案 file 中，用 crontab file 的方式来设定时程表。 \r\n例子 : \r\n#每天早上7点执行一次 /bin/ls : \r\n0 7 * * * /bin/ls \r\n在 12 月内, 每天的早上 6 点到 12 点中，每隔3个小时执行一次 /usr/bin/backup : \r\n0 6-12/3 * 12 * /usr/bin/backup \r\n周一到周五每天下午 5:00 寄一封信给 alex@domain.name : \r\n0 17 * * 1-5 mail -s &quot;hi&quot; alex@domain.name &lt; /tmp/maildata \r\n每月每天的午夜 0 点 20 分, 2 点 20 分, 4 点 20 分....执行 echo &quot;haha&quot; \r\n20 0-23/2 * * * echo &quot;haha&quot; \r\n注意 : \r\n当程序在你所指定的时间执行后，系统会寄一封信给你，显示该程序执行的内容，若是你不希望收到这样的信，请在每一行空一格之 \r\n后加上 &gt; /dev/null 2&gt;&amp;1 即可 \r\n例子2 : \r\n#每天早上6点10分 \r\n10 6 * * * date \r\n#每两个小时 \r\n0 */2 * * * date \r\n#晚上11点到早上8点之间每两个小时，早上8点 \r\n0 23-7/2，8 * * * date \r\n#每个月的4号和每个礼拜的礼拜一到礼拜三的早上11点 \r\n0 11 4 * mon-wed date \r\n#1月份日早上4点 \r\n0 4 1 jan * date \r\n范例 \r\n$crontab -l 列出用户目前的crontab.',9,0,0,1514648164,0,0,0),(210,1,'iptables的使用','','','#查看帮助\r\niptables -h\r\nman iptables\r\n\r\n列出iptables规则\r\niptables -L -n\r\n列出iptables规则并显示规则编号\r\niptables -L -n --line-numbers\r\n\r\n列出iptables nat表规则（默认是filter表）\r\niptables -L -n -t nat\r\n\r\n清除默认规则（注意默认是filter表，如果对nat表操作要加-t nat）\r\n#清楚所有规则\r\niptables -F \r\n\r\n#重启iptables发现规则依然存在，因为没有保存\r\nservice iptables restart\r\n\r\n#保存配置\r\nservice iptables save\r\n\r\n#禁止ssh登陆（若果服务器在机房，一定要小心）\r\niptables -A INPUT -p tcp --dport 22 -j DROP\r\n#删除规则\r\niptables -D INPUT -p tcp --dport 22 -j DROP\r\n\r\n-A, --append chain	追加到规则的最后一条\r\n-D, --delete chain [rulenum]	Delete rule rulenum (1 = first) from chain\r\n-I, --insert chain [rulenum]	Insert in chain as rulenum (default 1=first) 添加到规则的第一条\r\n-p, --proto  proto	protocol: by number or name, eg. &#039;tcp&#039;,常用协议有tcp、udp、icmp、all\r\n-j, --jump target 常见的行为有ACCEPT、DROP和REJECT三种，但一般不用REJECT，会带来安全隐患\r\n\r\n注意：INPUT和DROP这样的关键字需要大写\r\n\r\n#禁止192.168.33.0网段从eth0网卡接入\r\niptables -A INPUT -p tcp -i eth0 -s 192.168.33.0 -j DROP\r\niptables -A INPUT -p tcp --dport 22 -i eth0 -s 192.168.33.61  -j ACCEPT\r\n\r\n#禁止ip地址非192.168.10.10的所有类型数据接入\r\niptables -A INPUT ! -s 192.168.10.10 -j DROP\r\n\r\n#禁止ip地址非192.168.10.10的ping请求\r\niptables -I INPUT -p icmp --icmp-type 8 -s 192.168.50.100 -j DROP\r\n\r\n#扩展匹配：1.隐式扩展 2.显示扩展\r\n	#隐式扩展\r\n	-p tcp\r\n		--sport PORT 源端口\r\n		--dport PORT 目标端口\r\n\r\n	#显示扩展：使用额外的匹配规则\r\n	-m EXTENSTION --SUB-OPT\r\n	-p tcp --dport 22 与 -p tcp -m tcp --dport 22功能相同\r\n\r\n	state：状态扩展，接口ip_contrack追踪会话状态\r\n		NEW：新的连接请求\r\n		ESTABLISHED：已建立的连接请求\r\n		INVALID：非法连接\r\n		RELATED：相关联的连接\r\n	\r\n\r\n#匹配端口范围\r\niptables -I INPUT -p tcp --dport 22:80 -j DROP\r\n\r\n#匹配多个端口\r\niptables -I INPUT -p tcp -m multiport --dport 22,80,3306 -j ACCEPT\r\n\r\n#不允许源端口为80的数据流出\r\niptables -I OUTPUT -p tcp --sport 80 -j DROP\r\n',9,0,0,1514648205,0,0,0),(211,1,'sed简介','','','1. Sed简介\r\nsed 是一种在线编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。以下介绍的是Gnu版本的Sed 3.02。\r\n2. 定址\r\n可以通过定址来定位你所希望编辑的行，该地址用数字构成，用逗号分隔的两个行数表示以这两行为起止的行的范围（包括行数表示的那两行）。如1，3表示1，2，3行，美元符号($)表示最后一行。范围可以通过数据，正则表达式或者二者结合的方式确定 。\r\n\r\n3. Sed命令\r\n调用sed命令有两种形式：\r\n*\r\nsed [options] &#039;command&#039; file(s)\r\n*\r\nsed [options] -f scriptfile file(s)\r\na\\\r\n在当前行后面加入一行文本。\r\nb lable\r\n分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。\r\nc\\\r\n用新的文本改变本行的文本。\r\nd\r\n从模板块（Pattern space）位置删除行。\r\nD\r\n删除模板块的第一行。\r\ni\\\r\n在当前行上面插入文本。\r\nh\r\n拷贝模板块的内容到内存中的缓冲区。\r\nH\r\n追加模板块的内容到内存中的缓冲区\r\ng\r\n获得内存缓冲区的内容，并替代当前模板块中的文本。\r\nG\r\n获得内存缓冲区的内容，并追加到当前模板块文本的后面。\r\nl\r\n列表不能打印字符的清单。\r\nn\r\n读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。\r\nN\r\n追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。\r\np\r\n打印模板块的行。\r\nP（大写）\r\n打印模板块的第一行。\r\nq\r\n退出Sed。\r\nr file\r\n从file中读行。\r\nt label\r\nif分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。\r\nT label\r\n错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。\r\nw file\r\n写并追加模板块到file末尾。\r\nW file\r\n写并追加模板块的第一行到file末尾。\r\n!\r\n表示后面的命令对所有没有被选定的行发生作用。\r\ns/re/string\r\n用string替换正则表达式re。\r\n=\r\n打印当前行号码。\r\n#\r\n把注释扩展到下一个换行符以前。\r\n以下的是替换标记\r\n*\r\ng表示行内全面替换。\r\n*\r\np表示打印行。\r\n*\r\nw表示把行写入一个文件。\r\n*\r\nx表示互换模板块中的文本和缓冲区中的文本。\r\n*\r\ny表示把一个字符翻译为另外的字符（但是不用于正则表达式）\r\n\r\n4. 选项\r\n-e command, --expression=command\r\n允许多台编辑。\r\n-h, --help\r\n打印帮助，并显示bug列表的地址。\r\n-n, --quiet, --silent\r\n取消默认输出。\r\n-f, --filer=script-file\r\n引导sed脚本文件名。\r\n-V, --version\r\n打印版本和版权信息。\r\n\r\n5. 元字符集^\r\n锚定行的开始 如：/^sed/匹配所有以sed开头的行。 \r\n$\r\n锚定行的结束 如：/sed$/匹配所有以sed结尾的行。 \r\n.\r\n匹配一个非换行符的字符 如：/s.d/匹配s后接一个任意字符，然后是d。 \r\n*\r\n匹配零或多个字符 如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。 \r\n[]\r\n匹配一个指定范围内的字符，如/[Ss]ed/匹配sed和Sed。 \r\n[^]\r\n匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。 \r\n\\(..\\)\r\n保存匹配的字符，如s/\\(love\\)able/\\1rs，loveable被替换成lovers。 \r\n&amp;\r\n保存搜索字符用来替换其他字符，如s/love/**&amp;**/，love这成**love**。 \r\n\\&lt;\r\n锚定单词的开始，如:/\\&lt;love/匹配包含以love开头的单词的行。 \r\n\\&gt;\r\n锚定单词的结束，如/love\\&gt;/匹配包含以love结尾的单词的行。 \r\nx\\{m\\}\r\n重复字符x，m次，如：/o\\{5\\}/匹配包含5个o的行。 \r\nx\\{m,\\}\r\n重复字符x,至少m次，如：/o\\{5,\\}/匹配至少有5个o的行。 \r\nx\\{m,n\\}\r\n重复字符x，至少m次，不多于n次，如：/o\\{5,10\\}/匹配5--10个o的行。\r\n6. 实例\r\n删除：d命令\r\n*\r\n$ sed &#039;2d&#039; example-----删除example文件的第二行。\r\n*\r\n$ sed &#039;2,$d&#039; example-----删除example文件的第二行到末尾所有行。\r\n*\r\n$ sed &#039;$d&#039; example-----删除example文件的最后一行。\r\n*\r\n$ sed &#039;/test/&#039;d example-----删除example文件所有包含test的行。\r\n替换：s命令\r\n*\r\n$ sed &#039;s/test/mytest/g&#039; example-----在整行范围内把test替换为mytest。如果没有g标记，则只有每行第一个匹配的test被替换成mytest。\r\n*\r\n$ sed -n &#039;s/^test/mytest/p&#039; example-----(-n)选项和p标志一起使用表示只打印那些发生替换的行。也就是说，如果某一行开头的test被替换成mytest，就打印它。\r\n*\r\n$ sed &#039;s/^192.168.0.1/&amp;localhost/&#039; example-----&amp;符号表示替换换字符串中被找到的部份。所有以192.168.0.1开头的行都会被替换成它自已加 localhost，变成192.168.0.1localhost。\r\n*\r\n$ sed -n &#039;s/\\(love\\)able/\\1rs/p&#039; example-----love被标记为1，所有loveable会被替换成lovers，而且替换的行会被打印出来。\r\n*\r\n$ sed &#039;s#10#100#g&#039; example-----不论什么字符，紧跟着s命令的都被认为是新的分隔符，所以，“#”在这里是分隔符，代替了默认的“/”分隔符。表示把所有10替换成100。\r\n选定行的范围：逗号\r\n*\r\n$ sed -n &#039;/test/,/check/p&#039; example-----所有在模板test和check所确定的范围内的行都被打印。\r\n*\r\n$ sed -n &#039;5,/^test/p&#039; example-----打印从第五行开始到第一个包含以test开始的行之间的所有行。\r\n*\r\n$ sed &#039;/test/,/check/s/$/sed test/&#039; example-----对于模板test和west之间的行，每行的末尾用字符串sed test替换。\r\n多点编辑：e命令\r\n*\r\n$ sed -e &#039;1,5d&#039; -e &#039;s/test/check/&#039; example-----(-e)选项允许在同一行里执行多条命令。如例子所示，第一条命令删除1至5行，第二条命令用check替换test。命令的执 行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。\r\n*\r\n$ sed --expression=&#039;s/test/check/&#039; --expression=&#039;/love/d&#039; example-----一个比-e更好的命令是--expression。它能给sed表达式赋值。\r\n从文件读入：r命令\r\n*\r\n$ sed &#039;/test/r file&#039; example-----file里的内容被读进来，显示在与test匹配的行后面，如果匹配多行，则file的内容将显示在所有匹配行的下面。\r\n写入文件：w命令\r\n*\r\n$ sed -n &#039;/test/w file&#039; example-----在example中所有包含test的行都被写入file里。\r\n追加命令：a命令\r\n*\r\n$ sed &#039;/^test/a\\\\---&gt;this is a example&#039; example    &#039;-----&gt;this is a example&#039;被追加到以test开头的行后面，sed要求命令a后面有一个反斜杠。\r\n插入：i命令\r\n$ sed &#039;/test/i\\\\\r\nnew line\r\n-------------------------&#039; example\r\n如果test被匹配，则把反斜杠后面的文本插入到匹配行的前面。\r\n下一个：n命令\r\n*\r\n$ sed &#039;/test/{ n; s/aa/bb/; }&#039; example-----如果test被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb，并打印该行，然后继续。\r\n变形：y命令\r\n*\r\n$ sed &#039;1,10y/abcde/ABCDE/&#039; example-----把1--10行内所有abcde转变为大写，注意，正则表达式元字符不能使用这个命令。\r\n退出：q命令\r\n*\r\n$ sed &#039;10q&#039; example-----打印完第10行后，退出sed。\r\n保持和获取：h命令和G命令\r\n*\r\n$ sed -e &#039;/test/h&#039; -e &#039;$G example-----在sed处理文件的时候，每一行都被保存在一个叫模式空间的临时缓冲区中，除非行被删除或者输出被取消，否则所有被处理的行都将 打印在屏幕上。接着模式空间被清空，并存入新的一行等待处理。在这个例子里，匹配test的行被找到后，将存入模式空间，h命令将其复制并存入一个称为保 持缓存区的特殊缓冲区内。第二条语句的意思是，当到达最后一行后，G命令取出保持缓冲区的行，然后把它放回模式空间中，且追加到现在已经存在于模式空间中 的行的末尾。在这个例子中就是追加到最后一行。简单来说，任何包含test的行都被复制并追加到该文件的末尾。\r\n保持和互换：h命令和x命令\r\n*\r\n$ sed -e &#039;/test/h&#039; -e &#039;/check/x&#039; example -----互换模式空间和保持缓冲区的内容。也就是把包含test与check的行互换。\r\n7. 脚本\r\nSed脚本是一个sed的命令清单，启动Sed时以-f选项引导脚本文件名。Sed对于脚本中输入的命令非常挑剔，在命令的末尾不能有任何空白或文本，如果在一行中有多个命令，要用分号分隔。以#开头的行为注释行，且不能跨行。\r\n',9,0,0,1514648285,0,0,0),(212,1,'sort的使用','','','sort\r\nsort 命令对 File 参数指定的文件中的行排序，并将结果写到标准输出。如果 File 参数指定多个文件，那么 sort 命令将这些文件连接起来，并当作一个文件进行排序。\r\n\r\nsort语法\r\n\r\n \r\n[root@www ~]# sort [-fbMnrtuk] [file or stdin]\r\n选项与参数：\r\n-f  ：忽略大小写的差异，例如 A 与 a 视为编码相同；\r\n-b  ：忽略最前面的空格符部分；\r\n-M  ：以月份的名字来排序，例如 JAN, DEC 等等的排序方法；\r\n-n  ：使用『纯数字』进行排序(默认是以文字型态来排序的)；\r\n-r  ：反向排序；\r\n-u  ：就是 uniq ，相同的数据中，仅出现一行代表；\r\n-t  ：分隔符，默认是用 [tab] 键来分隔；\r\n-k  ：以那个区间 (field) 来进行排序的意思\r\n \r\n\r\n对/etc/passwd 的账号进行排序\r\n[root@www ~]# cat /etc/passwd | sort\r\nadm:x:3:4:adm:/var/adm:/sbin/nologin\r\napache:x:48:48:Apache:/var/www:/sbin/nologin\r\nbin:x:1:1:bin:/bin:/sbin/nologin\r\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\r\nsort 是默认以第一个数据来排序，而且默认是以字符串形式来排序,所以由字母 a 开始升序排序。\r\n\r\n \r\n\r\n/etc/passwd 内容是以 : 来分隔的，我想以第三栏来排序，该如何\r\n\r\n[root@www ~]# cat /etc/passwd | sort -t &#039;:&#039; -k 3\r\nroot:x:0:0:root:/root:/bin/bash\r\nuucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\r\noperator:x:11:0:operator:/root:/sbin/nologin\r\nbin:x:1:1:bin:/bin:/sbin/nologin\r\ngames:x:12:100:games:/usr/games:/sbin/nologin\r\n默认是以字符串来排序的，如果想要使用数字排序：\r\n\r\ncat /etc/passwd | sort -t &#039;:&#039; -k 3n\r\nroot:x:0:0:root:/root:/bin/bash\r\ndaemon:x:1:1:daemon:/usr/sbin:/bin/sh\r\nbin:x:2:2:bin:/bin:/bin/sh\r\n默认是升序排序，如果要倒序排序，如下\r\n\r\ncat /etc/passwd | sort -t &#039;:&#039; -k 3nr\r\nnobody:x:65534:65534:nobody:/nonexistent:/bin/sh\r\nntp:x:106:113::/home/ntp:/bin/false\r\nmessagebus:x:105:109::/var/run/dbus:/bin/false\r\nsshd:x:104:65534::/var/run/sshd:/usr/sbin/nologin\r\n \r\n\r\n如果要对/etc/passwd,先以第六个域的第2个字符到第4个字符进行正向排序，再基于第一个域进行反向排序。\r\n\r\ncat /etc/passwd |  sort -t&#039;:&#039; -k 6.2,6.4 -k 1r      \r\nsync:x:4:65534:sync:/bin:/bin/sync\r\nproxy:x:13:13:proxy:/bin:/bin/sh\r\nbin:x:2:2:bin:/bin:/bin/sh\r\nsys:x:3:3:sys:/dev:/bin/sh\r\n \r\n\r\n查看/etc/passwd有多少个shell:对/etc/passwd的第七个域进行排序，然后去重:\r\n\r\ncat /etc/passwd |  sort -t&#039;:&#039; -k 7 -u\r\nroot:x:0:0:root:/root:/bin/bash\r\nsyslog:x:101:102::/home/syslog:/bin/false\r\ndaemon:x:1:1:daemon:/usr/sbin:/bin/sh\r\nsync:x:4:65534:sync:/bin:/bin/sync\r\nsshd:x:104:65534::/var/run/sshd:/usr/sbin/nologin\r\n \r\n\r\nuniq\r\n uniq命令可以去除排序过的文件中的重复行，因此uniq经常和sort合用。也就是说，为了使uniq起作用，所有的重复行必须是相邻的。\r\n\r\nuniq语法\r\n\r\n[root@www ~]# uniq [-icu]\r\n选项与参数：\r\n-i   ：忽略大小写字符的不同；\r\n-c  ：进行计数\r\n-u  ：只显示唯一的行\r\n \r\n\r\ntestfile的内容如下\r\n\r\n \r\ncat testfile\r\nhello\r\nworld\r\nfriend\r\nhello\r\nworld\r\nhello\r\n \r\n \r\n\r\n直接删除未经排序的文件，将会发现没有任何行被删除\r\n\r\n \r\n#uniq testfile  \r\nhello\r\nworld\r\nfriend\r\nhello\r\nworld\r\nhello\r\n \r\n \r\n\r\n排序文件，默认是去重\r\n\r\n#cat testfile | sort |uniq\r\nfriend\r\nhello\r\nworld\r\n \r\n\r\n排序之后删除了重复行，同时在行首位置输出该行重复的次数\r\n\r\n#sort testfile | uniq -c\r\n1 friend\r\n3 hello\r\n2 world\r\n \r\n\r\n仅显示存在重复的行，并在行首显示该行重复的次数\r\n\r\n#sort testfile | uniq -dc\r\n3 hello\r\n2 world\r\n \r\n\r\n仅显示不重复的行\r\n\r\nsort testfile | uniq -u\r\nfriend  \r\n \r\n\r\ncut\r\ncut命令可以从一个文本文件或者文本流中提取文本列。\r\n\r\ncut语法\r\n\r\n[root@www ~]# cut -d&#039;分隔字符&#039; -f fields &lt;==用于有特定分隔字符\r\n[root@www ~]# cut -c 字符区间            &lt;==用于排列整齐的信息\r\n选项与参数：\r\n-d  ：后面接分隔字符。与 -f 一起使用；\r\n-f  ：依据 -d 的分隔字符将一段信息分割成为数段，用 -f 取出第几段的意思；\r\n-c  ：以字符 (characters) 的单位取出固定字符区间；\r\n \r\n\r\nPATH 变量如下\r\n\r\n[root@www ~]# echo $PATH\r\n/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/X11R6/bin:/usr/games\r\n# 1 | 2       | 3   | 4       | 5            | 6            | 7\r\n \r\n\r\n将 PATH 变量取出，我要找出第五个路径。\r\n\r\n#echo $PATH | cut -d &#039;:&#039; -f 5\r\n/usr/local/bin\r\n \r\n\r\n将 PATH 变量取出，我要找出第三和第五个路径。\r\n\r\n#echo $PATH | cut -d &#039;:&#039; -f 3,5\r\n/sbin:/usr/local/bin\r\n \r\n\r\n将 PATH 变量取出，我要找出第三到最后一个路径。\r\n\r\necho $PATH | cut -d &#039;:&#039; -f 3-\r\n/sbin:/usr/sbin:/usr/local/bin:/usr/X11R6/bin:/usr/games\r\n \r\n\r\n将 PATH 变量取出，我要找出第一到第三个路径。\r\n\r\n#echo $PATH | cut -d &#039;:&#039; -f 1-3\r\n/bin:/usr/bin:/sbin:\r\n \r\n \r\n\r\n将 PATH 变量取出，我要找出第一到第三，还有第五个路径。\r\n\r\necho $PATH | cut -d &#039;:&#039; -f 1-3,5\r\n/bin:/usr/bin:/sbin:/usr/local/bin\r\n \r\n\r\n实用例子:只显示/etc/passwd的用户和shell\r\n\r\n#cat /etc/passwd | cut -d &#039;:&#039; -f 1,7 \r\nroot:/bin/bash\r\ndaemon:/bin/sh\r\nbin:/bin/sh\r\n \r\n\r\n wc\r\n统计文件里面有多少单词，多少行，多少字符。\r\n\r\nwc语法\r\n\r\n[root@www ~]# wc [-lwm]\r\n选项与参数：\r\n-l  ：仅列出行；\r\n-w  ：仅列出多少字(英文单字)；\r\n-m  ：多少字符；\r\n \r\n\r\n默认使用wc统计/etc/passwd\r\n\r\n#wc /etc/passwd\r\n40   45 1719 /etc/passwd\r\n40是行数，45是单词数，1719是字节数\r\n\r\n \r\n\r\nwc的命令比较简单使用，每个参数使用如下：\r\n\r\n \r\n#wc -l /etc/passwd   #统计行数，在对记录数时，很常用\r\n40 /etc/passwd       #表示系统有40个账户\r\n\r\n#wc -w /etc/passwd  #统计单词出现次数\r\n45 /etc/passwd\r\n\r\n#wc -m /etc/passwd  #统计文件的字符数\r\n1719',9,0,0,1514648412,0,0,0),(213,1,'特殊文件: /dev/null和/dev/tty','','','    Linux系统提供了两个对Shell编程非常有用的特殊文件，/dev/null和/dev/tty。其中/dev/null将会丢掉所有写入它的数据，换句换说，当程序将数据写入到此文件时，会认为它已经成功完成写入数据的操作，但实际上什么事都没有做。如果你需要的是命令的退出状态，而非它的输出，此功能会非常有用，见如下Shell代码：\r\n    /&gt; vi test_dev_null.sh\r\n    \r\n    #!/bin/bash\r\n    if grep hello TestFile &gt; /dev/null\r\n    then\r\n        echo &quot;Found&quot;\r\n    else\r\n        echo &quot;NOT Found&quot;\r\n    fi\r\n    在vi中保存并退出后执行以下命令：\r\n    /&gt; chmod +x test_dev_null.sh  #使该文件成为可执行文件\r\n    /&gt; cat &gt; TestFile\r\n    hello my friend\r\n    CTRL + D                             #退出命令行文件编辑状态\r\n    /&gt; ./test_dev_null.sh\r\n    Found                                 #这里并没有输出grep命令的执行结果。\r\n    将以上Shell脚本做如下修改：\r\n    /&gt; vi test_dev_null.sh\r\n    \r\n    #!/bin/bash\r\n    if grep hello TestFile\r\n    then\r\n        echo &quot;Found&quot;\r\n    else\r\n        echo &quot;NOT Found&quot;\r\n    fi\r\n    在vi中保存退出后，再次执行该脚本：\r\n    /&gt; ./test_dev_null.sh\r\n    hello my friend                      #grep命令的执行结果被输出了。\r\n    Found\r\n    \r\n    下面我们再来看/dev/tty的用途。当程序打开此文件是，Linux会自动将它重定向到一个终端窗口，因此该文件对于读取人工输入时特别有用。见如下Shell代码：\r\n    /&gt; vi test_dev_tty.sh\r\n    \r\n    #!/bin/bash\r\n    printf &quot;Enter new password: &quot;    #提示输入\r\n    stty -echo                               #关闭自动打印输入字符的功能\r\n    read password &lt; /dev/tty         #读取密码\r\n    printf &quot;\\nEnter again: &quot;             #换行后提示再输入一次\r\n    read password2 &lt; /dev/tty       #再读取一次以确认\r\n    printf &quot;\\n&quot;                               #换行\r\n    stty echo                                #记着打开自动打印输入字符的功能\r\n    echo &quot;Password = &quot; $password #输出读入变量\r\n    echo &quot;Password2 = &quot; $password2\r\n    echo &quot;All Done&quot;\r\n\r\n    在vi中保存并退出后执行以下命令：\r\n    /&gt; chmod +x test_dev_tty.sh #使该文件成为可执行文件\r\n    /&gt; ./test_dev_tty\r\n    Enter new password:             #这里密码的输入被读入到脚本中的password变量\r\n    Enter again:                          #这里密码的输入被读入到脚本中的password2变量\r\n    Password = hello\r\n    Password2 = hello\r\n    All Done',9,0,0,1514648447,0,0,0),(214,1,'简单的命令跟踪','','','  Linux Shell提供了两种方式来跟踪Shell脚本中的命令，以帮助我们准确的定位程序中存在的问题。下面的代码为第一种方式，该方式会将Shell脚本中所有被执行的命令打印到终端，并在命令前加&quot;+&quot;：加号的后面还跟着一个空格。\r\n    /&gt; cat &gt; trace_all_command.sh\r\n    who | wc -l                          #这两条Shell命令将输出当前Linux服务器登录的用户数量\r\n    CTRL + D                            #退出命令行文件编辑状态\r\n    /&gt; chmod +x trace_all_command.sh\r\n    /&gt; sh -x ./trace_all_command.sh #Shell执行器的-x选项将打开脚本的执行跟踪功能。\r\n    + wc -l                               #被跟踪的两条Shell命令\r\n    + who\r\n    2                                       #实际输出结果。\r\n    Linux Shell提供的另一种方式可以只打印部分被执行的Shell命令，该方法在调试较为复杂的脚本时，显得尤为有用。\r\n    /&gt; cat &gt; trace_patial_command.sh\r\n    #! /bin/bash\r\n    set -x                                #从该命令之后打开跟踪功能\r\n    echo 1st echo                     #将被打印输出的Shell命令\r\n    set +x                               #该Shell命令也将被打印输出，然而在该命令被执行之后，所有的命令将不再打印输出\r\n    echo 2nd echo                    #该Shell命令将不再被打印输出。\r\n    CTRL + D                           #退出命令行文件编辑状态\r\n    /&gt; chmod +x trace_patial_command.sh\r\n    /&gt; ./trace_patial_command.sh\r\n    + echo 1st echo\r\n    1st echo\r\n    + set +x\r\n    2nd echo',9,0,0,1514648475,0,0,0),(215,1,'正则表达式基本语法描述','','','Linux Shell环境下提供了两种正则表达式规则，一个是基本正则表达式(BRE)，另一个是扩展正则表达式(ERE)。\r\n    下面是这两种表达式的语法列表，需要注意的是，如果没有明确指出的Meta字符，其将可同时用于BRE和ERE，否则将尽适用于指定的模式。\r\n\r\n正则元字符	模式含义	用例\r\n\\	通常用于关闭其后续字符的特殊意义，恢复其原意。	\\(...\\)，这里的括号仅仅表示括号。\r\n.	匹配任何单个字符。	a.b，将匹配abb、acb等\r\n*	匹配它之前的0-n个的单个字符。	a*b，将匹配ab、aab、aaab等。\r\n^	匹配紧接着的正则表达式，在行的起始处。	^ab，将匹配abc、abd等，但是不匹配cab。\r\n$	匹配紧接着的正则表达式，在行的结尾处。	ab$，将匹配ab、cab等，但是不匹配abc。\r\n[...]	方括号表达式，匹配其内部任何字符。其中-表示连续字符的范围，^符号置于方括号里第一个字符则有反向的含义，即匹配不在列表内(方括号)的任何字符。如果想让]和-表示其原意，需要将其放置在方括号的首字符位置，如[]ab]或[-ab]，如这两个字符同时存在，则将]放置在首字符位置，-放置在最尾部，如[]ab-]。	[a-bA-Z0-9!]表示所有的大小写字母，数字和感叹号。[^abc]表示a、b、c之外的所有字符。[Tt]om，可以匹配Tom和tom。\r\n\\{n,m\\}	区间表达式，匹配在它前面的单个字符重复出现的次数区间，\\{n\\}表示重复n次；\\{n,\\}表示至少重复n次；\\{n,m\\}表示重复n到m次。	ab\\{2\\}表示abb；ab\\{2,\\}表示abb、abbb等。ab\\{2,4\\}表示abb、abbb和abbbb。\r\n\\(...\\)	将圆括号之间的模式存储在特殊“保留空间”。最多可以将9个独立的子模式存储在单个模式中。匹配于子模式的文本，可以通过转义序列\\1到\\9，被重复使用在相同模式里。	\\(ab\\).*\\1表示ab组合出现两次，两次之间可存在任何数目的任何字符，如abcdab、abab等。\r\n{n,m}(ERE)	其功能等同于上面的\\{n,m\\}，只是不再写\\转义符了。	ab+匹配ab、abbb等，但是不匹配a。\r\n+(ERE)	和前面的星号相比，+匹配的是前面正则表达式的1-n个实例。	 \r\n?(ERE)	匹配前面正则表达式的0个或1个。	ab?仅匹配a或ab。\r\n|(ERE)	匹配于|符号前后的正则表达式。	(ab|cd)匹配ab或cd。\r\n[:alpha:]	匹配字母字符。	[[:alpha:]!]ab$匹配cab、dab和!ab。\r\n[:alnum:]	匹配字母和数字字符。	[[:alnum:]]ab$匹配1ab、aab。\r\n[:blank:]	匹配空格(space)和Tab字符。	[[:alnum:]]ab$匹配1ab、aab。\r\n[:cntrl:]	匹配控制字符。	 \r\n[:digit:]	匹配数字字符。	 \r\n[:graph:]	匹配非空格字符。	 \r\n[:lower:]	匹配小写字母字符。	 \r\n[:upper:]	匹配大写字母字符。	 \r\n[:punct:]	匹配标点字符。	 \r\n[:space:]	匹配空白(whitespace)字符。	 \r\n[:xdigit:]	匹配十六进制数字。	 \r\n\\w	匹配任何字母和数字组成的字符，等同于[[:alnum:]_]	 \r\n\\W	匹配任何非字母和数字组成的字符，等同于[^[:alnum:]_]	 \r\n\\&lt;\\&gt;	匹配单词的起始和结尾。	\\&lt;read匹配readme，me\\&gt;匹配readme。',9,0,0,1514648506,0,0,0),(216,1,'使用cut命令选定字段','','','    cut命令是用来剪下文本文件里的数据，文本文件可以是字段类型或是字符类型。下面给出应用实例：\r\n    /&gt; cat /etc/passwd\r\n    root:x:0:0:root:/root:/bin/bash\r\n    bin:x:1:1:bin:/bin:/sbin/nologin\r\n    daemon:x:2:2:daemon:/sbin:/sbin/nologin\r\n    adm:x:3:4:adm:/var/adm:/sbin/nologin\r\n    ... ...\r\n    /&gt; cut -d : -f 1,5 /etc/passwd     #-d后面的冒号表示字段之间的分隔符，-f表示取分割后的哪些字段\r\n    root:root                                 #这里取出的是第一个和第五个字段。\r\n    bin:bin\r\n    daemon:daemon\r\n    adm:adm\r\n    ... ...\r\n    /&gt; cut -d: -f 3- /etc/passwd       #从第三个字段开始显示，直到最后一个字段。\r\n    0:0:root:/root:/bin/bash\r\n    1:1:bin:/bin:/sbin/nologin\r\n    2:2:daemon:/sbin:/sbin/nologin\r\n    3:4:adm:/var/adm:/sbin/nologin\r\n    4:7:lp:/var/spool/lpd:/sbin/nologin\r\n    ... ...    \r\n    这里需要进一步说明的是，使用cut命令还可以剪切以字符数量为标量的部分字符，该功能通过-c选项实现，其不能与-d选项共存。\r\n    /&gt; cut -c 1-4 /etc/passwd          #取每行的前1-4个字符。\r\n    /&gt; cut -c-4 /etc/passwd            #取每行的前4个字符。 \r\n    root\r\n    bin:\r\n    daem\r\n    adm:\r\n    ... ...\r\n    /&gt; cut -c4- /etc/passwd            #取每行的第4个到最后字符。\r\n    t:x:0:0:root:/root:/bin/bash\r\n    :x:1:1:bin:/bin:/sbin/nologin\r\n    mon:x:2:2:daemon:/sbin:/sbin/nologin\r\n    :x:3:4:adm:/var/adm:/sbin/nologin\r\n    ... ...\r\n    /&gt; cut -c1,4 /etc/passwd           #取每行的第一个和第四个字符。\r\n    rt\r\n    b:\r\n    dm\r\n    a:\r\n    ... ...\r\n    /&gt; cut -c1-4,5 /etc/passwd        #取每行的1-4和第5个字符。\r\n    root:\r\n    bin:x\r\n    daemo\r\n    adm:x',9,0,0,1514648533,0,0,0),(217,1,'计算行数、字数以及字符数','','','    Linux提供了一个简单的工具wc用于完成该功能，见如下用例：\r\n    /&gt; echo This is a test of the emergency broadcast system | wc\r\n    1    9    49                              #1行，9个单词，49个字符\r\n    /&gt; echo Testing one two three | wc -c\r\n    22                                         #22个字符\r\n    /&gt; echo Testing one two three | wc -l\r\n    1                                           #1行\r\n    /&gt; echo Testing one two three | wc -w\r\n    4                                           #4个单词\r\n    /&gt; wc /etc/passwd /etc/group    #计算两个文件里的数据。\r\n    39   71  1933  /etc/passwd\r\n    62   62  906    /etc/group\r\n    101 133 2839  总用量',9,0,0,1514648551,0,0,0),(218,1,'提取开头或结尾数行','','','    有时，你会需要从文本文件里把几行字，多半是靠近开头或结尾的几行提取出来。如查看工作日志等操作。Linux Shell提供head和tail两个命令来完成此项工作。见如下用例：\r\n    /&gt; head -n 5 /etc/passwd           #显示输入文件的前五行。\r\n    root:x:0:0:root:/root:/bin/bash\r\n    bin:x:1:1:bin:/bin:/sbin/nologin\r\n    daemon:x:2:2:daemon:/sbin:/sbin/nologin\r\n    adm:x:3:4:adm:/var/adm:/sbin/nologin\r\n    lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\r\n\r\n    /&gt; tail -n 5 /etc/passwd             #显示输入文件的最后五行。\r\n    sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\r\n    mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/bash\r\n    pulse:x:496:494:PulseAudio System Daemon:/var/run/pulse:/sbin/nologin\r\n    gdm:x:42:42::/var/lib/gdm:/sbin/nologin\r\n    stephen:x:500:500:stephen:/home/stephen:/bin/bash\r\n    如果使用者想查看不间断增长的日志(如服务程序输出的)，可以使用tail的-f选项，这样可以让tail命令不会自动退出，必须通过CTRL+C命令强制退出，因此该选项不适合用于Shell脚本中，见如下用例：\r\n    /&gt; tail -f -n 5 my_server_log',9,0,0,1514648569,0,0,0),(219,1,'日常操作命令 ','','','**查看当前所在的工作目录\r\npwd\r\n\r\n**查看当前系统的时间 \r\ndate\r\n\r\n**查看有谁在线（哪些人登陆到了服务器）\r\nwho  查看当前在线\r\nlast 查看最近的登陆历史记录',9,0,0,1514648755,0,0,0),(220,1,'文件系统操作','','','ls /    查看根目录下的子节点（文件夹和文件）信息\r\nls -al  -a是显示隐藏文件   -l是以更详细的列表形式显示\r\n\r\n**切换目录\r\ncd  /home\r\n\r\n**创建文件夹\r\nmkdir aaa     这是相对路径的写法 \r\nmkdir -p aaa/bbb/ccc\r\nmkdir  /data    这是绝对路径的写法 \r\n\r\n**删除文件夹\r\nrmdir   可以删除空目录\r\nrm -r aaa   可以把aaa整个文件夹及其中的所有子节点全部删除\r\nrm -rf aaa   强制删除aaa\r\n\r\n**修改文件夹名称\r\nmv aaa angelababy\r\n\r\n**创建文件\r\ntouch  somefile.1   创建一个空文件\r\necho &quot;i miss you,my baby&quot; &gt; somefile.2  利用重定向“&gt;”的功能，将一条指令的输出结果写入到一个文件中，会覆盖原文件内容\r\necho &quot;huangxiaoming ,gun dan&quot; &gt;&gt; somefile.2     将一条指令的输出结果追加到一个文件中，不会覆盖原文件内容\r\n',9,0,0,1514648785,0,0,0),(221,1,'文件权限的操作','','','****linux文件权限的描述格式解读\r\ndrwxr-xr-x      （也可以用二进制表示  111 101 101  --&gt;  755）\r\n\r\nd：标识节点类型（d：文件夹   -：文件  l:链接）\r\nr：可读   w：可写    x：可执行 \r\n第一组rwx：  表示这个文件的拥有者对它的权限：可读可写可执行\r\n第二组r-x：  表示这个文件的所属组对它的权限：可读，不可写，可执行\r\n第三组r-x：  表示这个文件的其他用户（相对于上面两类用户）对它的权限：可读，不可写，可执行\r\n\r\n\r\n****修改文件权限\r\nchmod g-rw haha.dat    表示将haha.dat对所属组的rw权限取消\r\nchmod o-rw haha.dat 	表示将haha.dat对其他人的rw权限取消\r\nchmod u+x haha.dat      表示将haha.dat对所属用户的权限增加x\r\n\r\n也可以用数字的方式来修改权限\r\nchmod 664 haha.dat   \r\n就会修改成   rw-rw-r--\r\n\r\n如果要将一个文件夹的所有内容权限统一修改，则可以-R参数\r\nchmod -R 770 aaa/\r\nchown angela:angela aaa/    &lt;只有root能执行&gt;',9,0,0,1514649921,0,0,0),(222,1,'基本的用户管理','','','*****添加用户\r\nuseradd  angela\r\n要修改密码才能登陆 \r\npasswd angela  按提示输入密码即可\r\n\r\n\r\n**为用户配置sudo权限\r\n用root编辑 vi /etc/sudoers\r\n在文件的如下位置，为hadoop添加一行即可\r\nroot    ALL=(ALL)       ALL     \r\nhadoop  ALL=(ALL)       ALL\r\n\r\n然后，hadoop用户就可以用sudo来执行系统级别的指令\r\n[hadoop@shizhan ~]$ sudo useradd huangxiaoming',9,0,0,1514650033,0,0,0),(223,1,'系统管理操作','','','*****查看主机名\r\nhostname\r\n****修改主机名(重启后无效)\r\nhostname hadoop\r\n\r\n*****修改主机名(重启后永久生效)\r\nvi /ect/sysconfig/network\r\n****修改IP(重启后无效)\r\nifconfig eth0 192.168.12.22\r\n\r\n****修改IP(重启后永久生效)\r\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\r\n\r\n\r\nmount ****  挂载外部存储设备到文件系统中\r\nmkdir   /mnt/cdrom      创建一个目录，用来挂载\r\nmount -t iso9660 -o ro /dev/cdrom /mnt/cdrom/     将设备/dev/cdrom挂载到 挂载点 ：  /mnt/cdrom中\r\n\r\n*****umount\r\numount /mnt/cdrom\r\n\r\n\r\n*****统计文件或文件夹的大小\r\ndu -sh  /mnt/cdrom/Packages\r\ndf -h    查看磁盘的空间\r\n****关机\r\nhalt\r\n****重启\r\nreboot\r\n\r\n\r\n******配置主机之间的免密ssh登陆\r\n假如 A  要登陆  B\r\n在A上操作：\r\n%%首先生成密钥对\r\nssh-keygen   (提示时，直接回车即可)\r\n%%再将A自己的公钥拷贝并追加到B的授权列表文件authorized_keys中\r\nssh-copy-id   B\r\n\r\n\r\n\r\n******后台服务管理\r\nservice network status   查看指定服务的状态\r\nservice network stop     停止指定服务\r\nservice network start    启动指定服务\r\nservice network restart  重启指定服务\r\nservice --status-all  查看系统中所有的后台服务\r\n\r\n设置后台服务的自启配置\r\nchkconfig   查看所有服务器自启配置\r\nchkconfig iptables off   关掉指定服务的自动启动\r\nchkconfig iptables on   开启指定服务的自动启动\r\n\r\n\r\n*****系统启动级别管理\r\nvi  /etc/inittab\r\n\r\n# Default runlevel. The runlevels used are:\r\n#   0 - halt (Do NOT set initdefault to this)\r\n#   1 - Single user mode\r\n#   2 - Multiuser, without NFS (The same as 3, if you do not have networking)\r\n#   3 - Full multiuser mode\r\n#   4 - unused\r\n#   5 - X11\r\n#   6 - reboot (Do NOT set initdefault to this)\r\n#\r\nid:3:initdefault:',9,0,0,1514650071,0,0,0),(224,1,'iptables防火墙','','','Iptables也叫netfilter是Linux下自带的一款免费且优秀的基于包过滤的防火墙工具，它的功能十分强大，使用非常灵活，可以对流入、流出、流经服务器的数据包进行精细的控制。iptables是Linux2.4及2.6内核中集成的模块。\r\n    2. Iptables服务相关命令\r\n    1. 查看iptables状态\r\nservice iptables status\r\n    2. 开启/关闭iptables\r\nservice iptables start\r\nservice iptables stop\r\n    3. 查看iptables是否开机启动\r\nchkconfig iptables --list\r\n    4. 设置iptables开机启动/不启动\r\nchkconfig iptables on\r\nchkconfig iptables off\r\n\r\n在iptables中有四张表，分别是filter、nat、mangle和raw每一个表中都包含了各自不同的链，最常用的是filter表\r\n\r\n    • filter表：\r\nfilter是iptables默认使用的表，负责对流入、流出本机的数据包进行过滤，该表中定义了3个链：\r\n	INPOUT 	负责过滤所有目标地址是本机地址的数据包，就是过滤进入主机的数据包。\r\n	FORWARD	负责转发流经本机但不进入本机的数据包，起到转发的作用。\r\n	OUTPUT	负责处理所有源地址是本机地址的数据包，就是处理从主机发出去的数据包。',9,0,0,1514650533,0,0,0),(225,1,'redis','','','    1. 什么是Redis\r\nRedis是目前一个非常优秀的key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set有序集合)和hash（哈希类型）。\r\n    2. 为什么要安装Redis3集群\r\nRedis3.x支持集群模式，更加可靠！\r\n    3. 安装Redis3集群（6台Linux）\r\n参考文章：http://blog.csdn.net/myrainblues/article/details/25881535\r\n\r\n    1. 下载redis3的稳定版本，下载地址http://download.redis.io/releases/redis-3.0.7.tar.gz\r\n    2. 上传redis-3.0.7.tar.gz到服务器\r\n    3. 解压redis源码包\r\ntar -zxvf redis-3.0.7.tar.gz -C /usr/local/src/\r\n    4. 进入到源码包中，编译并安装redis\r\ncd /usr/local/src/redis-3.0.7/\r\nmake &amp;&amp; make install\r\n    5. 报错，缺少依赖的包\r\n\r\n    6. 配置本地YUM源并安装redis依赖的rpm包\r\nyum -y install gcc\r\n    7. 编译并安装\r\nmake &amp;&amp; make install\r\n    8. 报错，原因是没有安装jemalloc内存分配器，可以安装jemalloc或直接输入\r\nmake MALLOC=libc &amp;&amp; make install\r\n\r\n    9. 重新编译安装\r\nmake MALLOC=libc &amp;&amp; make install\r\n    10. 用同样的方式在其他的机器上编译安装redis\r\n    11. 在所有机器的/usr/local/下创建一个redis目录，然后拷贝redis自带的配置文件redis.conf到/usr/local/redis\r\nmkdir /usr/local/redis\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis\r\n    12. 修改所有机器的配置文件redis.conf\r\ndaemonize yes  #redis后台运行\r\ncluster-enabled yes  #开启集群把注释去掉\r\nappendonly yes  #开启aof日志，它会每次写操作都记录一条日志\r\nsed -i &#039;s/daemonize no/daemonize yes/&#039; /usr/local/redis/redis.conf\r\nsed -i &#039;s/# cluster-enabled yes/cluster-enabled yes/&#039; /usr/local/redis/redis.conf\r\nsed -i &#039;s/appendonly no/appendonly yes/&#039; /usr/local/redis/redis.conf\r\nsed -i &#039;s/# cluster-node-timeout 15000/cluster-node-timeout 5000/&#039; /usr/local/redis/redis.conf\r\n    13. 启动所有的redis节点\r\ncd /usr/local/redis\r\nredis-server redis.conf\r\n    14. 查看redis进程状态\r\nps -ef | grep redis\r\n\r\n    15. 配置集群：安装ruby和ruby gem工具（redis3集群配置需要ruby的gem工具，类似yum）\r\nyum -y install ruby rubygems\r\n(centos6.5的光盘可能缺失rubygems包，需要这样处理：\r\n先安装yum -y install ruby，\r\n再安装rubygems的依赖：\r\nyum install -y ruby-irb\r\nyum install -y ruby-rdoc\r\n再用rpm命令安装rubygems包\r\nrpm -ivh /root/rubygems-1.3.7-5.el6.noarch.rpm\r\n)\r\n    16. 使用gem下载redis集群的配置脚本\r\ngem install redis\r\n\r\n    17. gem需要上网才能下载，由于安装redis的服务器可能无法访问外网，可以找一台可以上网的服务器执行下面的命令\r\nyum -y install ruby rubygems\r\ngem install redis\r\n将下载好的redis gem（/usr/lib/ruby/gems/1.8/cache/redis-3.2.2.gem）拷贝到其他服务器\r\ncd /usr/lib/ruby/gems/1.8/cache\r\nfor n in {2..6}; do scp redis-3.2.2.gem 192.168.0.3$n:$PWD; done\r\n    18. 使用gem本地模式安装redis-3.2.2.gem\r\ngem install --local /usr/lib/ruby/gems/1.8/cache/redis-3.2.2.gem\r\n    19. 使用脚本配置redis集群（在一台机器上执行即可，想要把哪些节点配置成Master节点就放在后面）\r\ncd /usr/local/src/redis-3.0.7/src/\r\nservice iptables stop\r\n./redis-trib.rb create --replicas 1 192.168.0.34:6379 192.168.0.35:6379 192.168.0.36:6379 192.168.0.31:6379 192.168.0.32:6379 192.168.0.33:6379\r\n    20. 测试\r\nredis-cli -c -p 6379\r\n\r\n\r\n\r\n    4. Redis3伪分布式安装（1台Linux）\r\n    1. 下载redis3的稳定版本，下载地址http://download.redis.io/releases/redis-3.0.7.tar.gz\r\n    2. 上传redis-3.0.7.tar.gz到服务器\r\n3.解压redis源码包\r\ntar -zxvf redis-3.0.7.tar.gz -C /usr/local/src/\r\n4.进入到源码包中，编译并安装redis\r\ncd /usr/local/src/redis-3.0.7/\r\nmake &amp;&amp; make install\r\n5.在/usr/local/下创建一个redis目录，然后分别在/usr/local/redis目录创建6个文件夹7000,7001,7002,7003,7004,7005然后拷贝redis自带的配置文件redis.conf到这六个目录中\r\nmkdir /usr/local/redis\r\nmkdir /usr/local/redis/{7000,7001,7002,7003,7004,7005}\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7000\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7001\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7002\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7003\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7004\r\ncp /usr/local/src/redis-3.0.7/redis.conf /usr/local/redis/7005\r\n6.分别修改这六个目录中的配置文件\r\nport 7000 #端口要与其所在的文件名一致\r\npidfile /var/run/redis-7000.pid  #pid要与其所在的文件名一致\r\ndaemonize yes\r\ncluster-enabled yes\r\nappendonly yes\r\n    7. 分别进入到这六个目录启动redis进程\r\ncd /usr/local/redis/7000\r\nredis-server redis.conf\r\ncd /usr/local/redis/7001\r\nredis-server redis.conf\r\ncd /usr/local/redis/7002\r\nredis-server redis.conf\r\ncd /usr/local/redis/7003\r\nredis-server redis.conf\r\ncd /usr/local/redis/7004\r\nredis-server redis.conf\r\ncd /usr/local/redis/7005\r\nredis-server redis.conf',9,0,0,1514650699,0,0,0),(226,1,'nginx反向代理','','','反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。',10,0,0,1514651962,0,0,0),(227,1,'nginx负载均衡','','','负载均衡，英文名称为Load Balance，是指建立在现有网络结构之上，并提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。其原理就是数据流量分摊到多个服务器上执行，减轻每台服务器的压力，多台服务器共同完成工作任务，从而提高了数据的吞吐量。\r\n\r\n\r\n在http这个节下面配置一个叫upstream的，后面的名字可以随意取，但是要和location下的proxy_pass http://后的保持一致。\r\n\r\n    是在http里面的, 已有http, 不是在server里,在server外面\r\n    upstream tomcats { \r\n        server shizhan02:8080 weight=1;#weight表示多少个\r\n        server shizhan03:8080 weight=1;\r\n        server shizhan04:8080 weight=1;\r\n}\r\n#卸载server里\r\nlocation ~ .*\\.(jsp|do|action) {\r\n    proxy_pass http://tomcats;        #tomcats是后面的tomcat服务器组的逻辑组号\r\n}\r\n}',10,0,0,1514652038,0,0,0),(228,1,'nginx的安装','','','        3.1. 下载nginx\r\n官网：http://nginx.org/\r\n        3.2. 上传并解压nginx\r\ntar -zxvf nginx-1.8.1.tar.gz -C /usr/local/src\r\n        3.3. 编译nginx\r\n#进入到nginx源码目录\r\ncd /usr/local/src/nginx-1.8.1\r\n\r\n#检查安装环境,并指定将来要安装的路径\r\n./configure --prefix=/usr/local/nginx\r\n\r\n#缺包报错 ./configure: error: C compiler cc is not found\r\n\r\n#使用YUM安装缺少的包\r\nyum -y install gcc pcre-devel openssl openssl-devel\r\n\r\n#编译安装\r\nmake &amp;&amp; make install\r\n\r\n安装完后测试是否正常：\r\n/usr/loca/nginx/sbin/nginx\r\n查看端口是否有ngnix进程监听\r\nnetstat -ntlp | grep 80',10,0,0,1514652211,0,0,0),(229,1,'配置nginx','','','    1. 修改nginx配置文件\r\nserver {\r\n    listen       80;\r\n    server_name  nginx-01.itcast.cn;    #nginx所在服务器的主机名\r\n#反向代理的配置\r\nlocation / {             #拦截所有请求\r\n    root html;\r\n        proxy_pass http://192.168.0.21:8080;   #这里是代理走向的目标服务器：tomcat\r\n    }\r\n}\r\n    2. 启动tomcat-01上的tomcat\r\n\r\n3.启动nginx-01上的nginx\r\n./nginx\r\n\r\n重启:\r\nkill -HUP `cat /usr/local/nginx/logs/nginx.pid `\r\n参考网址:http://www.cnblogs.com/jianxie/p/3990377.html',10,0,0,1514652267,0,0,0),(230,1,'        4.2. 动静分离','','','动态资源 index.jsp\r\nlocation ~ .*\\.(jsp|do|action)$ {\r\n    proxy_pass http://tomcat-01.itcast.cn:8080;\r\n}\r\n\r\n#静态资源\r\nlocation ~ .*\\.(html|js|css|gif|jpg|jpeg|png)$ {\r\n    expires 3d;\r\n}',10,0,0,1514652307,0,0,0),(231,1,'利用keepalived实现高可靠（HA）','','','        5.1. 高可靠概念\r\nHA(High Available), 高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。\r\n\r\n        5.2. 高可靠软件keepalived\r\nkeepalive是一款可以实现高可靠的软件，通常部署在2台服务器上，分为一主一备。Keepalived可以对本机上的进程进行检测，一旦Master检测出某个进程出现问题，将自己切换成Backup状态，然后通知另外一个节点切换成Master状态。\r\n        5.3. keepalived安装\r\n下载keepalived官网:http://keepalived.org\r\n\r\n将keepalived解压到/usr/local/src目录下\r\ntar -zxvf  keepalived-1.2.19.tar.gz -C /usr/local/src\r\n\r\n进入到/usr/local/src/keepalived-1.2.19目录\r\ncd /usr/local/src/keepalived-1.2.19\r\n\r\n开始configure\r\n./configure --prefix=/usr/local/keepalived\r\n\r\n#编译并安装\r\nmake &amp;&amp; make install',10,0,0,1514652434,0,0,0),(232,1,'将keepalived添加到系统服务中','','','拷贝执行文件\r\ncp /usr/local/keepalived/sbin/keepalived /usr/sbin/\r\n将init.d文件拷贝到etc下,加入开机启动项\r\ncp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived\r\n将keepalived文件拷贝到etc下\r\ncp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ \r\n创建keepalived文件夹\r\nmkdir -p /etc/keepalived\r\n将keepalived配置文件拷贝到etc下\r\ncp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf\r\n添加可执行权限\r\n\r\nchmod +x /etc/init.d/keepalived\r\n\r\n##以上所有命令一次性执行：\r\ncp /usr/local/keepalived/sbin/keepalived /usr/sbin/\r\ncp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived\r\ncp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ \r\nmkdir -p /etc/keepalived\r\ncp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf\r\nchmod +x /etc/init.d/keepalived\r\nchkconfig --add keepalived	\r\nchkconfig keepalived on\r\n\r\n添加keepalived到开机启动\r\nchkconfig --add keepalived	\r\nchkconfig keepalived on',10,0,0,1514652565,0,0,0),(233,1,'配置keepalived虚拟IP','','','修改配置文件： /etc/keepalived/keepalived.conf\r\n#MASTER节点\r\nglobal_defs {\r\n}\r\nvrrp_instance VI_1 {\r\n    state MASTER   #指定A节点为主节点 备用节点上设置为BACKUP即可\r\n    interface eth0    #绑定虚拟IP的网络接口\r\n    virtual_router_id 51   #VRRP组名，两个节点的设置必须一样，以指明各个节点属于同一VRRP组\r\n    priority 100   #主节点的优先级（1-254之间），备用节点必须比主节点优先级低\r\n    advert_int 1  #组播信息发送间隔，两个节点设置必须一样\r\n    authentication {    #设置验证信息，两个节点必须一致\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {    #指定虚拟IP, 两个节点设置必须一样\r\n        192.168.33.60/24    #如果两个nginx的ip分别是192.168.33.61,,...62，则此处的虚拟ip跟它俩同一个网段即可\r\n    }\r\n}\r\n\r\n#BACKUP节点\r\nglobal_defs {\r\n}\r\nvrrp_instance VI_1 {\r\n    state BACKUP\r\n    interface eth0\r\n    virtual_router_id 51\r\n    priority 99\r\n    advert_int 1\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {\r\n        192.168.33.60/24\r\n    }\r\n}\r\n\r\n#分别启动两台机器上的keepalived\r\nservice keepalived start\r\n测试：\r\n杀掉master上的keepalived进程，你会发现，在slave机器上的eth0网卡多了一个ip地址\r\n查看ip地址的命令：  ip addr ',10,0,0,1514652607,0,0,0),(234,1,'配置keepalived心跳检查','','','原理：\r\nKeepalived并不跟nginx耦合，它俩完全不是一家人\r\n但是keepalived提供一个机制：让用户自定义一个shell脚本去检测用户自己的程序，返回状态给keepalived就可以了\r\n\r\n\r\n#MASTER节点\r\nglobal_defs {\r\n}\r\n\r\nvrrp_script chk_health {\r\n    script &quot;[[ `ps -ef | grep nginx | grep -v grep | wc -l` -ge 2 ]] &amp;&amp; exit 0 || exit 1&quot;\r\n    interval 1    #每隔1秒执行上述的脚本，去检查用户的程序ngnix\r\n    weight -2\r\n}\r\n\r\nvrrp_instance VI_1 {\r\n    state MASTER\r\n    interface eth0\r\n    virtual_router_id 1\r\n    priority 100\r\n    advert_int 2\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n\r\n    track_script {\r\n        chk_health\r\n    }\r\n\r\n    virtual_ipaddress {\r\n        10.0.0.10/24\r\n    }\r\n\r\n    notify_master &quot;/usr/local/keepalived/sbin/notify.sh master&quot;\r\n    notify_backup &quot;/usr/local/keepalived/sbin/notify.sh backup&quot;\r\n    notify_fault &quot;/usr/local/keepalived/sbin/notify.sh fault&quot;\r\n}\r\n\r\n#添加切换通知脚本\r\nvi /usr/local/keepalived/sbin/notify.sh\r\n#!/bin/bash\r\n\r\ncase &quot;$1&quot; in\r\n    master)\r\n        /usr/local/nginx/sbin/nginx\r\n        exit 0\r\n    ;;\r\nbackup)\r\n        /usr/local/nginx/sbin/nginx -s stop\r\n        /usr/local/nginx/sbin/nginx\r\n        exit 0\r\n    ;;\r\n    fault)\r\n        /usr/local/nginx/sbin/nginx -s stop\r\n        exit 0\r\n    ;;\r\n    *)\r\n        echo &#039;Usage: notify.sh {master|backup|fault}&#039;\r\n        exit 1\r\n    ;;\r\nesac\r\n\r\n#添加执行权限\r\nchmod +x /usr/local/keepalived/sbin/notify.sh\r\nglobal_defs {\r\n}\r\n\r\nvrrp_script chk_health {\r\n    script &quot;[[ `ps -ef | grep nginx | grep -v grep | wc -l` -ge 2 ]] &amp;&amp; exit 0 || exit 1&quot;\r\n    interval 1\r\n    weight -2\r\n}\r\n\r\nvrrp_instance VI_1 {\r\n    state BACKUP\r\n    interface eth0\r\n    virtual_router_id 1\r\n    priority 99\r\n    advert_int 1\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n\r\n    track_script {\r\n        chk_health\r\n    }\r\n\r\n    virtual_ipaddress {\r\n        10.0.0.10/24\r\n    }\r\n	\r\n    notify_master &quot;/usr/local/keepalived/sbin/notify.sh master&quot;\r\n    notify_backup &quot;/usr/local/keepalived/sbin/notify.sh backup&quot;\r\n    notify_fault &quot;/usr/local/keepalived/sbin/notify.sh fault&quot;\r\n}\r\n\r\n#在第二台机器上添加notify.sh脚本\r\n#分别在两台机器上启动keepalived\r\nservice keepalived start \r\nchkconfig keepalived on',10,0,0,1514652641,0,0,0),(235,1,'crond简介','','','前一天学习了 at 命令是针对仅运行一次的任务，循环运行的例行性计划任务，linux系统则是由 cron (crond) 这个系统服务来控制的。Linux 系统上面原本就有非常多的计划性工作，因此这个系统服务是默认启动的。另外, 由于使用者自己也可以设置计划任务，所以， Linux 系统也提供了使用者控制计划任务的命令 :crontab 命令。\r\n一、crond简介\r\ncrond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。\r\nLinux下的任务调度分为两类，系统任务调度和用户任务调度。\r\n系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。\r\n/etc/crontab文件包括下面几行：\r\n[root@localhost ~]# cat /etc/crontab \r\nSHELL=/bin/bash\r\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\r\nMAILTO=&quot;&quot;HOME=/\r\n# run-parts\r\n51 * * * * root run-parts /etc/cron.hourly\r\n24 7 * * * root run-parts /etc/cron.daily\r\n22 4 * * 0 root run-parts /etc/cron.weekly\r\n42 4 1 * * root run-parts /etc/cron.monthly\r\n[root@localhost ~]#\r\n前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。第六至九行表示的含义将在下个小节详细讲述。这里不在多说。\r\n用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab 文件都被保存在 /var/spool/cron目录中。其文件名与用户名一致。\r\n使用者权限文件：\r\n文件：\r\n/etc/cron.deny\r\n说明：\r\n该文件中所列用户不允许使用crontab命令\r\n文件：\r\n/etc/cron.allow\r\n说明：\r\n该文件中所列用户允许使用crontab命令\r\n文件：\r\n/var/spool/cron/\r\n说明：\r\n所有用户crontab文件存放的目录,以用户名命名\r\ncrontab文件的含义：\r\n用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下：\r\nminute   hour   day   month   week   command\r\n其中：\r\nminute： 表示分钟，可以是从0到59之间的任何整数。\r\nhour：表示小时，可以是从0到23之间的任何整数。\r\nday：表示日期，可以是从1到31之间的任何整数。\r\nmonth：表示月份，可以是从1到12之间的任何整数。\r\nweek：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。\r\ncommand：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。\r\n \r\n在以上各个字段中，还可以使用以下特殊字符：\r\n星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。\r\n逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\r\n中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\r\n正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。',9,0,0,1514653171,0,0,0),(236,1,'crond服务','','','安装crontab：\r\nyum install crontabs\r\n服务操作说明：\r\n/sbin/service crond start //启动服务\r\n/sbin/service crond stop //关闭服务\r\n/sbin/service crond restart //重启服务\r\n/sbin/service crond reload //重新载入配置\r\n查看crontab服务状态：\r\nservice crond status\r\n手动启动crontab服务：\r\nservice crond start\r\n查看crontab服务是否已设置为开机启动，执行命令：\r\nntsysv\r\n加入开机自动启动：\r\nchkconfig –level 35 crond on',9,0,0,1514653198,0,0,0),(237,1,'crontab命令详解','','','1．命令格式：\r\ncrontab [-u user] file\r\ncrontab [-u user] [ -e | -l | -r ]\r\n2．命令功能：\r\n通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常设合周期性的日志分析或数据备份等工作。\r\n3．命令参数：\r\n-u user：用来设定某个用户的crontab服务，例如，“-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。\r\nfile：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。\r\n-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。\r\n-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。\r\n-r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。\r\n-i：在删除用户的crontab文件时给确认提示。',9,0,0,1514653221,0,0,0),(238,1,'常用方法','','','1). 创建一个新的crontab文件\r\n在考虑向cron进程提交一个crontab文件之前，首先要做的一件事情就是设置环境变量EDITOR。cron进程根据它来确定使用哪个编辑器编辑crontab文件。9 9 %的UNIX和LINUX用户都使用vi，如果你也是这样，那么你就编辑$ HOME目录下的. profile文件，在其中加入这样一行：\r\nEDITOR=vi; export EDITOR\r\n然后保存并退出。不妨创建一个名为&lt;user&gt; cron的文件，其中&lt;user&gt;是用户名，例如， davecron。在该文件中加入如下的内容。\r\n      # (put your own initials here)echo the date to the console every\r\n      # 15minutes between 6pm and 6am\r\n      0,15,30,45 18-06 * * * /bin/echo &#039;date&#039; &gt; /dev/console\r\n    保存并退出。确信前面5个域用空格分隔。\r\n在上面的例子中，系统将每隔1 5分钟向控制台输出一次当前时间。如果系统崩溃或挂起，从最后所显示的时间就可以一眼看出系统是什么时间停止工作的。在有些系统中，用tty1来表示控制台，可以根据实际情况对上面的例子进行相应的修改。为了提交你刚刚创建的crontab文件，可以把这个新创建的文件作为cron命令的参数：\r\n     $ crontab davecron\r\n现在该文件已经提交给cron进程，它将每隔1 5分钟运行一次。\r\n同时，新创建文件的一个副本已经被放在/var/spool/cron目录中，文件名就是用户名(即dave)。\r\n2). 列出crontab文件\r\n   为了列出crontab文件，可以用：\r\n     $ crontab -l\r\n     0,15,30,45,18-06 * * * /bin/echo `date` &gt; dev/tty1\r\n你将会看到和上面类似的内容。可以使用这种方法在$ H O M E目录中对crontab文件做一备份：\r\n     $ crontab -l &gt; $HOME/mycron\r\n    这样，一旦不小心误删了crontab文件，可以用上一节所讲述的方法迅速恢复。\r\n3). 编辑crontab文件\r\n   如果希望添加、删除或编辑crontab文件中的条目，而E D I TO R环境变量又设置为v i，那么就可以用v i来编辑crontab文件，相应的命令为：\r\n     $ crontab -e\r\n可以像使用v i编辑其他任何文件那样修改crontab文件并退出。如果修改了某些条目或添加了新的条目，那么在保存该文件时， c r o n会对其进行必要的完整性检查。如果其中的某个域出现了超出允许范围的值，它会提示你。\r\n我们在编辑crontab文件时，没准会加入新的条目。例如，加入下面的一条：\r\n    # DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month\r\n     30 3 1,7,14,21,26 * * /bin/find -name &quot;core&#039; -exec rm {} \\;\r\n现在保存并退出。最好在crontab文件的每一个条目之上加入一条注释，这样就可以知道它的功能、运行时间，更为重要的是，知道这是哪位用户的作业。\r\n现在让我们使用前面讲过的crontab -l命令列出它的全部信息：\r\n    $ crontab -l \r\n    # (crondave installed on Tue May 4 13:07:43 1999)\r\n    # DT:ech the date to the console every 30 minites\r\n   0,15,30,45 18-06 * * * /bin/echo `date` &gt; /dev/tty1\r\n    # DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month\r\n    30 3 1,7,14,21,26 * * /bin/find -name &quot;core&#039; -exec rm {} \\;\r\n4). 删除crontab文件\r\n要删除crontab文件，可以用：\r\n    $ crontab -r\r\n5). 恢复丢失的crontab文件\r\n如果不小心误删了crontab文件，假设你在自己的$ H O M E目录下还有一个备份，那么可以将其拷贝到/var/spool/cron/&lt;username&gt;，其中&lt;username&gt;是用户名。如果由于权限问题无法完成拷贝，可以用：\r\n     $ crontab &lt;filename&gt;\r\n    其中，&lt;filename&gt;是你在$ H O M E目录中副本的文件名。\r\n我建议你在自己的$ H O M E目录中保存一个该文件的副本。我就有过类似的经历，有数次误删了crontab文件（因为r键紧挨在e键的右边）。这就是为什么有些系统文档建议不要直接编辑crontab文件，而是编辑该文件的一个副本，然后重新提交新的文件。\r\n有些crontab的变体有些怪异，所以在使用crontab命令时要格外小心。如果遗漏了任何选项，crontab可能会打开一个空文件，或者看起来像是个空文件。这时敲delete键退出，不要按&lt;Ctrl-D&gt;，否则你将丢失crontab文件。',9,0,0,1514653262,0,0,0),(239,1,'使用实例','','','实例1：每1分钟执行一次command\r\n命令：\r\n* * * * * command\r\n \r\n实例2：每小时的第3和第15分钟执行\r\n命令：\r\n3,15 * * * * command\r\n \r\n实例3：在上午8点到11点的第3和第15分钟执行\r\n命令：\r\n3,15 8-11 * * * command\r\n \r\n实例4：每隔两天的上午8点到11点的第3和第15分钟执行\r\n命令：\r\n3,15 8-11 */2 * * command\r\n \r\n实例5：每个星期一的上午8点到11点的第3和第15分钟执行\r\n命令：\r\n3,15 8-11 * * 1 command\r\n \r\n实例6：每晚的21:30重启smb \r\n命令：\r\n30 21 * * * /etc/init.d/smb restart\r\n \r\n实例7：每月1、10、22日的4 : 45重启smb \r\n命令：\r\n45 4 1,10,22 * * /etc/init.d/smb restart\r\n \r\n实例8：每周六、周日的1 : 10重启smb\r\n命令：\r\n10 1 * * 6,0 /etc/init.d/smb restart\r\n \r\n实例9：每天18 : 00至23 : 00之间每隔30分钟重启smb \r\n命令：\r\n0,30 18-23 * * * /etc/init.d/smb restart\r\n \r\n实例10：每星期六的晚上11 : 00 pm重启smb \r\n命令：\r\n0 23 * * 6 /etc/init.d/smb restart\r\n \r\n实例11：每一小时重启smb \r\n命令：\r\n* */1 * * * /etc/init.d/smb restart\r\n \r\n实例12：晚上11点到早上7点之间，每隔一小时重启smb \r\n命令：\r\n* 23-7/1 * * * /etc/init.d/smb restart\r\n \r\n实例13：每月的4号与每周一到周三的11点重启smb \r\n命令：\r\n0 11 4 * mon-wed /etc/init.d/smb restart\r\n \r\n实例14：一月一号的4点重启smb \r\n命令：\r\n0 4 1 jan * /etc/init.d/smb restart\r\n实例15：每小时执行/etc/cron.hourly目录内的脚本\r\n命令：\r\n01   *   *   *   *     root run-parts /etc/cron.hourly\r\n说明：\r\nrun-parts这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是目录名了',9,0,0,1514653289,0,0,0),(240,1,'使用注意事项','','','1. 注意环境变量问题\r\n有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。\r\n在crontab文件中定义多个调度任务时，需要特别注意的一个问题就是环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。\r\n不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shelll脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点：\r\n1）脚本中涉及文件路径时写全局路径；\r\n2）脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如：\r\ncat start_cbp.sh\r\n#!/bin/sh\r\nsource /etc/profile\r\nexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf\r\n/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp;\r\n3）当手动执行脚本OK，但是crontab死活不执行时。这时必须大胆怀疑是环境变量惹的祸，并可以尝试在crontab中直接引入环境变量解决问题。如：\r\n0 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh\r\n2. 注意清理系统用户的邮件日志\r\n每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。\r\n例如，可以在crontab文件中设置如下形式，忽略日志输出：\r\n0 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1\r\n“/dev/null 2&gt;&amp;1”表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。\r\n3. 系统级任务调度与用户级任务调度\r\n系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行，root用户的任务调度操作可以通过“crontab –uroot –e”来设置，也可以将调度任务直接写入/etc/crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。\r\n4. 其他注意事项\r\n新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。\r\n当crontab突然失效时，可以尝试/etc/init.d/crond restart解决问题。或者查看日志看某个job有没有执行/报错tail -f /var/log/cron。\r\n千万别乱运行crontab -r。它从Crontab目录（/var/spool/cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了。\r\n在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义\\%，如经常用的date ‘+%Y%m%d’在crontab里是不会执行的，应该换成date ‘+\\%Y\\%m\\%d\r\n',9,0,0,1514653313,0,0,0);
/*!40000 ALTER TABLE `resource_techn_article` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource_techn_article_cate`
--

DROP TABLE IF EXISTS `resource_techn_article_cate`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `resource_techn_article_cate` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `catename` varchar(100) NOT NULL DEFAULT '' COMMENT '文章分类名',
  `pid` int(11) NOT NULL DEFAULT '0' COMMENT '文章分类父ID',
  `path` varchar(255) NOT NULL DEFAULT '0,' COMMENT '全路径',
  `level` tinyint(4) NOT NULL DEFAULT '0' COMMENT '分类级别',
  `img` varchar(100) NOT NULL DEFAULT '' COMMENT '分类图片',
  `addtime` int(10) unsigned DEFAULT '0' COMMENT '添加时间',
  `uid` int(10) unsigned DEFAULT '0' COMMENT '创建者',
  `updatetime` int(10) unsigned DEFAULT '0' COMMENT '修改时间',
  `mid` int(10) unsigned DEFAULT '0' COMMENT '修改人',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8 COMMENT='技术文章分类表';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource_techn_article_cate`
--

LOCK TABLES `resource_techn_article_cate` WRITE;
/*!40000 ALTER TABLE `resource_techn_article_cate` DISABLE KEYS */;
INSERT INTO `resource_techn_article_cate` VALUES (1,'设计',0,'1',0,'',1474121898,1,0,0),(2,'前端',0,'2',0,'',1474121908,1,0,0),(3,'后端',0,'3',0,'',1474121919,1,0,0),(4,'运维',0,'4',0,'',1474121930,1,0,0),(5,'大数据',0,'5',0,'',1474121952,1,0,0),(6,'php',3,'3-6',1,'',1474122522,1,0,0),(7,'python',4,'4-7',1,'',1474123584,1,0,0),(8,'ELK',4,'4-8',1,'',1514601129,1,0,0),(9,'linux',4,'4-9',1,'',1514631605,1,0,0),(10,'nginx',4,'4-10',1,'',1514651900,1,0,0);
/*!40000 ALTER TABLE `resource_techn_article_cate` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2018-01-02 12:15:40
